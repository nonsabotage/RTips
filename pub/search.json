[
  {
    "objectID": "contents/utils/01_写真GPS/index.html",
    "href": "contents/utils/01_写真GPS/index.html",
    "title": "写真データのGISデータ化",
    "section": "",
    "text": "1 はじめに\niPhoneをはじめとしたスマートフォンで写真を撮ると位置情報が付与されている。 また、ミラーレス一眼レフカメラでもスマートフォンと連携して位置情報が付与される。 現場調査において位置情報というのは非常に重要な情報である。\nここでは、写真データに付与された位置情報をGISデータに変換する方法を紹介する。\n\n\n2 やり方\n大きく４つのステップに分かれている。\n\nJPEGへの変換\nJPEGから位置情報をCSVで出力する\nCSVをGeoJSONに変換する\n画像をサムネイル化する\n\n# convert HEIC to jpg\nmogrify -format jpg *.HEIC\n\n# extract gps coordates from jpgs to output.csv\nexiftool -gpslatitude -gpslongitude -n -csv *.jpg &gt; output.csv\n\n# convert output.csv to geojson\nogr2ogr -f \"GeoJSON\" output.geojson output.csv -oo X_POSSIBLE_NAMES=gpslongitude  -oo Y_POSSIBLE_NAMES=gpslatitude -oo KEEP_GEOM_COLUMNS=NO\n\n# create thumbnails\nmkdir -p thumbnails\nfor file in *.jpg; do \n    filename=$(basename \"$file\")\n    convert \"$file\" -thumbnail 200x \"thumbnails/$filename\"\ndone\nここまで為ておくことで、写真データを地図上に表示することができるようになる。 WebGISはもちろんであるがQGISでも可能である。たとえば、次をTipsに設定することで閲覧が可能となる。 これはプロジェクトフォルダーからの相対パスでファイルを指定している。ファイル名自体はSourceFileという名前の属性で保存されている。\n&lt;a href=\"file:///[% @project_folder %]/../../../09_写真/20231120-21_SiteVisit_Dulkadioglu/suzuki/[% SourceFile %]\"&gt;\n&lt;img src=\"file:///[% @project_folder %]/../../../09_写真/20231120-21_SiteVisit_Dulkadioglu/suzuki/[% SourceFile %]\" alt=\"hoge\" width=\"300\"&gt;\n&lt;/a&gt;\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Utils",
      "01_写真GPS"
    ]
  },
  {
    "objectID": "contents/test/bash/run_gdal.html",
    "href": "contents/test/bash/run_gdal.html",
    "title": "GDAL",
    "section": "",
    "text": "1 はじめに\n\ngdalを走らせられるかどうか\n日本語のパスでも問題なく動くには動くことがわかる\n\n\n\n2 works\n\n\nCode\ngdalinfo --version\n#&gt; GDAL 3.4.3, released 2022/04/22\n\n\n\n\nCode\ngdalinfo ./日本語フォルダ/stacking.tif\n#&gt; Driver: GTiff/GeoTIFF\n#&gt; Files: ./日本語フォルダ/stacking.tif\n#&gt; Size is 316, 192\n#&gt; Coordinate System is:\n#&gt; GEOGCRS[\"JGD2000\",\n#&gt;     DATUM[\"Japanese Geodetic Datum 2000\",\n#&gt;         ELLIPSOID[\"GRS 1980\",6378137,298.257222101004,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ID[\"EPSG\",6612]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433,\n#&gt;             ID[\"EPSG\",9122]]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"latitude\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433,\n#&gt;                 ID[\"EPSG\",9122]]],\n#&gt;         AXIS[\"longitude\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433,\n#&gt;                 ID[\"EPSG\",9122]]]]\n#&gt; Data axis to CRS axis mapping: 2,1\n#&gt; Origin = (138.937500000000000,35.899999999999999)\n#&gt; Pixel Size = (0.003125000000000,-0.002083333000000)\n#&gt; Metadata:\n#&gt;   AREA_OR_POINT=Area\n#&gt; Image Structure Metadata:\n#&gt;   COMPRESSION=LZW\n#&gt;   INTERLEAVE=PIXEL\n#&gt; Corner Coordinates:\n#&gt; Upper Left  ( 138.9375000,  35.9000000) (138d56'15.00\"E, 35d54' 0.00\"N)\n#&gt; Lower Left  ( 138.9375000,  35.5000001) (138d56'15.00\"E, 35d30' 0.00\"N)\n#&gt; Upper Right ( 139.9250000,  35.9000000) (139d55'30.00\"E, 35d54' 0.00\"N)\n#&gt; Lower Right ( 139.9250000,  35.5000001) (139d55'30.00\"E, 35d30' 0.00\"N)\n#&gt; Center      ( 139.4312500,  35.7000000) (139d25'52.50\"E, 35d42' 0.00\"N)\n#&gt; Band 1 Block=316x1 Type=Float64, ColorInterp=Gray\n#&gt;   NoData Value=0\n#&gt; Band 2 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 3 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 4 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 5 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 6 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 7 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 8 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 9 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 10 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 11 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 12 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Test",
      "Bash",
      "GDAL"
    ]
  },
  {
    "objectID": "contents/test/R/renv.html",
    "href": "contents/test/R/renv.html",
    "title": "renv",
    "section": "",
    "text": "1 renvはどのように指定すればいいか\n\nプロジェクトファイルがあるフォルダで指定するときはそのフォルダになる\nプロジェクトファイルがないフォルダで指定するときはルートフォルダになる\n正確には１番近い親フォルダになっているみたい\nなのでこの場合のhereの挙動には注意する\n\n\n\nCode\nhere::here()\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\ncur_dir &lt;- here::here(\"contents\", \"test\", \"R\")\nprint(cur_dir)\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R/contents/test/R\"\n# renv::init(cur_dir)\n\nprint(getwd())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\nprint(here::here())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\n\n\n\n\nCode\nrenv::activate()\n\n\n\n\nCode\nprint(getwd())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\nprint(here::here())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\nprint(cur_dir)\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R/contents/test/R\"\n\n\n\n\nCode\nlibrary(here)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Test",
      "R",
      "renv"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html",
    "href": "contents/notes/ggplot2/working.html",
    "title": "ggplot2 Tips",
    "section": "",
    "text": "ggplot2を覚えるのは難しいので具体的な描画事例を載せていく。 描画事例はシンプルなものを中心に載せていく。 機能の網羅性は欠ける資料であるが具体例なのでコピペですぐ動かせる内容ではある。\nすでに個別のファイルで作成した 事例があるのでそれを移植すること!\n\n\n\nggplot2\nrmarkdown\nggplot2 extensions",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#reference",
    "href": "contents/notes/ggplot2/working.html#reference",
    "title": "ggplot2 Tips",
    "section": "",
    "text": "ggplot2\nrmarkdown\nggplot2 extensions",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#カーネル密度と経験累積分布関数",
    "href": "contents/notes/ggplot2/working.html#カーネル密度と経験累積分布関数",
    "title": "ggplot2 Tips",
    "section": "2.1 カーネル密度と経験累積分布関数",
    "text": "2.1 カーネル密度と経験累積分布関数\n経験累積分布関数とカーネル密度プロットは常に描画すること. できればそれを組み合わせて描画すること。 経験累積分布関数はさらに目的変数の累積分布関数と比較をすること.\n\n\nCode\ndata &lt;- iris\ng_dens &lt;- \n    data %&gt;%\n    ggplot(aes(Sepal.Length, color = Species)) + \n    geom_line(stat = \"density\", size = 1.5) + \n    theme_bw() + \n    labs(x = \"Sepal.Length\", y = \"density\") + \n    scale_color_aaas()\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\nCode\ng_ecdf &lt;- \n    data %&gt;%\n    ggplot(aes(Sepal.Length, color = Species)) + \n    stat_ecdf(size = 1.5) + \n    theme_bw() + \n    labs(x = \"Sepal.Length\", y = \"density\") + \n    scale_color_aaas() +\n    guides(color = \"none\")\n\n\n\n\nCode\n\nplot_grid(\n    g_dens + labs(x = NULL), \n    g_ecdf, \n    rel_widths = c(1, 1), \n    ncol = 1,  \n    align = \"v\",  # 軸を合わせるのに必要 \n    axis = \"tblr\" # 余白を合わせるのに必要(特に凡例があったりなかったりするとき)\n)",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#関数曲線",
    "href": "contents/notes/ggplot2/working.html#関数曲線",
    "title": "ggplot2 Tips",
    "section": "2.2 関数曲線",
    "text": "2.2 関数曲線\n確率密度関数などを記述するときに使う.\n\n\nCode\ngenerate_dnorm &lt;- function (param) {\n    lift( partial )(\n        update_list(\n            param, .f = dnorm, .first = FALSE )\n        )\n}\n\nparam1 &lt;-\n    list( mean = 1.23 ) %&gt;%\n    update_list(\n        sd =  ~ .1\n    )\n#&gt; Warning: `update_list()` was deprecated in purrr 1.0.0.\nparam2 &lt;-\n    param1 %&gt;%\n    update_list(\n        mean = 1.48631\n    )\n\ndnorm1 &lt;- generate_dnorm( param1 )\n#&gt; Warning: `lift()` was deprecated in purrr 1.0.0.\n#&gt; Warning: The `.first` argument of `partial()` is deprecated as of purrr 0.3.0.\n#&gt; ℹ The deprecated feature was likely used in the purrr package.\n#&gt;   Please report the issue at &lt;https://github.com/tidyverse/purrr/issues&gt;.\ndnorm2 &lt;- generate_dnorm( param2 )\nfill_poly &lt;- data_frame(\n        x = seq(qnorm(.9, mean = param1$mean, sd = param1$sd), 2, length.out = 20)\n    ) %&gt;%\n    mutate(y = map_dbl(x, dnorm1))\n#&gt; Warning: `data_frame()` was deprecated in tibble 1.1.0.\n#&gt; ℹ Please use `tibble()` instead.\n\ngrh &lt;-\n    data_frame( x = seq(0, 2, .01) ) %&gt;%\n    ggplot() +\n    geom_abline(slope = 0, intercept = 0) +\n    geom_ribbon(data = fill_poly, aes(x = x, ymax = y, ymin = 0), fill = \"pink\", alpha = .5) +\n    stat_function(fun = dnorm1, colour = \"red\",  lwd = 1.2 ) +\n    stat_function(fun = dnorm2, colour = \"blue\", lwd = 1.2 ) +\n    ylim(c(0, 4)) +\n    scale_x_continuous(limits = c(0.9, 1.8), breaks = seq(0.9, 1.8, .1)) +\n    labs(x = \"\", y = \"\") +\n    theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.y  = element_blank(),\n        axis.text.x  = element_text(size = 12)\n    )\ngrh",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#確率楕円",
    "href": "contents/notes/ggplot2/working.html#確率楕円",
    "title": "ggplot2 Tips",
    "section": "2.3 確率楕円",
    "text": "2.3 確率楕円\nデフォルトでは、2次元t分布の95%確率となる領域を記述してくれる。 つまり、異常値の発見に役立つ. geomパラメータを変更することで塗りつぶしを行うこともできるし、 t分布の変わりに正規分布を使うことも可能である. 単純に円を記述することも可能である.\n\n\nCode\ngp &lt;-\n    iris %&gt;% \n    ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) + \n    theme_bw() + \n    labs(x = \"Sepal.Length\", y = \"Sepal.Width\") + \n    coord_equal() +\n    theme(\n        axis.ticks.length = unit(-2, \"mm\"), \n        axis.title.x = element_text(margin = margin(t = 1, unit = \"line\")),\n        axis.text.x  = element_text(margin = margin(t = 1, unit = \"line\")),\n        axis.title.y = element_text(margin = margin(r = 1, unit = \"line\")), \n        axis.text.y  = element_text(margin = margin(r = 1, unit = \"line\")),\n        plot.margin = margin(1, 0, 0, 0, \"line\")\n    ) + \n    scale_color_viridis_d()\n\ngp + \n    geom_point(size = 3, alpha = .5) + \n    stat_ellipse() + \n    stat_ellipse(type = \"norm\", lty = 2) + \n    stat_ellipse(type = \"euclid\", lty = 3)\n\n\n\n\n\n\n\n\n\n\n\nCode\ngp + \n    stat_ellipse(\n        aes(fill = Species), \n        type = \"norm\", \n        geom = \"polygon\", alpha = .2, color = \"transparent\") + \n    geom_point(size = 3, alpha = .5, position = \"jitter\") +\n    scale_fill_viridis_d()",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#annotate関数",
    "href": "contents/notes/ggplot2/working.html#annotate関数",
    "title": "ggplot2 Tips",
    "section": "2.4 annotate関数",
    "text": "2.4 annotate関数\n\n2.4.1 annotate\nannotate関数ではジオメトリを追加するが、 典型的なgeom functionとは方法がことなり、 ジオメトリの属性はデータフレームの変数ではなく ベクトルで与えらる。 テキストラベルなど小さなアノテーションを追加するとき、 あるいは、データフレームにしたくないデータを持っているときに有用である。\n\n\nCode\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + theme_bw()\nlift(plot_grid)(list(\n    p + annotate(\"text\", x = 4, y = 25, label = \"Some text\"), \n    p + annotate(\"text\", x = 2:5, y = 25, label = \"Some text\"), \n    p + annotate(\"rect\", xmin = 3, xmax = 4.2, ymin = 12, ymax = 21, alpha = .2),\n    p + annotate(\"segment\", x = 2.5, xend = 4, y = 15, yend = 25, colour = \"blue\"),\n    p + annotate(\"pointrange\", x = 3.5, y = 20, ymin = 12, ymax = 28, colour = \"red\", size = 1.5),\n    p + annotate(\"text\", x = 2:3, y = 20:21, label = c(\"my label\", \"label 2\")),\n    p + annotate(\"text\", x = 4, y = 25, label = \"italic(R) ^ 2 == 0.75\",   parse = TRUE),\n    p + annotate(\"text\", x = 4, y = 25, label = \"paste(italic(R) ^ 2, \\\" = .75\\\")\", parse = TRUE), \n    ncol = 2, \n    rel_heights = c(4, 4)\n))\n\n\n\n\n\n\n\n\n\nなんか頑張ったら、こんなのもかけた。。。 これがあれば特定の範囲のデータについて、分布を記述することが可能となる。 めっちゃこれが便利だというシーンは思い浮かばないけどキレイなグラフを 記述するという意味では良い経験になったと思う.\n\n\nCode\nd  &lt;- density(mtcars$mpg)\nd2 &lt;- density(mtcars$wt)\np + \n    annotate(\n        \"polygon\",\n        x = (5 - 4 * d$y), # シフトやスケールが必要\n        y = d$x,\n        color = \"red\", \n        fill = \"transparent\") +  \n    annotate(\n        \"polygon\", \n        x = d2$x, \n        y = d2$y * 6 + 5, \n        color = \"blue\", \n        fill = \"transparent\")",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#annotation",
    "href": "contents/notes/ggplot2/working.html#annotation",
    "title": "ggplot2 Tips",
    "section": "2.5 annotation",
    "text": "2.5 annotation\nすべてのパネルので同じ統計アノテーションとして使う特別なgeomである。 これらのアノテーションはscaleには影響を与えない.\n\n2.5.1 annotation_custom\n下記の例を見るとなんとなく使い方はわかる. あとは記述したい絵を考えたうえで、色々と工夫を重ねるのが一番良いと思う.\n適当なGrobを作成できれば後は勢いで作成できる?\n使い方の用途としては，全体を小さく書いておき メインは全体のうち拡大したい箇所，という方法にして置くのがわかりやすい気がする．\n\n\nCode\nbase &lt;- \n    iris %&gt;% \n    ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) + \n    geom_point() + \n    lims(y = c(2, 5))\n\ng &lt;-\n  ggplotGrob(\n      ggplot(iris, aes(x = Sepal.Length, color = Species)) +\n      geom_line(stat = \"density\") + \n      guides(color = FALSE) + \n      theme(\n          panel.background = element_rect(fill = \"transparent\"), \n          plot.background  = element_rect(fill = \"transparent\"), \n          panel.grid   = element_blank(), \n          axis.title.y = element_blank(), \n          axis.line.y  = element_blank(), \n          axis.text.y  = element_blank(),\n          axis.ticks   = element_blank()\n      ))\n#&gt; Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n#&gt; of ggplot2 3.3.4.\n \nbase + \n    annotation_custom(grob = g, xmin = 6, xmax = 8, ymin = 4, ymax = 5)\n\n\n\n\n\n\n\n\n\n\n\n2.5.2 annotation_logticks\nこれではないけどマイナーグリッドにチックを入れる方法として annotateにgeom = “segment”を使う方法が考えだされていたけど， 正直しんどい気がする．．．．\n\n\nCode\na &lt;- ggplot(msleep, aes(bodywt, brainwt)) +\n geom_point(na.rm = TRUE) +\n scale_x_log10(\n   breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n   labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n ) +\n scale_y_log10(\n   breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n   labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n ) +\n theme_bw()\n\nlift(plot_grid)(list(\n    a + annotation_logticks(),               # Default: log ticks on bottom and left\n    a + annotation_logticks(sides = \"lr\"),   # Log ticks for y, on left and right\n    a + annotation_logticks(sides = \"trbl\")  # All four sides\n))\n\n\n\n\n\n\n\n\n\n\n\n2.5.3 annotation_map\n\n\nCode\nif (require(\"maps\")) {\nusamap &lt;- map_data(\"state\")\n\nseal.sub &lt;- subset(seals, long &gt; -130 & lat &lt; 45 & lat &gt; 40)\nggplot(seal.sub, aes(x = long, y = lat)) +\n  annotation_map(usamap, fill = NA, colour = \"grey50\") +\n  geom_segment(aes(xend = long + delta_long, yend = lat + delta_lat))\n}\n#&gt; Loading required package: maps\n#&gt; \n#&gt; Attaching package: 'maps'\n#&gt; The following object is masked from 'package:purrr':\n#&gt; \n#&gt;     map\n\n\n\n\n\n\n\n\n\n\n\nCode\nif (require(\"maps\")) {\nseal2 &lt;- transform(seal.sub,\n  latr = cut(lat, 2),\n  longr = cut(long, 2))\n\nggplot(seal2,  aes(x = long, y = lat)) +\n  annotation_map(usamap, fill = NA, colour = \"grey50\") +\n  geom_segment(aes(xend = long + delta_long, yend = lat + delta_lat)) +\n  facet_grid(latr ~ longr, scales = \"free\", space = \"free\")\n}\n\n\n\n\n\n\n\n\n\n\n\n2.5.4 annotation_raster\n画像を読み込めば画像も行ける？ 16進数の色に変換しておけば良いのか?\n\n\nCode\n# Generate data\nrainbow &lt;- matrix(hcl(seq(0, 360, length.out = 50 * 50), 80, 70), nrow = 50)\nggplot(mtcars, aes(mpg, wt)) +\n  geom_point() +\n  annotation_raster(rainbow, 15, 20, 3, 4)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# To fill up whole plot\nggplot(mtcars, aes(mpg, wt)) +\n  annotation_raster(rainbow, -Inf, Inf, -Inf, Inf) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nCode\nrainbow2 &lt;- matrix(hcl(seq(0, 360, length.out = 10), 80, 70), nrow = 1)\nggplot(mtcars, aes(mpg, wt)) +\n  annotation_raster(rainbow2, -Inf, Inf, -Inf, Inf) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(png)\nlibrary(grid)\nimg &lt;- readPNG(system.file(\"img\", \"Rlogo.png\", package = \"png\"))\ng   &lt;- rasterGrob(img, interpolate = TRUE)\nqplot(1:10, 1:10, geom = \"blank\") + \n    annotation_custom(g, xmin = 1, xmax = 5, ymin = 1, ymax = 2) + \n    geom_point()\n#&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0.",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#凡例のハンドリング",
    "href": "contents/notes/ggplot2/working.html#凡例のハンドリング",
    "title": "ggplot2 Tips",
    "section": "2.6 凡例のハンドリング",
    "text": "2.6 凡例のハンドリング\n凡例の操作は難しい… とりあえず、次を操作できるようになりたい.\n\n凡例の出し入れ\n凡例の位置(panelの中か外か)\n凡例のタイトル\n凡例のキーの操作\n凡例の文字列\n\nまずは複数の凡例の順序と凡例自体の位置を制御する例. themeで見た目を整える事もできるけど, guides(guide_legend)でも 大概見た目を加工することができる. 以下の例では 標準的な出力に加えて、凡例を色々と操作してみた状態のプロット。\n\n\nCode\nbase &lt;- \n  ggplot(mpg, aes(displ, cty)) +\n  geom_point(aes(size = hwy, colour = cyl, shape = drv)) + \n  theme_bw()\n\nlift(plot_grid)(list(\n  base, \n  base + guides(\n   colour = guide_colourbar(order = 1, draw.llim = FALSE),\n   shape  = guide_legend(order = 2),\n   size   = guide_legend(\n     order = 3, \n     title = \"MyWay\", \n     keywidth = unit(2, \"cm\"),\n     reverse = 1, \n     title.position = \"top\", # or bottom, left, right\n     title.theme = element_text(color = \"red\", face = \"italic\"), \n     title.hjust = 1, \n     label.theme = element_text(size = 14),\n     direction = \"vertical\"\n    )\n ), \n ncol = 2\n))\n\n\n\n\n\n\n\n\n\nカラーバーなどでの操作を細かく行うための使い方。 guidesとかlegendとかではないけど、一応凡例が操作されているといいうことで. これってスゴい便利なのでは？ 例えば普通の連続値の場合にはscale_fill_continuousを使う.\n\n\nCode\ndf &lt;- expand.grid(X1 = 1:10, X2 = 1:10)\ndf$value &lt;- df$X1 * df$X2\n\np &lt;- ggplot(df, aes(X1, X2)) + geom_tile(aes(fill = value)) + theme_bw() + coord_equal()\n\n# Coloursteps guide is the default for binned colour scales\nlift(plot_grid)(list(\n  p , \n  p + \n    geom_hline(yintercept = 1:10, color = \"lightgrey\", lty = 2) + \n    geom_vline(xintercept = 1:10, color = \"lightgrey\", lty = 2),\n  p + scale_fill_binned(), \n  p + scale_fill_binned(breaks = c(10, 25, 50)), \n  p + scale_fill_binned(breaks = c(10, 25, 50), guide = guide_coloursteps(even.steps = FALSE)), \n  p + scale_fill_binned(show.limits = TRUE),\n  ncol = 2\n))\n\n\n\n\n\n\n\n\n\n凡例をpanelの中に記述する場合の書き方は次の通り.\n\n\nCode\ndf &lt;- tibble(x = 1:3, y = 1:3, z = c(\"a\", \"b\", \"c\"))\nbase &lt;- \n  df %&gt;%\n  ggplot(aes(x, y, color = z), size = 3) + \n  geom_point()\n\nlift(plot_grid)(list(\n  base, \n  base + theme(\n    legend.position = c(0, 1),       # アンカーの相対座標\n    legend.justification = c(0, 1)), # アンカーの相対座標と紐付ける凡例の相対座標\n  base + theme(\n    legend.position = c(.5, .5), \n    legend.justification = c(.5, .5)),\n  base + theme(\n    legend.position = c(.5, .5), \n    legend.justification = c(0, 1)), \n  base + theme(\n    legend.position = c(1, 0), \n    legend.justification = c(1, 0)\n  ), \n  ncol = 2\n))\n\n\n\n\n\n\n\n\n\nマニュアルで凡例を作成する. ここを参考にした. これはしっくりするやり方だと思う.\n\n\nCode\n\ncolors &lt;- c(\"Sepal Width\" = \"blue\", \"Petal Length\" = \"red\", \"Petal Width\" = \"orange\")\n\nggplot(iris, aes(x = Sepal.Length)) +\n    geom_line(aes(y = Sepal.Width, color = \"Sepal Width\"), size = 1.5) +\n    geom_line(aes(y = Petal.Length, color = \"Petal Length\"), size = 1.5) +\n    geom_line(aes(y = Petal.Width, color = \"Petal Width\"), size = 1.5) +\n    labs(x = \"Year\",\n         y = \"(%)\",\n         color = \"Legend\") +\n    scale_color_manual(values = colors)\n\n\n\n\n\n\n\n\n\n凡例を消したい場合の対処方法. ここを参考にしている.\n\n\nCode\n# レイヤー単位で指定\np &lt;- \n  ggplot(ToothGrowth, aes(x = dose, y = len))+ \n  geom_boxplot(aes(fill = dose), show.legend = FALSE) +\n  scale_fill_viridis_d()\np\n#&gt; Warning: Continuous x aesthetic\n#&gt; ℹ did you forget `aes(group = ...)`?\n#&gt; Warning: The following aesthetics were dropped during statistical transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping structure in\n#&gt;   the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n#&gt;   variable into a factor?\n\n\n\n\n\n\n\n\n\n\n\nCode\n# グラフを作成してから指定\np + theme(legend.position = \"none\")\n#&gt; Warning: Continuous x aesthetic\n#&gt; ℹ did you forget `aes(group = ...)`?\n#&gt; Warning: The following aesthetics were dropped during statistical transformation: fill\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping structure in\n#&gt;   the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n#&gt;   variable into a factor?\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 特定の軸に対して処理をしたい場合の指定\nmtcars$cyl&lt;-as.factor(mtcars$cyl)\nmtcars$gear &lt;- as.factor(mtcars$gear)\n\n# Scatter plot\np2 &lt;- ggplot(data = mtcars, aes(x = mpg, y = wt))+\n    geom_point(aes(color = cyl, size = qsec, shape = gear)) +\n  scale_color_viridis_d()\np2\n\n\n\n\n\n\n\n\n\n\n\nCode\np2 + guides(color = \"none\", size = \"none\")\n\n\n\n\n\n\n\n\n\n\n2.6.1 文字や位置を微調整したい\nstackoverflowが参考になる.\n水平方向に調整する.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(mtcars, aes(factor(cyl), fill = factor(cyl))) + \n  geom_bar() +\n  coord_flip() +\n  scale_fill_brewer(\"Cyl\", palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = 'top', \n        legend.spacing.x = unit(1.0, 'cm'))\n\n\n\n\n\n\n\n\n\n鉛直方向に調整する.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(mtcars, aes(y = factor(cyl), fill = factor(cyl))) + \n  geom_bar() +\n  theme(legend.spacing.y = unit(1.0, 'cm'))  +\n  ## important additional element\n  guides(fill = guide_legend(byrow = TRUE))\n\n\n\n\n\n\n\n\n\nカラーバーなどは個別に微調整するみたい. バーの長さも自分で調整ができる見たい.\n\n\nCode\nggplot(mtcars, aes(mpg, wt)) +\n  geom_point(aes(fill = hp), pch = I(21), size = 5)+\n  scale_fill_viridis_c(guide = FALSE) +\n  theme_classic(base_size = 14) +\n  theme(legend.position = 'top', \n        legend.spacing.x = unit(0.5, 'cm'),\n        legend.text = element_text(margin = margin(t = 10))) +\n  guides(fill = guide_colorbar(title = \"HP\",\n                               label.position = \"bottom\",\n                               title.position = \"left\", title.vjust = 1,\n                               # draw border around the legend\n                               frame.colour = \"black\",\n                               barwidth = 25,\n                               barheight = 2)) \n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(mtcars) +\n  aes(x = cyl, fill = factor(cyl)) +\n  geom_bar() +\n  scale_fill_brewer(\"Cyl\", palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.key.size = unit(1, \"cm\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndraw_key_polygon3 &lt;- function(data, params, size) {\n  lwd &lt;- min(data$size, min(size) / 4)\n  \n  grid::rectGrob(\n    width = grid::unit(0.6, \"npc\"),\n    height = grid::unit(0.6, \"npc\"),\n    gp = grid::gpar(\n      col = data$colour,\n      fill = alpha(data$fill, data$alpha),\n      lty = data$linetype,\n      lwd = lwd * .pt,\n      linejoin = \"mitre\"\n    ))\n}\n\n### this step is not needed anymore per tjebo's comment below\n### see also: https://ggplot2.tidyverse.org/reference/draw_key.html\n# register new key drawing function, \n# the effect is global & persistent throughout the R session\n# GeomBar$draw_key = draw_key_polygon3\n\nggplot(mtcars) +\n  aes(x = cyl, fill = factor(cyl)) +\n  geom_bar(key_glyph = \"polygon3\") +\n  scale_fill_brewer(\"Cyl\", palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.key = element_rect(color = NA, fill = NA),\n        legend.key.size = unit(1.5, \"cm\")) +\n  theme(legend.title.align = 0.5)",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#テキスト",
    "href": "contents/notes/ggplot2/working.html#テキスト",
    "title": "ggplot2 Tips",
    "section": "2.7 テキスト",
    "text": "2.7 テキスト\n\n2.7.1 hjust/vjusts\n凡例の位置設定と同じ考え方が，どうやらテキストも同様の考え方で対応でてきる気がする. これをみると、ラベルの左下が(0, 0)で、右上が(1, 1)となっている. ラベルの右端を点xに合わせたい場合には、hjust=1に調整するなど下のものを 参考としてもらいたい.\n\n\nCode\n\nd &lt;- tibble(\n  x = 1:3 %&gt;% rep(3), \n  y = 1:3 %&gt;% rep(each = 3), \n  hjust = c(0, .5, 1) %&gt;% rep(3), \n  vjust = c(0, .5, 1) %&gt;% rep(each = 3)\n) %&gt;%\n  mutate(\n    t = sprintf(\"h:%s\\nv:%s\", hjust, vjust)\n  )\n\ng &lt;- \n  d %&gt;% \n  ggplot(aes(x, y, label = t)) + \n  coord_equal() + \n  theme_bw()\n\nplot(\n  g +\n    geom_label(aes(hjust = hjust, vjust = vjust)) + \n    geom_point(color = \"gold\", size = 2)\n  )\n\n\n\n\n\n\n\n\n\n\n\n2.7.2 position\nposition系の関数を使うと気の利いた調整が行える.\n\n\nCode\ndf &lt;- data.frame(\n  x = factor(c(1, 1, 2, 2)),\n  y = c(1, 3, 2, 1),\n  grp = c(\"a\", \"b\", \"a\", \"b\")\n)\n\n\n\n\nCode\nggplot(data = df, aes(x, y, group = grp)) +\n  geom_col(aes(fill = grp), position = \"dodge\") +\n  # x軸の中心からそれぞれどれだけ離すか\n  geom_text(aes(label = y), position = position_dodge(0.9))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Use you can't nudge and dodge text, so instead adjust the y position\nggplot(data = df, aes(x, y, group = grp)) +\n  geom_col(aes(fill = grp), position = \"dodge\") +\n  geom_text(\n    aes(label = y, y = y + 0.05),\n    position = position_dodge(0.9),\n    vjust = 0\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# To place text in the middle of each bar in a stacked barplot, you\n# need to set the vjust parameter of position_stack()\nggplot(data = df, aes(x, y, group = grp)) +\n geom_col(aes(fill = grp)) +\n # スタックされているところは，つまり，境界のところ\n # この場合のvjustはスタックされているところで調整してくれている(文字の大きさでない)\n geom_text(aes(label = y), position = position_stack(vjust = 0.5))",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#facet_grid",
    "href": "contents/notes/ggplot2/working.html#facet_grid",
    "title": "ggplot2 Tips",
    "section": "2.8 facet_grid",
    "text": "2.8 facet_grid\n基本的な使い方はいいが，ここではストリップラベルをずらす例を紹介．\n\n\nCode\ndata &lt;- \n  tibble(\n    rows = sprintf(\"row_%d\", 1:2), \n    cols = sprintf(\"col_%d\", 1:2)\n  ) %&gt;%\n  tidyr::expand(rows, cols) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    data = list(tibble(x = rnorm(1000), y = rnorm(1000)))\n  ) %&gt;%\n  unnest(data)\n\n\n\n\nCode\ndata %&gt;%\n  ggplot() + \n  geom_point(aes(x, y), size = 1) + \n  coord_equal(xlim = c(-3, 3), ylim = c(-3, 3)) + \n  theme_cowplot() + \n  facet_grid(\n    rows = vars(rows), \n    cols = vars(cols), \n    switch = \"y\" # 行のラベルを左に置く\n  ) + \n  scale_y_continuous(position = \"right\") + \n  theme(\n    # RStudioの予測にも出てこないけど，\n    # leftにおいたyのstripにアングルを与えるには次を使うこと\n    strip.text.y.left = element_text(angle = 0, face = \"bold\")\n  )",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#summary_stat",
    "href": "contents/notes/ggplot2/working.html#summary_stat",
    "title": "ggplot2 Tips",
    "section": "2.9 summary_stat",
    "text": "2.9 summary_stat\ngeom関数とstat関数は一緒の関係であるが, stat_summaryにより関数を適用してプロットを行うことが可能となる. 正直使いにくいイメージがあるので，個人的には 事前に集計したレイヤーを重ねるという方法を常に選択しがちである．\n\n\nCode\np &lt;- \n  ggplot(\n    data = filter(penguins, complete.cases(penguins)), \n    mapping = aes(\n      x = species,\n      y = bill_length_mm\n    )\n  ) + \n  theme_minimal() + \n  theme(\n    panel.background = element_rect(color = \"black\")\n  )\n\np + \n  geom_point() + \n  stat_summary(fun = \"mean\", color = \"red\", size = 4)\n#&gt; Warning: Removed 3 rows containing missing values (`geom_segment()`).",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#マルチレベルのwrap",
    "href": "contents/notes/ggplot2/working.html#マルチレベルのwrap",
    "title": "ggplot2 Tips",
    "section": "2.10 マルチレベルのwrap",
    "text": "2.10 マルチレベルのwrap\n自然言語処理でドキュメントごとに単語頻度上位N件の 度数をバープロットで描画するときなど， ドキュメントでfacetしてその中でwordを並び変えてプロットしたいときがある．\nこのとき，何も考えずに行うと期待したプロットが作成できない． これに対処するには，並び換えを全体で行えるようにkeyを調整した上で， プロットする際にキーラベル名を調整するという処理が必要である.\n\n\nCode\ndata &lt;- \n  tibble(\n    key   = c(\"a\", \"a\", \"b\", \"b\"), \n    value = c(1, 2, 2, 1), \n    term  = c(\"w1\", \"w2\", \"w1\", \"w2\")\n  )\n\n# 並び変えてプロットしても\n# グラフは同じ順番になってしまう\n# これをfacetごとに降順に並び変えたい\ndata %&gt;% \n  group_by(key) %&gt;% \n  arrange(value) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(reorder(term, value), value)) + \n  geom_col() + \n  facet_wrap(vars(key), scales = \"free\")\n\n\n\n\n\n\n\n\n\n一応このようにやれば対応が可能である.\n\n\nCode\ndata %&gt;%\n  mutate(new_key = factor(str_c(term, key, sep = \"___\"))) %&gt;%\n  arrange(value) %&gt;%\n  ggplot(aes(reorder(new_key, value), value)) + \n  geom_col() + \n  facet_wrap(vars(key), scales = \"free\") + \n  scale_x_discrete(label = function(x) sub(\"___.+$\", \"\", x), drop = TRUE)",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#trans",
    "href": "contents/notes/ggplot2/working.html#trans",
    "title": "ggplot2 Tips",
    "section": "2.11 trans",
    "text": "2.11 trans\ntrans系やscale系の関数を使うことで，データをggplot内で変換することが可能となるはず． たとえばscale_x_logなどがそれに該当する.\n\n\nCode\ntibble(\n  x = 1:10, \n  y = 1:10\n) %&gt;% \n  ggplot(aes(x, y)) + \n  geom_line() + \n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n同じことを違う遣り方で実行してみる.\n\n\nCode\ntibble(\n  x = 1:10, \n  y = 1:10\n) %&gt;% \n  ggplot(aes(x, y)) + \n  geom_line() + \n  scale_y_continuous(trans = \"log10\")\n\n\n\n\n\n\n\n\n\nさらに別のやり方を試す.\n\n\nCode\ntibble(\n  x = 1:10, \n  y = 1:10\n) %&gt;% \n  ggplot(aes(x, y)) + \n  geom_line() + \n  scale_y_continuous(\n    trans = scales::trans_new(\"log10\", log10, function(x) 10 ** x), \n    minor_breaks = seq(1, 10, .5), \n    breaks = c(1, 6, 10)\n  ) + \n  theme(\n    axis.ticks.length = unit(-2, \"mm\")\n  )",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#グラフの順番に凡例の順番を合わす",
    "href": "contents/notes/ggplot2/working.html#グラフの順番に凡例の順番を合わす",
    "title": "ggplot2 Tips",
    "section": "2.12 グラフの順番に凡例の順番を合わす",
    "text": "2.12 グラフの順番に凡例の順番を合わす\n参考文献\n\n\nCode\nlibrary(tidyverse)  # 1.3.0\n\nusa_crop_yields &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv\") %&gt;%\n  rename_with(~ gsub(\" \\\\(tonnes per hectare\\\\)\", \"\", .)) %&gt;%\n  pivot_longer(Wheat:Bananas, names_to = \"crop\", values_to = \"yield\") %&gt;%\n  rename_with(tolower) %&gt;%\n  filter(entity == \"United States\", !is.na(yield)) %&gt;%\n  select(year, crop, yield)\n#&gt; Rows: 13075 Columns: 14\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr  (2): Entity, Code\n#&gt; dbl (12): Year, Wheat (tonnes per hectare), Rice (tonnes per hectare), Maize...\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nusa_crop_yields\n#&gt; # A tibble: 522 × 3\n#&gt;     year crop     yield\n#&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1  1961 Wheat     1.61\n#&gt;  2  1961 Rice      3.82\n#&gt;  3  1961 Maize     3.92\n#&gt;  4  1961 Soybeans  1.69\n#&gt;  5  1961 Potatoes 22.2 \n#&gt;  6  1961 Beans     1.54\n#&gt;  7  1961 Peas      1.19\n#&gt;  8  1961 Barley    1.65\n#&gt;  9  1961 Bananas  10.5 \n#&gt; 10  1962 Wheat     1.68\n#&gt; # ℹ 512 more rows\n\n\n\n\nCode\nusa_crop_yields %&gt;%\n  mutate(crop = fct_reorder2(crop, year, yield)) %&gt;%\n  ggplot(aes(year, yield, color = crop)) +\n  geom_line() +\n  labs(x = NULL, y = NULL, color = NULL)",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#histgram",
    "href": "contents/notes/ggplot2/working.html#histgram",
    "title": "ggplot2 Tips",
    "section": "2.13 histgram",
    "text": "2.13 histgram\nヒストグラムをきれいに書きたい.\n\n\nCode\ndata &lt;- tibble(a = runif(100), b = sample(c(\"a\", \"b\"), 100, replace = TRUE))\ndata %&gt;%\n  ggplot(aes(x = a)) + \n  geom_histogram(aes(fill = b), center = 0, breaks = seq(0, 1., .1), color = \"lightgrey\", closed = \"left\") +\n  geom_text(\n    stat = \"bin\",\n    # count, dnesity, ncount, ndensity\n    aes(label = ..count.., y = ..count.., fill = b),\n    breaks = seq(0, 1., .1),\n    closed = \"left\",\n    vjust = -.5,\n    position = position_stack(vjust = .5)\n  ) + \n  geom_text(\n    stat = \"bin\", \n    aes(x = a, label = after_stat(count), y = after_stat(count)), \n    breaks = seq(0, 1., .1), \n    closed = \"left\", \n    vjust = -.5, \n    data = data\n  ) +\n  scale_x_continuous(breaks = seq(0, 1., .1))\n#&gt; Warning in geom_text(stat = \"bin\", aes(label = ..count.., y = ..count.., :\n#&gt; Ignoring unknown aesthetics: fill\n#&gt; Warning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `after_stat(count)` instead.\n\n\n\n\n\n\n\n\n\n左に閉じたときにどのような挙動になるのかを確認したい. [0, .1), [.1, .2), …., [.9, 1.]となっていることがわかる.\n\n\nCode\ndata &lt;- tibble(a = sample(c(0, .1, .5,  1.), 100, TRUE), b = sample(c(\"a\", \"b\"), 100, replace = TRUE))\ndata %&gt;%\n  ggplot(aes(x = a)) + \n  geom_histogram(\n    aes(fill = b), \n    center = 0,\n    breaks = seq(0, 1., .1), \n    color = \"lightgrey\", \n    closed = \"left\"\n  ) +\n  geom_text(\n    stat = \"bin\",\n    aes(label = ..count.., y = ..count.., group = b),\n    breaks = seq(0, 1., .1),\n    closed = \"left\",\n    vjust = -.5,\n    position = position_stack(vjust = .5)\n  ) + \n  scale_x_continuous(breaks = seq(0, 1., .1))",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#geom_smooth",
    "href": "contents/notes/ggplot2/working.html#geom_smooth",
    "title": "ggplot2 Tips",
    "section": "2.14 geom_smooth",
    "text": "2.14 geom_smooth\n\n\nCode\ndf &lt;- tibble(\n    x = seq(1, 100, 1), \n    e = runif(n = 100)\n) |&gt; \n    mutate(y = x * .05 + e)\n\ndf |&gt; \n    ggplot(aes(x, y)) + \n    geom_point() + \n    geom_line(data = ~ . |&gt; filter(x &lt;= 50), col = \"red\") + \n    geom_smooth(data = ~ . |&gt; filter(x &gt; 50), col = \"blue\")\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#ggmosaic",
    "href": "contents/notes/ggplot2/working.html#ggmosaic",
    "title": "ggplot2 Tips",
    "section": "3.1 ggmosaic",
    "text": "3.1 ggmosaic\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggmosaic)\n#&gt; \n#&gt; Attaching package: 'ggmosaic'\n#&gt; The following object is masked from 'package:GGally':\n#&gt; \n#&gt;     happy\nlibrary(dplyr)\n\ncc &lt;- count(diamonds, cut, clarity)\n\nggplot(cc) + \n    geom_mosaic(\n        aes(weight = n, x = product(cut), fill = clarity)\n    )\n#&gt; Warning: `unite_()` was deprecated in tidyr 1.2.0.\n#&gt; ℹ Please use `unite()` instead.\n#&gt; ℹ The deprecated feature was likely used in the ggmosaic package.\n#&gt;   Please report the issue at &lt;https://github.com/haleyjeppson/ggmosaic&gt;.",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#ggokabeito",
    "href": "contents/notes/ggplot2/working.html#ggokabeito",
    "title": "ggplot2 Tips",
    "section": "3.2 ggokabeito",
    "text": "3.2 ggokabeito\nsafecolorらしい．\n\n\nCode\niris |&gt; \n    ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) + \n    geom_point(size = 4) + \n    theme_bw() + \n    scale_color_okabe_ito()",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#ggprism",
    "href": "contents/notes/ggplot2/working.html#ggprism",
    "title": "ggplot2 Tips",
    "section": "3.3 ggprism",
    "text": "3.3 ggprism\nマイナーチック を打てるようです.\n他にも色々軸の加工が可能なためいろいろとチェックしてみるのがいい．\n\n3.3.1 Minor ticks\n\n\nCode\np &lt;- ggplot(\n  data = ToothGrowth, \n  mapping = aes(\n    x = factor(supp), \n    y = len\n  )\n) + \n  geom_boxplot(aes(fill = factor(supp))) + \n  theme_prism() + \n  theme(legend.position = \"none\")\np1 &lt;- p + scale_y_continuous(guide = guide_prism_minor())\np2 &lt;- p + guides(y = guide_prism_minor())\np2\n\n\n\n\n\n\n\n\n\n\n\nCode\np &lt;- \n  ggplot(\n    data = ToothGrowth, \n    mapping = aes(\n      x = factor(dose), \n      y = len\n    )\n  ) + \n  stat_summary(\n    mapping = aes(fill = factor(dose)), na.rm = TRUE, \n    geom = \"col\", fun = mean, colour = \"black\", size = .9\n  ) + \n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n# 書きたかったグラフはこれや！！！！！\n# これが土木学会のグラフや\np + \n  scale_y_continuous(\n    guide = \"prism_minor\", \n    limits = c(0, 30), \n    expand = c(0, 0), \n    minor_breaks = seq(0, 30)\n  ) + \n  theme(\n    axis.ticks.length.y = unit(10, \"pt\"), \n    prism.ticks.length.y = unit(5, \"pt\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Brackets axis\n\n\nCode\np1 &lt;- ggplot(ToothGrowth, aes(x = factor(dose), y = len)) + \n  geom_jitter(aes(shape = factor(dose)), width = 0.2, size = 2) + \n  scale_shape_prism() + \n  theme_prism() + \n  theme(legend.position = \"none\") + \n  scale_y_continuous(limits = c(0, 40), guide = \"prism_offset\")\n\np2 &lt;- p1 + scale_x_discrete(guide = \"prism_bracket\")\n\np2\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Border with minor ticks\n\n\nCode\nbase &lt;- \n  ggplot(mpg, aes(x = displ, y = cty)) +\n  geom_point(aes(colour = class))\nbase\n\n\n\n\n\n\n\n\n\nannotation_ticksを使うをスゴく簡単にチックを打てる.\n\n\nCode\np &lt;- base + theme_prism(border = TRUE) +\n  theme(legend.position = c(0.8, 0.75)) +\n  coord_cartesian(clip = \"off\") + \n  guides(\n    x = \"prism_minor\", y = \"prism_minor\"\n  )\np_annot &lt;- \n  p + \n  annotation_ticks(\n    sides = \"tr\", \n    type = \"both\", \n    size = 1,\n    outside = TRUE,\n    tick.length = unit(14/2, \"pt\"),\n    minor.length = unit(14/4, \"pt\"))\np_annot\n\n\n\n\n\n\n\n\n\n離散値の場合にも出来るようでしたがあまりにも トリッキーにつき省略いたします.",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#gghalves",
    "href": "contents/notes/ggplot2/working.html#gghalves",
    "title": "ggplot2 Tips",
    "section": "3.4 gghalves",
    "text": "3.4 gghalves\n\n(gghalves)[https://github.com/erocoar/gghalves]\n\nggplo2によるハーフ＆ハーフのプロットの合成を容易にする。\n\n\nCode\nlibrary(gghalves)\n\n\n\n\nCode\niris %&gt;%\n  ggplot(aes(Species, Sepal.Width, color = Species)) + \n  geom_half_point() + \n  geom_half_violin()",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#ggally",
    "href": "contents/notes/ggplot2/working.html#ggally",
    "title": "ggplot2 Tips",
    "section": "3.5 ggally",
    "text": "3.5 ggally\n\nggally\n\n\n\nCode\nlibrary(GGally)\n\n\n\n3.5.1 Types of Plots\n\ncontinuous: when both x and y variables are continuous\ncomboHorizontal: when x is continuous and y is discrete\ncomboVorizontal: when x is discrete and y is continuous\ndiscrete: when both x and y variables are discrete\nna: when all x data and all y data is NA\n\n\n\n3.5.2 ggpairs\nいろいろカスタマイズができるのはわかるが、 やりたいものがないとどのようなグラフを作成すべきであるのかがわからない。 こんため\n\n3.5.2.1 basic example\n\n\nCode\npm &lt;- \n  iris %&gt;%\n  ggpairs(\n    mapping = aes(color = Species), \n    columns = c(\"Sepal.Length\", \"Petal.Length\", \"Species\")\n  ) + \n  theme_bw()\npm\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCode\npm &lt;- \n  iris %&gt;%\n  ggpairs(\n    mapping = aes(color = Species), \n    columns = c(\"Sepal.Length\", \"Petal.Length\", \"Species\"), \n    lower   = list(\n      continuous = \"smooth\", \n      combo = \"facetdensity\", \n      mappping = aes(color = Species)\n    )\n  ) + \n  theme_bw()\npm\n\n\n\n\n\n\n\n\n\n\n\nCode\npm &lt;- \n  iris %&gt;%\n  ggpairs(\n    mapping = aes(color = Species), \n    columns = c(\"Sepal.Length\", \"Petal.Length\", \"Species\"), \n    upper = \"blank\", \n    diag  = NULL, \n    lower   = list(\n      continuous = \"smooth\", \n      combo = \"facetdensity\", \n      mappping = aes()\n    )\n  ) + \n  theme_bw()\npm\n\n\n\n\n\n\n\n\n\n\n\n3.5.2.2 custom functions\n\n\nCode\ndata(tips, package = \"reshape\")\nmy_bin &lt;- function(data, mapping, ..., low = \"#132B43\", high = \"#56B1F7\") {\n  ggplot(data = data, mapping = mapping) +\n    geom_bin2d(...) +\n    scale_fill_gradient(low = low, high = high) + \n    theme(\n      axis.text = element_text(color = \"red\")\n    )\n}\npm &lt;- ggpairs(\n  tips, \n  columns = c(\"total_bill\", \"time\", \"tip\"), \n  lower = list(\n    continuous = my_bin\n  )\n)\npm\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCode\npm &lt;- ggpairs(\n  tips, columns = c(\"total_bill\", \"time\", \"tip\"),\n  lower = list(\n    combo = wrap(\"facethist\", binwidth = 1),\n    continuous = wrap(my_bin, binwidth = c(5, 0.5), high = \"red\")\n  )\n)\npm\n\n\n\n\n\n\n\n\n\n\n\n3.5.2.3 plot matrix subsetting\n\n\nCode\npm &lt;- ggpairs(tips, columns = c(\"total_bill\", \"time\", \"tip\"))\np  &lt;- pm[3, 1]\np  &lt;- p + aes(color = time)\np\n\n\n\n\n\n\n\n\n\n\n\nCode\npm[3,1] &lt;- p\npm\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCode\ncustom_car &lt;- ggpairs(mtcars[, c(\"mpg\", \"wt\", \"cyl\")], \n                      upper = \"blank\", title = \"Custom Example\")\n# ggplot example taken from example(geom_text)\nplot &lt;- ggplot2::ggplot(mtcars, ggplot2::aes(x=wt, y=mpg, label=rownames(mtcars)))\nplot &lt;- plot +\n    ggplot2::geom_text(ggplot2::aes(colour=factor(cyl)), size = 3) +\n    ggplot2::scale_colour_discrete(l=40)\ncustom_car[1, 2] &lt;- plot\npersonal_plot &lt;- ggally_text(\n  \"ggpairs allows you\\nto put in your\\nown plot.\\nLike that one.\\n &lt;---\"\n)\ncustom_car[1, 3] &lt;- personal_plot\n(custom_car)\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Remove binwidth warning from ggplot2\npm &lt;- ggpairs(tips, 2:3, lower = list(combo = wrap(\"facethist\", binwidth = 0.5)))\npm\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Remove panel grid lines from correlation plots\npm &lt;- ggpairs(\n  flea, columns = 2:4,\n  upper = list(continuous = wrap(ggally_cor, display_grid = FALSE))\n)\npm + theme(\n  strip.background = element_rect(fill = \"black\"), \n  strip.text = element_text(color = \"white\"))\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.3 ggnetworkmap\n\n\nCode\nsuppressMessages(library(ggplot2))\nsuppressMessages(library(maps))\nsuppressMessages(library(network))\nsuppressMessages(library(sna))\n\nairports &lt;- read.csv(\"http://datasets.flowingdata.com/tuts/maparcs/airports.csv\", header = TRUE)\nrownames(airports) &lt;- airports$iata\n\n# select some random flights\nset.seed(1234)\nflights &lt;- data.frame(\n  origin = sample(airports[200:400, ]$iata, 200, replace = TRUE),\n  destination = sample(airports[200:400, ]$iata, 200, replace = TRUE)\n)\n\n# convert to network\nflights &lt;- network(flights, directed = TRUE)\n\n# add geographic coordinates\nflights %v% \"lat\" &lt;- airports[ network.vertex.names(flights), \"lat\" ]\nflights %v% \"lon\" &lt;- airports[ network.vertex.names(flights), \"long\" ]\n\n# drop isolated airports\ndelete.vertices(flights, which(degree(flights) &lt; 2))\n\n# compute degree centrality\nflights %v% \"degree\" &lt;- degree(flights, gmode = \"digraph\")\n\n# add random groups\nflights %v% \"mygroup\" &lt;- sample(letters[1:4], network.size(flights), replace = TRUE)\n\n# create a map of the USA\nusa &lt;- ggplot(map_data(\"usa\"), aes(x = long, y = lat)) +\n  geom_polygon(aes(group = group), color = \"grey65\",\n               fill = \"#f9f9f9\", size = 0.2)\n\n# trim flights\ndelete.vertices(flights, which(flights %v% \"lon\" &lt; min(usa$data$long)))\ndelete.vertices(flights, which(flights %v% \"lon\" &gt; max(usa$data$long)))\ndelete.vertices(flights, which(flights %v% \"lat\" &lt; min(usa$data$lat)))\ndelete.vertices(flights, which(flights %v% \"lat\" &gt; max(usa$data$lat)))\n\n# overlay network data to map\nggnetworkmap(usa, flights, size = 4, great.circles = TRUE,\n             node.group = mygroup, segment.color = \"steelblue\",\n             ring.group = degree, weight = degree)",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#gganimate",
    "href": "contents/notes/ggplot2/working.html#gganimate",
    "title": "ggplot2 Tips",
    "section": "3.6 gganimate",
    "text": "3.6 gganimate\ngithub\n\n\nCode\nlibrary(gganimate)\n\nggplot(mtcars, aes(factor(cyl), mpg)) + \n  geom_boxplot() + \n  transition_states(\n    gear, \n    transition_length = 2, \n    state_length = 1\n  ) + \n  enter_fade() + \n  exit_shrink() + \n  ease_aes('sine-in-out')",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#waffle",
    "href": "contents/notes/ggplot2/working.html#waffle",
    "title": "ggplot2 Tips",
    "section": "3.7 waffle",
    "text": "3.7 waffle\n\n\nCode\nwaffle(c(30, 25, 20, 5), rows = 8)",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#ggupset",
    "href": "contents/notes/ggplot2/working.html#ggupset",
    "title": "ggplot2 Tips",
    "section": "3.8 ggupset",
    "text": "3.8 ggupset\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggupset)\nggplot(tidy_movies[1:100, ], aes(x=Genres)) +\n  geom_bar() +\n  scale_x_upset(reverse = TRUE, sets=c(\"Drama\", \"Action\"))\n#&gt; Warning: Removed 33 rows containing non-finite values (`stat_count()`).\n\n\n\n\n\n\n\n\n\nCode\n\n ggplot(tidy_movies[1:100, ], aes(x=Genres)) +\n   geom_bar() +\n   scale_x_upset(n_intersections = 5, ytrans=\"sqrt\")\n#&gt; Warning: Removed 28 rows containing non-finite values (`stat_count()`).\n\n\n\n\n\n\n\n\n\nCode\n\n ggplot(tidy_movies[1:100, ], aes(x=Genres, y=year)) +\n   geom_boxplot() +\n   scale_x_upset(intersections = list(c(\"Drama\", \"Comedy\"), c(\"Short\"), c(\"Short\", \"Animation\")),\n                 sets = c(\"Drama\", \"Comedy\", \"Short\", \"Animation\", \"Horror\"))\n#&gt; Warning: Removed 90 rows containing missing values (`stat_boxplot()`).",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#heat-map-with-geom_tile",
    "href": "contents/notes/ggplot2/working.html#heat-map-with-geom_tile",
    "title": "ggplot2 Tips",
    "section": "3.9 Heat map with geom_tile",
    "text": "3.9 Heat map with geom_tile\n\n3.9.1 Sample Data\n\n\nCode\nlibrary(reshape)\n#&gt; \n#&gt; Attaching package: 'reshape'\n#&gt; The following object is masked from 'package:cowplot':\n#&gt; \n#&gt;     stamp\n#&gt; The following object is masked from 'package:lubridate':\n#&gt; \n#&gt;     stamp\n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     rename\n#&gt; The following objects are masked from 'package:tidyr':\n#&gt; \n#&gt;     expand, smiths\n\n# Data \nset.seed(8)\nm &lt;- matrix(round(rnorm(200), 2), 10, 10)\n#&gt; Warning in matrix(round(rnorm(200), 2), 10, 10): data length differs from size\n#&gt; of matrix: [200 != 10 x 10]\ncolnames(m) &lt;- paste(\"Col\", 1:10)\nrownames(m) &lt;- paste(\"Row\", 1:10)\n\n# Transform the matrix in long format\ndf &lt;- melt(m)\n#&gt; Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\n#&gt; the caller; using TRUE\n#&gt; Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\n#&gt; the caller; using TRUE\ncolnames(df) &lt;- c(\"x\", \"y\", \"value\")\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile() + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(\n    color = \"white\", \n    lwd = 1.5, \n    linetype = 1.\n  ) + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(color = \"black\") + \n  geom_text(aes(label = value), color = \"white\", size = 4) + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\n3.9.2 Color Palette\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(color = \"black\") + \n  scale_fill_gradient(low = \"white\", high = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(color = \"black\") + \n  scale_fill_gradient2(\n    low = \"#075AFF\", \n    mid = \"#FFFFCC\", \n    high = \"#FF0000\"\n  ) + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(color = \"black\") + \n  scale_fill_gradientn(colors = hcl.colors(20, \"RdYlGn\")) +\n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\n3.9.3 Legend customization\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(color = \"black\") + \n  coord_fixed() + \n  guides(\n    fill = guide_colourbar(barwidth = .5, barheight = 20)\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y, fill = value)) +\n  geom_tile(color = \"black\") +\n  coord_fixed() +\n  guides(fill = guide_colourbar(title = \"Title\")) \n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y, fill = value)) +\n  geom_tile(color = \"black\") +\n  coord_fixed() +\n  guides(fill = guide_colourbar(label = FALSE,\n                                ticks = FALSE)) \n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y, fill = value)) +\n  geom_tile(color = \"black\") +\n  coord_fixed() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#alluvial-plot",
    "href": "contents/notes/ggplot2/working.html#alluvial-plot",
    "title": "ggplot2 Tips",
    "section": "3.10 Alluvial plot",
    "text": "3.10 Alluvial plot\n\n3.10.1 sample dataset\n\n\nCode\ndata(\"vaccinations\")\nvaccinations\n#&gt;        survey freq subject  response start_date   end_date\n#&gt; 1   ms153_NSA   48       1    Always 2010-09-22 2010-10-25\n#&gt; 2   ms153_NSA    9       2    Always 2010-09-22 2010-10-25\n#&gt; 3   ms153_NSA   66       3    Always 2010-09-22 2010-10-25\n#&gt; 4   ms153_NSA    1       4    Always 2010-09-22 2010-10-25\n#&gt; 5   ms153_NSA   11       5    Always 2010-09-22 2010-10-25\n#&gt; 6   ms153_NSA    1       6    Always 2010-09-22 2010-10-25\n#&gt; 7   ms153_NSA    5       7    Always 2010-09-22 2010-10-25\n#&gt; 8   ms153_NSA    4       8    Always 2010-09-22 2010-10-25\n#&gt; 9   ms153_NSA   24       9   Missing 2010-09-22 2010-10-25\n#&gt; 10  ms153_NSA    4      10   Missing 2010-09-22 2010-10-25\n#&gt; 11  ms153_NSA   29      11   Missing 2010-09-22 2010-10-25\n#&gt; 12  ms153_NSA    1      12   Missing 2010-09-22 2010-10-25\n#&gt; 13  ms153_NSA    1      13   Missing 2010-09-22 2010-10-25\n#&gt; 14  ms153_NSA    9      14   Missing 2010-09-22 2010-10-25\n#&gt; 15  ms153_NSA   10      15   Missing 2010-09-22 2010-10-25\n#&gt; 16  ms153_NSA   14      16   Missing 2010-09-22 2010-10-25\n#&gt; 17  ms153_NSA    3      17     Never 2010-09-22 2010-10-25\n#&gt; 18  ms153_NSA    7      18     Never 2010-09-22 2010-10-25\n#&gt; 19  ms153_NSA   42      19     Never 2010-09-22 2010-10-25\n#&gt; 20  ms153_NSA   20      20     Never 2010-09-22 2010-10-25\n#&gt; 21  ms153_NSA    2      21     Never 2010-09-22 2010-10-25\n#&gt; 22  ms153_NSA   36      22     Never 2010-09-22 2010-10-25\n#&gt; 23  ms153_NSA    7      23     Never 2010-09-22 2010-10-25\n#&gt; 24  ms153_NSA    2      24     Never 2010-09-22 2010-10-25\n#&gt; 25  ms153_NSA    4      25     Never 2010-09-22 2010-10-25\n#&gt; 26  ms153_NSA    9      26     Never 2010-09-22 2010-10-25\n#&gt; 27  ms153_NSA   48      27 Sometimes 2010-09-22 2010-10-25\n#&gt; 28  ms153_NSA   11      28 Sometimes 2010-09-22 2010-10-25\n#&gt; 29  ms153_NSA   99      29 Sometimes 2010-09-22 2010-10-25\n#&gt; 30  ms153_NSA    3      30 Sometimes 2010-09-22 2010-10-25\n#&gt; 31  ms153_NSA   41      31 Sometimes 2010-09-22 2010-10-25\n#&gt; 32  ms153_NSA  168      32 Sometimes 2010-09-22 2010-10-25\n#&gt; 33  ms153_NSA    2      33 Sometimes 2010-09-22 2010-10-25\n#&gt; 34  ms153_NSA   23      34 Sometimes 2010-09-22 2010-10-25\n#&gt; 35  ms153_NSA   22      35 Sometimes 2010-09-22 2010-10-25\n#&gt; 36  ms153_NSA   38      36 Sometimes 2010-09-22 2010-10-25\n#&gt; 37  ms153_NSA    1      37 Sometimes 2010-09-22 2010-10-25\n#&gt; 38  ms153_NSA   15      38 Sometimes 2010-09-22 2010-10-25\n#&gt; 39  ms153_NSA  113      39 Sometimes 2010-09-22 2010-10-25\n#&gt; 40  ms432_NSA   48       1    Always 2015-06-04 2015-10-05\n#&gt; 41  ms432_NSA    9       2    Always 2015-06-04 2015-10-05\n#&gt; 42  ms432_NSA   66       3   Missing 2015-06-04 2015-10-05\n#&gt; 43  ms432_NSA    1       4   Missing 2015-06-04 2015-10-05\n#&gt; 44  ms432_NSA   11       5   Missing 2015-06-04 2015-10-05\n#&gt; 45  ms432_NSA    1       6     Never 2015-06-04 2015-10-05\n#&gt; 46  ms432_NSA    5       7 Sometimes 2015-06-04 2015-10-05\n#&gt; 47  ms432_NSA    4       8 Sometimes 2015-06-04 2015-10-05\n#&gt; 48  ms432_NSA   24       9    Always 2015-06-04 2015-10-05\n#&gt; 49  ms432_NSA    4      10    Always 2015-06-04 2015-10-05\n#&gt; 50  ms432_NSA   29      11   Missing 2015-06-04 2015-10-05\n#&gt; 51  ms432_NSA    1      12   Missing 2015-06-04 2015-10-05\n#&gt; 52  ms432_NSA    1      13   Missing 2015-06-04 2015-10-05\n#&gt; 53  ms432_NSA    9      14   Missing 2015-06-04 2015-10-05\n#&gt; 54  ms432_NSA   10      15 Sometimes 2015-06-04 2015-10-05\n#&gt; 55  ms432_NSA   14      16 Sometimes 2015-06-04 2015-10-05\n#&gt; 56  ms432_NSA    3      17    Always 2015-06-04 2015-10-05\n#&gt; 57  ms432_NSA    7      18   Missing 2015-06-04 2015-10-05\n#&gt; 58  ms432_NSA   42      19   Missing 2015-06-04 2015-10-05\n#&gt; 59  ms432_NSA   20      20   Missing 2015-06-04 2015-10-05\n#&gt; 60  ms432_NSA    2      21     Never 2015-06-04 2015-10-05\n#&gt; 61  ms432_NSA   36      22     Never 2015-06-04 2015-10-05\n#&gt; 62  ms432_NSA    7      23     Never 2015-06-04 2015-10-05\n#&gt; 63  ms432_NSA    2      24 Sometimes 2015-06-04 2015-10-05\n#&gt; 64  ms432_NSA    4      25 Sometimes 2015-06-04 2015-10-05\n#&gt; 65  ms432_NSA    9      26 Sometimes 2015-06-04 2015-10-05\n#&gt; 66  ms432_NSA   48      27    Always 2015-06-04 2015-10-05\n#&gt; 67  ms432_NSA   11      28    Always 2015-06-04 2015-10-05\n#&gt; 68  ms432_NSA   99      29   Missing 2015-06-04 2015-10-05\n#&gt; 69  ms432_NSA    3      30   Missing 2015-06-04 2015-10-05\n#&gt; 70  ms432_NSA   41      31   Missing 2015-06-04 2015-10-05\n#&gt; 71  ms432_NSA  168      32   Missing 2015-06-04 2015-10-05\n#&gt; 72  ms432_NSA    2      33     Never 2015-06-04 2015-10-05\n#&gt; 73  ms432_NSA   23      34     Never 2015-06-04 2015-10-05\n#&gt; 74  ms432_NSA   22      35     Never 2015-06-04 2015-10-05\n#&gt; 75  ms432_NSA   38      36 Sometimes 2015-06-04 2015-10-05\n#&gt; 76  ms432_NSA    1      37 Sometimes 2015-06-04 2015-10-05\n#&gt; 77  ms432_NSA   15      38 Sometimes 2015-06-04 2015-10-05\n#&gt; 78  ms432_NSA  113      39 Sometimes 2015-06-04 2015-10-05\n#&gt; 79  ms460_NSA   48       1    Always 2016-09-27 2016-10-25\n#&gt; 80  ms460_NSA    9       2 Sometimes 2016-09-27 2016-10-25\n#&gt; 81  ms460_NSA   66       3    Always 2016-09-27 2016-10-25\n#&gt; 82  ms460_NSA    1       4     Never 2016-09-27 2016-10-25\n#&gt; 83  ms460_NSA   11       5 Sometimes 2016-09-27 2016-10-25\n#&gt; 84  ms460_NSA    1       6    Always 2016-09-27 2016-10-25\n#&gt; 85  ms460_NSA    5       7    Always 2016-09-27 2016-10-25\n#&gt; 86  ms460_NSA    4       8 Sometimes 2016-09-27 2016-10-25\n#&gt; 87  ms460_NSA   24       9    Always 2016-09-27 2016-10-25\n#&gt; 88  ms460_NSA    4      10 Sometimes 2016-09-27 2016-10-25\n#&gt; 89  ms460_NSA   29      11    Always 2016-09-27 2016-10-25\n#&gt; 90  ms460_NSA    1      12   Missing 2016-09-27 2016-10-25\n#&gt; 91  ms460_NSA    1      13     Never 2016-09-27 2016-10-25\n#&gt; 92  ms460_NSA    9      14 Sometimes 2016-09-27 2016-10-25\n#&gt; 93  ms460_NSA   10      15    Always 2016-09-27 2016-10-25\n#&gt; 94  ms460_NSA   14      16 Sometimes 2016-09-27 2016-10-25\n#&gt; 95  ms460_NSA    3      17    Always 2016-09-27 2016-10-25\n#&gt; 96  ms460_NSA    7      18    Always 2016-09-27 2016-10-25\n#&gt; 97  ms460_NSA   42      19     Never 2016-09-27 2016-10-25\n#&gt; 98  ms460_NSA   20      20 Sometimes 2016-09-27 2016-10-25\n#&gt; 99  ms460_NSA    2      21   Missing 2016-09-27 2016-10-25\n#&gt; 100 ms460_NSA   36      22     Never 2016-09-27 2016-10-25\n#&gt; 101 ms460_NSA    7      23 Sometimes 2016-09-27 2016-10-25\n#&gt; 102 ms460_NSA    2      24    Always 2016-09-27 2016-10-25\n#&gt; 103 ms460_NSA    4      25     Never 2016-09-27 2016-10-25\n#&gt; 104 ms460_NSA    9      26 Sometimes 2016-09-27 2016-10-25\n#&gt; 105 ms460_NSA   48      27    Always 2016-09-27 2016-10-25\n#&gt; 106 ms460_NSA   11      28 Sometimes 2016-09-27 2016-10-25\n#&gt; 107 ms460_NSA   99      29    Always 2016-09-27 2016-10-25\n#&gt; 108 ms460_NSA    3      30   Missing 2016-09-27 2016-10-25\n#&gt; 109 ms460_NSA   41      31     Never 2016-09-27 2016-10-25\n#&gt; 110 ms460_NSA  168      32 Sometimes 2016-09-27 2016-10-25\n#&gt; 111 ms460_NSA    2      33    Always 2016-09-27 2016-10-25\n#&gt; 112 ms460_NSA   23      34     Never 2016-09-27 2016-10-25\n#&gt; 113 ms460_NSA   22      35 Sometimes 2016-09-27 2016-10-25\n#&gt; 114 ms460_NSA   38      36    Always 2016-09-27 2016-10-25\n#&gt; 115 ms460_NSA    1      37   Missing 2016-09-27 2016-10-25\n#&gt; 116 ms460_NSA   15      38     Never 2016-09-27 2016-10-25\n#&gt; 117 ms460_NSA  113      39 Sometimes 2016-09-27 2016-10-25\n\n\n\n\n3.10.2 plot\n次の２つを使うことができるようになるのでコレを使うこと. 曲線の形は色々と選択することが可能である.\n\ngeom_alluvium\ngeom_stratum\n\n\n\nCode\n\nggplot(\n  data = vaccinations, \n  aes(\n    axis1 = survey, \n    axis2 = response, \n    y = freq\n  )) + \n  geom_alluvium(aes(fill = response), curve_type = \"sine\") + \n  geom_stratum() + \n  geom_text(\n    stat = \"stratum\", \n    aes(label = after_stat(stratum))\n  ) + \n  scale_x_discrete(limits = c(\"Survey\", \"Response\"), expand = c(.15, .05)) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nもちろん色のカスタマイズも可能である.\n\n\nCode\ncolors &lt;- hcl.colors(4, \"Red-Blue\")\n\n\nggplot(\n  data = vaccinations, \n  aes(\n    axis1 = survey, \n    axis2 = response, \n    y = freq\n  ) \n) + \n  geom_alluvium(aes(fill = response)) + \n  geom_stratum() + \n  geom_text(\n    stat = \"stratum\", \n    aes(label = after_stat(stratum))\n  ) + \n  scale_x_discrete(limits = c(\"Survey\", \"Response\"), expand = c(.15, .05)) + \n  scale_fill_manual(values = colors) + \n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nCode\nhcl.colors(4, \"Red-Blue\")\n#&gt; [1] \"#A93154\" \"#BC569E\" \"#B788CD\" \"#AEB6E5\"",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#ggbumps",
    "href": "contents/notes/ggplot2/working.html#ggbumps",
    "title": "ggplot2 Tips",
    "section": "3.11 ggbumps",
    "text": "3.11 ggbumps\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggbump)\nlibrary(tidyverse)\n\n\ngithub\n\n\nCode\n\ndf &lt;- tibble(country = c(\"India\", \"India\", \"India\", \"Sweden\", \"Sweden\", \"Sweden\", \"Germany\", \"Germany\", \"Germany\", \"Finland\", \"Finland\", \"Finland\"),\n             year = c(2011, 2012, 2013, 2011, 2012, 2013, 2011, 2012, 2013, 2011, 2012, 2013),\n             value = c(492, 246, 246, 369, 123, 492, 246, 369, 123, 123, 492, 369))\n\ndf &lt;- df %&gt;% \n  group_by(year) %&gt;% \n  mutate(rank = rank(value, ties.method = \"random\")) %&gt;% \n  ungroup()\n\nknitr::kable(head(df))\n\n\n\n\n\ncountry\nyear\nvalue\nrank\n\n\n\n\nIndia\n2011\n492\n4\n\n\nIndia\n2012\n246\n2\n\n\nIndia\n2013\n246\n2\n\n\nSweden\n2011\n369\n3\n\n\nSweden\n2012\n123\n1\n\n\nSweden\n2013\n492\n4\n\n\n\n\n\n\n\nCode\nggplot(df, aes(year, rank, color = country)) +\n    geom_bump()\n\n\n\n\n\n\n\n\n\n\n\nCode\npacman::p_load(tidyverse, cowplot, wesanderson)\nggplot(df, aes(year, rank, color = country)) +\n  geom_point(size = 7) +\n  geom_text(data = df %&gt;% filter(year == min(year)),\n            aes(x = year - .1, label = country), size = 5, hjust = 1) +\n  geom_text(data = df %&gt;% filter(year == max(year)),\n            aes(x = year + .1, label = country), size = 5, hjust = 0) +\n  geom_bump(size = 2, smooth = 8) +\n  scale_x_continuous(limits = c(2010.6, 2013.4),\n                     breaks = seq(2011, 2013, 1)) +\n  theme_minimal_grid(font_size = 14, line_size = 0) +\n  theme(legend.position = \"none\",\n        panel.grid.major = element_blank()) +\n  labs(y = \"RANK\",\n       x = NULL) +\n  scale_y_reverse() +\n  scale_color_manual(values = wes_palette(n = 4, name = \"GrandBudapest1\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Original df\ndf &lt;- tibble(season = c(\"Spring\", \"Pre-season\", \"Summer\", \"Season finale\", \"Autumn\", \"Winter\", \n                        \"Spring\", \"Pre-season\", \"Summer\", \"Season finale\", \"Autumn\", \"Winter\", \n                        \"Spring\", \"Pre-season\", \"Summer\", \"Season finale\", \"Autumn\", \"Winter\",\n                        \"Spring\", \"Pre-season\", \"Summer\", \"Season finale\", \"Autumn\", \"Winter\"),\n             rank = c(1, 3, 4, 2, 1, 4,\n                      2, 4, 1, 3, 2, 3,\n                      4, 1, 2, 4, 4, 1,\n                      3, 2, 3, 1, 3, 2),\n             player = c(rep(\"David\", 6),\n                        rep(\"Anna\", 6),\n                        rep(\"Franz\", 6),\n                        rep(\"Ika\", 6)))\n\n# Create factors and order factor\n# チュートリアルでは文字列などでも描画出来ていたけどここではうまくいかなった\n# x軸を数値に変更すると上手く描画できる\ndf &lt;- df %&gt;% \n  mutate(season = factor(season, levels = unique(season), ordered = TRUE)) \n\n# Add manual axis labels to plot\nggplot(df, aes(season, rank, color = player)) +\n  geom_bump(size = 2, smooth = 20, show.legend = F) +\n  geom_point(size = 5, aes(shape = player)) +\n  theme_minimal_grid(font_size = 10, line_size = 0) +\n  theme(panel.grid.major = element_blank(),\n        axis.ticks = element_blank()) +\n  scale_color_manual(values = wes_palette(n = 4, name = \"IsleofDogs1\"))\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#ggridges",
    "href": "contents/notes/ggplot2/working.html#ggridges",
    "title": "ggplot2 Tips",
    "section": "3.12 ggridges",
    "text": "3.12 ggridges\n公式ページをみるのが１番．どのようなグラフが描けるのかは，サンプルを見て欲しい．グラフの幅が拡がること間違いなし．\n\n\nCode\ndata &lt;- data.frame(x = 1:5, y = rep(1, 5), height = c(0, 1, 3, 4, 2))\nggplot(data, aes(x, y, height = height)) + geom_ridgeline()\n\n\n\n\n\n\n\n\n\n\n\nCode\n\ndata &lt;- data.frame(x = 1:5, y = rep(1, 5), height = c(0, 1, -1, 3, 2))\nplot_base &lt;- ggplot(data, aes(x, y, height = height))\n\nplot_base + geom_ridgeline() | plot_base + geom_ridgeline(min_height = -2)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(iris, aes(x=Sepal.Length, y=Species, fill = factor(stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\", calc_ecdf = TRUE,\n    quantiles = 4, quantile_lines = TRUE\n  ) +\n  scale_fill_viridis_d(name = \"Quartiles\")\n#&gt; Warning: `stat(quantile)` was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `after_stat(quantile)` instead.\n#&gt; Picking joint bandwidth of 0.181\n#&gt; Warning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use the `linewidth` aesthetic instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(iris, aes(x = Sepal.Length, y = Species, fill = factor(stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE,\n    quantiles = c(0.025, 0.975)\n  ) +\n  scale_fill_manual(\n    name = \"Probability\", values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  )\n#&gt; Picking joint bandwidth of 0.181\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(iris, aes(x = Sepal.Length, y = Species)) +\n  geom_density_ridges(\n    jittered_points = TRUE, quantile_lines = TRUE, scale = 0.9, alpha = 0.7,\n    vline_size = 1, vline_color = \"red\",\n    point_size = 0.4, point_alpha = 1,\n    position = position_raincloud(adjust_vlines = TRUE)\n  )\n#&gt; Picking joint bandwidth of 0.181\n\n\n\n\n\n\n\n\n\nスケールは縦スケールである．重なりを避けたい場合には小さくすればいいし，重なりがあっても強調したい場合には大きい値を設定すればよい.\n\n\nCode\nggplot(lincoln_weather, aes(x = `Mean Temperature [F]`, y = Month, fill = stat(x))) +\n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\", option = \"C\") +\n  labs(title = 'Temperatures in Lincoln NE in 2016')\n#&gt; Picking joint bandwidth of 3.37",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/working.html#ggblend",
    "href": "contents/notes/ggplot2/working.html#ggblend",
    "title": "ggplot2 Tips",
    "section": "3.13 ggblend",
    "text": "3.13 ggblend\nlink\n透過色でグラフを記述すると，レンダリングの順番でグラフの見え方が変わる. pngにしているとブレンドが機能しないとのことである. RStudioのレンダリングをcario pngにするとRStudio上でレンダリングすることが可能であった.\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggblend)\ntheme_set(ggdist::theme_ggdist() + theme(\n  plot.title = element_text(size = rel(1), lineheight = 1.1, face = \"bold\"),\n  plot.subtitle = element_text(face = \"italic\"),\n  panel.border = element_rect(color = \"gray75\", fill = NA)\n))\n\nset.seed(1234)\ndf_a = data.frame(x = rnorm(500, 0), y = rnorm(500, 1), set = \"a\")\ndf_b = data.frame(x = rnorm(500, 1), y = rnorm(500, 2), set = \"b\")\n\ndf_ab = rbind(df_a, df_b) |&gt;\n  transform(order = \"draw a then b\")\n\ndf_ba = rbind(df_b, df_a) |&gt;\n  transform(order = \"draw b then a\")\n\ndf = rbind(df_ab, df_ba)\n\n\n\n\nCode\ndf |&gt;\n  ggplot(aes(x, y, color = set)) +\n  geom_point(size = 3, alpha = 0.5) +\n  scale_color_brewer(palette = \"Set1\") +\n  facet_grid(~ order) +\n  labs(title = \"geom_point() without blending\", subtitle = \"Draw order matters.\")\n\n\n\n\n\n\n\n\n\n\n\nCode\noptions(ggblend.check_blend = FALSE)\ndf |&gt;\n  ggplot(aes(x, y, color = set)) +\n  geom_point(size = 3, alpha = 0.5) |&gt; blend(\"multiply\") +\n  scale_color_brewer(palette = \"Set1\") +\n  facet_grid(~ order) +\n  labs(\n    title = \"geom_point(alpha = 0.5) |&gt; blend('multiply')\",\n    subtitle = \"Draw order does not matter, but color is too dark.\"\n  )\n#&gt; Warning in drawDetails.GridGroup(x, recording = FALSE): Group definition failed\n\n#&gt; Warning in drawDetails.GridGroup(x, recording = FALSE): Group definition failed",
    "crumbs": [
      "Notes",
      "ggplot2"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/119_欠測データ処理の基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/119_欠測データ処理の基礎.html",
    "title": "欠損データ処理",
    "section": "",
    "text": "MCAR(完全にランダム)\nMAR(条件付けたときにランダム)\nNMAR(ランダムとは言えない情況)\n\nMCARは欠測値を削除して良い． NMARのときには何もできない． MARのときは条件を探す必要がある.\nMARは次式で表される．つまり変数\\(X\\)の欠損を表す\\(K\\)は変数\\(Y\\)で条件づけたときに， 変数\\(X\\)の値によらない． たとえば性別で条件づけることで体重に関する回答の欠損の有無は 体重によらないと考えられる．\n\\[\nX \\perp K_i|Y\n\\]\nまあ色々あるけど多重代入法から傾向スコアマッチングによる効果推定がデフォルトでよい.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "欠損データ処理"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/119_欠測データ処理の基礎.html#欠測のメカニズム",
    "href": "contents/books/05_統計的因果推論の理論と実際/119_欠測データ処理の基礎.html#欠測のメカニズム",
    "title": "欠損データ処理",
    "section": "",
    "text": "MCAR(完全にランダム)\nMAR(条件付けたときにランダム)\nNMAR(ランダムとは言えない情況)\n\nMCARは欠測値を削除して良い． NMARのときには何もできない． MARのときは条件を探す必要がある.\nMARは次式で表される．つまり変数\\(X\\)の欠損を表す\\(K\\)は変数\\(Y\\)で条件づけたときに， 変数\\(X\\)の値によらない． たとえば性別で条件づけることで体重に関する回答の欠損の有無は 体重によらないと考えられる．\n\\[\nX \\perp K_i|Y\n\\]\nまあ色々あるけど多重代入法から傾向スコアマッチングによる効果推定がデフォルトでよい.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "欠損データ処理"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html",
    "title": "操作変数の非遵守への対処",
    "section": "",
    "text": "操作変数法による応用として，実験兼研究における無作為割り付けが守られない場合の対処法を考える．\n\n\n小学生の学力にテレビの視聴が与える影響を計りたいとする． 理論的には，無作為にサンプリングした小学生に，無作為にテレビの視聴時間を割り当てることで， 計測することができる．\nしかし，このような実験は必ずしも対象者が条件を守るとは限らない． 実験者が監視することは出来ないので，たとえば「視聴時間０」と割り付けられた対象者が， 本当にテレビを見ていないのかを管理することが出来ない． このような状況を非遵守という．\n\n\n処置の割り付け変数\\(T_i\\)とは別に，新たに\\(D_i\\)を個体\\(i\\)が 実際に処置を受けたかどうか表す変数として定義する． たとえば\\(D_i(T_i=0)=1\\)とは処理が割り付けられていないが実際には処置を受けたということを 表す．\\(T_i\\)と\\(D_i\\)の組合せで各個人がどのような振る舞いをするのかが分類できる.\n\n\n\n\n\n\nCode\ndata14 &lt;- read_csv(\"./causality/data14.csv\", show_col_types = FALSE)\ndata14 |&gt; head()\n#&gt; # A tibble: 6 × 7\n#&gt;      y3    t1    d1   y0t   y1t   d0t   d1t\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    70     0     0    70    79     0     1\n#&gt; 2    74     0     0    74    82     0     1\n#&gt; 3    69     0     0    69    78     0     1\n#&gt; 4    81     0     0    81    89     0     1\n#&gt; 5    75     0     0    75    75     0     0\n#&gt; 6    69     0     0    69    69     0     0\n\n\n\n\n\n単調性は次式で表される．これは一言でいうと，処置が割り付けられたときにおこなわず， 割り付けられていないときにおこなう，という天邪鬼が存在していない，という仮定である.\n\\[\nD_i(T_i=1)\\ge D_i(T_i=0)\n\\]\n天邪鬼は実際には少ないため，上記の仮定は妥当なものと考えられる． これ以外の常に処置を受ける人，常に処置を受けない人は，処置効果を求められないので， 効果を計測することが出来ない． このため結局は，遵守者のみを対象にした推定をおこなうことになる.\n\\[\nCACE = E[Y_i(1)-Y_i(0)|C]\n\\]\n\n\nCode\n# d1tは処置が割り付けられたときに実際に取る行動\n# d0tは処理が割り付けられなかったときに実際に取る行動\n# d1t==1 & d0t ==0 は遵守者を抽出していることになる\nwith(subset(data14, d1t==1 & d0t==0), {\n    print(\n        mean(y1t) - mean(y0t)\n    )\n})\n#&gt; [1] 7.875\n\n\nとは言え遵守者を抽出出来ないので意味はない気もする．\n\n\n\n小学生のTVの話に戻ると，実際に制御することは出来ないが， TVを見ること，見ないことの奨励をすることは出来，奨励するのかどうかは 無作為化することが出来る．\n基本的に過小推定することしか出来ない． しかし，過小であることがわかっているので，それはそれで意味がある値となる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数の非遵守への対処"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#非遵守",
    "href": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#非遵守",
    "title": "操作変数の非遵守への対処",
    "section": "",
    "text": "小学生の学力にテレビの視聴が与える影響を計りたいとする． 理論的には，無作為にサンプリングした小学生に，無作為にテレビの視聴時間を割り当てることで， 計測することができる．\nしかし，このような実験は必ずしも対象者が条件を守るとは限らない． 実験者が監視することは出来ないので，たとえば「視聴時間０」と割り付けられた対象者が， 本当にテレビを見ていないのかを管理することが出来ない． このような状況を非遵守という．\n\n\n処置の割り付け変数\\(T_i\\)とは別に，新たに\\(D_i\\)を個体\\(i\\)が 実際に処置を受けたかどうか表す変数として定義する． たとえば\\(D_i(T_i=0)=1\\)とは処理が割り付けられていないが実際には処置を受けたということを 表す．\\(T_i\\)と\\(D_i\\)の組合せで各個人がどのような振る舞いをするのかが分類できる.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数の非遵守への対処"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#使用するデータ",
    "href": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#使用するデータ",
    "title": "操作変数の非遵守への対処",
    "section": "",
    "text": "Code\ndata14 &lt;- read_csv(\"./causality/data14.csv\", show_col_types = FALSE)\ndata14 |&gt; head()\n#&gt; # A tibble: 6 × 7\n#&gt;      y3    t1    d1   y0t   y1t   d0t   d1t\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    70     0     0    70    79     0     1\n#&gt; 2    74     0     0    74    82     0     1\n#&gt; 3    69     0     0    69    78     0     1\n#&gt; 4    81     0     0    81    89     0     1\n#&gt; 5    75     0     0    75    75     0     0\n#&gt; 6    69     0     0    69    69     0     0",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数の非遵守への対処"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#単調性の仮定と推定対象者",
    "href": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#単調性の仮定と推定対象者",
    "title": "操作変数の非遵守への対処",
    "section": "",
    "text": "単調性は次式で表される．これは一言でいうと，処置が割り付けられたときにおこなわず， 割り付けられていないときにおこなう，という天邪鬼が存在していない，という仮定である.\n\\[\nD_i(T_i=1)\\ge D_i(T_i=0)\n\\]\n天邪鬼は実際には少ないため，上記の仮定は妥当なものと考えられる． これ以外の常に処置を受ける人，常に処置を受けない人は，処置効果を求められないので， 効果を計測することが出来ない． このため結局は，遵守者のみを対象にした推定をおこなうことになる.\n\\[\nCACE = E[Y_i(1)-Y_i(0)|C]\n\\]\n\n\nCode\n# d1tは処置が割り付けられたときに実際に取る行動\n# d0tは処理が割り付けられなかったときに実際に取る行動\n# d1t==1 & d0t ==0 は遵守者を抽出していることになる\nwith(subset(data14, d1t==1 & d0t==0), {\n    print(\n        mean(y1t) - mean(y0t)\n    )\n})\n#&gt; [1] 7.875\n\n\nとは言え遵守者を抽出出来ないので意味はない気もする．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数の非遵守への対処"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#無作為化奨励デザインと４つの推定量",
    "href": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#無作為化奨励デザインと４つの推定量",
    "title": "操作変数の非遵守への対処",
    "section": "",
    "text": "小学生のTVの話に戻ると，実際に制御することは出来ないが， TVを見ること，見ないことの奨励をすることは出来，奨励するのかどうかは 無作為化することが出来る．\n基本的に過小推定することしか出来ない． しかし，過小であることがわかっているので，それはそれで意味がある値となる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数の非遵守への対処"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html",
    "title": "傾向スコアによる層化解析法および重み付き法",
    "section": "",
    "text": "ここまでに傾向スコアマッチングによりATTを推定できること， 仮定が満たされているならば共分散分析によりATEが推定できることも確認した． しかし，ATEを推定するための共分散分析の仮定を満たすことは現実的には難しい.\nそこで登場するのが傾向スコアを用いた層化解析手法である.\n\n\nまず層化抽出とは，母集団をいくつかのグループに分けて， それぞれのグループで無作為抽出する方法である． このときのグループは調べる共変量が似通った値になるように調整する． たとえば「県民性」を調べたいときには 日本全国からサンプリングするよりも４７都道府県別でサンプリングする方がよいのは 直感的に理解できる． 層別は「意味がある」特性が行うことが重要である．\n層別サンプリングは交絡因子を使って層にわけることで， 交絡因子の影響を受けるするという考えであり， フィッシャーの三原則の「局所管理」にあたる作法である．\n層ごとの推定値の情報をまとめたものを全体の推定値とする． \\(K\\)層に分けた場合のATEは次式で求められる.\n\\[\n\\begin{align}\n\\hat{\\tau}_{\\text{ATE}}&=\\sum_{k=1}^{K}\\frac{n_k}{N}[\\overline{Y_k}(1)-\\overline{Y_k}(0)]\\\\\n\\text{var}(\\hat{\\tau}_{\\text{ATE}})&=\\sum_{k=1}^{K}\\left(\\frac{n_k}{N}\\right)^2[\\overline{Y_k}(1)-\\overline{Y_k}(0)]\n\\end{align}\n\\]\n交絡変数をなくそうとして多変量で層別にすると， パターンは膨大になるので，傾向スコアで層別するのがやはり基本となる．\n\n\n\n\nCode\ndata11 &lt;- read_csv(\"./causality/data11.csv\", show_col_types = FALSE)\nwith(data11, {\n    print(\n        mean(y1t) - mean(y0t)\n    )\n})\n#&gt; [1] 3.755947\n\n\n\n\nCode\ndata11 |&gt; head()\n#&gt; # A tibble: 6 × 10\n#&gt;      y0t   y1t     y3    t1     x1     x2      x3      x4    x5     x6\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 12.7   23.2  12.7       0  2.75   3.08   1.74   1.04    0.708 -0.411\n#&gt; 2 61.6   64.7  64.7       1  0.640 -1.20  -0.0539 1.67    2.11   2.47 \n#&gt; 3 -0.275 -1.97 -0.275     0 -0.473  0.747  0.961  0.956   0.223 -1.06 \n#&gt; 4 25.8   38.4  25.8       0  1.27  -0.828 -0.962  0.00882 0.668  1.81 \n#&gt; 5 24.8   21.0  24.8       0  0.640 -0.492  0.630  0.114   0.936  0.886\n#&gt; 6 46.8   48.3  48.3       1  0.846 -0.855  0.159  1.05    1.29   2.05\n\n\n上記がATEのバイアスのない推定値である． この値が算出できるのかを傾向モデルを使った層別サンプリングで試す．\n\n\nCode\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nsub    &lt;- 5\nm.out2 &lt;- matchit(\n    t1 ~ x1 + x2 + x3 + x4 + x5 + x6, \n    data      = data11, \n    method    = \"subclass\", \n    subclass  = sub, \n    estimand  = \"ATE\", \n    min.n     = 2\n)\nm.data2 &lt;- match.data(m.out2)\nm.data2\n#&gt; # A tibble: 1,000 × 13\n#&gt;        y0t   y1t      y3    t1     x1     x2      x3       x4       x5     x6\n#&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1  12.7   23.2   12.7       0  2.75   3.08   1.74    1.04     0.708   -0.411\n#&gt;  2  61.6   64.7   64.7       1  0.640 -1.20  -0.0539  1.67     2.11     2.47 \n#&gt;  3  -0.275 -1.97  -0.275     0 -0.473  0.747  0.961   0.956    0.223   -1.06 \n#&gt;  4  25.8   38.4   25.8       0  1.27  -0.828 -0.962   0.00882  0.668    1.81 \n#&gt;  5  24.8   21.0   24.8       0  0.640 -0.492  0.630   0.114    0.936    0.886\n#&gt;  6  46.8   48.3   48.3       1  0.846 -0.855  0.159   1.05     1.29     2.05 \n#&gt;  7  31.0   34.6   34.6       1  0.680 -0.106  0.304   0.362    1.66     0.992\n#&gt;  8  53.3   54.3   54.3       1  1.70   2.02   2.23    2.88     1.58     1.22 \n#&gt;  9  23.6   20.6   23.6       0 -0.752 -0.567  0.440   1.56     0.686    0.263\n#&gt; 10 -17.8   11.4  -17.8       0  1.05   0.397 -2.04   -0.851   -0.00748 -0.232\n#&gt; # ℹ 990 more rows\n#&gt; # ℹ 3 more variables: distance &lt;dbl&gt;, weights &lt;dbl&gt;, subclass &lt;fct&gt;\n\n\n上記で層を割り当てることが出来たので， 層ごとに解析して，その結果を集約すれば終わりである.\n\n\nCode\nlibrary(lmtest); library(sandwich);\n#&gt; Warning: package 'lmtest' was built under R version 4.3.2\n#&gt; Loading required package: zoo\n#&gt; \n#&gt; Attaching package: 'zoo'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\nfits &lt;- \n    m.data2 |&gt; \n    group_by(subclass) |&gt; \n    summarise(\n        fit = list(lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = cur_data()))\n    )\n#&gt; Warning: There was 1 warning in `summarise()`.\n#&gt; ℹ In argument: `fit = list(lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data =\n#&gt;   cur_data()))`.\n#&gt; ℹ In group 1: `subclass = 1`.\n#&gt; Caused by warning:\n#&gt; ! `cur_data()` was deprecated in dplyr 1.1.0.\n#&gt; ℹ Please use `pick()` instead.\nclass_size &lt;- \n    m.data2 |&gt; \n    count(subclass, name = \"N\")\nfit_stats &lt;- \n    fits |&gt; \n    mutate(coef = map(fit, ~ tidy(.x))) |&gt; \n    select(!fit) |&gt; \n    unnest(coef) |&gt; \n    filter(term == \"t1\") \nate &lt;- \n    fit_stats|&gt; \n    left_join(class_size, by = \"subclass\") |&gt; \n    mutate( \n        psvar = std.error ^ 2\n    ) |&gt; \n    select(\n        subclass, \n        psp = estimate, \n        psvar, \n        nps = N, \n        p.value\n    )\n    \nate\n#&gt; # A tibble: 5 × 5\n#&gt;   subclass   psp psvar   nps  p.value\n#&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1 1        8.21  0.401   200 5.16e-28\n#&gt; 2 2        4.48  0.567   200 1.26e- 8\n#&gt; 3 3        3.82  0.488   200 1.44e- 7\n#&gt; 4 4        2.12  0.493   200 2.86e- 3\n#&gt; 5 5        0.114 0.411   200 8.58e- 1\n\n\n上記の結果を集約する.\n\n\nCode\nestimated &lt;- \n    ate |&gt; \n    summarise(\n        tauhat = sum(nps/sum(nps) * psp), \n        vartau = sum((nps/sum(nps)) ^ 2 * psvar), \n        settau = sqrt(vartau)\n    )\n\nestimated\n#&gt; # A tibble: 1 × 3\n#&gt;   tauhat vartau settau\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   3.75 0.0944  0.307\n\n\n\n\n\n\nStd.Mean Diffの値が0に近く, Var.Ratioの値が1に近ければ， バランシングが取れていることを意味する. ここでは出力結果を省略する.\n\n\nCode\nsummary(m.out2)\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, \n#&gt;     method = \"subclass\", estimand = \"ATE\", subclass = sub, min.n = 2)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.3584          0.6007     0.9897    0.1755\n#&gt; x1              0.9655        0.9919         -0.0280     0.7966    0.0210\n#&gt; x2              0.9596        0.9810         -0.0211     0.8841    0.0200\n#&gt; x3              1.1451        0.9162          0.2213     1.0303    0.0616\n#&gt; x4              1.2246        0.8986          0.3371     0.9352    0.1017\n#&gt; x5              1.2874        0.7913          0.4971     1.0065    0.1461\n#&gt; x6              1.2896        0.8382          0.4753     0.9166    0.1376\n#&gt;          eCDF Max\n#&gt; distance   0.3232\n#&gt; x1         0.0540\n#&gt; x2         0.0524\n#&gt; x3         0.1002\n#&gt; x4         0.1797\n#&gt; x5         0.2387\n#&gt; x6         0.2212\n#&gt; \n#&gt; Summary of Balance Across Subclasses\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.3902        0.3882          0.0148     1.0303    0.0095\n#&gt; x1              0.9917        0.9727          0.0202     0.7583    0.0190\n#&gt; x2              0.9844        0.9713          0.0129     0.9283    0.0213\n#&gt; x3              1.0157        1.0030          0.0123     1.0600    0.0107\n#&gt; x4              1.0413        1.0228          0.0191     0.9487    0.0205\n#&gt; x5              0.9994        0.9760          0.0235     1.0893    0.0100\n#&gt; x6              1.0112        1.0121         -0.0009     0.9771    0.0083\n#&gt;          eCDF Max\n#&gt; distance   0.0339\n#&gt; x1         0.0536\n#&gt; x2         0.0474\n#&gt; x3         0.0408\n#&gt; x4         0.0553\n#&gt; x5         0.0324\n#&gt; x6         0.0328\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All            611.       389\n#&gt; Matched (ESS)  570.38     325\n#&gt; Matched        611.       389\n#&gt; Unmatched        0.         0\n#&gt; Discarded        0.         0\n\n\n\n\nCode\ndiffa &lt;- abs(summary(m.out2)$sum.all[,3])\ndiffb &lt;- abs(summary(m.out2)$sum.acros[,3])\ndiff1 &lt;- rev(diffa)\ndiff2 &lt;- rev(diffb)\n\nmaxx    &lt;- max(diff1, diff2)\nlabels0 &lt;- rownames(summary(m.out2)$sum.all)\nlabels1 &lt;- rev(labels0)\n\ndotchart(diff1, xlim = c(0, maxx), labels = c(labels1))\nabline(v = .0, col = 8)\nabline(v = .1, col = 8)\nabline(v = .05, lty = 2, col = 8)\n\npar(new = TRUE)\ndotchart(diff2, xlim = c(0, maxx), labels = c(labels1), \n         pch = 16, xlab = \"Absolute Standardized Mean Difference\")\n\n\n\n\n\n\n\n\n\n\n\n\n調査標本における重み付けと似ている． 調査標本ではある個体がサンプリングされる確率が等確率にならない， つまり，ランダムサンプリングが実現できない現象である．\n例えば男子6000人，女子4000人の学校で1000人を対象にしたアンケートを行うとき， 女子ロッカーに関することのため女子の人数を多くしたアンケートをしたいと考える． このとき，男子120人，女子880人をサンプリングすると， 男子は120/6000なので0.02, 女子は880/4000なので0.22の確率でサンプリングされる．\nそこでサンプリングされた個人の意見はサンプリング確率の逆数， この場合だと男子は50人, 女子は4.8人分の価値があると解釈する． これが，重み付け法である．\n\n\n標本調査では抽出確率の逆数を使っていた． 傾向スコアは共変量\\(X\\)があたてられたとき，処置に割り付けられる確率という意味であることから， その逆数を重みとして使うことは自然である． これを逆重み付け法(IPW)とおいう.\n無作為化実験から得られるであろう解析結果に一致するように， 処置群の個体に対して「傾向スコアの逆数」を乗じて， 統制群の個体に対して「１－傾向スコア」の逆数を乗じることで，データの重み付けを算出する． つまり出にくい方の値が出たときにはその価値を重視するというものとなる．\n\\[\nw_i = \\frac{T_i}{\\hat{e_i}}+\\frac{1-T_i}{1-\\hat{e_i}}\n\\]\n上記の計算は傾向スコアを算出したのちに，そのスコアを重みとした 回帰分析を行うことで算出が可能である.\n\n\nCode\nmodel1  &lt;- glm(t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, family = binomial(link = \"logit\"))\nps1     &lt;- model1$fitted.values\nweights &lt;- data11$t1 / ps1 + (1 - data11$t1) / (1 - ps1)\n\n\n\n\nCode\nmodel2 &lt;- lm(y3 ~ t1, data = data11, weights = weights)\nmodel3 &lt;- lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11, weights = weights)\n\n\n\n\nCode\ntidy(model2)\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    31.9       1.06     29.9  4.09e-141\n#&gt; 2 t1              3.20      1.51      2.13 3.38e-  2\n\n\n\n\nCode\ntidy(model3)\n#&gt; # A tibble: 8 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.36      0.341      3.98 7.44e-  5\n#&gt; 2 t1             3.82      0.311     12.3  2.66e- 32\n#&gt; 3 x1             1.54      0.191      8.05 2.29e- 15\n#&gt; 4 x2             0.454     0.198      2.30 2.19e-  2\n#&gt; 5 x3            -0.765     0.198     -3.87 1.15e-  4\n#&gt; 6 x4             7.64      0.208     36.7  2.87e-187\n#&gt; 7 x5             9.68      0.195     49.6  9.98e-271\n#&gt; 8 x6            11.3       0.187     60.2  0",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアによる層化解析法および重み付き法"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによる層化解析法",
    "href": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによる層化解析法",
    "title": "傾向スコアによる層化解析法および重み付き法",
    "section": "",
    "text": "まず層化抽出とは，母集団をいくつかのグループに分けて， それぞれのグループで無作為抽出する方法である． このときのグループは調べる共変量が似通った値になるように調整する． たとえば「県民性」を調べたいときには 日本全国からサンプリングするよりも４７都道府県別でサンプリングする方がよいのは 直感的に理解できる． 層別は「意味がある」特性が行うことが重要である．\n層別サンプリングは交絡因子を使って層にわけることで， 交絡因子の影響を受けるするという考えであり， フィッシャーの三原則の「局所管理」にあたる作法である．\n層ごとの推定値の情報をまとめたものを全体の推定値とする． \\(K\\)層に分けた場合のATEは次式で求められる.\n\\[\n\\begin{align}\n\\hat{\\tau}_{\\text{ATE}}&=\\sum_{k=1}^{K}\\frac{n_k}{N}[\\overline{Y_k}(1)-\\overline{Y_k}(0)]\\\\\n\\text{var}(\\hat{\\tau}_{\\text{ATE}})&=\\sum_{k=1}^{K}\\left(\\frac{n_k}{N}\\right)^2[\\overline{Y_k}(1)-\\overline{Y_k}(0)]\n\\end{align}\n\\]\n交絡変数をなくそうとして多変量で層別にすると， パターンは膨大になるので，傾向スコアで層別するのがやはり基本となる．\n\n\n\n\nCode\ndata11 &lt;- read_csv(\"./causality/data11.csv\", show_col_types = FALSE)\nwith(data11, {\n    print(\n        mean(y1t) - mean(y0t)\n    )\n})\n#&gt; [1] 3.755947\n\n\n\n\nCode\ndata11 |&gt; head()\n#&gt; # A tibble: 6 × 10\n#&gt;      y0t   y1t     y3    t1     x1     x2      x3      x4    x5     x6\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 12.7   23.2  12.7       0  2.75   3.08   1.74   1.04    0.708 -0.411\n#&gt; 2 61.6   64.7  64.7       1  0.640 -1.20  -0.0539 1.67    2.11   2.47 \n#&gt; 3 -0.275 -1.97 -0.275     0 -0.473  0.747  0.961  0.956   0.223 -1.06 \n#&gt; 4 25.8   38.4  25.8       0  1.27  -0.828 -0.962  0.00882 0.668  1.81 \n#&gt; 5 24.8   21.0  24.8       0  0.640 -0.492  0.630  0.114   0.936  0.886\n#&gt; 6 46.8   48.3  48.3       1  0.846 -0.855  0.159  1.05    1.29   2.05\n\n\n上記がATEのバイアスのない推定値である． この値が算出できるのかを傾向モデルを使った層別サンプリングで試す．\n\n\nCode\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nsub    &lt;- 5\nm.out2 &lt;- matchit(\n    t1 ~ x1 + x2 + x3 + x4 + x5 + x6, \n    data      = data11, \n    method    = \"subclass\", \n    subclass  = sub, \n    estimand  = \"ATE\", \n    min.n     = 2\n)\nm.data2 &lt;- match.data(m.out2)\nm.data2\n#&gt; # A tibble: 1,000 × 13\n#&gt;        y0t   y1t      y3    t1     x1     x2      x3       x4       x5     x6\n#&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1  12.7   23.2   12.7       0  2.75   3.08   1.74    1.04     0.708   -0.411\n#&gt;  2  61.6   64.7   64.7       1  0.640 -1.20  -0.0539  1.67     2.11     2.47 \n#&gt;  3  -0.275 -1.97  -0.275     0 -0.473  0.747  0.961   0.956    0.223   -1.06 \n#&gt;  4  25.8   38.4   25.8       0  1.27  -0.828 -0.962   0.00882  0.668    1.81 \n#&gt;  5  24.8   21.0   24.8       0  0.640 -0.492  0.630   0.114    0.936    0.886\n#&gt;  6  46.8   48.3   48.3       1  0.846 -0.855  0.159   1.05     1.29     2.05 \n#&gt;  7  31.0   34.6   34.6       1  0.680 -0.106  0.304   0.362    1.66     0.992\n#&gt;  8  53.3   54.3   54.3       1  1.70   2.02   2.23    2.88     1.58     1.22 \n#&gt;  9  23.6   20.6   23.6       0 -0.752 -0.567  0.440   1.56     0.686    0.263\n#&gt; 10 -17.8   11.4  -17.8       0  1.05   0.397 -2.04   -0.851   -0.00748 -0.232\n#&gt; # ℹ 990 more rows\n#&gt; # ℹ 3 more variables: distance &lt;dbl&gt;, weights &lt;dbl&gt;, subclass &lt;fct&gt;\n\n\n上記で層を割り当てることが出来たので， 層ごとに解析して，その結果を集約すれば終わりである.\n\n\nCode\nlibrary(lmtest); library(sandwich);\n#&gt; Warning: package 'lmtest' was built under R version 4.3.2\n#&gt; Loading required package: zoo\n#&gt; \n#&gt; Attaching package: 'zoo'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\nfits &lt;- \n    m.data2 |&gt; \n    group_by(subclass) |&gt; \n    summarise(\n        fit = list(lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = cur_data()))\n    )\n#&gt; Warning: There was 1 warning in `summarise()`.\n#&gt; ℹ In argument: `fit = list(lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data =\n#&gt;   cur_data()))`.\n#&gt; ℹ In group 1: `subclass = 1`.\n#&gt; Caused by warning:\n#&gt; ! `cur_data()` was deprecated in dplyr 1.1.0.\n#&gt; ℹ Please use `pick()` instead.\nclass_size &lt;- \n    m.data2 |&gt; \n    count(subclass, name = \"N\")\nfit_stats &lt;- \n    fits |&gt; \n    mutate(coef = map(fit, ~ tidy(.x))) |&gt; \n    select(!fit) |&gt; \n    unnest(coef) |&gt; \n    filter(term == \"t1\") \nate &lt;- \n    fit_stats|&gt; \n    left_join(class_size, by = \"subclass\") |&gt; \n    mutate( \n        psvar = std.error ^ 2\n    ) |&gt; \n    select(\n        subclass, \n        psp = estimate, \n        psvar, \n        nps = N, \n        p.value\n    )\n    \nate\n#&gt; # A tibble: 5 × 5\n#&gt;   subclass   psp psvar   nps  p.value\n#&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1 1        8.21  0.401   200 5.16e-28\n#&gt; 2 2        4.48  0.567   200 1.26e- 8\n#&gt; 3 3        3.82  0.488   200 1.44e- 7\n#&gt; 4 4        2.12  0.493   200 2.86e- 3\n#&gt; 5 5        0.114 0.411   200 8.58e- 1\n\n\n上記の結果を集約する.\n\n\nCode\nestimated &lt;- \n    ate |&gt; \n    summarise(\n        tauhat = sum(nps/sum(nps) * psp), \n        vartau = sum((nps/sum(nps)) ^ 2 * psvar), \n        settau = sqrt(vartau)\n    )\n\nestimated\n#&gt; # A tibble: 1 × 3\n#&gt;   tauhat vartau settau\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   3.75 0.0944  0.307",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアによる層化解析法および重み付き法"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによるバランシングの評価",
    "href": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによるバランシングの評価",
    "title": "傾向スコアによる層化解析法および重み付き法",
    "section": "",
    "text": "Std.Mean Diffの値が0に近く, Var.Ratioの値が1に近ければ， バランシングが取れていることを意味する. ここでは出力結果を省略する.\n\n\nCode\nsummary(m.out2)\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, \n#&gt;     method = \"subclass\", estimand = \"ATE\", subclass = sub, min.n = 2)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.3584          0.6007     0.9897    0.1755\n#&gt; x1              0.9655        0.9919         -0.0280     0.7966    0.0210\n#&gt; x2              0.9596        0.9810         -0.0211     0.8841    0.0200\n#&gt; x3              1.1451        0.9162          0.2213     1.0303    0.0616\n#&gt; x4              1.2246        0.8986          0.3371     0.9352    0.1017\n#&gt; x5              1.2874        0.7913          0.4971     1.0065    0.1461\n#&gt; x6              1.2896        0.8382          0.4753     0.9166    0.1376\n#&gt;          eCDF Max\n#&gt; distance   0.3232\n#&gt; x1         0.0540\n#&gt; x2         0.0524\n#&gt; x3         0.1002\n#&gt; x4         0.1797\n#&gt; x5         0.2387\n#&gt; x6         0.2212\n#&gt; \n#&gt; Summary of Balance Across Subclasses\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.3902        0.3882          0.0148     1.0303    0.0095\n#&gt; x1              0.9917        0.9727          0.0202     0.7583    0.0190\n#&gt; x2              0.9844        0.9713          0.0129     0.9283    0.0213\n#&gt; x3              1.0157        1.0030          0.0123     1.0600    0.0107\n#&gt; x4              1.0413        1.0228          0.0191     0.9487    0.0205\n#&gt; x5              0.9994        0.9760          0.0235     1.0893    0.0100\n#&gt; x6              1.0112        1.0121         -0.0009     0.9771    0.0083\n#&gt;          eCDF Max\n#&gt; distance   0.0339\n#&gt; x1         0.0536\n#&gt; x2         0.0474\n#&gt; x3         0.0408\n#&gt; x4         0.0553\n#&gt; x5         0.0324\n#&gt; x6         0.0328\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All            611.       389\n#&gt; Matched (ESS)  570.38     325\n#&gt; Matched        611.       389\n#&gt; Unmatched        0.         0\n#&gt; Discarded        0.         0\n\n\n\n\nCode\ndiffa &lt;- abs(summary(m.out2)$sum.all[,3])\ndiffb &lt;- abs(summary(m.out2)$sum.acros[,3])\ndiff1 &lt;- rev(diffa)\ndiff2 &lt;- rev(diffb)\n\nmaxx    &lt;- max(diff1, diff2)\nlabels0 &lt;- rownames(summary(m.out2)$sum.all)\nlabels1 &lt;- rev(labels0)\n\ndotchart(diff1, xlim = c(0, maxx), labels = c(labels1))\nabline(v = .0, col = 8)\nabline(v = .1, col = 8)\nabline(v = .05, lty = 2, col = 8)\n\npar(new = TRUE)\ndotchart(diff2, xlim = c(0, maxx), labels = c(labels1), \n         pch = 16, xlab = \"Absolute Standardized Mean Difference\")",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアによる層化解析法および重み付き法"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによる重み付け法",
    "href": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによる重み付け法",
    "title": "傾向スコアによる層化解析法および重み付き法",
    "section": "",
    "text": "調査標本における重み付けと似ている． 調査標本ではある個体がサンプリングされる確率が等確率にならない， つまり，ランダムサンプリングが実現できない現象である．\n例えば男子6000人，女子4000人の学校で1000人を対象にしたアンケートを行うとき， 女子ロッカーに関することのため女子の人数を多くしたアンケートをしたいと考える． このとき，男子120人，女子880人をサンプリングすると， 男子は120/6000なので0.02, 女子は880/4000なので0.22の確率でサンプリングされる．\nそこでサンプリングされた個人の意見はサンプリング確率の逆数， この場合だと男子は50人, 女子は4.8人分の価値があると解釈する． これが，重み付け法である．\n\n\n標本調査では抽出確率の逆数を使っていた． 傾向スコアは共変量\\(X\\)があたてられたとき，処置に割り付けられる確率という意味であることから， その逆数を重みとして使うことは自然である． これを逆重み付け法(IPW)とおいう.\n無作為化実験から得られるであろう解析結果に一致するように， 処置群の個体に対して「傾向スコアの逆数」を乗じて， 統制群の個体に対して「１－傾向スコア」の逆数を乗じることで，データの重み付けを算出する． つまり出にくい方の値が出たときにはその価値を重視するというものとなる．\n\\[\nw_i = \\frac{T_i}{\\hat{e_i}}+\\frac{1-T_i}{1-\\hat{e_i}}\n\\]\n上記の計算は傾向スコアを算出したのちに，そのスコアを重みとした 回帰分析を行うことで算出が可能である.\n\n\nCode\nmodel1  &lt;- glm(t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, family = binomial(link = \"logit\"))\nps1     &lt;- model1$fitted.values\nweights &lt;- data11$t1 / ps1 + (1 - data11$t1) / (1 - ps1)\n\n\n\n\nCode\nmodel2 &lt;- lm(y3 ~ t1, data = data11, weights = weights)\nmodel3 &lt;- lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11, weights = weights)\n\n\n\n\nCode\ntidy(model2)\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    31.9       1.06     29.9  4.09e-141\n#&gt; 2 t1              3.20      1.51      2.13 3.38e-  2\n\n\n\n\nCode\ntidy(model3)\n#&gt; # A tibble: 8 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.36      0.341      3.98 7.44e-  5\n#&gt; 2 t1             3.82      0.311     12.3  2.66e- 32\n#&gt; 3 x1             1.54      0.191      8.05 2.29e- 15\n#&gt; 4 x2             0.454     0.198      2.30 2.19e-  2\n#&gt; 5 x3            -0.765     0.198     -3.87 1.15e-  4\n#&gt; 6 x4             7.64      0.208     36.7  2.87e-187\n#&gt; 7 x5             9.68      0.195     49.6  9.98e-271\n#&gt; 8 x6            11.3       0.187     60.2  0",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアによる層化解析法および重み付き法"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html",
    "title": "傾向スコア",
    "section": "",
    "text": "実験研究がよいのは，処置の有無を無作為に割り付けられることで， 共変量の分布が確率的に同じになることにより， 交絡が発生しないことである．\n観察研究における因果推論が難しいのは，処置群と統制群が傾向的に異なっている可能性のためである．\n傾向スコアによる分析は「準実験」と呼ばれる． 処置の無作為な割り付けが出来ない場合でも， 割り付けや比較対象の集団について研究者が何らかの統制を行うことで 無作為化をマネすデザインのことをいう．\n傾向スコアマッチングとは傾向スコアが同じペアを同一だと見なし， 処置群の処置効果を測るための手法とするものである. 共変量で調整されていれば，結果の違いは，共変量以外の要因からなり， それは主として処置効果からくるものと考えられる． この考え方の妥当性は，これまでと同じように条件付き独立性が満たされるという考え方になる.\n\\[\n\\{Y(1), Y(0)\\} \\perp T |X\n\\] 上記のマッチングがうまく出来れば, 本来観測不可能である「処置群の処置効果(ATT)」を推定することが可能となる．傾向スコアによりATEを推定するには，スコアごとの層別解析をなんらかの方法で平均すればよい.\n\\[\n\\tau_{ATT}=E[Y_i(1)-Y_i(0)|T_i=1]=E[Y_i(1)|T_i=1]-E[T_i(0)|T_i=1]\n\\] なお，これまで調べていたのは「処置の平均効果(ATE)」である．これは，処置と結果変数を独立となるようにすることで，推定が行える.\n\\[\n\\tau_{ATE}=E[Y_i(1)-Y_i(0)|T_i]=E[Y_i(1)|T_i]-E[T_i(0)|T_i]=E[Y_i(1)]-E[T_i(0)]\n\\]\nとはいえ，まずはATTとATEは異なるということをまずは知っておく必要がある．\n\n\n共変量が単変量の場合には共分散分析により解析することが出来た． しかし，通常共変量は多変量である．これに対して傾向スコアが開発された． 傾向スコアとはバランシングスコアの１つであり， 観測される共変量\\(X\\)の関数\\(b(X)\\)が与えられたときの\\(X\\)の条件つき分布が， 処置群と統制群において同じとなる関数である.\nとにかくバランシングスコアを見るようにして，バランシングスコアが悪いときに，マッチングを使ったATT推定を行ってはならない. マッチしたからといってどの程度マッチに意味があるかは，別であるよね，という話だと理解した．\n\n\nCode\ndata10a &lt;- read_csv(\"./causality/data10a.csv\", show_col_types = FALSE)\n\n# 平均処置効果\nwith(data10a, {\n    print(mean(y1t) - mean(y0t))\n})\n#&gt; [1] 9.95\n\n# 処置群の平均処置効果\nwith(data10a, {\n    print(mean(y1t[t1==1])-mean(y0t[t1==1]))   \n})\n#&gt; [1] 9.090909\n\n# ナイーブな平均処置効果\nwith(data10a, {\n    print(mean(y1t[t1==1])-mean(y0t[t1==0]))   \n})\n#&gt; [1] 17.39394\n\n\nナイーブな平均処置結果が使えないことがよくわかる.\nこのような結果が起こるのは共変量の分布が異なることによる. 処置群と統制群で平均値や分散が同じになることを指して「バランスが取れている」ということになる．\n\n\nCode\ndata10a |&gt; \n    group_by(t1) |&gt;\n    summarise(quantile = list(stack(quantile(x1)))) |&gt; \n    unnest(quantile) |&gt; \n    pivot_wider(id_cols = t1, names_from = \"ind\", values_from = \"values\")\n#&gt; # A tibble: 2 × 6\n#&gt;      t1  `0%` `25%` `50%` `75%` `100%`\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1     0    66  70      74    78     88\n#&gt; 2     1    73  77.5    83    89     92\n\n\n\n\n\n共変量が多変量である場合を考える. 多変量である場合には上記のように 共変量の分布が同一であるのかを確認することは容易ではない．\n\n\nCode\ndata10b &lt;- read_csv(\"./causality/data10b.csv\", show_col_types = FALSE)\nprint(data10b |&gt; head())\n#&gt; # A tibble: 6 × 8\n#&gt;     y0t   y1t    t1     y    x1    x2    x3   ps1\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    63    74     0    63    66    76    75  0.12\n#&gt; 2    55    70     0    55    70    75    55  0.05\n#&gt; 3    59    69     0    59    70    60    73  0.19\n#&gt; 4    71    77     1    77    73    76    82  0.43\n#&gt; 5    73    78     0    73    73    79    78  0.35\n#&gt; 6    66    77     0    66    74    72    79  0.41\n\n\nそこで傾向スコアを考える．傾向スコアとは，共変量\\(X\\)が与えられたとき， 処置に割り付けらあれる確率と定義される．\n\\[\ne(X) = \\text{Pr}(T_i=1|X)\n\\]\n傾向スコアは最も粗いバランシングスコアである． ここで粗いとは，共変量の情報を一つの値に集約しているという意味である． 最も細かいバランシングスコアは共変量が取り得る値の組合せになる.\n傾向スコアが同程度で異なる処置がされたデータの組合せを選び， 処置群の処置群の平均処置効果を求めることになる\n\n\n\n\n定理１（バランシング）：処置の割り付け\\(T\\)を観測された共変量\\(X\\)は，傾向スコア\\(e(X)\\)が和えられたとき条件付き独立である．すなわち傾向スコアが\\(e(X)\\)が同じであれば処置群と統制群で共変量の分布は同じである.\n\n\\[\nX \\perp T|e(X)\n\\]\n\n定理２（条件付き独立）：傾向スコア\\(e(X)\\)が与えられれば潜在的結果変数\\({Y(1), Y(2)}\\)と割り付け変数\\(T\\)は条件付き独立である．すなわり傾向スコアが同じ個体であれば，処置への割り付けえは無作為と見なせる\n\n\\[\n\\{Y(0), Y(1)\\} \\perp T|e(X)\n\\]\n上記の定理はこれまでに使っていた条件付き独立性（無交絡性）と条件付き正直性である．\n\\[\n\\{Y(1), Y(0)\\} \\perp T|X\\\\\n0 &lt; \\text{Pr}(T_i=1|X) &lt; 1\n\\]\n条件付き独立性が述べているのは観測された共変量のみが処置の割り付けに影響を与えている ということであり，潜在的結果変数とは独立ということである. 二つ目の式で述べていることは実用的には傾向スコアが０になっても１になってもいけない ということである．処置群と統制群のどちらにも観測値があるということを意味している．\n何がいいたいのかというと観測されない共変量\\(\\mathcal{U}\\)がないという想定だし， それが合った場合にどのような結果となるのかは何もいえない. それを調べる場合には別のモデルを使う必要がある.\n\n\n\n省略\n\n\n\n\n\nCode\ndata03 &lt;- read_csv(\"./causality/data03.csv\", show_col_types = FALSE)\nsummary(data03)\n#&gt;        x1              y3              t1           y0t             y1t       \n#&gt;  Min.   :70.00   Min.   :63.00   Min.   :0.0   Min.   :62.00   Min.   :71.00  \n#&gt;  1st Qu.:73.75   1st Qu.:73.75   1st Qu.:0.0   1st Qu.:66.50   1st Qu.:75.50  \n#&gt;  Median :80.00   Median :77.00   Median :0.5   Median :71.00   Median :81.50  \n#&gt;  Mean   :80.00   Mean   :77.25   Mean   :0.5   Mean   :72.20   Mean   :82.00  \n#&gt;  3rd Qu.:86.25   3rd Qu.:82.00   3rd Qu.:1.0   3rd Qu.:78.75   3rd Qu.:88.75  \n#&gt;  Max.   :90.00   Max.   :91.00   Max.   :1.0   Max.   :82.00   Max.   :92.00\n\n\n\n\nCode\n# 傾向スコア\npsmodel &lt;- glm(t1 ~ x1, family = binomial(link = \"logit\"), data = data03)\nps3 &lt;- round(psmodel$fitted.values, 4)\nps4 &lt;- c(rep(.8, 5), rep(.6, 5), rep(.4, 5), rep(.2, 5))\n\nn1 &lt;- 1000\nx1 &lt;- runif(n1, -10, 10)\ne1 &lt;- rlogis(n1, location = 0, scale = 1)\ntstar &lt;- .5 + 1.1 * x1 + e1\nt1 &lt;- NULL\nt1[tstar &gt;  0] &lt;- 1\nt1[tstar &lt;= 0] &lt;- 0\n\n\ndf2 &lt;- with(data03, data.frame(x1, y3, t1, ps3, ps4))\nprint(df2)\n#&gt;    x1 y3 t1    ps3 ps4\n#&gt; 1  70 74  1 0.7751 0.8\n#&gt; 2  70 63  0 0.7751 0.8\n#&gt; 3  70 73  1 0.7751 0.8\n#&gt; 4  70 71  1 0.7751 0.8\n#&gt; 5  70 74  1 0.7751 0.8\n#&gt; 6  75 67  0 0.6499 0.6\n#&gt; 7  75 77  1 0.6499 0.6\n#&gt; 8  75 68  0 0.6499 0.6\n#&gt; 9  75 77  1 0.6499 0.6\n#&gt; 10 75 78  1 0.6499 0.6\n#&gt; 11 85 88  1 0.3501 0.4\n#&gt; 12 85 77  0 0.3501 0.4\n#&gt; 13 85 76  0 0.3501 0.4\n#&gt; 14 85 86  1 0.3501 0.4\n#&gt; 15 85 78  0 0.3501 0.4\n#&gt; 16 90 81  0 0.2249 0.2\n#&gt; 17 90 91  1 0.2249 0.2\n#&gt; 18 90 82  0 0.2249 0.2\n#&gt; 19 90 82  0 0.2249 0.2\n#&gt; 20 90 82  0 0.2249 0.2\n\n\n\n\n\n傾向スコアとは，共変量\\(X\\)が与えられたときに処置に割り付けられる確率であり， ロジスティック回帰モデルによる確率の予測値である。 実務ではもろもろ処理をパッケージで済ます．\n\n\nCode\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nm.out &lt;- matchit(t1 ~ x1, data = data03)\nps5 &lt;- m.out$model$fitted.values\nsummary(ps3)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.2249  0.3188  0.5000  0.5000  0.6812  0.7751\nsummary(ps5)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.2249  0.3188  0.5000  0.5000  0.6812  0.7751\ncor(ps3, ps5)\n#&gt; [1] 1\n\n\nパッケージで算出した傾向スコアとロジスティックス回帰モデルで算出した傾向スコアが 一致していることがわかる.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#バランシングスコア",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#バランシングスコア",
    "title": "傾向スコア",
    "section": "",
    "text": "共変量が単変量の場合には共分散分析により解析することが出来た． しかし，通常共変量は多変量である．これに対して傾向スコアが開発された． 傾向スコアとはバランシングスコアの１つであり， 観測される共変量\\(X\\)の関数\\(b(X)\\)が与えられたときの\\(X\\)の条件つき分布が， 処置群と統制群において同じとなる関数である.\nとにかくバランシングスコアを見るようにして，バランシングスコアが悪いときに，マッチングを使ったATT推定を行ってはならない. マッチしたからといってどの程度マッチに意味があるかは，別であるよね，という話だと理解した．\n\n\nCode\ndata10a &lt;- read_csv(\"./causality/data10a.csv\", show_col_types = FALSE)\n\n# 平均処置効果\nwith(data10a, {\n    print(mean(y1t) - mean(y0t))\n})\n#&gt; [1] 9.95\n\n# 処置群の平均処置効果\nwith(data10a, {\n    print(mean(y1t[t1==1])-mean(y0t[t1==1]))   \n})\n#&gt; [1] 9.090909\n\n# ナイーブな平均処置効果\nwith(data10a, {\n    print(mean(y1t[t1==1])-mean(y0t[t1==0]))   \n})\n#&gt; [1] 17.39394\n\n\nナイーブな平均処置結果が使えないことがよくわかる.\nこのような結果が起こるのは共変量の分布が異なることによる. 処置群と統制群で平均値や分散が同じになることを指して「バランスが取れている」ということになる．\n\n\nCode\ndata10a |&gt; \n    group_by(t1) |&gt;\n    summarise(quantile = list(stack(quantile(x1)))) |&gt; \n    unnest(quantile) |&gt; \n    pivot_wider(id_cols = t1, names_from = \"ind\", values_from = \"values\")\n#&gt; # A tibble: 2 × 6\n#&gt;      t1  `0%` `25%` `50%` `75%` `100%`\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1     0    66  70      74    78     88\n#&gt; 2     1    73  77.5    83    89     92",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコア-1",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコア-1",
    "title": "傾向スコア",
    "section": "",
    "text": "共変量が多変量である場合を考える. 多変量である場合には上記のように 共変量の分布が同一であるのかを確認することは容易ではない．\n\n\nCode\ndata10b &lt;- read_csv(\"./causality/data10b.csv\", show_col_types = FALSE)\nprint(data10b |&gt; head())\n#&gt; # A tibble: 6 × 8\n#&gt;     y0t   y1t    t1     y    x1    x2    x3   ps1\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    63    74     0    63    66    76    75  0.12\n#&gt; 2    55    70     0    55    70    75    55  0.05\n#&gt; 3    59    69     0    59    70    60    73  0.19\n#&gt; 4    71    77     1    77    73    76    82  0.43\n#&gt; 5    73    78     0    73    73    79    78  0.35\n#&gt; 6    66    77     0    66    74    72    79  0.41\n\n\nそこで傾向スコアを考える．傾向スコアとは，共変量\\(X\\)が与えられたとき， 処置に割り付けらあれる確率と定義される．\n\\[\ne(X) = \\text{Pr}(T_i=1|X)\n\\]\n傾向スコアは最も粗いバランシングスコアである． ここで粗いとは，共変量の情報を一つの値に集約しているという意味である． 最も細かいバランシングスコアは共変量が取り得る値の組合せになる.\n傾向スコアが同程度で異なる処置がされたデータの組合せを選び， 処置群の処置群の平均処置効果を求めることになる",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコア定理",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコア定理",
    "title": "傾向スコア",
    "section": "",
    "text": "定理１（バランシング）：処置の割り付け\\(T\\)を観測された共変量\\(X\\)は，傾向スコア\\(e(X)\\)が和えられたとき条件付き独立である．すなわち傾向スコアが\\(e(X)\\)が同じであれば処置群と統制群で共変量の分布は同じである.\n\n\\[\nX \\perp T|e(X)\n\\]\n\n定理２（条件付き独立）：傾向スコア\\(e(X)\\)が与えられれば潜在的結果変数\\({Y(1), Y(2)}\\)と割り付け変数\\(T\\)は条件付き独立である．すなわり傾向スコアが同じ個体であれば，処置への割り付けえは無作為と見なせる\n\n\\[\n\\{Y(0), Y(1)\\} \\perp T|e(X)\n\\]\n上記の定理はこれまでに使っていた条件付き独立性（無交絡性）と条件付き正直性である．\n\\[\n\\{Y(1), Y(0)\\} \\perp T|X\\\\\n0 &lt; \\text{Pr}(T_i=1|X) &lt; 1\n\\]\n条件付き独立性が述べているのは観測された共変量のみが処置の割り付けに影響を与えている ということであり，潜在的結果変数とは独立ということである. 二つ目の式で述べていることは実用的には傾向スコアが０になっても１になってもいけない ということである．処置群と統制群のどちらにも観測値があるということを意味している．\n何がいいたいのかというと観測されない共変量\\(\\mathcal{U}\\)がないという想定だし， それが合った場合にどのような結果となるのかは何もいえない. それを調べる場合には別のモデルを使う必要がある.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコアのモデル化",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコアのモデル化",
    "title": "傾向スコア",
    "section": "",
    "text": "省略",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコアの算出例",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコアの算出例",
    "title": "傾向スコア",
    "section": "",
    "text": "Code\ndata03 &lt;- read_csv(\"./causality/data03.csv\", show_col_types = FALSE)\nsummary(data03)\n#&gt;        x1              y3              t1           y0t             y1t       \n#&gt;  Min.   :70.00   Min.   :63.00   Min.   :0.0   Min.   :62.00   Min.   :71.00  \n#&gt;  1st Qu.:73.75   1st Qu.:73.75   1st Qu.:0.0   1st Qu.:66.50   1st Qu.:75.50  \n#&gt;  Median :80.00   Median :77.00   Median :0.5   Median :71.00   Median :81.50  \n#&gt;  Mean   :80.00   Mean   :77.25   Mean   :0.5   Mean   :72.20   Mean   :82.00  \n#&gt;  3rd Qu.:86.25   3rd Qu.:82.00   3rd Qu.:1.0   3rd Qu.:78.75   3rd Qu.:88.75  \n#&gt;  Max.   :90.00   Max.   :91.00   Max.   :1.0   Max.   :82.00   Max.   :92.00\n\n\n\n\nCode\n# 傾向スコア\npsmodel &lt;- glm(t1 ~ x1, family = binomial(link = \"logit\"), data = data03)\nps3 &lt;- round(psmodel$fitted.values, 4)\nps4 &lt;- c(rep(.8, 5), rep(.6, 5), rep(.4, 5), rep(.2, 5))\n\nn1 &lt;- 1000\nx1 &lt;- runif(n1, -10, 10)\ne1 &lt;- rlogis(n1, location = 0, scale = 1)\ntstar &lt;- .5 + 1.1 * x1 + e1\nt1 &lt;- NULL\nt1[tstar &gt;  0] &lt;- 1\nt1[tstar &lt;= 0] &lt;- 0\n\n\ndf2 &lt;- with(data03, data.frame(x1, y3, t1, ps3, ps4))\nprint(df2)\n#&gt;    x1 y3 t1    ps3 ps4\n#&gt; 1  70 74  1 0.7751 0.8\n#&gt; 2  70 63  0 0.7751 0.8\n#&gt; 3  70 73  1 0.7751 0.8\n#&gt; 4  70 71  1 0.7751 0.8\n#&gt; 5  70 74  1 0.7751 0.8\n#&gt; 6  75 67  0 0.6499 0.6\n#&gt; 7  75 77  1 0.6499 0.6\n#&gt; 8  75 68  0 0.6499 0.6\n#&gt; 9  75 77  1 0.6499 0.6\n#&gt; 10 75 78  1 0.6499 0.6\n#&gt; 11 85 88  1 0.3501 0.4\n#&gt; 12 85 77  0 0.3501 0.4\n#&gt; 13 85 76  0 0.3501 0.4\n#&gt; 14 85 86  1 0.3501 0.4\n#&gt; 15 85 78  0 0.3501 0.4\n#&gt; 16 90 81  0 0.2249 0.2\n#&gt; 17 90 91  1 0.2249 0.2\n#&gt; 18 90 82  0 0.2249 0.2\n#&gt; 19 90 82  0 0.2249 0.2\n#&gt; 20 90 82  0 0.2249 0.2",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#rパッケージによる傾向スコアのモデル化",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#rパッケージによる傾向スコアのモデル化",
    "title": "傾向スコア",
    "section": "",
    "text": "傾向スコアとは，共変量\\(X\\)が与えられたときに処置に割り付けられる確率であり， ロジスティック回帰モデルによる確率の予測値である。 実務ではもろもろ処理をパッケージで済ます．\n\n\nCode\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nm.out &lt;- matchit(t1 ~ x1, data = data03)\nps5 &lt;- m.out$model$fitted.values\nsummary(ps3)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.2249  0.3188  0.5000  0.5000  0.6812  0.7751\nsummary(ps5)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.2249  0.3188  0.5000  0.5000  0.6812  0.7751\ncor(ps3, ps5)\n#&gt; [1] 1\n\n\nパッケージで算出した傾向スコアとロジスティックス回帰モデルで算出した傾向スコアが 一致していることがわかる.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html",
    "title": "最小二乗法の仮定と診断2",
    "section": "",
    "text": "ここで紹介する家庭は次の三つである.\n\n完全な多重共線性が存在しない\n誤差項の分散均一の仮定\n誤差項の世紀性の仮定\n\n\n\n変数X1とX2との相関が強くなることで，パラメータの不偏性には影響ないが， 相関が強くなるにつれて標準誤差が大きくなることになる． 結果的に検定が棄却されにくくなる．\n\n\nVIFによる診断がある.\n\\[\n\\text{VIF}=\\frac{1}{1-R_j^2}\n\\]\nここで\\(R_j^2\\)は説明変数\\(j\\)を除いた他の説明変数で，説明変数\\(j\\)を重回帰分析したときの， 決定係数である．決定係数が大きいとき，つまり残りの変数で説明できるときには，VIFが大きくなる． VIFが大きいことは多重共線性の存在を疑うことになる.\n交絡因子として取り込んだ共変量同士で多重共線性があっても， 着目した変数との多重共線性がなければ，偏回帰係数の推定には影響がない． ただし共変量の回帰係数は解釈可能ではないということに注意する.\n\n\n\n\nたとえば，年収と食費の関係を調べたとする． 年収が少ない場合には食費に使える予算はほぼ一定であること対して， 年収が増えると食費に使う予算のばらつきが大きくなったとする． これは年収が増えることで食費に対する贅沢が可能となり， 質素派と贅沢派という個人の属性が影響し分散が大きくなり， 年収で食費を回帰した場合には分散不均一の仮定を満たさなくなる．\n不均一分散はそれ自体が発見であるし，様々なモデルが既に開発されている．\n\n\n分散が不均一であっても推定量の不偏性には影響しない．一方で標準誤差が大きくなる. これにより最小二乗推定量が最良線形不偏推定量を満たさないことになる.\n\n\n\nブルーシュ・ペイガン検定を行うことでよい. Rではlmtest::bptestを使うことで出来る.\n\n\n\n説明変数が分散に与える影響がわかっている場合，加重最小二乗法を使うことで， 不均一分散に対処することが可能である.\n関数系の推定は作法があるのでそれに従うこと.\n\n\nCode\ndata08b &lt;- read_csv(\"./causality/data08b.csv\", show_col_types = FALSE)\nsummary(data08b)\n#&gt;        y1                y2                 x1           \n#&gt;  Min.   :-2.4949   Min.   :-8.17685   Min.   :-0.997053  \n#&gt;  1st Qu.:-0.5390   1st Qu.:-0.52864   1st Qu.:-0.477269  \n#&gt;  Median : 0.1584   Median :-0.02624   Median :-0.008905  \n#&gt;  Mean   : 0.1779   Mean   : 0.18978   Mean   : 0.012520  \n#&gt;  3rd Qu.: 0.8890   3rd Qu.: 0.79394   3rd Qu.: 0.525134  \n#&gt;  Max.   : 3.3866   Max.   :10.32879   Max.   : 0.997758\n\n\n\n\nCode\n# 加重最小二乗法\nmodel5 &lt;- lm(y2 ~ x1, weights = 1/ exp(1.5 * x1), data = data08b)\nmodel5\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y2 ~ x1, data = data08b, weights = 1/exp(1.5 * x1))\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;      0.1689       0.4748\n\n\nなお関数系は一般に知ることが出来ないので， 通常は下記の実行可能一般化最小二乗法における関数系の推定手順を用いた分析が行われている。\n\n\nCode\nmodel4    &lt;- lm(y2 ~ x1, data = data08b)\nlog_resid &lt;- log(residuals(model4) ** 2)\nmodel6 &lt;- lm(log_resid ~ x1, data = data08b)\nhhat1  &lt;- predict(model6)\nhhat2  &lt;- exp(hhat1)\nmodel7 &lt;- lm(y2 ~ x1, weights = 1/ hhat2, data = data08b)\nsummary(model7)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y2 ~ x1, data = data08b, weights = 1/hhat2)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.9655 -1.2527 -0.0158  1.2804  5.8643 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.11808    0.04006   2.947  0.00328 ** \n#&gt; x1           0.38348    0.05509   6.961  6.1e-12 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.883 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.04631,    Adjusted R-squared:  0.04535 \n#&gt; F-statistic: 48.46 on 1 and 998 DF,  p-value: 6.102e-12\n\n\n\n\n\n\nここまでの５つの仮定が満たされていればこれは通常問題にならない． 一般に誤差項の正規性は問題になるが， 誤差項に正規性があるのかどうかは， 最小二乗推定量が最良線形不偏推定量であるのかどうかには影響がない．\n本来正規性について気にすべき問題は， 誤差項でなくパラメータの推定量の正規性である． パラメータの推定値は標本平均であるため中心極限定理からデータサイズに依存して， 正規分布となる．一般にはデータサイズが３０以上で正規分布として扱うことができるとされている．\n一方で誤差項の正規性はそれなりに意味を持つ． つまり，すべての共変量をモデルに取り込むことは現実的ではなく， 多くの場合には誤差項として表現することになる． 誤差項で様々な共変量をまとめて表現することにあるため， 誤差項が正規性を持つことは適切なモデリングが行えているのかの重要な指標となる.\n次の例では，データがベータ分布に従っている場合，データが正規分布に従っている場合の ２つの例で回帰分析を行い，その際の誤差項の分布を比べてみる. また誤差項をジャック・ベラの正規性検定をおこなう. (ライブラリが入らずできななかった)\nベータ分布に従うものは偏りが大きいことがグラフから見て取れる.\n\n\nCode\ndata08c &lt;- read_csv(\"./causality/data08c.csv\", show_col_types = FALSE)\nmodel1  &lt;- lm(y1 ~ x1, data = data08c)\nresid1  &lt;- residuals(model1)\nmodel2  &lt;- lm(y2 ~ x1, data = data08c)\nresid2  &lt;- residuals(model2)\n\n\n\n\nCode\nhist(resid1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nhist(resid2)",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断2"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#完全な多重共線性が存在しない",
    "href": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#完全な多重共線性が存在しない",
    "title": "最小二乗法の仮定と診断2",
    "section": "",
    "text": "変数X1とX2との相関が強くなることで，パラメータの不偏性には影響ないが， 相関が強くなるにつれて標準誤差が大きくなることになる． 結果的に検定が棄却されにくくなる．\n\n\nVIFによる診断がある.\n\\[\n\\text{VIF}=\\frac{1}{1-R_j^2}\n\\]\nここで\\(R_j^2\\)は説明変数\\(j\\)を除いた他の説明変数で，説明変数\\(j\\)を重回帰分析したときの， 決定係数である．決定係数が大きいとき，つまり残りの変数で説明できるときには，VIFが大きくなる． VIFが大きいことは多重共線性の存在を疑うことになる.\n交絡因子として取り込んだ共変量同士で多重共線性があっても， 着目した変数との多重共線性がなければ，偏回帰係数の推定には影響がない． ただし共変量の回帰係数は解釈可能ではないということに注意する.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断2"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#誤差項の分散均一性",
    "href": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#誤差項の分散均一性",
    "title": "最小二乗法の仮定と診断2",
    "section": "",
    "text": "たとえば，年収と食費の関係を調べたとする． 年収が少ない場合には食費に使える予算はほぼ一定であること対して， 年収が増えると食費に使う予算のばらつきが大きくなったとする． これは年収が増えることで食費に対する贅沢が可能となり， 質素派と贅沢派という個人の属性が影響し分散が大きくなり， 年収で食費を回帰した場合には分散不均一の仮定を満たさなくなる．\n不均一分散はそれ自体が発見であるし，様々なモデルが既に開発されている．\n\n\n分散が不均一であっても推定量の不偏性には影響しない．一方で標準誤差が大きくなる. これにより最小二乗推定量が最良線形不偏推定量を満たさないことになる.\n\n\n\nブルーシュ・ペイガン検定を行うことでよい. Rではlmtest::bptestを使うことで出来る.\n\n\n\n説明変数が分散に与える影響がわかっている場合，加重最小二乗法を使うことで， 不均一分散に対処することが可能である.\n関数系の推定は作法があるのでそれに従うこと.\n\n\nCode\ndata08b &lt;- read_csv(\"./causality/data08b.csv\", show_col_types = FALSE)\nsummary(data08b)\n#&gt;        y1                y2                 x1           \n#&gt;  Min.   :-2.4949   Min.   :-8.17685   Min.   :-0.997053  \n#&gt;  1st Qu.:-0.5390   1st Qu.:-0.52864   1st Qu.:-0.477269  \n#&gt;  Median : 0.1584   Median :-0.02624   Median :-0.008905  \n#&gt;  Mean   : 0.1779   Mean   : 0.18978   Mean   : 0.012520  \n#&gt;  3rd Qu.: 0.8890   3rd Qu.: 0.79394   3rd Qu.: 0.525134  \n#&gt;  Max.   : 3.3866   Max.   :10.32879   Max.   : 0.997758\n\n\n\n\nCode\n# 加重最小二乗法\nmodel5 &lt;- lm(y2 ~ x1, weights = 1/ exp(1.5 * x1), data = data08b)\nmodel5\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y2 ~ x1, data = data08b, weights = 1/exp(1.5 * x1))\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;      0.1689       0.4748\n\n\nなお関数系は一般に知ることが出来ないので， 通常は下記の実行可能一般化最小二乗法における関数系の推定手順を用いた分析が行われている。\n\n\nCode\nmodel4    &lt;- lm(y2 ~ x1, data = data08b)\nlog_resid &lt;- log(residuals(model4) ** 2)\nmodel6 &lt;- lm(log_resid ~ x1, data = data08b)\nhhat1  &lt;- predict(model6)\nhhat2  &lt;- exp(hhat1)\nmodel7 &lt;- lm(y2 ~ x1, weights = 1/ hhat2, data = data08b)\nsummary(model7)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y2 ~ x1, data = data08b, weights = 1/hhat2)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.9655 -1.2527 -0.0158  1.2804  5.8643 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.11808    0.04006   2.947  0.00328 ** \n#&gt; x1           0.38348    0.05509   6.961  6.1e-12 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.883 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.04631,    Adjusted R-squared:  0.04535 \n#&gt; F-statistic: 48.46 on 1 and 998 DF,  p-value: 6.102e-12",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断2"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#誤差項の正規性",
    "href": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#誤差項の正規性",
    "title": "最小二乗法の仮定と診断2",
    "section": "",
    "text": "ここまでの５つの仮定が満たされていればこれは通常問題にならない． 一般に誤差項の正規性は問題になるが， 誤差項に正規性があるのかどうかは， 最小二乗推定量が最良線形不偏推定量であるのかどうかには影響がない．\n本来正規性について気にすべき問題は， 誤差項でなくパラメータの推定量の正規性である． パラメータの推定値は標本平均であるため中心極限定理からデータサイズに依存して， 正規分布となる．一般にはデータサイズが３０以上で正規分布として扱うことができるとされている．\n一方で誤差項の正規性はそれなりに意味を持つ． つまり，すべての共変量をモデルに取り込むことは現実的ではなく， 多くの場合には誤差項として表現することになる． 誤差項で様々な共変量をまとめて表現することにあるため， 誤差項が正規性を持つことは適切なモデリングが行えているのかの重要な指標となる.\n次の例では，データがベータ分布に従っている場合，データが正規分布に従っている場合の ２つの例で回帰分析を行い，その際の誤差項の分布を比べてみる. また誤差項をジャック・ベラの正規性検定をおこなう. (ライブラリが入らずできななかった)\nベータ分布に従うものは偏りが大きいことがグラフから見て取れる.\n\n\nCode\ndata08c &lt;- read_csv(\"./causality/data08c.csv\", show_col_types = FALSE)\nmodel1  &lt;- lm(y1 ~ x1, data = data08c)\nresid1  &lt;- residuals(model1)\nmodel2  &lt;- lm(y2 ~ x1, data = data08c)\nresid2  &lt;- residuals(model2)\n\n\n\n\nCode\nhist(resid1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nhist(resid2)",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断2"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/106_図で理解する重回帰モデルの基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/106_図で理解する重回帰モデルの基礎.html",
    "title": "重回帰分析の基礎",
    "section": "",
    "text": "チョコレートの消費量とノーベル章受賞者の数が回帰分析により有意な関係があると 算出されたとき，因果推論を考えるにはまず交絡因子の探索が必要となる． つまり，チョコレートの消費量とノーベル章受賞者のどちらにも影響を与えると考えられる 変数を探すことになる．\nそのような変数が明示できなければ，なんらかの因果関係があると解釈を進めていくことが出来る.\n交絡因子についての検討はデータ分析担当者の責任のもと行うべきであるとともに， 後に検証可能な形で分析結果を共有してくことが必須である.\n重回帰モデルを考えたときある説明変数Aと別の説明変数Bに相関があるときには， AとBが交絡していることとなり， AとBの相関を除いた上で回帰分析することが望ましい． 共分散分析ではそのような場合の因果推論に役に立つ．\n重回帰分析における回帰係数とは当該変数以外で説明できる当該変数の分散を除いた， 当該変数の分散によるものである． 他の変数から受ける影響を統計的に除外， つまり交絡を取り除いて推定してると考えることが出来る.\n\n\nCode\n# 共分散分析\n# 重回帰分析は残差に対して単回帰を繰り返すことと同じ\n\ndata03 &lt;- read_csv(\"./causality/data03.csv\", locale = locale(encoding = \"UTF-8\"))\n#&gt; Rows: 20 Columns: 5\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (5): x1, y3, t1, y0t, y1t\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nmodel1 &lt;- lm(t1 ~ x1, data = data03)\net1    &lt;- resid(model1)\nmodel2 &lt;- lm(y3 ~ et1, data = data03)\nsummary(model2)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ et1, data = data03)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -8.409 -4.328 -0.250  4.828  6.910 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   77.250      1.236  62.475  &lt; 2e-16 ***\n#&gt; et1            9.816      2.758   3.559  0.00224 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.53 on 18 degrees of freedom\n#&gt; Multiple R-squared:  0.413,  Adjusted R-squared:  0.3804 \n#&gt; F-statistic: 12.67 on 1 and 18 DF,  p-value: 0.002242\n\n\n\n\n処置の割り付けが無作為化されてていても， 実際にはたまたま共変量がうまくバランスしていないことがある． そのような時にも共変量を考慮した モデル化を行い共分散分析を行うことで，バランシングできることがある．\nまた処置の無作為割り付け自体は成功しており，交絡因子を気にする必要がない場合でも， 結果辺陬の変動を説明することに寄与する変数があれば，共分散分析のモデルに取り入れることが望ましい. 不偏性をもったまま決定係数を減らすことが出来る.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重回帰分析の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/106_図で理解する重回帰モデルの基礎.html#実験研究における共分散分析の活用",
    "href": "contents/books/05_統計的因果推論の理論と実際/106_図で理解する重回帰モデルの基礎.html#実験研究における共分散分析の活用",
    "title": "重回帰分析の基礎",
    "section": "",
    "text": "処置の割り付けが無作為化されてていても， 実際にはたまたま共変量がうまくバランスしていないことがある． そのような時にも共変量を考慮した モデル化を行い共分散分析を行うことで，バランシングできることがある．\nまた処置の無作為割り付け自体は成功しており，交絡因子を気にする必要がない場合でも， 結果辺陬の変動を説明することに寄与する変数があれば，共分散分析のモデルに取り入れることが望ましい. 不偏性をもったまま決定係数を減らすことが出来る.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重回帰分析の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html",
    "title": "標準誤差と信頼区間",
    "section": "",
    "text": "本章では標準誤差とは何か，信頼区間とはなにか，具体的に検討したのち２標本t検定の メカニズムについて解説する.\n無作為に割り付けされている場合には，線形回帰分析で定量化しt.testで検定が行える．\n\n\n標本平均が従う分布の標準偏差である. いつも思うけどこれくらいの計算も出来なくなるのではないか，という一末の不安がある． これは何度も計算すれば解消さるのだろうか・・・\n\n\n\n信頼区間について納得できる説明がされている．t.testのやり方について説明されている. そういえば，\\(t\\)検定の場合，自由度は\\(n-1\\)とするけど，標本平均と標本分散を使っているので自由度\\(n-2\\)のような気がするのだけど，どうなのだろうか？",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "標準誤差と信頼区間"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html#標準誤差",
    "href": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html#標準誤差",
    "title": "標準誤差と信頼区間",
    "section": "",
    "text": "標本平均が従う分布の標準偏差である. いつも思うけどこれくらいの計算も出来なくなるのではないか，という一末の不安がある． これは何度も計算すれば解消さるのだろうか・・・",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "標準誤差と信頼区間"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html#信頼区間",
    "href": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html#信頼区間",
    "title": "標準誤差と信頼区間",
    "section": "",
    "text": "信頼区間について納得できる説明がされている．t.testのやり方について説明されている. そういえば，\\(t\\)検定の場合，自由度は\\(n-1\\)とするけど，標本平均と標本分散を使っているので自由度\\(n-2\\)のような気がするのだけど，どうなのだろうか？",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "標準誤差と信頼区間"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "この枠組みでは因果推論を「欠測データの問題」として扱う．\n\n\n20人の学生を対象にした数学の試験結果を考える． 10人は補習講義を受けており，試験結果から補習講義の効果を知りたいとする． しかし，補習講義を受けた人は受けていない場合の結果はなく，その逆もまた然りである． つまり，補習講義の有無による個々人の試験結果の違いの分布を得ることは出来ない．\n補習講義の有無がランダムに割り付けられている場合には評価が出来るものの， 補習講義を受けるの者はそれまでの成績が悪い場合である．\n仮にすべての生徒に対して補修講義の有無が観測されているならば 単に平均値を比較することでよい．．\n\n\n\n補習講義を受けたどうかを「補修講義を受ける確率」として考えるとことで 補習講義の有無をまとめて表現することが可能となる． 具体的には\\(Y_i(0)\\)を補習講義を受けていない場合の潜在的結果， \\(Y_i(1)\\)を補習講義を受けた場合の潜在的結果といして，次の式でモデル化する.\n\\[\n\\begin{align}\nY_i &= (1-T_i)Y_i(0) + T_iY_i(1) \\\\\nT_i &\\in {0, 1}\n\\end{align}\n\\]\n\n\nCode\npath   &lt;- \"./causality/data02.csv\"\ndata02 &lt;- read_csv(path, locale = locale(encoding = \"UTF-8\"), show_col_types = FALSE)\ndata02 |&gt; summary()\n#&gt;        x1              y3             t1            y0              y1       \n#&gt;  Min.   :58.00   Min.   :61.0   Min.   :0.0   Min.   :72.00   Min.   :61.00  \n#&gt;  1st Qu.:76.25   1st Qu.:75.0   1st Qu.:0.0   1st Qu.:75.00   1st Qu.:74.25  \n#&gt;  Median :83.50   Median :76.5   Median :0.0   Median :77.00   Median :75.50  \n#&gt;  Mean   :81.95   Mean   :76.6   Mean   :0.3   Mean   :77.79   Mean   :73.83  \n#&gt;  3rd Qu.:87.25   3rd Qu.:80.0   3rd Qu.:1.0   3rd Qu.:80.00   3rd Qu.:76.75  \n#&gt;  Max.   :96.00   Max.   :87.0   Max.   :1.0   Max.   :87.00   Max.   :80.00  \n#&gt;                                               NA's   :6       NA's   :14     \n#&gt;       y0t            y1t       \n#&gt;  Min.   :52.0   Min.   :61.00  \n#&gt;  1st Qu.:69.5   1st Qu.:79.25  \n#&gt;  Median :75.0   Median :84.50  \n#&gt;  Mean   :73.8   Mean   :83.85  \n#&gt;  3rd Qu.:78.5   3rd Qu.:89.00  \n#&gt;  Max.   :87.0   Max.   :97.00  \n#&gt; \n\n\ny0, y1が条件付きの値，つまり実際の結果であり，y0t, y1tがもしどちらも観測可能である場合の結果である． y0t, y1tが観測できるときにはt検定などで比較が可能である.\n\n\n\n個体因果結果とは\\(\\tau_i = Y_i(1) - Y_i(0)\\)であり，つまりは個体差である. 通常はどちらか一方しか観測できないのでこの値を算出することは出来ない. また推測も出来ない.\n\n\nCode\nwith(data02, {\n    # 個体ごとの因果効果(理想的)\n    print(y1t - y0t)\n})\n#&gt;  [1]  8  9 10 13  9  9 11 12 10 10  9 10 10  9 12 12 10  9 10  9\n\n\n\n\n\n個体因果効果は観測も推測も出来ないので， 個体の母集団に対する平均的な因果効果を考えることになる． つまり正しく推定が行えていないことがわかる.\n\n\nCode\nwith(data02, {\n    # 母集団への平均的な効果の推定\n    print(mean(y1t, na.rm = TRUE) - mean(y0t, na.rm = TRUE))\n    # しかし，この値をこのまま実際の結果に当てはめると結果が大きく異なる\n    print(mean(y1, na.rm = TRUE) - mean(y0, na.rm = TRUE))\n})\n#&gt; [1] 10.05\n#&gt; [1] -3.952381\n\n\n\n\n\nある処置が取られた対象に対してどれくらい影響があったのかについて知る． これは処置群の平均処置効果と呼ばれるものである. 次の式で定義される. これは処置を受けた人の処置を受けていなかったときの結果\\(E[Y_i(0)|T_i=1\\)が含まれている． このような値を推定することも出来るのが統計的因果推論の面白さになる.\n\\[\n\\tau_{\\text{ATT}}=E[Y_i(1)-Y_i(0)|T_i=1]=E[Y_i(1)|T_i=1]-E[Y_i(0)|T_i=1]\n\\]\nこれを計算してみるとかなりよい値を推定する.\n\n\nCode\nwith(subset(data02, t1==1), {\n    print(mean(y1t)-mean(y0t))\n})\n#&gt; [1] 9.333333\n\n\n\n\n\n処置効果２におけるナイーブな推定は大きく外れていた． このような場合交絡因子を疑うことが重要な点である． 今回の場合には，補習講義を受けたかどうかは入学時の成績が考慮されており， この部分が交絡している．\n処置と結果変数のどちらにも影響する変数が交絡因子である． たとえば，補習授業の効果を知りたいとき，補習講義を受けるかどうかがテストの点で決まり， 効果を測るときにもテストの点を使うとすると，学力が交絡しており，補習講義の効果を適切に推定することが困難になる.\n平均処置効果を適切似推定するには２つの比較可能な集団を作る必要がある． 比較可能な集団とは共変量の分布は異なるが，原因変数と結果変数だけ異なる である．\n\n\n\n観測されない交絡を統制するにはどうすればよいのか． 理想はランダムサンプリングでの割り付けである. 一般に無作為割り付けがされている実験を実験研究といい，無作為割り付けされていない実験を観察研究という．\n\n\n\n内的妥当性とはある限られた条件での因果である．外的妥当性とは条件が変更された場合における妥当性である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#具体例",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#具体例",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "20人の学生を対象にした数学の試験結果を考える． 10人は補習講義を受けており，試験結果から補習講義の効果を知りたいとする． しかし，補習講義を受けた人は受けていない場合の結果はなく，その逆もまた然りである． つまり，補習講義の有無による個々人の試験結果の違いの分布を得ることは出来ない．\n補習講義の有無がランダムに割り付けられている場合には評価が出来るものの， 補習講義を受けるの者はそれまでの成績が悪い場合である．\n仮にすべての生徒に対して補修講義の有無が観測されているならば 単に平均値を比較することでよい．．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#理論",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#理論",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "補習講義を受けたどうかを「補修講義を受ける確率」として考えるとことで 補習講義の有無をまとめて表現することが可能となる． 具体的には\\(Y_i(0)\\)を補習講義を受けていない場合の潜在的結果， \\(Y_i(1)\\)を補習講義を受けた場合の潜在的結果といして，次の式でモデル化する.\n\\[\n\\begin{align}\nY_i &= (1-T_i)Y_i(0) + T_iY_i(1) \\\\\nT_i &\\in {0, 1}\n\\end{align}\n\\]\n\n\nCode\npath   &lt;- \"./causality/data02.csv\"\ndata02 &lt;- read_csv(path, locale = locale(encoding = \"UTF-8\"), show_col_types = FALSE)\ndata02 |&gt; summary()\n#&gt;        x1              y3             t1            y0              y1       \n#&gt;  Min.   :58.00   Min.   :61.0   Min.   :0.0   Min.   :72.00   Min.   :61.00  \n#&gt;  1st Qu.:76.25   1st Qu.:75.0   1st Qu.:0.0   1st Qu.:75.00   1st Qu.:74.25  \n#&gt;  Median :83.50   Median :76.5   Median :0.0   Median :77.00   Median :75.50  \n#&gt;  Mean   :81.95   Mean   :76.6   Mean   :0.3   Mean   :77.79   Mean   :73.83  \n#&gt;  3rd Qu.:87.25   3rd Qu.:80.0   3rd Qu.:1.0   3rd Qu.:80.00   3rd Qu.:76.75  \n#&gt;  Max.   :96.00   Max.   :87.0   Max.   :1.0   Max.   :87.00   Max.   :80.00  \n#&gt;                                               NA's   :6       NA's   :14     \n#&gt;       y0t            y1t       \n#&gt;  Min.   :52.0   Min.   :61.00  \n#&gt;  1st Qu.:69.5   1st Qu.:79.25  \n#&gt;  Median :75.0   Median :84.50  \n#&gt;  Mean   :73.8   Mean   :83.85  \n#&gt;  3rd Qu.:78.5   3rd Qu.:89.00  \n#&gt;  Max.   :87.0   Max.   :97.00  \n#&gt; \n\n\ny0, y1が条件付きの値，つまり実際の結果であり，y0t, y1tがもしどちらも観測可能である場合の結果である． y0t, y1tが観測できるときにはt検定などで比較が可能である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果1個体因果結果",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果1個体因果結果",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "個体因果結果とは\\(\\tau_i = Y_i(1) - Y_i(0)\\)であり，つまりは個体差である. 通常はどちらか一方しか観測できないのでこの値を算出することは出来ない. また推測も出来ない.\n\n\nCode\nwith(data02, {\n    # 個体ごとの因果効果(理想的)\n    print(y1t - y0t)\n})\n#&gt;  [1]  8  9 10 13  9  9 11 12 10 10  9 10 10  9 12 12 10  9 10  9",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果2平均処置効果",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果2平均処置効果",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "個体因果効果は観測も推測も出来ないので， 個体の母集団に対する平均的な因果効果を考えることになる． つまり正しく推定が行えていないことがわかる.\n\n\nCode\nwith(data02, {\n    # 母集団への平均的な効果の推定\n    print(mean(y1t, na.rm = TRUE) - mean(y0t, na.rm = TRUE))\n    # しかし，この値をこのまま実際の結果に当てはめると結果が大きく異なる\n    print(mean(y1, na.rm = TRUE) - mean(y0, na.rm = TRUE))\n})\n#&gt; [1] 10.05\n#&gt; [1] -3.952381",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果3処置群の平均処置効果",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果3処置群の平均処置効果",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "ある処置が取られた対象に対してどれくらい影響があったのかについて知る． これは処置群の平均処置効果と呼ばれるものである. 次の式で定義される. これは処置を受けた人の処置を受けていなかったときの結果\\(E[Y_i(0)|T_i=1\\)が含まれている． このような値を推定することも出来るのが統計的因果推論の面白さになる.\n\\[\n\\tau_{\\text{ATT}}=E[Y_i(1)-Y_i(0)|T_i=1]=E[Y_i(1)|T_i=1]-E[Y_i(0)|T_i=1]\n\\]\nこれを計算してみるとかなりよい値を推定する.\n\n\nCode\nwith(subset(data02, t1==1), {\n    print(mean(y1t)-mean(y0t))\n})\n#&gt; [1] 9.333333",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#交絡因子",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#交絡因子",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "処置効果２におけるナイーブな推定は大きく外れていた． このような場合交絡因子を疑うことが重要な点である． 今回の場合には，補習講義を受けたかどうかは入学時の成績が考慮されており， この部分が交絡している．\n処置と結果変数のどちらにも影響する変数が交絡因子である． たとえば，補習授業の効果を知りたいとき，補習講義を受けるかどうかがテストの点で決まり， 効果を測るときにもテストの点を使うとすると，学力が交絡しており，補習講義の効果を適切に推定することが困難になる.\n平均処置効果を適切似推定するには２つの比較可能な集団を作る必要がある． 比較可能な集団とは共変量の分布は異なるが，原因変数と結果変数だけ異なる である．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#無作為抽出と無作為割り付け",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#無作為抽出と無作為割り付け",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "観測されない交絡を統制するにはどうすればよいのか． 理想はランダムサンプリングでの割り付けである. 一般に無作為割り付けがされている実験を実験研究といい，無作為割り付けされていない実験を観察研究という．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#内的妥当性と外的妥当性",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#内的妥当性と外的妥当性",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "内的妥当性とはある限られた条件での因果である．外的妥当性とは条件が変更された場合における妥当性である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/index.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/index.html",
    "title": "数値シミュレーションで身につける統計の仕組み",
    "section": "",
    "text": "1 はじめに\n\n『統計シミュレーションで身につける統計の仕組み』の読書ノートです\n誤植が多い図書なので次のサポートページをよくみること\nサポートページ\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch06_適切な検定のためのサンプルサイズ設計.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch06_適切な検定のためのサンプルサイズ設計.html",
    "title": "ch06 サンプルサイズ",
    "section": "",
    "text": "統計的検定において、エラー確率を管理するには、 次のことを検定する前に決めておく必要がある。\n\n確率モデルと検定する母数\nその母数についての帰無仮説\n\\(\\alpha\\)と\\(\\beta\\)\n計算する検定統計量\nサンプルサイズ",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch06 サンプルサイズ"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch06_適切な検定のためのサンプルサイズ設計.html#標本のt検定のサンプルサイズ",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch06_適切な検定のためのサンプルサイズ設計.html#標本のt検定のサンプルサイズ",
    "title": "ch06 サンプルサイズ",
    "section": "2.1 1標本のt検定のサンプルサイズ",
    "text": "2.1 1標本のt検定のサンプルサイズ\n\n小さめのサンプルサイズnを適当に決める\n見積もった効果量\\(\\delta_0\\)と\\(n\\)から非心度\\(\\lambda\\)を計算する\n帰無頒布の自由度を求めて\\(\\alpha\\)の臨界値を計算する\n臨界値と非心度からタイプ２エラーを求める\nタイプ２が\\(\\beta\\)を下回っていればそこで終了しそうでないなあ\\(n\\)を増やす\n\n\n\nCode\nt2e_ttest &lt;- function(alpah, delta, n) {\n  df &lt;- n - 1\n  lambda &lt;- delta * sqrt(n)\n  cv &lt;- qt(p = 1 - alpha / 2, df = df)\n  type2error &lt;- pt(q = cv, df = df, ncp = lambda)\n  return(type2error)\n}\n\n# 設定と準備\nalpha &lt;- .05\nbeta &lt;- .2\ndelta &lt;- .5\n\niter &lt;- 1000\n\nfor (n in 5:iter) {\n  type2error &lt;- t2e_ttest(alpha, delta, n)\n  if (type2error &lt; beta) {\n    break\n  }\n}\n\nprint(n)\n#&gt; [1] 34\n\nprint(type2error)\n#&gt; [1] 0.1922233",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch06 サンプルサイズ"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch04_母数の推定のシミュレーション.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch04_母数の推定のシミュレーション.html",
    "title": "ch04 母数の推定",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch04  母数の推定"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch02_プログラミングの基礎.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch02_プログラミングの基礎.html",
    "title": "ch02 プログラミングの基礎",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch02 プログラミングの基礎"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/index.html",
    "href": "contents/books/02_因果推論ミックステープ/index.html",
    "title": "因果推論ミックステープ",
    "section": "",
    "text": "1 はじめに\n\n『因果推論ミックステープ』の読書ノートです\n因果推論の初中級レベルとのことです\n本書をきっかけとして計量経済学についても学習してもらいたいとのことです\n本書はサイバーエージェントのゼミが誕生したとのこと\n\n非常にすごいことだと思います\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch10_合成コントロール法.html",
    "href": "contents/books/02_因果推論ミックステープ/ch10_合成コントロール法.html",
    "title": "ch10 合成コントロール",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch10 合成コントロール"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch08_パネルデータ.html",
    "href": "contents/books/02_因果推論ミックステープ/ch08_パネルデータ.html",
    "title": "ch08 パネルデータ",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch08 パネルデータ"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch06_回帰不連続デザイン.html",
    "href": "contents/books/02_因果推論ミックステープ/ch06_回帰不連続デザイン.html",
    "title": "ch06 回帰不連続デザイン",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch06 回帰不連続デザイン"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html",
    "href": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html",
    "title": "ch04 潜在アウトカム因果モデル",
    "section": "",
    "text": "潜在アウトカムとは、次のスイッチング方程式により決定される。\n\\[\nY_i = D_i Y_i^1 + (1-D_i)Y_i^0\n\\]\nここで\\(D_i\\)はそのユニットが処置を受けたときには\\(1\\), 受けなかった場合は\\(0\\)となる。 \\(D_i=1\\)のときには\\(Y_i=Y_i^1\\)となり、\\(D_i=0\\)のときには\\(Y_i=Y_i^0\\)となる。\nこの表記を用いて、ユニット固有の処置効果を２つの状態の差として定義する。\n\\[\n\\delta_i = Y_i^1 - Y_i^0\n\\]\nこの式が意味するところはあるユニットに対する処置効果を知るには、 処置が施された場合の結果、施されていない場合の結果のどちらも必要であり、 現実には知り得ない情報っであるということである.\n\n\n個体ごとの効果を知ることは出来ないので、 平均的な処置効果\\(ATE(Average treatment effect)\\)を調べることにする。\n\\[\n\\begin{align}\nATE &= E[\\delta_i]\\\\\n&=E[Y_i^1 - Y_i^0] \\\\\n&=E[Y_i^1] - E[Y_i^0]\n\\end{align}\n\\]\nATEを考えることで処置効果を推定することができる。 ただし、いずれにせよあるユニットにはどちらかしか観測できないのに注意する。\n次に関心があるのは処置群の平均処置効果である\\(ATT(average treatment effect for the treated group)\\)である。処置群においても、処置効果はユニットごとに異なるので、平均的にどの程度の効果があるのかを知りたいということになる。\n\\[\n\\begin{align}\nATT &= E[\\delta_i\\mid D_i = 1]\\\\\n&=E[Y_i^1 - Y_i^0\\mid D_i = 1] \\\\\n&=E[Y_i^1\\mid D_i = 1] - E[Y_i^0\\mid D_i = 1]\n\\end{align}\n\\]\n最期に関心があるのは、コントロール群、または未処理群に対する平均処置効果\\(ATU(average treatement effect for the untreated)\\)である。\n\\[\n\\begin{align}\nATU &= E[\\delta_i\\mid D_i = 0]\\\\\n&=E[Y_i^1 - Y_i^0\\mid D_i = 0] \\\\\n&=E[Y_i^1\\mid D_i = 0] - E[Y_i^0\\mid D_i = 0]\n\\end{align}\n\\]\n研究ではこれらの効果を 観測できる情報からどのように推定するのかが重要となる。\nところで, \\(ATE\\)の推定値として２つの処置群の単純差(Simple difference in mean outcomes: SDO)を考える。この値はデータから推定できる値である。\n\\[\nATE =　E[Y_i^1\\mid D=1] - E[Y_i^0\\mid D=0]\n\\]\nこの統計処理自体は正しいものの、誤解を招きかねないことには注意が必要である。 処置の割り当てがランダムでないときには、処置群とコントロール群に根本的な差が生じ津ことが知られている。\n\\[\n\\begin{align}\nE[Y_i^1\\mid D_i = 0] - E[Y_i^0\\mid D_i = 0] &= ATE + E[Y^0\\mid D=1]-E[Y^0\\mid D=0]+(1-\\pi)(ATT - ATU)\n\\end{align}\n\\]\n上記の結果は下の数式を整理したものである。\n\\[\n\\begin{align}\nATE&=\\pi ATT + (1-\\pi)ATU\\\\\n&=\\pi E[Y^1\\mid D=1]-\\pi E[Y^0\\mid D = 1] + (1-\\pi)E[Y^1\\mid D=0]-(1-\\pi)E[Y^0\\mid D = 0]\\\\\n&=\\{\\pi E[Y^1\\mid D=1]+(1-\\pi)E[Y^1\\mid D=0]\\}-{\\pi E[Y^0\\mid D = 1]+(1-\\pi)E[Y^0\\mid D = 0]}\n\\end{align}\n\\]\n\\(\\pi\\)は処置を受けた人の割合である。第一式は\\(ATT\\)の定義から、 処置を受けた人、受けていない人の処置効果に分会しているということになる。\nこれを見ると、単純な差にはATEだけでなく２つの項がある。 まずは次の式は選択バイアスと呼ばれるものである。 処置を受けていない場合の平均的なアウトカムが、 処置を受けているかどうかで変わるのでこのように呼ばれる。\n\\[\nE[Y^0\\mid D=1]-E[Y^0\\mid D=0]\n\\]\n次の式は異質性のある処置効果にともなうバイアスである。\n\\[\n(1-\\pi)(ATT - ATU)\n\\]\n上記２つの項は反実仮想を含んでいるため計算することが出来ない。 いずれにせよバイアスを含んでいることはわかる。\n\n\n\nSDOを使ってATEを推定するにあたり最も信頼できるのは、 処置を受けるのかどうかとアウトカムが独立であるときである。\n\\[\n(Y^1, Y^0) \\mathop{\\perp\\!\\!\\!\\!\\perp} D\n\\]\nランダム化によって実現したいのは、次の状態である。\n\\[\n\\begin{align}\nE[Y^1\\mid D=1]-E[Y^1\\mid D=0]&=0\\\\\nE[Y^0\\mid D=1]-E[Y^0\\mid D=0]&=0\n\\end{align}\n\\] ランダム化により明らかに選択バイアスを除くことができる。\nまた異質性のある処置効果についても取り除くことができる。\n\\[\nATT-ATU=E[Y^1\\mid D=1]-E[Y^0\\mid  D=1]-E[Y^1\\mid D=0]+E[Y^0\\mid D=0]\n\\]\n\n\n\nRubinはこの種の計算をいつくかの仮定が伴うとして、その仮定をSUTVAとまとめています。具体的には次の３つです。\n\n処置の均質性\n処置の外部性がないこと\n\nユニット間で影響しあわないこと\n\n一般均衡効果がないこと",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch04 潜在アウトカム因果モデル"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#平均処置効果",
    "href": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#平均処置効果",
    "title": "ch04 潜在アウトカム因果モデル",
    "section": "",
    "text": "個体ごとの効果を知ることは出来ないので、 平均的な処置効果\\(ATE(Average treatment effect)\\)を調べることにする。\n\\[\n\\begin{align}\nATE &= E[\\delta_i]\\\\\n&=E[Y_i^1 - Y_i^0] \\\\\n&=E[Y_i^1] - E[Y_i^0]\n\\end{align}\n\\]\nATEを考えることで処置効果を推定することができる。 ただし、いずれにせよあるユニットにはどちらかしか観測できないのに注意する。\n次に関心があるのは処置群の平均処置効果である\\(ATT(average treatment effect for the treated group)\\)である。処置群においても、処置効果はユニットごとに異なるので、平均的にどの程度の効果があるのかを知りたいということになる。\n\\[\n\\begin{align}\nATT &= E[\\delta_i\\mid D_i = 1]\\\\\n&=E[Y_i^1 - Y_i^0\\mid D_i = 1] \\\\\n&=E[Y_i^1\\mid D_i = 1] - E[Y_i^0\\mid D_i = 1]\n\\end{align}\n\\]\n最期に関心があるのは、コントロール群、または未処理群に対する平均処置効果\\(ATU(average treatement effect for the untreated)\\)である。\n\\[\n\\begin{align}\nATU &= E[\\delta_i\\mid D_i = 0]\\\\\n&=E[Y_i^1 - Y_i^0\\mid D_i = 0] \\\\\n&=E[Y_i^1\\mid D_i = 0] - E[Y_i^0\\mid D_i = 0]\n\\end{align}\n\\]\n研究ではこれらの効果を 観測できる情報からどのように推定するのかが重要となる。\nところで, \\(ATE\\)の推定値として２つの処置群の単純差(Simple difference in mean outcomes: SDO)を考える。この値はデータから推定できる値である。\n\\[\nATE =　E[Y_i^1\\mid D=1] - E[Y_i^0\\mid D=0]\n\\]\nこの統計処理自体は正しいものの、誤解を招きかねないことには注意が必要である。 処置の割り当てがランダムでないときには、処置群とコントロール群に根本的な差が生じ津ことが知られている。\n\\[\n\\begin{align}\nE[Y_i^1\\mid D_i = 0] - E[Y_i^0\\mid D_i = 0] &= ATE + E[Y^0\\mid D=1]-E[Y^0\\mid D=0]+(1-\\pi)(ATT - ATU)\n\\end{align}\n\\]\n上記の結果は下の数式を整理したものである。\n\\[\n\\begin{align}\nATE&=\\pi ATT + (1-\\pi)ATU\\\\\n&=\\pi E[Y^1\\mid D=1]-\\pi E[Y^0\\mid D = 1] + (1-\\pi)E[Y^1\\mid D=0]-(1-\\pi)E[Y^0\\mid D = 0]\\\\\n&=\\{\\pi E[Y^1\\mid D=1]+(1-\\pi)E[Y^1\\mid D=0]\\}-{\\pi E[Y^0\\mid D = 1]+(1-\\pi)E[Y^0\\mid D = 0]}\n\\end{align}\n\\]\n\\(\\pi\\)は処置を受けた人の割合である。第一式は\\(ATT\\)の定義から、 処置を受けた人、受けていない人の処置効果に分会しているということになる。\nこれを見ると、単純な差にはATEだけでなく２つの項がある。 まずは次の式は選択バイアスと呼ばれるものである。 処置を受けていない場合の平均的なアウトカムが、 処置を受けているかどうかで変わるのでこのように呼ばれる。\n\\[\nE[Y^0\\mid D=1]-E[Y^0\\mid D=0]\n\\]\n次の式は異質性のある処置効果にともなうバイアスである。\n\\[\n(1-\\pi)(ATT - ATU)\n\\]\n上記２つの項は反実仮想を含んでいるため計算することが出来ない。 いずれにせよバイアスを含んでいることはわかる。",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch04 潜在アウトカム因果モデル"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#独立性の仮定",
    "href": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#独立性の仮定",
    "title": "ch04 潜在アウトカム因果モデル",
    "section": "",
    "text": "SDOを使ってATEを推定するにあたり最も信頼できるのは、 処置を受けるのかどうかとアウトカムが独立であるときである。\n\\[\n(Y^1, Y^0) \\mathop{\\perp\\!\\!\\!\\!\\perp} D\n\\]\nランダム化によって実現したいのは、次の状態である。\n\\[\n\\begin{align}\nE[Y^1\\mid D=1]-E[Y^1\\mid D=0]&=0\\\\\nE[Y^0\\mid D=1]-E[Y^0\\mid D=0]&=0\n\\end{align}\n\\] ランダム化により明らかに選択バイアスを除くことができる。\nまた異質性のある処置効果についても取り除くことができる。\n\\[\nATT-ATU=E[Y^1\\mid D=1]-E[Y^0\\mid  D=1]-E[Y^1\\mid D=0]+E[Y^0\\mid D=0]\n\\]",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch04 潜在アウトカム因果モデル"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#sutva",
    "href": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#sutva",
    "title": "ch04 潜在アウトカム因果モデル",
    "section": "",
    "text": "Rubinはこの種の計算をいつくかの仮定が伴うとして、その仮定をSUTVAとまとめています。具体的には次の３つです。\n\n処置の均質性\n処置の外部性がないこと\n\nユニット間で影響しあわないこと\n\n一般均衡効果がないこと",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch04 潜在アウトカム因果モデル"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch02_確率と回帰の概要.html",
    "href": "contents/books/02_因果推論ミックステープ/ch02_確率と回帰の概要.html",
    "title": "ch02 確率と回帰の概要",
    "section": "",
    "text": "Note\n\n\n\nランダム過程とは、何度も繰り返し可能であり、そのたびに異なるアウトカムが得られる過程である。 標本空間とはランダム過程における起こり得るアウトカムの集合である。\n\n\n\n\n\\(x\\)の値ごとに分割した母集団の誤差項の期待値に関する、次のような仮定がある。\n$$\n(u x) = (u)\n$$ さらに、回帰分析における自明な仮定として次がある。\n\\[\n\\textrm{E}(u) = 0\n\\]\nよって、回帰分析の文脈でいえば説明変数で条件付けることで、誤差項の期待値はすべて０となる、という解釈になる。\n\n\n\n\n\n\nNote\n\n\n\n残差と誤差項は異なる。残差とは、標本値と推定値の差であり、誤差項とはアウトカムに影響を与える要因のうちモデルが補足できない全て要因を含んだ値である。実態としては残差は計算することが出来るが、誤差項は含めることができない。\n\n\n\n\nCode\nset.seed(1)\ntb &lt;- tibble(\nx = rnorm(10000),\nu = rnorm(10000),\ny = 5.5*x + 12*u\n)\n\nreg_tb &lt;- tb %&gt;%\nlm(y ~ x, .) %&gt;%\nprint()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, data = .)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            x  \n#&gt;    -0.04991      5.55690\nreg_tb$coefficients\n#&gt; (Intercept)           x \n#&gt; -0.04990882  5.55690164\ntb &lt;- tb %&gt;%\nmutate(\nyhat1 = predict(lm(y ~ x, .)),\nyhat2 = 0.0732608 + 5.685033*x,\nuhat1 = residuals(lm(y ~ x, .)),\nuhat2 = y - yhat2\n)\nsummary(tb[-1:-3])\n#&gt;      yhat1               yhat2              uhat1              uhat2         \n#&gt;  Min.   :-20.45096   Min.   :-20.7982   Min.   :-51.5275   Min.   :-51.5247  \n#&gt;  1st Qu.: -3.79189   1st Qu.: -3.7550   1st Qu.: -8.1520   1st Qu.: -8.2751  \n#&gt;  Median : -0.13842   Median : -0.0173   Median : -0.1727   Median : -0.3147  \n#&gt;  Mean   : -0.08624   Mean   :  0.0361   Mean   :  0.0000   Mean   : -0.1223  \n#&gt;  3rd Qu.:  3.71578   3rd Qu.:  3.9258   3rd Qu.:  7.9778   3rd Qu.:  7.8506  \n#&gt;  Max.   : 21.12342   Max.   : 21.7348   Max.   : 44.7176   Max.   : 44.4416\ntb %&gt;%\nlm(y ~ x, .) %&gt;%\nggplot(aes(x=x, y=y)) +\nggtitle(\"OLS Regression Line\") +\ngeom_point(size = 0.05, color = \"black\", alpha = 0.5) +\ngeom_smooth(method = lm, color = \"black\") +\nannotate(\"text\", x = -1.5, y = 30, color = \"red\",\nlabel = paste(\"Intercept = \", -0.0732608)) +\nannotate(\"text\", x = 1.5, y = -30, color = \"blue\",\nlabel = paste(\"Slope =\", 5.685033))\n#&gt; `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch02 確率と回帰の概要"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch02_確率と回帰の概要.html#平均独立",
    "href": "contents/books/02_因果推論ミックステープ/ch02_確率と回帰の概要.html#平均独立",
    "title": "ch02 確率と回帰の概要",
    "section": "",
    "text": "\\(x\\)の値ごとに分割した母集団の誤差項の期待値に関する、次のような仮定がある。\n$$\n(u x) = (u)\n$$ さらに、回帰分析における自明な仮定として次がある。\n\\[\n\\textrm{E}(u) = 0\n\\]\nよって、回帰分析の文脈でいえば説明変数で条件付けることで、誤差項の期待値はすべて０となる、という解釈になる。\n\n\n\n\n\n\nNote\n\n\n\n残差と誤差項は異なる。残差とは、標本値と推定値の差であり、誤差項とはアウトカムに影響を与える要因のうちモデルが補足できない全て要因を含んだ値である。実態としては残差は計算することが出来るが、誤差項は含めることができない。\n\n\n\n\nCode\nset.seed(1)\ntb &lt;- tibble(\nx = rnorm(10000),\nu = rnorm(10000),\ny = 5.5*x + 12*u\n)\n\nreg_tb &lt;- tb %&gt;%\nlm(y ~ x, .) %&gt;%\nprint()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, data = .)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            x  \n#&gt;    -0.04991      5.55690\nreg_tb$coefficients\n#&gt; (Intercept)           x \n#&gt; -0.04990882  5.55690164\ntb &lt;- tb %&gt;%\nmutate(\nyhat1 = predict(lm(y ~ x, .)),\nyhat2 = 0.0732608 + 5.685033*x,\nuhat1 = residuals(lm(y ~ x, .)),\nuhat2 = y - yhat2\n)\nsummary(tb[-1:-3])\n#&gt;      yhat1               yhat2              uhat1              uhat2         \n#&gt;  Min.   :-20.45096   Min.   :-20.7982   Min.   :-51.5275   Min.   :-51.5247  \n#&gt;  1st Qu.: -3.79189   1st Qu.: -3.7550   1st Qu.: -8.1520   1st Qu.: -8.2751  \n#&gt;  Median : -0.13842   Median : -0.0173   Median : -0.1727   Median : -0.3147  \n#&gt;  Mean   : -0.08624   Mean   :  0.0361   Mean   :  0.0000   Mean   : -0.1223  \n#&gt;  3rd Qu.:  3.71578   3rd Qu.:  3.9258   3rd Qu.:  7.9778   3rd Qu.:  7.8506  \n#&gt;  Max.   : 21.12342   Max.   : 21.7348   Max.   : 44.7176   Max.   : 44.4416\ntb %&gt;%\nlm(y ~ x, .) %&gt;%\nggplot(aes(x=x, y=y)) +\nggtitle(\"OLS Regression Line\") +\ngeom_point(size = 0.05, color = \"black\", alpha = 0.5) +\ngeom_smooth(method = lm, color = \"black\") +\nannotate(\"text\", x = -1.5, y = 30, color = \"red\",\nlabel = paste(\"Intercept = \", -0.0732608)) +\nannotate(\"text\", x = 1.5, y = -30, color = \"blue\",\nlabel = paste(\"Slope =\", 5.685033))\n#&gt; `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch02 確率と回帰の概要"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/index.html",
    "href": "contents/books/01_Rによる実証分析_2e/index.html",
    "title": "Rによる実証分析 2nd",
    "section": "",
    "text": "1 はじめに\n\nRによる実証分析 2eの読書ノートです\n計量経済学をバックグラウンドとして回帰分析の基礎から解説しています\n応用として因果推論についても解説しています\n出版社HP\n著者サポートページ\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch11_操作変数法.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch11_操作変数法.html",
    "title": "ch11 操作変数法",
    "section": "",
    "text": "1 Setup\n\n\n2 はじめに\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch11 操作変数法"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch09_マッチング法.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch09_マッチング法.html",
    "title": "ch09 マッチング法",
    "section": "",
    "text": "1 Setup\n\n\n2 計算練習\nRを使って実際にマッチング法から因果分析を行っていく。\n\n\nCode\npath &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap09/wage_training.csv\")\nwagedata &lt;- read_csv(path, show_col_types = FALSE)\nwagedata |&gt; \n    head() |&gt; \n    paged_table()\n\n\n\n  \n\n\n\n\nwagea：研修期間後の賃金\nD：研修参加の有無\nyears：経験年数\nwageb：研修以前の賃金\n\n\n\nCode\nwagedata |&gt; \n    summarise(\n        mean_treated = mean(wagea[D == 1]), \n        mean_controlled = mean(wagea[D == 0]), \n        mean_diff = mean_treated - mean_controlled\n    )\n#&gt; # A tibble: 1 × 3\n#&gt;   mean_treated mean_controlled mean_diff\n#&gt;          &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1         25.7            26.8     -1.13\n\n\n上記の結果をみると、研修をおこなった人の方が賃金が低いという結果が出ている。 これは、他の要因がコントロールされている状況下では、その通りであるが実際には、 次でみるように研修参加が経験年数などと相関していることがわかる。 経験年数などは賃金と相関することが高いと考えられるため、 この値を制御した上で比較する必要がある。\n\n\nCode\nlibrary(correlation)\nwagedata |&gt; \n    select(-wagea) |&gt; \n    correlation() |&gt; \n    paged_table()\n\n\n\n  \n\n\n\nそこでマッチングを行うことにする。\n\n\nCode\nm.out &lt;- matchit(D ~ years + wageb,\n                 data     = wagedata,\n                 method   = \"nearest\",\n                 distance = \"scaled_euclidean\",\n                 replace  = TRUE)\nm.out\n#&gt; A matchit object\n#&gt;  - method: 1:1 nearest neighbor matching with replacement\n#&gt;  - distance: Scaled Euclidean\n#&gt;  - number of obs.: 800 (original), 362 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: years, wageb\n\n\nバランスさせる前と後で統計量が変わっていることがわかる。 バランスさせると統計量が非常に近くなっていることがわかる。 注意点としてはUnmatchedなデータが多数発生してることは忘れないことにする。\nStd. Mean DiffというのはMean TreatedとMeaan Controlの差を、 treatedの標準偏差で割った値である。\n\n\nCode\nm.out |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = D ~ years + wageb, data = wagedata, method = \"nearest\", \n#&gt;     distance = \"scaled_euclidean\", replace = TRUE)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;       Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\n#&gt; years        8.2311       10.9377         -0.4970     0.6556    0.1044   0.1722\n#&gt; wageb       24.4874       26.6637         -0.4734     0.6734    0.0989   0.2094\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;       Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\n#&gt; years        8.2311        8.2101          0.0039     1.0147    0.0044   0.0210\n#&gt; wageb       24.4874       24.4958         -0.0018     0.9905    0.0008   0.0042\n#&gt;       Std. Pair Dist.\n#&gt; years          0.0239\n#&gt; wageb          0.0037\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All            562.       238\n#&gt; Matched (ESS)   88.78     238\n#&gt; Matched        124.       238\n#&gt; Unmatched      438.         0\n#&gt; Discarded        0.         0\n\n\n\n\nCode\nwagedata |&gt; group_by(D) |&gt; summarise(across(c(years), .fns = list(mean = mean, sd = sd)))\n#&gt; # A tibble: 2 × 3\n#&gt;       D years_mean years_sd\n#&gt;   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     0      10.9      6.73\n#&gt; 2     1       8.23     5.45\n\n\nStd. Mean Diffの絶対値をプロットしたものをラブプロットという。 ラブプロットでマッチングがうまくいっているのかどうかを判定することができる。\n\n\nCode\nm.out |&gt; \n    summary() |&gt; \n    plot(xlim = c(0, 1.5))\n\n\n\n\n\n\n\n\n\nマッチングに使われたデータは次である。重複については重みで管理されているみたいです。 このあたりは詳しい使い方を調べる必要がある。\n\n\nCode\nmatch.data(m.out) \n#&gt; # A tibble: 362 × 7\n#&gt;    wagea     D years wageb  educ female weights\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1    19     1     4    18    17      1   1    \n#&gt;  2    20     1     2    18    15      1   1    \n#&gt;  3    36     1    14    34    14      1   1    \n#&gt;  4    18     0     4    18    11      1   2.61 \n#&gt;  5    29     0    19    29    15      0   0.521\n#&gt;  6    21     1     3    20    18      1   1    \n#&gt;  7    31     0    13    31    15      1   1.04 \n#&gt;  8    26     0    13    26    14      0   2.61 \n#&gt;  9    35     0    19    34    15      1   0.521\n#&gt; 10    33     0    13    33    15      1   0.521\n#&gt; # ℹ 352 more rows\n\n\n\n\nCode\nmatch.data(m.out) |&gt; \n    group_by(D) |&gt; \n    skim() |&gt; \n    as_tibble() |&gt; \n    paged_table()\n\n\n\n  \n\n\n\nこの状態であれば通常の回帰分析を使うことで対象を推定することができる。\n\n\nCode\nmatch.data(m.out) |&gt; \n    lm(wagea ~ D + years + wageb, educ, female, data = _, weigths = weights) |&gt; \n    tidy()\n#&gt; Warning: In lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, \n#&gt;     ...) :\n#&gt;  extra argument 'weigths' will be disregarded\n#&gt; # A tibble: 4 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.48    0.209        7.10 7.67e- 12\n#&gt; 2 D             -0.386   0.0961      -4.01 7.40e-  5\n#&gt; 3 years          0.142   0.00948     14.9  8.02e- 39\n#&gt; 4 wageb          0.916   0.0102      89.8  1.48e-232\n\n\nまた、傾向スコアによるマッチングも行える。\n\n\nCode\nm.out &lt;- matchit(D ~ years + wageb,\n                 data     = wagedata,\n                 method   = \"nearest\",\n                 distance = \"glm\",\n                 replace  = TRUE)\n\nm.out %&gt;% \n  match.data() %&gt;% \n  lm(wagea ~ D,\n       data = .,\n     weights = weights) %&gt;% \n  tidy()\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)   25.1       0.369     68.0  3.92e-221\n#&gt; 2 D              0.613     0.479      1.28 2.01e-  1\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch09 マッチング法"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#欠落変数",
    "href": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#欠落変数",
    "title": "ch07 内生変数と外生変数",
    "section": "2.1 欠落変数",
    "text": "2.1 欠落変数\nもとのモデルは内生変数がない次の状態を考える。この状態であれば通常の回帰分析で推定することができる。\n$$\nY_i = _0 + 1X{1i} + 2X{2i} + _i\n$$\nここで\\(X_2\\)が観測できない状態を考える。\n$$ \\[\\begin{align}\n\nY_i &= \\alpha_0 + \\beta_1X_{1i} + \\eta_i \\\\\n\\alpha_0 &= \\beta_0 + \\beta_2\\textrm{E}[X_{2i}] \\\\\n\\eta_i &= \\beta_2(X_{2i}-\\textrm{E}[X_{2i}])+\\epsilon_i\n\n\n\\end{align}\\] $$\n単回帰モデルで正しく\\(\\beta\\)を推定するためには、\\(X_{i}\\)が誤差項と外生的変数であることが求められる。 下記の結果をみると\\(X_{1i}\\)は外生的であるため欠落したままでは回帰分析は適していない\n\\[\n\\begin{align}\n\\textrm{E}(X_{1i}) &= \\textrm{E}\\{X_{1i}[\\beta_2(X_{2i}-\\textrm{E}[X_{2i}])+\\epsilon_i]\\} \\\\\n&=\\beta_2\\textrm{E}{X_{1i}(X_{2i}-\\textrm{E}[X_{2i}])} + E[X_{1i}\\epsilon_i]\\\\\n&=\\beta_2\\textrm{Cov}(X_{1i}., X_{2i})\n\\end{align}\n\\]",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch07 内生変数と外生変数"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#測定誤差",
    "href": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#測定誤差",
    "title": "ch07 内生変数と外生変数",
    "section": "2.2 測定誤差",
    "text": "2.2 測定誤差\n測定誤差があると、過小バイアスがかかる。ただし符号は変化しない。\n\n\nCode\nn &lt;- 200\ne &lt;- rnorm(n)\nX &lt;- rnorm(n)\nu &lt;- runif(n, -1, 1)\nW &lt;- X + u \nb0 &lt;- 1\nb1 &lt;- 2\nY &lt;- b0 + X * b1 + e\n\nlm(\n    Y ~ W\n)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Y ~ W)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            W  \n#&gt;       1.040        1.463",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch07 内生変数と外生変数"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#同時性",
    "href": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#同時性",
    "title": "ch07 内生変数と外生変数",
    "section": "2.3 同時性",
    "text": "2.3 同時性\nこれも通常の回帰分析では正しく推定することは出来ない。",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch07 内生変数と外生変数"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch05_推測統計の基礎.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch05_推測統計の基礎.html",
    "title": "ch05 推測統計の基礎",
    "section": "",
    "text": "1 Setup\n\n\n2 平均値の検定\n\n\nCode\ndata_path &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap05/distributions.csv\")\nsimdata &lt;- read_csv(data_path, show_col_types = FALSE)\nsimdata\n#&gt; # A tibble: 100 × 2\n#&gt;    distA  distB\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1  2.83  0.816\n#&gt;  2  2.66  0.755\n#&gt;  3  2.64 -0.971\n#&gt;  4  1.31 -0.572\n#&gt;  5  2.29  0.450\n#&gt;  6  1.44  0.848\n#&gt;  7  2.56  0.130\n#&gt;  8  1.30 -0.640\n#&gt;  9  1.39  0.196\n#&gt; 10  1.27  0.817\n#&gt; # ℹ 90 more rows\n\n\n\n\nCode\nsimdata |&gt; \n    summarise(mean_A = mean(distA), var_A = var(distA)) |&gt; \n    mutate(t.value = sqrt(100) / sqrt(var_A) * mean_A)\n#&gt; # A tibble: 1 × 3\n#&gt;   mean_A var_A t.value\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1   2.04 0.373    33.4\n\n\n\n\n3 回帰係数の検定\n線形モデルでシミュレーションして回帰係数の値をみてみる。 次の結果からモデル通りの値が出ていることがわかる。\n\n\nCode\nX &lt;- rnorm(1000, 0, 1)\nY &lt;- 1 + 5 * X + rnorm(1000, 0, 1)\nbeta1 &lt;- lm(Y ~ X)$coefficients[2]\nbeta1\n#&gt;        X \n#&gt; 4.998162\n\n\n上記の推定の流れをモンテカルロシミュレーションする。だいたい5に近い値が出ていることがわかる。\n\n\nCode\nn_simulations &lt;- 1000\nbetas &lt;- replicate(n_simulations, {\n    X &lt;- rnorm(1000, 0, 1)\n    Y &lt;- 1 + 5 * X + rnorm(1000, 0, 1)\n    lm(Y ~ X)$coefficients[2]\n})\n\nhist(betas)\n\n\n\n\n\n\n\n\n\n分析例としてwageデータを使う.\n\n\nCode\nwagedata &lt;- read_csv(here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap03/wage.csv\"), show_col_types = FALSE)\nfit &lt;- \n    wagedata |&gt; \n    lm(log(wage) ~ educ + exper, data = _) \n\nfit |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = log(wage) ~ educ + exper, data = wagedata)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1.93442 -0.26396  0.02404  0.27287  1.42863 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 4.666034   0.063790   73.15   &lt;2e-16 ***\n#&gt; educ        0.093168   0.003612   25.80   &lt;2e-16 ***\n#&gt; exper       0.040657   0.002334   17.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4017 on 3007 degrees of freedom\n#&gt; Multiple R-squared:  0.1813, Adjusted R-squared:  0.1808 \n#&gt; F-statistic:   333 on 2 and 3007 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nconfint(fit) \n#&gt;                  2.5 %     97.5 %\n#&gt; (Intercept) 4.54095799 4.79111090\n#&gt; educ        0.08608627 0.10024978\n#&gt; exper       0.03608017 0.04523456\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch05 推測統計の基礎"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#独立性と相関係数",
    "href": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#独立性と相関係数",
    "title": "ch03 確率論の基礎",
    "section": "2.1 独立性と相関係数",
    "text": "2.1 独立性と相関係数\n無相関であるとういことは、独立を意味しないことの例を示す。\n\n\nCode\nx &lt;- rnorm(100000, 50, 10)\nz &lt;- - ((x - 50) ** 2) / 10\ncor(x, z)\n#&gt; [1] 0.00199626\n\n\n\n\nCode\nplot(x, z)",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch03 確率論の基礎"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#確率密度",
    "href": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#確率密度",
    "title": "ch03 確率論の基礎",
    "section": "2.2 確率密度",
    "text": "2.2 確率密度\n確率変数\\(X\\)がある値を取る確率はゼロになってしまう。そこで、ある値に収束するように、確率関数を微分した確率密度関数を使う。",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch03 確率論の基礎"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#データによる条件付き期待値の推定",
    "href": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#データによる条件付き期待値の推定",
    "title": "ch03 確率論の基礎",
    "section": "2.3 データによる条件付き期待値の推定",
    "text": "2.3 データによる条件付き期待値の推定\n1976年にアメリカ合衆国の男性労働者を対象に実施された賃金調査結果データの1部分を扱う。educが教育年数であり、experは職業経験根数、wageは1時間あたり賃金です。\n\n\nCode\npath &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap03/wage.csv\")\nwage &lt;- read_csv(path, show_col_types = FALSE)\n\nwage |&gt; \n    head() |&gt; \n    paged_table()\n\n\n\n  \n\n\n\n\n\nCode\nsummary(wage)\n#&gt;       educ           exper             wage       \n#&gt;  Min.   : 1.00   Min.   : 0.000   Min.   : 100.0  \n#&gt;  1st Qu.:12.00   1st Qu.: 6.000   1st Qu.: 394.2  \n#&gt;  Median :13.00   Median : 8.000   Median : 537.5  \n#&gt;  Mean   :13.26   Mean   : 8.856   Mean   : 577.3  \n#&gt;  3rd Qu.:16.00   3rd Qu.:11.000   3rd Qu.: 708.8  \n#&gt;  Max.   :18.00   Max.   :23.000   Max.   :2404.0\n\n\n\n\nCode\nplot(wage)\n\n\n\n\n\n\n\n\n\nこのデータに対して、educ = 12の個人だけを対象にサマリーする。とおもったけど面倒であるので、ここではすべてを対象にする。 これを見るだけでも、どうやら教育年数により収入の平均値が変化していそうなことがわかる。\n\n\nCode\nwage_summary &lt;- \n    wage |&gt; \n    group_by(educ) |&gt; \n    skim() |&gt; \n    as_tibble() |&gt; \n    filter(skim_variable == \"wage\")\n\nwage_summary |&gt; paged_table()\n\n\n\n  \n\n\n\n\n\nCode\nwage_summary |&gt; \n    ggplot(aes(x = educ, y = numeric.mean)) +\n    geom_linerange(aes(ymin = numeric.p25, ymax = numeric.p75), lwd = 2) + \n    geom_point(color = \"red\", size = 5)",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch03 確率論の基礎"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch01_回帰分析の目的.html#身長と体重",
    "href": "contents/books/01_Rによる実証分析_2e/ch01_回帰分析の目的.html#身長と体重",
    "title": "ch01 回帰分析の目的",
    "section": "2.1 身長と体重",
    "text": "2.1 身長と体重\nある年代の人の身長と体重をプロットとすると「正の相関」が見られる。 一方で、この相関からは「体重を増やせば身長が伸びる」という因果関係を得ることはできない。 よく使われる例であるが、これを別の言い方に変えると、相関はマクロな指標のため入力も出力も統計的であるが、 因果は個別の人についても適用される。\n\n\n\n\n\n\nNote\n\n\n\nある集団の平均的な体重が増えるとも身長も増えるのが相関であり、ある人のことについて言及できるのが因果である",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch01 回帰分析の目的"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch01_回帰分析の目的.html#疑似相関",
    "href": "contents/books/01_Rによる実証分析_2e/ch01_回帰分析の目的.html#疑似相関",
    "title": "ch01 回帰分析の目的",
    "section": "2.2 疑似相関",
    "text": "2.2 疑似相関\n次のグラフは、一見因果関係がありそうな２つの支出であるが、これは経済自体が成長している時期であるため、 どのような指標とも医療への支出額が相関関係をもつ。 統計的には「所得」という因子が交絡しているということになる。\n\ndata(\"USPersonalExpenditure\")\nUSPersonalExpenditure &lt;-\n    USPersonalExpenditure |&gt;\n    t() |&gt; \n    as.data.frame() \n\nUSPersonalExpenditure |&gt; \n    ggplot(aes(x = `Food and Tobacco`, y = `Medical and Health`)) +\n    geom_path() + \n    geom_point(size = 10) + \n    labs(\n        x = \"食料品およびタバコへの支出総額(Food and tobacco)\", \n        y = \"医療および健康への支出総額\"\n    ) \n\n\n\n\n\n\n\n\nその他のカラムとの関係をみてみる。\n\n\nCode\npairs  &lt;- combn(syms(names(USPersonalExpenditure)), 2, simplify = FALSE)\ngraphs &lt;- \n    pairs |&gt; \n    map(\\(x) {\n        ggplot(USPersonalExpenditure, aes(!!x[[1]], !!x[[2]])) + \n            geom_path() + \n            geom_point(size = 5)\n    }) |&gt; \n    list_modify(ncol = 2)\n\ndo.call(grid.arrange, graphs)\n\n\n\n\n\n\n\n\n\nこのように一般にはデータからだけでは因果関係に言及することは困難である。 データ分析のまえに事前にフィールドワークや文献調査をおこない信頼できる因果関係を仮説立てた上で、その仮説をモデルにより検証する。\nただし近年では古典的なビッグデータを使った因果特定の研究もある。",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch01 回帰分析の目的"
    ]
  },
  {
    "objectID": "contents/libs/sf/working.html",
    "href": "contents/libs/sf/working.html",
    "title": "sf",
    "section": "",
    "text": "sfはRでGISを扱うためのパッケージです。\nsfはSimple Featuresの略で、空間データを扱うための標準的なデータモデルです。\nsfは、Rのtidyverseとの親和性が高く、dplyrのような文法で空間データを扱うことができます。",
    "crumbs": [
      "libs",
      "sf"
    ]
  },
  {
    "objectID": "contents/libs/sf/working.html#idw",
    "href": "contents/libs/sf/working.html#idw",
    "title": "sf",
    "section": "2.1 IDW",
    "text": "2.1 IDW\nデータを作成する。\n\n\nCode\nlocations &lt;- \n  tibble(\n    longitude = c(10, 10.5, 11),\n    latitude = c(10, 14.5, 18),\n    measurement = c(100, 150, 120)\n  ) |&gt;\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nregion &lt;- st_as_sf(data.frame(\n  x = c(10, 10, 20, 20),\n  y = c(10, 20, 20, 10)\n), coords = c(\"x\", \"y\"), crs = 4326)\n\n\n逆距離補間をおこなう。\n\n\nCode\ngrid &lt;- st_make_grid(region, what = \"centers\")\nidw_result &lt;- idw(formula = measurement ~ 1, locations, grid)\n#&gt; [inverse distance weighted interpolation]\nidw_result\n#&gt; Simple feature collection with 100 features and 2 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 10.5 ymin: 10.5 xmax: 19.5 ymax: 19.5\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    var1.pred var1.var          geometry\n#&gt; 1   101.6566       NA POINT (10.5 10.5)\n#&gt; 2   106.8246       NA POINT (11.5 10.5)\n#&gt; 3   112.7551       NA POINT (12.5 10.5)\n#&gt; 4   116.9918       NA POINT (13.5 10.5)\n#&gt; 5   119.5808       NA POINT (14.5 10.5)\n#&gt; 6   121.1055       NA POINT (15.5 10.5)\n#&gt; 7   122.0098       NA POINT (16.5 10.5)\n#&gt; 8   122.5585       NA POINT (17.5 10.5)\n#&gt; 9   122.9007       NA POINT (18.5 10.5)\n#&gt; 10  123.1201       NA POINT (19.5 10.5)\n\n\n\n\nCode\nplot(\n  st_make_grid(\n    region, \n    cellsize = 1,\n    what = \"polygons\")\n)\nplot(select(idw_result, var1.pred), add = TRUE)\nplot(locations, add = TRUE, col = \"red\", cex = 3, pch = 19)\n\n\n\n\n\n\n\n\n\nラスタ化する.\n\n\nCode\nplot(\n  rasterize(\n    idw_result, \n    rast(\n      extent = st_bbox(region), \n      resolution = 1\n    ),\n    field = \"var1.pred\"\n  )\n)\n\n\n\n\n\n\n\n\n\n距離の次数を変更してみる。\n\n\nCode\n# IDW補間の実行、距離のべき乗を3に設定\nidw_result_power3 &lt;- idw(formula = measurement ~ 1, locations, grid, idp = 3)\n#&gt; [inverse distance weighted interpolation]\nidw_result_power3\n#&gt; Simple feature collection with 100 features and 2 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 10.5 ymin: 10.5 xmax: 19.5 ymax: 19.5\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    var1.pred var1.var          geometry\n#&gt; 1   100.2868       NA POINT (10.5 10.5)\n#&gt; 2   102.7593       NA POINT (11.5 10.5)\n#&gt; 3   108.0653       NA POINT (12.5 10.5)\n#&gt; 4   113.3959       NA POINT (13.5 10.5)\n#&gt; 5   117.2178       NA POINT (14.5 10.5)\n#&gt; 6   119.6376       NA POINT (15.5 10.5)\n#&gt; 7   121.1204       NA POINT (16.5 10.5)\n#&gt; 8   122.0318       NA POINT (17.5 10.5)\n#&gt; 9   122.6014       NA POINT (18.5 10.5)\n#&gt; 10  122.9651       NA POINT (19.5 10.5)",
    "crumbs": [
      "libs",
      "sf"
    ]
  },
  {
    "objectID": "contents/libs/sf/working.html#st_make_grid",
    "href": "contents/libs/sf/working.html#st_make_grid",
    "title": "sf",
    "section": "3.1 st_make_grid",
    "text": "3.1 st_make_grid\n\n空間データをグリッドに変換する関数です。\n以下のように、グリッドのセルの大きさを指定することができます。\n\n\n\nCode\n# 例として、ある地域のポリゴンを定義\nregion &lt;- st_as_sf(data.frame(\n  x = c(10, 10, 20, 20),\n  y = c(10, 20, 20, 10)\n), coords = c(\"x\", \"y\"), crs = 4326)\n\n\n\n\nCode\ngrid &lt;- st_make_grid(region, cellsize = 1, what = \"polygons\")\ngrid\n#&gt; Geometry set for 100 features \n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 10 ymin: 10 xmax: 20 ymax: 20\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 5 geometries:\n#&gt; POLYGON ((10 10, 11 10, 11 11, 10 11, 10 10))\n#&gt; POLYGON ((11 10, 12 10, 12 11, 11 11, 11 10))\n#&gt; POLYGON ((12 10, 13 10, 13 11, 12 11, 12 10))\n#&gt; POLYGON ((13 10, 14 10, 14 11, 13 11, 13 10))\n#&gt; POLYGON ((14 10, 15 10, 15 11, 14 11, 14 10))\n\n\n\n\nCode\nplot(grid)\nplot(region, add = TRUE)\n# ポイントデータを作成することもできる\nplot(\n  st_make_grid(region, cellsize = 1, what = \"centers\"),\n  col = \"red\", \n  add = TRUE\n)\nplot(\n  st_make_grid(region, cellsize = 1, what = \"corners\"),\n  col = \"blue\", \n  add = TRUE\n)\n\n\n\n\n\n\n\n\n\nセルサイズはXYで変更することもできる。\n\n\nCode\nplot(grid)\nplot(\n  st_make_grid(region, cellsize = c(1, 2), what = \"centers\"),\n  col = \"red\", \n  add = TRUE\n)",
    "crumbs": [
      "libs",
      "sf"
    ]
  },
  {
    "objectID": "contents/libs/igraph/index.html",
    "href": "contents/libs/igraph/index.html",
    "title": "igraph",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/igraph\")\n\n\n\n\nCode\nbox::use(\n  igraph[...],\n  ggplot2[...],\n  cowplot[...], \n  showtext[showtext_auto], \n  sysfonts[font_add_google],\n)\nlibrary(cowplot)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 はじめに\nigraphはグラフ構造の解析を行うためのオープンソースのパッケージである。 RだけでなくPython, Mathmaticaでも使うことができる。\nここでは、Rでのigraphの使い方を学ぶこととする。\n\nR interface\nR reference\nigraph\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/libs/ggmosaic/working.html",
    "href": "contents/libs/ggmosaic/working.html",
    "title": "ggmosaic",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/ggmosaic\")\n\n\n\n\nCode\nlibrary(ggmosaic)\nlibrary(showtext)\nlibrary(dplyr)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 はじめに\nggmosaicで美しいモザイクプロットを作成することができる。\nHPが再発サイトである。\n\n\n2 Example\n\n\nCode\nhappy %&gt;% \n  mutate(finrela = forcats::fct_recode(finrela,\n    \"far below     \" = \"far below average\",\n    \"    below\" = \"below average\",\n    \"average\" = \"average\",\n    \"above    \" = \"above average\", \n    \"l\\n   far above\" = \"far above average\")) %&gt;% \n  ggplot() +\n  geom_mosaic(aes(x = product(finrela), fill=health), show.legend = FALSE) +\n  theme_mosaic() +\n  scale_fill_manual(values = c(\"#4575B4\", \"#ABD9E9\", \"#FEE090\", \"#F46D43\"))\n#&gt; Warning: `unite_()` was deprecated in tidyr 1.2.0.\n#&gt; ℹ Please use `unite()` instead.\n#&gt; ℹ The deprecated feature was likely used in the ggmosaic package.\n#&gt;   Please report the issue at &lt;https://github.com/haleyjeppson/ggmosaic&gt;.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "libs",
      "ggmosaic"
    ]
  },
  {
    "objectID": "contents/libs/gganimate/working.html",
    "href": "contents/libs/gganimate/working.html",
    "title": "gganimate",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/gganimate\")\n\n\n\n1 はじめに\n\nGithub\n\n\n\n2 An Example\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "libs",
      "gganimate"
    ]
  },
  {
    "objectID": "contents/libs/ComplexHeatmap/working.html",
    "href": "contents/libs/ComplexHeatmap/working.html",
    "title": "ComplexHeatmap",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/ComplexHeatmap\")\n\n\n\n\nCode\nlibrary(ComplexHeatmap)\nlibrary(showtext)\nlibrary(dplyr)\nlibrary(ggplot2)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 はじめに\nComplexHeatmapで美しいモザイクプロットを作成することができる。\n\n\n2 Example\n\n\nCode\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"ComplexHeatmap\")\nlibrary(ComplexHeatmap)\n\n\n\n\nCode\n# サンプルデータの生成\nset.seed(123)\ndata &lt;- matrix(rnorm(100), nrow=10, ncol=10)\n\n# ヒートマップの作成\nHeatmap(data, \n        name = \"my_heatmap\", \n        cluster_rows = TRUE, \n        cluster_columns = TRUE,\n        show_row_dend = TRUE, \n        show_column_dend = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "libs",
      "ComplexHeatmap"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html",
    "title": "Working with Layouts and Composition",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggfittext\",\n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Working with Layouts and Composition"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#discrete",
    "href": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#discrete",
    "title": "Working with Layouts and Composition",
    "section": "2.1 discrete",
    "text": "2.1 discrete\n\n\nCode\npal &lt;- c(\"#3c89d9\", \"#1ec99b\", \"#F7B01B\", \"#a26e7c\")\n\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = season)\n  ) +\n  geom_point() +\n  scale_color_manual(values = pal)",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Working with Layouts and Composition"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#continuous",
    "href": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#continuous",
    "title": "Working with Layouts and Composition",
    "section": "2.2 Continuous",
    "text": "2.2 Continuous\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = humidity)\n  ) +\n  geom_point() +\n  scale_color_viridis_c() + \n  theme(\n      legend.position = \"bottom\", \n      legend.justification = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = humidity)\n  ) +\n  geom_point() +\n  scale_color_viridis_c(\n      guide = \"colorbar\"\n  ) + \n  theme(\n      legend.position = c(.25, .85), \n      legend.direction = \"horizontal\"\n  )\n\n\n\n\n\n\n\n\n\n次は凡例を離散的に表現しているが、 プロットの色自体は色分けされていないことに注意する。\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = humidity)\n  ) +\n  geom_point() +\n  scale_color_viridis_c(\n      guide = \"colorsteps\",\n  ) + \n  theme(\n      legend.position = c(.25, .85), \n      legend.direction = \"horizontal\"\n  )\n\n\n\n\n\n\n\n\n\n次はプロットの色自体も離散的にしている。つまり、viridis_bを使っている。\n\n\nCode\nbikes |&gt; \n    ggplot(\n        aes(x = temp_feel, y = count, color = humidity)\n    ) + \n    geom_point() + \n    scale_color_viridis_b(\n        guide = \"colorsteps\"\n    ) + \n    theme(\n        legend.position = c(.25, .85), \n        legend.direction = \"horizontal\"\n    )\n\n\n\n\n\n\n\n\n\n細かく凡例を調整したいときには次のようにguide_colorstepsを使う。\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = humidity)\n  ) +\n  geom_point() +\n  scale_color_viridis_b(\n    breaks = seq(30, 90, 10),\n    limits = c(30, 100),\n    guide = guide_colorsteps(\n      title.position = \"top\",\n      title.hjust = .5,\n      show.limits = TRUE,\n      frame.colour = \"black\",\n      frame.linewidth = 3,\n      barwidth = unit(8, \"lines\"), \n      ticks.linewidth = 1\n    )\n  ) +\n  theme(\n    legend.position = c(.25, .85),\n    legend.direction = \"horizontal\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = humidity)\n  ) +\n  geom_point() +\n  scale_color_viridis_c(\n    breaks = 3:10*10,\n    limits = c(30, 100),\n    guide = guide_colorbar(\n      title.position = \"top\",\n      title.hjust = .5,\n      ticks = FALSE,\n      barwidth = unit(20, \"lines\"),\n      barheight = unit(.6, \"lines\")\n    )\n  ) +\n  theme(\n    legend.position = \"top\"\n  )",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Working with Layouts and Composition"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#key-glyphs",
    "href": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#key-glyphs",
    "title": "Working with Layouts and Composition",
    "section": "2.3 Key Glyphs",
    "text": "2.3 Key Glyphs\n\n\nCode\nggplot(\n    bikes,\n    aes(x = lubridate::week(date),\n        y = count,\n        color = day_night)\n  ) +\n  stat_summary(\n    geom = \"line\",\n    fun = sum,\n    size = 1\n  ) +\n  scale_color_manual(\n    values = c(\"#28A87D\", \"#663399\"),\n    name = NULL\n  ) +\n  theme(\n    legend.text = element_text(size = 16)\n  )\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = lubridate::week(date),\n        y = count,\n        color = day_night)\n  ) +\n  stat_summary(\n    geom = \"line\",\n    fun = sum,\n    key_glyph = \"timeseries\",\n    size = 1\n  ) +\n  scale_color_manual(\n    values = c(\"#28A87D\", \"#663399\"),\n    name = NULL\n  ) +\n  theme(\n    legend.text = element_text(size = 16)\n  )",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Working with Layouts and Composition"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#patchwork",
    "href": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#patchwork",
    "title": "Working with Layouts and Composition",
    "section": "3.1 patchwork",
    "text": "3.1 patchwork\n\n\nCode\ntheme_std &lt;- theme_set(theme_minimal(base_size = 18, base_family = \"Noto Sans\"))\ntheme_update(\n  panel.grid = element_blank(),\n  axis.text = element_text(color = \"grey50\", size = 12),\n  axis.title = element_text(color = \"grey40\", face = \"bold\"),\n  axis.title.x = element_text(margin = margin(t = 12)),\n  axis.title.y = element_text(margin = margin(r = 12)),\n  axis.line = element_line(color = \"grey80\", size = .4),\n  legend.text = element_text(color = \"grey50\", size = 12),\n  plot.tag = element_text(size = 40, margin = margin(b = 15)),\n  plot.background = element_rect(fill = \"white\", color = \"white\")\n)\n#&gt; Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n#&gt; ℹ Please use the `linewidth` argument instead.\n\nbikes_sorted &lt;-\n  bikes %&gt;%\n  filter(!is.na(weather_type)) %&gt;%\n  group_by(weather_type) %&gt;%\n  mutate(sum = sum(count)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    weather_type = forcats::fct_reorder(\n      str_to_title(str_wrap(weather_type, 5)), sum\n    )\n  )\n\np1 &lt;- ggplot(\n    bikes_sorted,\n    aes(x = weather_type, y = count, color = weather_type)\n  ) +\n  geom_hline(yintercept = 0, color = \"grey80\", size = .4) +\n  stat_summary(\n    geom = \"point\", fun = \"sum\", size = 12\n  ) +\n  stat_summary(\n    geom = \"linerange\", ymin = 0, fun.max = function(y) sum(y),\n    size = 2, show.legend = FALSE\n  ) +\n  coord_flip(ylim = c(0, NA), clip = \"off\") +\n  scale_y_continuous(\n    expand = c(0, 0), limits = c(0, 8500000),\n    labels = scales::comma_format(scale = .0001, suffix = \"K\")\n  ) +\n  scale_color_viridis_d(\n    option = \"magma\", direction = -1, begin = .1, end = .9, name = NULL,\n    guide = guide_legend(override.aes = list(size = 7))\n  ) +\n  labs(\n    x = NULL, y = \"Sum of reported bike shares\", tag = \"P1\",\n  ) +\n  theme(\n    axis.line.y = element_blank(),\n    axis.text.y = element_text(color = \"grey50\", face = \"bold\",\n                               margin = margin(r = 15), lineheight = .9)\n  )\n\n\np2 &lt;- bikes_sorted %&gt;%\n  filter(season == \"winter\", is_weekend == TRUE, day_night == \"night\") %&gt;%\n  group_by(weather_type, .drop = FALSE) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ggplot(\n      aes(x = weather_type, y = id, color = weather_type)\n    ) +\n    geom_point(size = 4.5) +\n    scale_color_viridis_d(\n      option = \"magma\", direction = -1, begin = .1, end = .9, name = NULL,\n      guide = guide_legend(override.aes = list(size = 7))\n    ) +\n    labs(\n      x = NULL, y = \"Reported bike shares on\\nweekend winter nights\", tag = \"P2\",\n    ) +\n    coord_cartesian(ylim = c(.5, NA), clip = \"off\")\n\n\nmy_colors &lt;- c(\"#cc0000\", \"#000080\")\n\np3 &lt;- bikes %&gt;%\n  group_by(week = lubridate::week(date), day_night, year) %&gt;%\n  summarize(count = sum(count)) %&gt;%\n  group_by(week, day_night) %&gt;%\n  mutate(avg = mean(count)) %&gt;%\n  ggplot(aes(x = week, y = count, group = interaction(day_night, year))) +\n    geom_line(color = \"grey65\", size = 1) +\n    geom_line(aes(y = avg, color = day_night), stat = \"unique\", size = 1.7) +\n    annotate(\n      geom = \"text\", label = c(\"Day\", \"Night\"), color = my_colors,\n      x = c(5, 18), y = c(125000, 29000), size = 8, fontface = \"bold\"\n    ) +\n    scale_x_continuous(breaks = c(1, 1:10*5)) +\n    scale_y_continuous(labels = scales::comma_format()) +\n    scale_color_manual(values = my_colors, guide = \"none\") +\n    labs(\n      x = \"Week of the Year\", y = \"Reported bike shares\\n(cumulative # per week)\", tag = \"P3\",\n    )\n#&gt; `summarise()` has grouped output by 'week', 'day_night'. You can override using\n#&gt; the `.groups` argument.\n\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n(p1 + p2) / p3\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + p2) / p3 + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n((p1 + p2) / p3 & theme(legend.justification = \"top\")) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + p2) / p3 & theme(legend.position = \"none\", plot.background = element_rect(color = \"black\", size = 3))\n#&gt; Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n#&gt; ℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n((p1 + p2) / p3 & theme(legend.position = \"none\")) +\n  plot_layout(heights = c(.2, .1), widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\npicasso &lt;- \"\nAAAAAA#BBBB\nCCCCCCCCC##\nCCCCCCCCC##\"\n(p1 + p2 + p3 & theme(legend.position = \"none\")) + plot_layout(design = picasso)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntheme_std &lt;- theme_set(theme_minimal(base_size = 18, base_family = \"Noto Sans\"))\ntheme_update(\n  panel.grid = element_blank(),\n  axis.text = element_text(color = \"grey50\", size = 12),\n  axis.title = element_text(color = \"grey40\", face = \"bold\"),\n  axis.title.x = element_text(margin = margin(t = 12)),\n  axis.title.y = element_text(margin = margin(r = 12)),\n  axis.line = element_line(color = \"grey80\", size = .4),\n  legend.text = element_text(color = \"grey50\", size = 12),\n  plot.tag = element_text(size = 40, margin = margin(b = 15)),\n  plot.background = element_rect(fill = \"white\", color = \"white\")\n)\n\nbikes_sorted &lt;-\n  bikes %&gt;%\n  filter(!is.na(weather_type)) %&gt;%\n  group_by(weather_type) %&gt;%\n  mutate(sum = sum(count)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    weather_type = forcats::fct_reorder(\n      str_to_title(str_wrap(weather_type, 5)), sum\n    )\n  )\n\np1 &lt;- ggplot(\n    bikes_sorted,\n    aes(x = weather_type, y = count, color = weather_type)\n  ) +\n  geom_hline(yintercept = 0, color = \"grey80\", size = .4) +\n  stat_summary(\n    geom = \"point\", fun = \"sum\", size = 12\n  ) +\n  stat_summary(\n    geom = \"linerange\", ymin = 0, fun.max = function(y) sum(y),\n    size = 2, show.legend = FALSE\n  ) +\n  coord_flip(ylim = c(0, NA), clip = \"off\") +\n  scale_y_continuous(\n    expand = c(0, 0), limits = c(0, 8500000),\n    labels = scales::comma_format(scale = .0001, suffix = \"K\")\n  ) +\n  scale_color_viridis_d(\n    option = \"magma\", direction = -1, begin = .1, end = .9, name = NULL,\n    guide = guide_legend(override.aes = list(size = 7))\n  ) +\n  labs(\n    x = NULL, y = \"Sum of reported bike shares\", tag = \"P1\",\n  ) +\n  theme(\n    axis.line.y = element_blank(),\n    axis.text.y = element_text(color = \"grey50\", face = \"bold\",\n                               margin = margin(r = 15), lineheight = .9)\n  )\n\n\np2 &lt;- bikes_sorted %&gt;%\n  filter(season == \"winter\", is_weekend == TRUE, day_night == \"night\") %&gt;%\n  group_by(weather_type, .drop = FALSE) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ggplot(\n      aes(x = weather_type, y = id, color = weather_type)\n    ) +\n    geom_point(size = 4.5) +\n    scale_color_viridis_d(\n      option = \"magma\", direction = -1, begin = .1, end = .9, name = NULL,\n      guide = guide_legend(override.aes = list(size = 7))\n    ) +\n    labs(\n      x = NULL, y = \"Reported bike shares on\\nweekend winter nights\", tag = \"P2\",\n    ) +\n    coord_cartesian(ylim = c(.5, NA), clip = \"off\")\n\n\nmy_colors &lt;- c(\"#cc0000\", \"#000080\")\n\np3 &lt;- bikes %&gt;%\n  group_by(week = lubridate::week(date), day_night, year) %&gt;%\n  summarize(count = sum(count)) %&gt;%\n  group_by(week, day_night) %&gt;%\n  mutate(avg = mean(count)) %&gt;%\n  ggplot(aes(x = week, y = count, group = interaction(day_night, year))) +\n    geom_line(color = \"grey65\", size = 1) +\n    geom_line(aes(y = avg, color = day_night), stat = \"unique\", size = 1.7) +\n    annotate(\n      geom = \"text\", label = c(\"Day\", \"Night\"), color = my_colors,\n      x = c(5, 18), y = c(125000, 29000), size = 8, fontface = \"bold\"\n    ) +\n    scale_x_continuous(breaks = c(1, 1:10*5)) +\n    scale_y_continuous(labels = scales::comma_format()) +\n    scale_color_manual(values = my_colors, guide = \"none\") +\n    labs(\n      x = \"Week of the Year\", y = \"Reported bike shares\\n(cumulative # per week)\", tag = \"P3\",\n    )\n#&gt; `summarise()` has grouped output by 'week', 'day_night'. You can override using\n#&gt; the `.groups` argument.\n\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n(p1 + p2) / p3\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + p2) / p3 + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n((p1 + p2) / p3 & theme(legend.justification = \"top\")) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + p2) / p3 & theme(legend.position = \"none\", plot.background = element_rect(color = \"black\", size = 3))\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n((p1 + p2) / p3 & theme(legend.position = \"none\")) +\n  plot_layout(heights = c(.2, .1), widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\npicasso &lt;- \"\nAAAAAA#BBBB\nCCCCCCCCC##\nCCCCCCCCC##\"\n(p1 + p2 + p3 & theme(legend.position = \"none\")) + plot_layout(design = picasso)\n\n\n\n\n\n\n\n\n\n\n\nCode\n\ntext &lt;- tibble(\n  x = 0, y = 0, label = \"Lorem ipsum dolor sit amet, **consectetur adipiscing elit**, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation &lt;b style='color:#000080;'&gt;ullamco laboris nisi&lt;/b&gt; ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat &lt;b style='color:#cc0000;'&gt;cupidatat non proident&lt;/b&gt;, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n)\n\npt &lt;- ggplot(text, aes(x = x, y = y)) +\n  ggtext::geom_textbox(\n    aes(label = label),\n    box.color = NA, width = unit(23, \"lines\"),\n    color = \"grey40\", size = 6.5, lineheight = 1.4\n  ) +\n  coord_cartesian(expand = FALSE, clip = \"off\") +\n  theme_void()\n\npt\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + pt) / p3\n\n\n\n\n\n\n\n\n\n\n\nCode\np1 + inset_element(p2, l = .6, b = .1, r = 1, t = .6)\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + inset_element(p2, l = .6, b = .1, r = 1, t = .6) + pt) / p3",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Working with Layouts and Composition"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch04_working_with_labels_and_annotations.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch04_working_with_labels_and_annotations.html",
    "title": "Working with Labels and Annotations",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\n\n\n\n\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 Setup\n\n\nCode\nbikes &lt;- read_csv(\n    here(cur_dir, \"ggplot2-course-data/london-bikes-custom.csv\"), \n    col_types = \"Dcfffilllddddc\"\n)\n\nbikes$season &lt;- forcats::fct_inorder(bikes$season)\n\ntheme_set(\n    theme_light(\n        base_size = 14, \n        base_family = \"Noto Sans\"\n    )\n)\n\nglimpse(bikes)\n#&gt; Rows: 1,454\n#&gt; Columns: 14\n#&gt; $ date         &lt;date&gt; 2015-01-04, 2015-01-04, 2015-01-05, 2015-01-05, 2015-01-…\n#&gt; $ day_night    &lt;chr&gt; \"day\", \"night\", \"day\", \"night\", \"day\", \"night\", \"day\", \"n…\n#&gt; $ year         &lt;fct&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 201…\n#&gt; $ month        &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#&gt; $ season       &lt;fct&gt; winter, winter, winter, winter, winter, winter, winter, w…\n#&gt; $ count        &lt;int&gt; 6830, 2404, 14763, 5609, 14501, 6112, 16358, 4706, 9971, …\n#&gt; $ is_workday   &lt;lgl&gt; FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n#&gt; $ is_weekend   &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#&gt; $ is_holiday   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n#&gt; $ temp         &lt;dbl&gt; 2.166667, 2.791667, 8.958333, 7.125000, 9.000000, 6.70833…\n#&gt; $ temp_feel    &lt;dbl&gt; -0.750000, 2.041667, 7.708333, 5.708333, 6.458333, 4.2083…\n#&gt; $ humidity     &lt;dbl&gt; 95.16667, 93.37500, 81.08333, 79.54167, 80.20833, 77.5833…\n#&gt; $ wind_speed   &lt;dbl&gt; 10.416667, 4.583333, 8.666667, 9.041667, 19.208333, 12.79…\n#&gt; $ weather_type &lt;chr&gt; \"broken clouds\", \"clear\", \"broken clouds\", \"cloudy\", \"bro…\n\n\n\n\n2 Labels + theme\n\n\nCode\n\ng &lt;- ggplot(\n    bikes, \n    aes(x = temp_feel, y = count, color = season)\n) +\n    geom_point(\n        alpha = .5\n    ) + \n    labs(\n        x = \"Feels Like temperature (℉)\", \n        y = \"Reported bike shares\", \n        title = \"TfL bike shareing trends\", \n        subtitle = \"Reporeted bike rents versus Feels Like temperature in London\", \n        caption= \"Data: TfL\", \n        color = \"Season: \", \n        tag = \"1.\"\n    )\n\nplot(g)\n\n\n\n\n\n\n\n\n\n上記のグラフをカスタマイズする.\n\n\nCode\ng + \n    theme(\n        plot.title = element_text(face = \"bold\"), \n        plot.title.position = \"plot\", \n        axis.text = element_text(\n            color = \"#28a87d\", \n            family = \"Noto Sans\", \n            face = \"italic\", \n            hjust = 1, \n            vjust = 0, \n            angle = 45, \n            lineheight = 1.3, \n            margin = margin(10, 0, 20, 0), \n            debug = TRUE\n        ), \n        axis.text.x = element_text(\n            margin = margin(0, 12, -8, 0)\n        ), \n        plot.tag = element_text(\n            margin = margin(0, 12, -8, 0), \n            debug = TRUE\n        )\n    )\n\n\n\n\n\n\n\n\n\n\n\n3 Labels + scale\n\n\nCode\ng &lt;- \n    g + \n    labs(\n        tag = NULL, \n        title = NULL, \n        subtitle = NULL\n    )\n\ng + \n    scale_y_continuous(\n        breaks = 0:4 * 15000, \n        labels = scales::comma_format(\n            suffix = \"\\n bikes shared\"\n        ), \n        name = NULL\n    ) + \n    theme(\n        axis.text.y = element_text(\n            hjust = .5\n        )\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\ng +\n  scale_y_continuous(\n    breaks = 0:4*15000,\n    labels = function(y) y / 1000,\n    name = \"Reported bike shares in thousands\",\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = season, y = count)\n  ) +\n  geom_boxplot() +\n  scale_x_discrete(\n    name = NULL,\n    labels = stringr::str_to_title\n  )\n\n\n\n\n\n\n\n\n\n\n\n4 Labels + element_markdown\nggtextパッケージを使うことで対応することができる.\n\n\nCode\n# install.packages(\"ggtext\")\n\ng +\n  ggtitle(\"**TfL bike sharing trends by _season_**\") +\n  theme(\n    plot.title = ggtext::element_markdown()\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggtext\")\n\ng +\n  ggtitle(\"&lt;b style='font-size:25pt'&gt;TfL&lt;/b&gt; bike sharing trends by &lt;i style='color:#28a87d;'&gt;season&lt;/i&gt;\") +\n  theme(\n    plot.title = ggtext::element_markdown()\n  )\n\n\n\n\n\n\n\n\n\n\n\n5 Labels + facet\n\n\nCode\ncodes &lt;- c(\n  `TRUE` = \"Workday\",\n  `FALSE` = \"Weekend or Holiday\"\n)\n\ng +\n  facet_grid(\n    day_night ~ is_workday,\n    scales = \"free\",\n    space = \"free\",\n    labeller = labeller(\n      day_night = stringr::str_to_title,\n      is_workday = codes\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n6 Handling Long Labels\n\n\nCode\nggplot(\n    bikes,\n    aes(x = stringr::str_wrap(weather_type, 6),\n        y = count)\n  ) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nタイトルを調整することも可能です.\n\n\nCode\ng +\n  ggtitle(\"TfL bike sharing trends in 2015 and 2016 by season for day and night periods\") +\n  theme(\n    plot.title =\n      ggtext::element_textbox_simple(\n          margin = margin(t = 12, b = 12), \n          lineheight = .9, \n          fill = \"grey90\", \n          padding = margin(rep(12, 4)), \n          r = unit(9, \"pt\"),\n          box.color = \"grey40\",  \n          halign = .5, \n          size = 20),\n    plot.title.position = \"plot\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n7 Annotations with annotate\n\n\nCode\nggplot(bikes, aes(humidity, temp)) +\n  geom_point(size = 2, color = \"grey\") +\n  annotate(\n    geom = \"text\",\n    x = c(90, 50),\n    y = c(27.5, 3.5),\n    label = c(\"Text A\", \"Text B\"),\n    color = c(\"black\", \"firebrick\"),\n    size = c(5, 10),\n    fontface = c(\"plain\", \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n\nggplot(bikes, aes(humidity, temp)) +\n  annotate(\n    geom = \"rect\",\n    xmin = -Inf,\n    xmax = 60,\n    ymin = 20,\n    ymax = Inf,\n    fill = \"#663399\"\n  ) +\n  geom_point(size = 2, color = \"grey\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n\nggplot(bikes, aes(humidity, temp)) +\n  geom_point(size = 2, color = \"grey\") +\n  annotate(\n    geom = \"text\",\n    x = 90,\n    y = 27.5,\n    label = \"Some\\nadditional\\ntext\",\n    size = 6,\n    lineheight = .9\n  ) +\n  annotate(\n    geom = \"curve\",\n    x = 90, xend = 82,\n    y = 25, yend = 18.5, \n    curvature = -.5, \n    lwd = 5, \n    arrow = arrow(\n        length = unit(20, \"pt\"), \n        type = \"closed\", \n        ends = \"both\", \n        \n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n8 Annotations with geom\n\n\nCode\nggplot(\n    filter(bikes, temp &gt;= 27),\n    aes(x = humidity, y = temp)\n  ) +\n  geom_point(\n    data = bikes,\n    color = \"grey65\", alpha = .3\n  ) +\n  geom_point(size = 2.5) +\n  geom_text(\n    aes(label = season),\n    nudge_x = .3,\n    hjust = 0\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n\nggplot(\n    filter(bikes, temp &gt;= 27),\n    aes(x = humidity, y = temp,\n        color = season == \"summer\")\n  ) +\n  geom_point(\n    data = bikes,\n    color = \"grey65\", alpha = .3\n  ) +\n  geom_point(size = 2.5) +\n  ggrepel::geom_text_repel(\n    aes(label = str_to_title(season))\n  ) +\n  scale_color_manual(\n    values = c(\"firebrick\", \"black\"),\n    guide = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, temp &gt;= 27),\n    aes(\n        x = humidity, \n        y = temp,\n        color = season == \"summer\")\n  ) +\n  geom_point(\n    data  = bikes,\n    color = \"grey65\", \n    alpha = .3\n  ) +\n  geom_point(size = 2.5) +\n  ggrepel::geom_text_repel(\n    aes(label = str_to_title(season)),\n    ## force to the right\n    xlim = c(NA, 35), \n    hjust = 1\n  ) +\n  scale_color_manual(\n    values = c(\"firebrick\", \"black\"),\n    guide  = \"none\"\n  ) +\n  xlim(25, NA)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, temp &gt; 20 & season != \"summer\"),\n    aes(x = humidity, y = temp,\n        color = season)\n  ) +\n  geom_point(\n    data = bikes,\n    color = \"grey65\", alpha = .3\n  ) +\n  geom_point() +\n  ggforce::geom_mark_rect(\n    aes(label = str_to_title(season))\n  ) +\n  scale_color_brewer(\n    palette = \"Dark2\",\n    guide = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, temp &gt; 20 & season != \"summer\"),\n    aes(x = humidity, y = temp,\n        color = season)\n  ) +\n  geom_point(\n    data = bikes,\n    color = \"grey65\", alpha = .3\n  ) +\n  geom_point() +\n  ggforce::geom_mark_rect(\n    aes(label = str_to_title(season)),\n    expand = unit(5, \"pt\"),\n    radius = unit(0, \"pt\"),\n    con.cap = unit(0, \"pt\"),\n    label.buffer = unit(15, \"pt\"),\n    con.type = \"straight\",\n    label.fill = \"transparent\"\n  ) +\n  scale_color_brewer(\n    palette = \"Dark2\",\n    guide = \"none\"\n  ) +\n  ylim(NA, 35)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = humidity, y = temp,\n        color = season == \"summer\")\n  ) +\n  geom_point(alpha = .4) +\n  ggforce::geom_mark_hull(\n    aes(label = str_to_title(season),\n        filter = season == \"summer\",\n        description = \"June to August\"),\n    expand = unit(10, \"pt\")\n  ) +\n  scale_color_manual(\n    values = c(\"grey65\", \"firebrick\"),\n    guide = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n9 Adding Images\n\n\nCode\nurl &lt;- \"https://d33wubrfki0l68.cloudfront.net/dbb07b06a7b3fe056db386fef0b158cc2fd33cb9/8b491/assets/img/2022conf/logo-rstudio-conf.png\"\nimg &lt;- magick::image_read(url)\nimg &lt;- magick::image_negate(img)\n\nimg\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(bikes, aes(date, temp_feel)) +\n  annotation_custom(\n    grid::rasterGrob(\n      image = img,\n      x = .47,\n      y = 1.15,\n      width = .9\n    )\n  ) +\n  geom_point(color = \"#71a5d4\") +\n  coord_cartesian(clip = \"off\") +\n  theme(\n    plot.margin = margin(90, 10, 10, 10)\n  )\n\n\n\n\n\n\n\n\n\n\n\n10 Exercise 1\nggtextを調べろ\n\n\n11 Exercise 2\n\n\nCode\n\ng &lt;- \n    palmerpenguins::penguins |&gt; \n    filter(complete.cases(pick(everything()))) |&gt; \n    # mutate(body_mass_kg_fct = body_mass_g |&gt; \n    #            set_units(\"g\") |&gt; \n    #            set_units(\"kg\") |&gt; \n    #            round(0) |&gt; \n    #            as.character() |&gt; \n    #            fct_inseq(ordered = TRUE)) |&gt; \n    ggplot() + \n    geom_point(\n        aes(\n            x = bill_length_mm, \n            y = bill_depth_mm, \n            color = species, \n            size = body_mass_g / 1000\n        ), \n        alpha = .1\n    ) + \n    labs(\n        title = \"Bill dimensions fo brush-tailed penguins Pygoscelis spec\", \n        x = \"Bill length(&lt;i&gt;mm&lt;/i&gt;)\", \n        y = \"Bill depth(&lt;i&gt;mm&lt;/i&gt;)\", \n        caption = \"Horst AM, Hill AP, Gorman KB(2020). palmerpenguins R package version 0.1.0\", \n        size = \"Body mass\"\n    ) + \n    scale_y_continuous(\n        breaks = seq(12.5, 22.5, 2.5), \n        labels = \\(x) format(x, digits = 3), \n        limits = c(12.5, 22.5), \n        expand = expansion(0, 0),\n    ) + \n    scale_x_continuous(\n        breaks = seq(30, 60, 5), \n        limits = c(30, 60), \n        expand = expansion(0, 0),\n    ) +\n    scale_color_manual(\n        values =  c(\"Adelie\" = \"red\", \"Chinstrap\" = \"blue\", \"Gentoo\" = \"green\"), \n        guide = \"none\"\n    ) + \n    scale_size_continuous(\n        breaks = 3:6,\n        labels = \\(x) paste(x, \"kg\")\n    ) + \n    theme(\n        panel.grid.minor = element_blank(), \n        plot.title.position = \"plot\", \n        plot.title = element_text(size = rel(1.3), margin = margin(b = 1, unit = \"line\")), \n        plot.caption = element_text(hjust = 1, color = \"grey80\"), \n        plot.caption.position = \"plot\", \n        axis.title.y = element_markdown(\n            size = rel(1.1), margin = margin(10, 10, 10, 10, \"pt\")), \n        axis.title.x = element_markdown(\n            size = rel(1.1), margin = margin(10, 10, 10, 10, \"pt\")),\n        axis.ticks = element_blank()\n    ) + \n    guides(\n        size = guide_legend(\n            override.aes = list(color = \"grey80\")\n        )\n    )\n\n\ng\n\n\n\n\n\n\n\n\n\n上記に、テキストボックスを追加する。と思ったけどどうすればいいのかがよくわからない・・・・\n\n\nCode\ncodes &lt;- c(\n    \"Adelie\" = \"&lt;span style='font-size:20px'&gt;&lt;b&gt;P.adelie&lt;/b&gt;&lt;/span&gt;&lt;br&gt;(Adelie penguin)\", \n    \"Chinstrap\" = \"&lt;span style='font-size:20px'&gt;&lt;b&gt;P.antarctica&lt;/b&gt;&lt;/span&gt;&lt;br&gt;(Chinstrap pengui)\", \n    \"Gentoo\" = \"&lt;span style='font-size:20px'&gt;&lt;b&gt;P.papua&lt;/b&gt;&lt;/span&gt;&lt;br&gt;(Gentoo pengui)\"\n)\n\nd &lt;- \n    palmerpenguins::penguins |&gt; \n    filter(complete.cases(pick(everything()))) |&gt; \n    group_by(species) |&gt; \n    summarise(across(c(bill_length_mm, bill_depth_mm), mean)) |&gt; \n    ungroup() |&gt; \n    mutate(label = codes[species]) \n\ng + \n    geom_richtext(\n        data = d, \n        aes(x = bill_length_mm, y = bill_depth_mm, label = label), \n        fill = \"white\", \n        color = \"transparent\", \n        alpha = .1, \n        label.padding = unit(.7, units = \"lines\"), \n        label.r = unit(10, \"pt\"), \n    ) +\n    geom_richtext(\n        data = d, \n        aes(x = bill_length_mm, y = bill_depth_mm, label = label), \n        fill = \"transparent\", \n        color = c(\"red\", \"blue\", \"green\"),\n        label.padding = unit(.7, units = \"lines\"), \n        label.r = unit(10, \"pt\"), \n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Working with Labels and Annotations"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch02_concepts_of_the_ggplot2_part2.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch02_concepts_of_the_ggplot2_part2.html",
    "title": "Concepts of the ggplot2 Packages pt 2",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\n\n\n\n\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"ggplot2\", \"readr\", \"tibble\", \"tidyr\", \"forcats\", \n  \"stringr\",\n  \"lubridate\", \"here\", \"systemfonts\", \"magick\", \n  \"scales\", \"grid\",\n  \"grDevices\", \"colorspace\", \"viridis\", \n  \"RColorBrewer\", \"rcartocolor\",\n  \"scico\", \"ggsci\", \"ggthemes\", \"nord\", \n  \"MetBrewer\", \"ggrepel\",\n  \"ggforce\", \"ggtext\", \"ggdist\", \"ggbeeswarm\", \n  \"gghalves\", \"patchwork\", \n  \"palmerpenguins\", \"rnaturalearth\", \"sf\", \"rmapshaper\", \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 Setup\n\n\nCode\nlibrary(tidyverse)\n\nbikes &lt;- readr::read_csv(\n  here::here(cur_dir, \"ggplot2-course-data\", \"london-bikes-custom.csv\"),\n  col_types = \"Dcfffilllddddc\"\n)\n\nbikes$season &lt;- forcats::fct_inorder(bikes$season)\n\ntheme_set(theme_light(base_size = 14, base_family = \"Noto Sans\"))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  plot.title = element_text(face = \"bold\"),\n  legend.position = \"top\",\n  plot.title.position = \"plot\"\n)\n\ninvisible(Sys.setlocale(\"LC_TIME\", \"C\"))\n\n\n\n\nCode\ng &lt;-\n  ggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = season,\n        group = day_night)\n  ) +\n  geom_point(\n    alpha = .5\n  ) +\n  geom_smooth(\n    method = \"lm\",\n    color = \"black\"\n  )\n\n\n\n\n2 Facets\n\n\nCode\ng +\n  facet_wrap(\n    ~ is_workday + day_night\n  )\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCode\ng +\n  facet_wrap(\n    ~ day_night,\n    ncol = 1,\n    strip.position = \"bottom\"\n  )\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCode\ng +\n  facet_grid(\n    day_night ~ is_workday,\n    scales = \"free\",\n    switch = \"y\"\n  ) + \n  scale_y_continuous(position = \"right\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCode\ng +\n  facet_grid(\n    rows = vars(day_night),\n    cols = vars(is_workday)\n  )\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCode\ng +\n  facet_grid(\n    day_night ~ is_workday, \n    scales = c(\"free_y\"), \n    space = c(\"free_y\")\n  )\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    diamonds,\n    aes(x = carat, y = price)\n  ) +\n  geom_point(\n    alpha = .3,\n    color = \"white\"\n  ) +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE,\n    color = \"dodgerblue\"\n  ) +\n  facet_grid(\n    cut ~ clarity,\n    space = \"free_x\",\n    scales = \"free_x\"\n  ) + \n  theme_dark()\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n3 Scales\n\n\nCode\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = season)\n  ) +\n  geom_point() +\n  scale_x_date(\n        name = NULL,\n        date_breaks = \"6 months\",\n        date_labels = \"%b '%y\"\n  ) +\n  scale_y_continuous(\n      trans = \"log10\", \n      name = \"Reported bike shares\", \n      breaks = seq(0, 60000, by = 15000), \n      labels = \\(x) paste0(x, \" bikes\"), \n      limits = c(NA, 60000), \n      expand = expansion(0, 0), # add, mult\n      # guide = \"none\"\n  ) +\n  scale_color_discrete()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = season, y = count)\n  ) +\n  geom_boxplot() +\n  scale_x_discrete(\n    name = \"Period\",\n    labels = c(\"Dec-Feb\", \"Mar-May\", \"Jun-Aug\", \"Sep-Nov\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = season)\n  ) +\n  geom_point() +\n  scale_color_discrete(\n    name = \"Season:\",\n    type = c(\"#69b0d4\", \"#00CB79\", \"#F7B01B\", \"#a78f5f\")\n  )\n\n\n\n\n\n\n\n\n\n色についてもscale_colorの中でマニュアルで指定することが可能である.\n\n\nCode\nmy_colors &lt;- c(\n  `winter` = \"#3c89d9\",\n  `spring` = \"#1ec99b\",\n  `summer` = \"#F7B01B\",\n  `autumn` = \"#a26e7c\"\n)\n\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = season)\n  ) +\n  geom_point() +\n  scale_color_discrete(\n    name = \"Season:\",\n    type = my_colors\n  )\n\n\n\n\n\n\n\n\n\nNAは別の値で決めることができる. パステルカラーにブラックはよくわかるので良さそうである.\n\n\nCode\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = weather_type)\n  ) +\n  geom_point() +\n  scale_color_manual(\n    name = \"Season:\",\n    values = brewer.pal(n = 6, name = \"Pastel1\"),\n    na.value = \"black\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = weather_type)\n  ) +\n  geom_point() +\n  rcartocolor::scale_color_carto_d(\n    name = \"Season:\",\n    palette = \"Pastel\",\n    na.value = \"black\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nfacet &lt;-\n  ggplot(\n    diamonds,\n    aes(x = carat, y = price)\n  ) +\n  geom_point(\n    alpha = .3\n  ) +\n  geom_smooth(\n    aes(color = cut),\n    method = \"lm\",\n    se = FALSE\n  ) +\n  facet_grid(\n    cut ~ clarity,\n    space = \"free_x\",\n    scales = \"free_x\"\n  )\n\nfacet + \n    scale_x_continuous(breaks = 0:5) + \n    scale_y_continuous(\n        limits = c(0, 30000), \n        breaks = 0:3 * 10000, \n        labels = \\(x) paste0(\"$\", format(x, big.mark = \",\", trim = TRUE))\n    ) + \n    scale_color_brewer(palette = \"Set2\") + \n    theme(legend.position = \"none\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n#&gt; Warning: Removed 131 rows containing missing values (`geom_smooth()`).\n\n\n\n\n\n\n\n\n\n\n\n4 Coordinates Systems\n線形の座標系は次である.\n\ncoord_cartesian()\ncoord_fixed()\ncoord_flip()\n\n非線形の座標系として次がある.\n\ncoord_polar()\ncoord_map()\ncoord_sf()\ncoord_trans()\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = season, y = count)\n  ) +\n  geom_boxplot() +\n  coord_cartesian(\n      ylim = c(NA, 15000)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = season, y = count)\n  ) +\n  geom_boxplot() +\n  scale_y_continuous(\n      limits = c(NA, 15000)\n  )\n#&gt; Warning: Removed 575 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nclip = \"off\"を使うとpanel領域をはみ出してグラフを記述することが出来る.\n\n\nCode\nggplot(\n    bikes,\n    aes(x = season, y = count)\n  ) +\n  geom_boxplot() +\n  coord_cartesian(\n    ylim = c(NA, 15000),\n    clip = \"off\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, is_holiday == TRUE),\n    aes(x = temp_feel, y = count)\n  ) +\n  geom_point() +\n  geom_text(\n    aes(label = season),\n    nudge_x = .3,\n    hjust = 0, \n    vjust = -1\n  ) +\n  coord_cartesian(\n    clip = \"off\"\n  )\n\n\n\n\n\n\n\n\n\nパディングをすべて消すことも可能である.\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count)\n  ) +\n  geom_point() +\n  coord_cartesian(\n    expand = FALSE,\n    clip = \"off\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = temp)\n  ) +\n  geom_point() +\n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = temp)\n  ) +\n  geom_point() +\n  coord_fixed(ratio = 4) # y / x\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(y = weather_type)\n  ) +\n  geom_bar() +\n  coord_cartesian()\n\n\n\n\n\n\n\n\n\nreorderすることも出来る.\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(y = fct_rev(fct_infreq(weather_type)))\n  ) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(x = weather_type,\n        fill = weather_type)\n  ) +\n  geom_bar() +\n  coord_polar(theta = \"y\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(x = fct_infreq(weather_type),\n        fill = weather_type)\n  ) +\n  geom_bar(width = 1) +\n  coord_polar()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(x = fct_infreq(weather_type),\n        fill = weather_type)\n  ) +\n  geom_bar(width = 1) +\n  coord_cartesian()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(x = 1, fill = weather_type)\n  ) +\n  geom_bar(position = \"stack\") +\n  coord_polar(theta = \"y\") \n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(x = 1, fill = weather_type)\n  ) +\n  geom_bar(position = \"stack\") +\n  coord_cartesian() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp, y = count,\n        group = day_night)\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  coord_trans(y = \"log10\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp, y = count,\n        group = day_night)\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_y_log10()\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Spaital Coordiate\n\n\nCode\ncountries &lt;- rnaturalearth::ne_countries(\n  returnclass = \"sf\"\n)\n\n\n\nggplot() +\n  geom_sf(\n    data = countries,\n    color = \"#79dfbd\",\n    fill = \"#28a87d\",\n    size = .3\n  ) +\n  coord_sf(\n    crs = \"+proj=moll\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\noceans &lt;- rnaturalearth::ne_download(\n  category = \"physical\", type = \"ocean\", returnclass = \"sf\"\n)\nggplot() +\n  geom_sf(\n    data = oceans,\n    fill = \"#d8f1f6\",\n    color = \"white\"\n  ) +\n  geom_sf(\n    data = countries,\n    aes(fill = economy),\n    color = \"white\",\n    size = .3\n  ) +\n  coord_sf(\n    crs = \"+proj=bonne +lat_1=10\"\n  ) +\n  scale_fill_viridis_d(option = \"magma\") +\n  theme_void() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 2"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/index.html",
    "href": "contents/website/analizeUSCensus/index.html",
    "title": "Analyzing US Census Data",
    "section": "",
    "text": "1 はじめに\n\nアメリカのセンサスデータのデータ分析\n空間データ分析のトレーニングとして有用\nリンクス\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch06_mapping_census_data_with_r.html",
    "href": "contents/website/analizeUSCensus/ch06_mapping_census_data_with_r.html",
    "title": "Mapping Census data with R",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\n\n\n\n\nCode\nlibrary(sf)\nlibrary(here)\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(spdep)\n\ncur_dir &lt;- here()\noptions(tigris_use_cache = TRUE)\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Mapping Census data with R"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch12_choropleths.html",
    "href": "contents/website/leaflet4r/chapters/ch12_choropleths.html",
    "title": "Choropleths",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Choropleths"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch12_choropleths.html#basic-stats-map",
    "href": "contents/website/leaflet4r/chapters/ch12_choropleths.html#basic-stats-map",
    "title": "Choropleths",
    "section": "1.1 Basic stats map",
    "text": "1.1 Basic stats map\n\n\nCode\nm &lt;- leaflet(states) %&gt;%\n  setView(-96, 37.8, 4) %&gt;%\n  addProviderTiles(\"MapBox\", options = providerTileOptions(\n    id = \"mapbox.light\",\n    accessToken = Sys.getenv('MAPBOX_ACCESS_TOKEN')))\n\nm\n\n\n\n\n\n\n\n\nCode\nm %&gt;% addPolygons()",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Choropleths"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch10_Legends.html",
    "href": "contents/website/leaflet4r/chapters/ch10_Legends.html",
    "title": "Legends",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")\n\n\n\n1 Lgends\n\n\nCode\ncountries &lt;- sf::st_read(\"https://rstudio.github.io/leaflet/json/countries.geojson\")\n#&gt; Reading layer `countries' from data source \n#&gt;   `https://rstudio.github.io/leaflet/json/countries.geojson' \n#&gt;   using driver `GeoJSON'\n#&gt; Simple feature collection with 177 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513\n#&gt; Geodetic CRS:  WGS 84\n\n\n\n\nCode\nmap &lt;- leaflet(countries) %&gt;% addTiles()\n\n\naddLegend関数を使うことで凡例を追加することができる.\n\n\nCode\npal &lt;- colorNumeric(\n  palette = \"YlGnBu\",\n  domain = countries$gdp_md_est\n)\nmap %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~pal(gdp_md_est)\n  ) %&gt;%\n  addLegend(\n      \"bottomright\", \n      pal = pal, \n      values = ~gdp_md_est,\n      title = \"Est. GDP (2010)\",\n      labFormat = labelFormat(prefix = \"$\"),\n      opacity = 1\n  )\n\n\n\n\n\n\n異なる凡例を使うのも簡単である.\n\n\nCode\nqpal &lt;- colorQuantile(\"RdYlBu\", countries$gdp_md_est, n = 5)\nmap %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~qpal(gdp_md_est)\n  ) %&gt;%\n  addLegend(pal = qpal, values = ~gdp_md_est, opacity = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Legends"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch08_shiny_integration.html",
    "href": "contents/website/leaflet4r/chapters/ch08_shiny_integration.html",
    "title": "Shiny Integration",
    "section": "",
    "text": "Quartoのプロジェクトの中に置いていると shinyを動かすことができないんだけど、 基本的にはleafletでもshinyで使えますという話し.\nまた, Proxyとしてマウスの位置やクリック位置などを検出することができるので, それを使えばリッチなWebアプリが作成できますという主旨のことが書いてある。\n作るときに忘れず参照することが大事である.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Shiny Integration"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch06_geojson.html",
    "href": "contents/website/leaflet4r/chapters/ch06_geojson.html",
    "title": "Working with GeoJSON and TopoJSON",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Working with GeoJSON and TopoJSON"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch06_geojson.html#styling-raw-geojson-topojson",
    "href": "contents/website/leaflet4r/chapters/ch06_geojson.html#styling-raw-geojson-topojson",
    "title": "Working with GeoJSON and TopoJSON",
    "section": "1.1 Styling raw GeoJSON / TopoJSON",
    "text": "1.1 Styling raw GeoJSON / TopoJSON\nGeoJSONなどはいくつかの方法でスタイルを直接変更することができる.\n\n\nCode\nlibrary(jsonlite)\n#&gt; \n#&gt; Attaching package: 'jsonlite'\n#&gt; The following object is masked from 'package:purrr':\n#&gt; \n#&gt;     flatten\n#&gt; The following objects are masked from 'package:rjson':\n#&gt; \n#&gt;     fromJSON, toJSON\n#&gt; The following object is masked from 'package:shiny':\n#&gt; \n#&gt;     validate\n\n# From http://data.okfn.org/data/datasets/geo-boundaries-world-110m\ngeojson &lt;- \n    readLines(\n        \"https://rstudio.github.io/leaflet/json/countries.geojson\", \n        warn = FALSE) %&gt;%\n  paste(collapse = \"\\n\") %&gt;%\n  fromJSON(simplifyVector = FALSE)\n\n# Default styles for all features\ngeojson$style = list(\n  weight = 1,\n  color = \"#555555\",\n  opacity = 1,\n  fillOpacity = 0.8\n)\n\n# Gather GDP estimate from all countries\ngdp_md_est &lt;- sapply(geojson$features, function(feat) {\n  feat$properties$gdp_md_est\n})\n# Gather population estimate from all countries\npop_est &lt;- sapply(geojson$features, function(feat) {\n  max(1, feat$properties$pop_est)\n})\n\n# Color by per-capita GDP using quantiles\npal &lt;- colorQuantile(\"Greens\", gdp_md_est / pop_est)\n# Add a properties$style list to each feature\ngeojson$features &lt;- lapply(geojson$features, function(feat) {\n  feat$properties$style &lt;- list(\n    fillColor = pal(\n      feat$properties$gdp_md_est / max(1, feat$properties$pop_est)\n    )\n  )\n  feat\n})\n\n# Add the now-styled GeoJSON object to the map\nleaflet() %&gt;% addGeoJSON(geojson)",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Working with GeoJSON and TopoJSON"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html",
    "href": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html",
    "title": "Popups and Shapes",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Popups and Shapes"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html#customizing-marker-labels",
    "href": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html#customizing-marker-labels",
    "title": "Popups and Shapes",
    "section": "2.1 Customizing Marker Labels",
    "text": "2.1 Customizing Marker Labels\n\n\nCode\n# Change Text Size and text Only and also a custom CSS\nleaflet() %&gt;% addTiles() %&gt;% setView(-118.456554, 34.09, 13) %&gt;%\n  addMarkers(\n    lng = -118.456554, lat = 34.105,\n    label = \"Default Label\",\n    labelOptions = labelOptions(noHide = T)) %&gt;%\n  addMarkers(\n    lng = -118.456554, lat = 34.095,\n    label = \"Label w/o surrounding box\",\n    labelOptions = labelOptions(noHide = T, textOnly = TRUE)) %&gt;%\n  addMarkers(\n    lng = -118.456554, lat = 34.085,\n    label = \"label w/ textsize 15px\",\n    labelOptions = labelOptions(noHide = T, textsize = \"15px\")) %&gt;%\n  addMarkers(\n    lng = -118.456554, lat = 34.075,\n    label = \"Label w/ custom CSS style\",\n    labelOptions = labelOptions(noHide = T, direction = \"bottom\",\n      style = list(\n        \"color\" = \"red\",\n        \"font-family\" = \"serif\",\n        \"font-style\" = \"italic\",\n        \"box-shadow\" = \"3px 3px rgba(0,0,0,0.25)\",\n        \"font-size\" = \"12px\",\n        \"border-color\" = \"rgba(0,0,0,0.5)\"\n      )))",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Popups and Shapes"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html#labels-without-markers",
    "href": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html#labels-without-markers",
    "title": "Popups and Shapes",
    "section": "2.2 Labels without markers",
    "text": "2.2 Labels without markers\n\n\nCode\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addLabelOnlyMarkers(\n    data = breweries91,\n    label = as.character(breweries91$brewery),\n    group = \"brew\",\n    labelOptions = leaflet::labelOptions(\n      noHide = TRUE,\n      direction = \"bottom\",\n      textOnly = TRUE,\n      offset = c(0, -10),\n      opacity = 1\n    )\n  )",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Popups and Shapes"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch02_basemaps.html",
    "href": "contents/website/leaflet4r/chapters/ch02_basemaps.html",
    "title": "Basemap",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)\n\n\n\n1 Default\n\nOpenstreet Mapが使われる.\n\n\n\nCode\nm &lt;- leaflet() %&gt;% setView(lng = -71.0589, lat = 42.3601, zoom = 12)\nm %&gt;% addTiles()\n\n\n\n\n\n\n\n\n2 Third-Party Tiles\n\n\nCode\nm %&gt;% addProviderTiles(providers$CartoDB.Positron)\n\n\n\n\n\n\n\n\n3 Custom Tile URL Template\n\n\nCode\nleaflet() %&gt;% addTiles() %&gt;% setView(-93.65, 42.0285, zoom = 4) %&gt;%\n  addWMSTiles(\n    \"http://mesonet.agron.iastate.edu/cgi-bin/wms/nexrad/n0r.cgi\",\n    layers = \"nexrad-n0r-900913\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n    attribution = \"Weather data © 2012 IEM Nexrad\"\n  )\n\n\n\n\n\n\n\n\n4 Combining Tile Layers\nbasemapは重ねることが可能である. ちなみにvector tileをどのように重ねたらよいのかはまだわかっていない。\n\n\nCode\nm |&gt; \n  addProviderTiles(providers$MtbMap) |&gt;\n  addProviderTiles(providers$Stamen.TonerLines,\n    options = providerTileOptions(opacity = 0.35)) |&gt;\n  addProviderTiles(providers$Stamen.TonerLabels)\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Basemap"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html",
    "href": "contents/website/leaflet4r/index.html",
    "title": "Leaflet for R",
    "section": "",
    "text": "Leaflet for Rの学習ノートです\nAPI reference\nすべてではなく気になった部分だけを記録しています\nalbersusaパッケージはここインストールしています",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#文字列へのリンク",
    "href": "contents/website/leaflet4r/index.html#文字列へのリンク",
    "title": "Leaflet for R",
    "section": "3.1 文字列へのリンク",
    "text": "3.1 文字列へのリンク\nleaflet map",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#画像へのリンク",
    "href": "contents/website/leaflet4r/index.html#画像へのリンク",
    "title": "Leaflet for R",
    "section": "3.2 画像へのリンク",
    "text": "3.2 画像へのリンク\nlightboxの効果はないのに注意すること.\n\n\n\n画像へのリンク作成",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#geojson",
    "href": "contents/website/leaflet4r/index.html#geojson",
    "title": "Leaflet for R",
    "section": "3.3 GeoJSON",
    "text": "3.3 GeoJSON\n\n\nCode\ngeojson &lt;- \"https://www.geospatial.jp/ckan/dataset/669fc4ff-317b-48cf-a43d-9340ff04ff18/resource/ccff82d4-2353-49e6-9845-371ee6d622de/download/biomasspowerstation42.geojson\"\ngeojson &lt;- \"https://www.geospatial.jp/ckan/dataset/a4f2f167-1433-47ea-b7be-ce88660a30a2/resource/72f6ed4e-a68c-4082-a90f-dd7e5423d87a/download/p35_18_03.geojson\"\nx &lt;- fromJSON(file = geojson)\ny &lt;- read_sf(geojson)\nytext &lt;- \n    y %&gt;%\n    \"st_geometry&lt;-\"(NULL) %&gt;%\n    transpose() %&gt;%\n    map( ~ glue(\"{names(.x)} = {.x}\")) %&gt;%\n    map_chr(~ str_c(.x, collapse = \"&lt;br/&gt;\"))\nas_longstyle_htmltable &lt;- function(data) {\n    data %&gt;%\n        mutate(across(everything(), as.character)) %&gt;%\n        rowwise() %&gt;%\n        summarise(htmltbl = list(tibble(`属性` = names(cur_data()), `値` = flatten_chr(cur_data())))) %&gt;%\n        mutate(htmltbl = map(htmltbl, ~ kable(.x, format = \"html\"))) %&gt;%\n        mutate(htmltbl = map(htmltbl, ~ str_replace(.x, \"&lt;table&gt;\", \"&lt;table style='border: solid 1px #f8f8f8;border-collapse:collapse;height:auto;max-height:35vh;overflow-y:scroll'&gt;\"))) %&gt;%\n        mutate(htmltbl = map(htmltbl, ~ str_replace_all(.x, \"&lt;th style=\\\"text-align:left;\\\"&gt;\", \"&lt;th style=\\\"text-align:center;background-color:black;color:white\\\"&gt;\"))) %&gt;%\n        mutate(htmltbl = map(htmltbl, ~ str_replace_all(.x, \"&lt;tr&gt;\", \"&lt;tr style='border: solid 1px #f8f8f8;'&gt;\"))) %&gt;%\n        mutate(htmltbl = map(htmltbl, ~ str_replace_all(.x, \"&lt;td style=\\\"text-align:left;\\\"&gt;\", \"&lt;td style=\\\"text-align:left;padding:0.5em\\\"&gt;\")))\n}\n\nyattr      &lt;- \"st_geometry&lt;-\"(y, NULL)\nytabletext &lt;- as_longstyle_htmltable(yattr)\n#&gt; Warning: There was 1 warning in `summarise()`.\n#&gt; ℹ In argument: `htmltbl = list(tibble(属性 = names(cur_data()), 値 =\n#&gt;   flatten_chr(cur_data())))`.\n#&gt; ℹ In row 1.\n#&gt; Caused by warning:\n#&gt; ! `cur_data()` was deprecated in dplyr 1.1.0.\n#&gt; ℹ Please use `pick()` instead.\n\n\nrm(m)\nm &lt;- \n    leaflet(\n        width = \"100%\",\n        height = \"800\"\n    ) %&gt;%\n    addTiles(urlTemplate = \"https://cyberjapandata.gsi.go.jp/xyz/seamlessphoto/{z}/{x}/{y}.jpg\", group = \"空中写真\", layerId = \"photo\") %&gt;%\n    addTiles(urlTemplate = \"http://cyberjapandata.gsi.go.jp/xyz/std/{z}/{x}/{y}.png\", group = \"標準地図\", layerId = \"gsi\") %&gt;%\n    addTiles(urlTemplate = \"https://tile.geospatial.jp/ks-shinsuisoutei/A31-19_83_SHP_tile/{z}/{x}/{y}.png\", group = \"MLIT\") %&gt;%\n    addFeatures(data = st_geometry(y), popup = ytabletext$htmltbl, group = \"new\") %&gt;%\n    addLayersControl(\n        baseGroup     = c(\"標準地図\", \"空中写真\"), \n        overlayGroups = c(\"new\", \"MLIT\")\n    )\n    \n\nm %&gt;%\n    addFeatures(data = st_geometry(y), popup = ytabletext$htmltbl, group = \"new2\") %&gt;%\n    addLayersControl(\n        baseGroup     = c(\"標準地図\", \"空中写真\"), \n        overlayGroups = c(\"new\", \"new2\")\n    )",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#geotiff",
    "href": "contents/website/leaflet4r/index.html#geotiff",
    "title": "Leaflet for R",
    "section": "3.4 GeoTiff",
    "text": "3.4 GeoTiff\n\n\nCode\nlibrary(stars)\nlibrary(leafem)\n\nr &lt;- read_stars(here(cur_dir, \"data/DCWD_map.tif\"))\nleaflet() %&gt;%\n    addTiles(group = \"OpenStreetMap\") %&gt;% \n    addStarsRGB(r)",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#gps",
    "href": "contents/website/leaflet4r/index.html#gps",
    "title": "Leaflet for R",
    "section": "3.5 GPS",
    "text": "3.5 GPS\n\n\nCode\nlibrary(leaflet)\nlibrary(leaflet.extras)\n\nmap &lt;- leaflet() %&gt;% addTiles()\n\nmap &lt;- addControlGPS(\n    map, \n    options = gpsOptions(\n        position = \"topleft\", \n        activate = TRUE, \n        autoCenter = TRUE, \n        maxZoom = 10, \n        setView = TRUE\n    )\n)\n\nactivateGPS(map)",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#vector-tile",
    "href": "contents/website/leaflet4r/index.html#vector-tile",
    "title": "Leaflet for R",
    "section": "3.6 Vector tile",
    "text": "3.6 Vector tile\n\n\nCode\nlibrary(leaflet)\nlibrary(htmltools)\n\nm &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 174.768, lat = -36.852, zoom = 10)\n\nm_html &lt;- as.character(m)\n\nvector_tile_js &lt;- \"\nL.vectorGrid.protobuf('https://cyberjapandata.gsi.go.jp/xyz/experimental_bvmap/{z}/{x}/{y}.pbf').addTo(map);\n\"\n\nbrowsable(\n  tagList(\n    tags$head(tags$script(src = \"https://unpkg.com/leaflet.vectorgrid/dist/Leaflet.VectorGrid.js\")),\n    HTML(m_html),\n    tags$script(HTML(vector_tile_js))\n  )\n)",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/index.html",
    "href": "contents/website/r4ds2e/index.html",
    "title": "R for Data Science 2e",
    "section": "",
    "text": "1 はじめに\n\nR for Data Science 2eの学習ノートです\nすべてではなく気になった部分だけを記録しています\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "R for Data Science 2e",
      "Introduction"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Tips Site",
    "section": "",
    "text": "1 はじめに\n\nRに関するTips, 勉強ノートです\nサイト、図書、勉強会などの情報を一箇所にまとめるのが目的です\nRは常に進化しており、知らないことは損をします\nデータサイエンスを極めましょう\n\n\n\n2 構成\ncontents/{web/book/adhoc/library}/source_name/section/chapter.qmdを基本的な構成とします.\n\n\n3 開発環境\n\nR 4.3.1を使います\nhereパッケージを使います\nrenvパッケージを使いマス\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html",
    "title": "23 Arrow",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/r4ds2e/Import/\")\nCode\nlibrary(tidyverse)\nlibrary(arrow)\nlibrary(dbplyr)\nlibrary(duckdb)\nlibrary(here)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html#advantages",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html#advantages",
    "title": "23 Arrow",
    "section": "4.1 Advantages",
    "text": "4.1 Advantages\n\nParquetは効率的なエンコードでファイルサイズを小さくする\nCSVに持たせることが出来ない情報を持たせることができる\nカラム指向である\nチャンク化されていても,異なるチャンクを同時扱うことが出来る",
    "crumbs": [
      "R",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html#partitioning",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html#partitioning",
    "title": "23 Arrow",
    "section": "4.2 Partitioning",
    "text": "4.2 Partitioning\n\nデータが巨大になるほど1つのファイルで管理するのが困難となる\nチャンク化は20MB以下になることと2GB以上になることを避ける\nChekoutYearを軸にチャンク化をおこなう\nISBNがint64やnullだと出力が出来ないのでここでは文字列として出力する\n\n\n\nCode\npq_path &lt;- here(cur_dir, \"data/seattle-library-checkouts\")\nseattle_csv |&gt; \n    group_by(CheckoutYear) |&gt; \n    write_dataset(path = pq_path, format = \"parquet\")\n\n\n生成したデータを確認する.\n\n\nCode\ntibble(\n    files = list.files(pq_path, recursive = TRUE), \n    size_MB = file.size(here(pq_path, files)) / 1024^2\n)\n#&gt; # A tibble: 18 × 2\n#&gt;    files                            size_MB\n#&gt;    &lt;chr&gt;                              &lt;dbl&gt;\n#&gt;  1 CheckoutYear=2005/part-0.parquet    109.\n#&gt;  2 CheckoutYear=2006/part-0.parquet    164.\n#&gt;  3 CheckoutYear=2007/part-0.parquet    178.\n#&gt;  4 CheckoutYear=2008/part-0.parquet    195.\n#&gt;  5 CheckoutYear=2009/part-0.parquet    214.\n#&gt;  6 CheckoutYear=2010/part-0.parquet    222.\n#&gt;  7 CheckoutYear=2011/part-0.parquet    239.\n#&gt;  8 CheckoutYear=2012/part-0.parquet    249.\n#&gt;  9 CheckoutYear=2013/part-0.parquet    269.\n#&gt; 10 CheckoutYear=2014/part-0.parquet    282.\n#&gt; 11 CheckoutYear=2015/part-0.parquet    294.\n#&gt; 12 CheckoutYear=2016/part-0.parquet    300.\n#&gt; 13 CheckoutYear=2017/part-0.parquet    304.\n#&gt; 14 CheckoutYear=2018/part-0.parquet    292.\n#&gt; 15 CheckoutYear=2019/part-0.parquet    288.\n#&gt; 16 CheckoutYear=2020/part-0.parquet    151.\n#&gt; 17 CheckoutYear=2021/part-0.parquet    229.\n#&gt; 18 CheckoutYear=2022/part-0.parquet    241.",
    "crumbs": [
      "R",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html#using-dplyr-with-arrow",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html#using-dplyr-with-arrow",
    "title": "23 Arrow",
    "section": "4.3 Using dplyr with arrow",
    "text": "4.3 Using dplyr with arrow\n\n\nCode\nseattle_pq &lt;- open_dataset(pq_path)\nseattle_pq\n#&gt; FileSystemDataset with 18 Parquet files\n#&gt; UsageClass: string\n#&gt; CheckoutType: string\n#&gt; MaterialType: string\n#&gt; CheckoutMonth: int64\n#&gt; Checkouts: int64\n#&gt; Title: string\n#&gt; ISBN: string\n#&gt; Creator: string\n#&gt; Subjects: string\n#&gt; Publisher: string\n#&gt; PublicationYear: string\n#&gt; CheckoutYear: int32\n\n\ndplyrの構文がそのまま使える. collectを実行したときに, コードが実行される.\n\n\nCode\nquery &lt;- \n    seattle_pq |&gt; \n    filter(CheckoutYear &gt;= 2008, MaterialType == \"BOOK\") |&gt; \n    group_by(CheckoutYear, CheckoutMonth) |&gt; \n    summarise(TotalCheckouts = sum(Checkouts)) |&gt; \n    arrange(CheckoutYear, CheckoutMonth)\nquery\n#&gt; FileSystemDataset (query)\n#&gt; CheckoutYear: int32\n#&gt; CheckoutMonth: int64\n#&gt; TotalCheckouts: int64\n#&gt; \n#&gt; * Grouped by CheckoutYear\n#&gt; * Sorted by CheckoutYear [asc], CheckoutMonth [asc]\n#&gt; See $.data for the source Arrow object\n\n\n前述したように, collectで評価される.\n\n\nCode\nquery |&gt; collect()\n#&gt; # A tibble: 178 × 3\n#&gt; # Groups:   CheckoutYear [15]\n#&gt;    CheckoutYear CheckoutMonth TotalCheckouts\n#&gt;           &lt;int&gt;         &lt;int&gt;          &lt;int&gt;\n#&gt;  1         2008             1         343384\n#&gt;  2         2008             2         315806\n#&gt;  3         2008             3         291710\n#&gt;  4         2008             4         341452\n#&gt;  5         2008             5         328072\n#&gt;  6         2008             6         356621\n#&gt;  7         2008             7         388959\n#&gt;  8         2008             8         363506\n#&gt;  9         2008             9         353561\n#&gt; 10         2008            10         354779\n#&gt; # ℹ 168 more rows",
    "crumbs": [
      "R",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html#performance",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html#performance",
    "title": "23 Arrow",
    "section": "4.4 Performance",
    "text": "4.4 Performance\nデータが絞られるほどCSVよりも高速に動作する.\n\n\nCode\nbench::mark(\n    {\n        seattle_csv |&gt; \n        filter(CheckoutYear &gt;= 2008, MaterialType == \"BOOK\") |&gt; \n        group_by(CheckoutYear, CheckoutMonth) |&gt; \n        summarise(TotalCheckouts = sum(Checkouts)) |&gt; \n        arrange(CheckoutYear, CheckoutMonth) |&gt; \n        collect()\n    }, \n    {\n        seattle_pq |&gt; \n        filter(CheckoutYear &gt;= 2008, MaterialType == \"BOOK\") |&gt; \n        group_by(CheckoutYear, CheckoutMonth) |&gt; \n        summarise(TotalCheckouts = sum(Checkouts)) |&gt; \n        arrange(CheckoutYear, CheckoutMonth) |&gt; \n        collect()\n    }, \n    check = FALSE, \n    max_iterations = 1\n)\n#&gt; Warning: Some expressions had a GC in every iteration; so filtering is\n#&gt; disabled.\n#&gt; # A tibble: 2 × 6\n#&gt;   expression                             min median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt;                           &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 { collect(arrange(summarise(group_b… 18.3s  18.3s    0.0547     218KB    0    \n#&gt; 2 { collect(arrange(summarise(group_b…  2.1s   2.1s    0.475      218KB    0.475",
    "crumbs": [
      "R",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html#advance",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html#advance",
    "title": "23 Arrow",
    "section": "4.5 Advance",
    "text": "4.5 Advance\nparquetはカラム指向なので同じカラム指向のリレーショナルデータベースであるDuckDBと非常に相性がよい. DuckDBはカラム指向でデータの高速処理を指向して開発されたデータベースである. サーバーレスで動作する.\n\n\nCode\nseattle_pq |&gt; \n  to_duckdb() |&gt;\n  filter(CheckoutYear &gt;= 2008, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutYear) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutYear)) |&gt;\n  collect() |&gt; \n  system.time()\n#&gt; Warning: Missing values are always removed in SQL aggregation functions.\n#&gt; Use `na.rm = TRUE` to silence this warning\n#&gt; This warning is displayed once every 8 hours.\n#&gt;    user  system elapsed \n#&gt;    1.44    0.21    2.69",
    "crumbs": [
      "R",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch01_map_object.html",
    "href": "contents/website/leaflet4r/chapters/ch01_map_object.html",
    "title": "The Map Widget",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)\n\n\n\n1 ランダムメモ\n\n基本的にはleafletのAPIドックと同じものが動こう\n\n\n\n2 プリミティブな図形\n\ndata frame作成してしまうのが１番簡単である\n\n\n\nCode\n# add some circles to a map\ndf = data.frame(Lat = 1:10, Long = rnorm(10))\nleaflet(df) |&gt; addCircles()\n\n\n\n\n\n\n\n\nCode\nlibrary(maps)\n#&gt; \n#&gt; Attaching package: 'maps'\n#&gt; The following object is masked from 'package:purrr':\n#&gt; \n#&gt;     map\nmapStates = map(\"state\", fill = TRUE, plot = FALSE)\nleaflet(data = mapStates) %&gt;% addTiles() %&gt;%\n  addPolygons(fillColor = topo.colors(10, alpha = NULL), stroke = FALSE)\n\n\n\n\n\n\n上記の場合にはデータはリスト形になっている. NAがあるとペンが外れるGISではよくあるプリミティブな形状で、 記述することができる。\n\n\nCode\nmapStates |&gt; str(1)\n#&gt; List of 4\n#&gt;  $ x    : num [1:15599] -87.5 -87.5 -87.5 -87.5 -87.6 ...\n#&gt;  $ y    : num [1:15599] 30.4 30.4 30.4 30.3 30.3 ...\n#&gt;  $ range: num [1:4] -124.7 -67 25.1 49.4\n#&gt;  $ names: chr [1:63] \"alabama\" \"arizona\" \"arkansas\" \"california\" ...\n#&gt;  - attr(*, \"class\")= chr \"map\"\n\n\n\n\n3 Formula interface\nRらしい記述を使うことが可能である.\n\n\nCode\nm = leaflet() %&gt;% addTiles()\ndf = data.frame(\n  lat = rnorm(100),\n  lng = rnorm(100),\n  size = runif(100, 5, 20),\n  color = sample(colors(), 100)\n)\nm = leaflet(df) %&gt;% addTiles()\nm %&gt;% addCircleMarkers(radius = ~size, color = ~color, fill = FALSE)\n#&gt; Assuming \"lng\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\n\n\nCode\nm %&gt;% addCircleMarkers(radius = runif(100, 4, 10), color = c('red'))\n#&gt; Assuming \"lng\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Leaflet for R",
      "The Map Widget"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch03_markers.html",
    "href": "contents/website/leaflet4r/chapters/ch03_markers.html",
    "title": "Markers",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Markers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch03_markers.html#customizing-marker-icons",
    "href": "contents/website/leaflet4r/chapters/ch03_markers.html#customizing-marker-icons",
    "title": "Markers",
    "section": "1.1 Customizing Marker Icons",
    "text": "1.1 Customizing Marker Icons\n\n\nCode\ngreenLeafIcon &lt;- makeIcon(\n  iconUrl = \"https://leafletjs.com/examples/custom-icons/leaf-green.png\",\n  iconWidth = 38, iconHeight = 95,\n  iconAnchorX = 22, iconAnchorY = 94,\n  shadowUrl = \"https://leafletjs.com/examples/custom-icons/leaf-shadow.png\",\n  shadowWidth = 50, shadowHeight = 64,\n  shadowAnchorX = 4, shadowAnchorY = 62\n)\n\nleaflet(data = quakes[1:4,]) %&gt;% addTiles() %&gt;%\n  addMarkers(~long, ~lat, icon = greenLeafIcon)\n\n\n\n\n\n\n\n\nCode\noceanIcons &lt;- iconList(\n  ship = makeIcon(\"ferry-18.png\", \"ferry-18@2x.png\", 18, 18),\n  pirate = makeIcon(\"danger-24.png\", \"danger-24@2x.png\", 24, 24)\n)\n\n# Some fake data\ndf &lt;- sp::SpatialPointsDataFrame(\n  cbind(\n    (runif(20) - .5) * 10 - 90.620130,  # lng\n    (runif(20) - .5) * 3.8 + 25.638077  # lat\n  ),\n  data.frame(type = factor(\n    ifelse(runif(20) &gt; 0.75, \"pirate\", \"ship\"),\n    c(\"ship\", \"pirate\")\n  ))\n)\n\nleaflet (df) |&gt; \n    addTiles() |&gt; \n    addMarkers(icon = ~ oceanIcons[type])",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Markers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch03_markers.html#marker-clusters",
    "href": "contents/website/leaflet4r/chapters/ch03_markers.html#marker-clusters",
    "title": "Markers",
    "section": "1.2 Marker Clusters",
    "text": "1.2 Marker Clusters\n\n\nCode\nleaflet(quakes) %&gt;% addTiles() %&gt;% addMarkers(\n  clusterOptions = markerClusterOptions()\n)\n#&gt; Assuming \"long\" and \"lat\" are longitude and latitude, respectively",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Markers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch05_lines_and_shapes.html",
    "href": "contents/website/leaflet4r/chapters/ch05_lines_and_shapes.html",
    "title": "Lines and Shapes",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Lines and Shapes"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch05_lines_and_shapes.html#simplifying-complex-polygons",
    "href": "contents/website/leaflet4r/chapters/ch05_lines_and_shapes.html#simplifying-complex-polygons",
    "title": "Lines and Shapes",
    "section": "1.1 Simplifying complex polygons",
    "text": "1.1 Simplifying complex polygons\nデータが複雑が重たいときにはシンプリファイする必要がある.\n```{r}\nsimplified &lt;- rmapshaper::ms_simplify(fullsize)\nobject.size(simplified)\n```",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Lines and Shapes"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch07_raster_images.html",
    "href": "contents/website/leaflet4r/chapters/ch07_raster_images.html",
    "title": "Raster Images",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")\n\n\n2次元のSpatRasterオブジェクトについては,addRasterImageにより表示することが可能である. 読み込んだラスターデータは,EPSGは3857に変換されるとともに, 各セルの値はRGBAに変換される.\n\n1 Large Raster\nterra::resampleを使って解像度を調整すること.\n\n\n2 Projection Performance\n大量のデータがある場合にはleaflet::projectRasterFroLeaflet を使って事前にデータを変換しておくのがよい.\nその上でaddRasterImageでproject=FALSEにしておく.\n\n\n3 Coloring\n色々できるという感じです.\n\n\n4 Example\n\n\nCode\nr &lt;- rast(here(data_dir, \"FG-GML-5339-05-DEM5A.tif\")) \n\npal &lt;- \n    colorNumeric(\n        c(\"#0C2C84\", \"#41B6C4\", \"#FFFFCC\"), \n        values(r),\n        na.color = \"transparent\")\n\nleaflet() |&gt; \n    addTiles() |&gt;\n    addRasterImage(r, colors = pal, opacity = .5) |&gt; \n    addLegend(\n        pal = pal, \n        values = values(r), \n        title = \"Surface terrain\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Raster Images"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch09_Colors.html",
    "href": "contents/website/leaflet4r/chapters/ch09_Colors.html",
    "title": "Colors",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Colors"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch09_Colors.html#coloring-continuous-data",
    "href": "contents/website/leaflet4r/chapters/ch09_Colors.html#coloring-continuous-data",
    "title": "Colors",
    "section": "2.1 Coloring continuous data",
    "text": "2.1 Coloring continuous data\n\n\nCode\ncountries &lt;- sf::st_read(\"https://rstudio.github.io/leaflet/json/countries.geojson\")\n#&gt; Reading layer `countries' from data source \n#&gt;   `https://rstudio.github.io/leaflet/json/countries.geojson' \n#&gt;   using driver `GeoJSON'\n#&gt; Simple feature collection with 177 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513\n#&gt; Geodetic CRS:  WGS 84\nmap &lt;- leaflet(countries)\n\n\n\n\nCode\npar(mar = c(5,5,0,0), cex = 0.8)\nhist(countries$gdp_md_est, breaks = 20, main = \"\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a continuous palette function\npal &lt;- colorNumeric(\n  palette = \"Blues\",\n  domain = countries$gdp_md_est)\n\n# Apply the function to provide RGB colors to addPolygons\nmap %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~pal(gdp_md_est))\n\n\n\n\n\n\n\n\nCode\nbinpal &lt;- colorBin(\"Blues\", countries$gdp_md_est, 6, pretty = FALSE)\n\nmap %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~binpal(gdp_md_est))\n\n\n\n\n\n\n\n\nCode\nqpal &lt;- colorQuantile(\"Blues\", countries$gdp_md_est, n = 7)\nmap %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~qpal(gdp_md_est))",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Colors"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch09_Colors.html#coloring-categorical-data",
    "href": "contents/website/leaflet4r/chapters/ch09_Colors.html#coloring-categorical-data",
    "title": "Colors",
    "section": "2.2 Coloring categorical data",
    "text": "2.2 Coloring categorical data\n\n\nCode\n# Make up some random levels. (TODO: Better example)\ncountries$category &lt;- factor(sample.int(5L, nrow(countries), TRUE))\n\nfactpal &lt;- colorFactor(topo.colors(5), countries$category)\n\nleaflet(countries) %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~factpal(category))",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Colors"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html",
    "href": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html",
    "title": "Show/Hide Layers",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Show/Hide Layers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html#interactive-layer-display",
    "href": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html#interactive-layer-display",
    "title": "Show/Hide Layers",
    "section": "1.1 Interactive Layer Display",
    "text": "1.1 Interactive Layer Display\n\n\nCode\noutline &lt;- quakes[chull(quakes$long, quakes$lat),]\n\nmap &lt;- leaflet(quakes) %&gt;%\n  # Base groups\n  addTiles(group = \"OSM (default)\") %&gt;%\n  addProviderTiles(providers$Stamen.Toner, group = \"Toner\") %&gt;%\n  addProviderTiles(providers$Stamen.TonerLite, group = \"Toner Lite\") %&gt;%\n  # Overlay groups\n  addCircles(~long, ~lat, ~10^mag/5, stroke = F, group = \"Quakes\") %&gt;%\n  addPolygons(data = outline, lng = ~long, lat = ~lat,\n    fill = F, weight = 2, color = \"#FFFFCC\", group = \"Outline\") %&gt;%\n  # Layers control\n  addLayersControl(\n    baseGroups = c(\"OSM (default)\", \"Toner\", \"Toner Lite\"),\n    overlayGroups = c(\"Quakes\", \"Outline\"),\n    options = layersControlOptions(collapsed = FALSE)\n  )\nmap",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Show/Hide Layers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html#programmatic-layer-display",
    "href": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html#programmatic-layer-display",
    "title": "Show/Hide Layers",
    "section": "1.2 Programmatic Layer Display",
    "text": "1.2 Programmatic Layer Display\nshowGroupやhideGroupを使うことでレイヤーの表示、 非表示をプログラムから管理することが可能となる.\nこれは特にShinyから操作するときに意味を持つ.\n\n\nCode\nmap %&gt;% hideGroup(\"Outline\")",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Show/Hide Layers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch13_projection.html",
    "href": "contents/website/leaflet4r/chapters/ch13_projection.html",
    "title": "Projections",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")\n\n\nleafletではWGS84で特定できる座標値であることが期待されている. 標準では, 3857で表示される. その他の座標系で表示させたいときには Proj4Leafletプラグインを組み込む必要がある.\n\n1 Defining a custom CRS\n\n\nCode\ncrs &lt;- leafletCRS(crsClass = \"L.Proj.CRS\", code = \"ESRI:102003\",\n  proj4def = \"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\",\n  resolutions = 1.5^(25:15))\n\n\n\n\n2 Dsiplaying basemap tiles with cusotm projections\nbasemapが対応していないのでレンダリングが出来ない.\n\n\nCode\n\nepsg3006 &lt;- leafletCRS(crsClass = \"L.Proj.CRS\", code = \"EPSG:3006\",\n  proj4def = \"+proj=utm +zone=33 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\",\n  resolutions = 2^(13:-1), # 8192 down to 0.5\n  origin = c(0, 0)\n)\n\ntile_url &lt;- \"http://api.geosition.com/tile/osm-bright-3006/{z}/{x}/{y}.png\"\ntile_attrib &lt;- \"Map data &copy; &lt;a href='http://www.openstreetmap.org/copyright'&gt;OpenStreetMap contributors&lt;/a&gt;, Imagery &copy; 2013 &lt;a href='http://www.kartena.se/'&gt;Kartena&lt;/a&gt;\"\n\nleaflet(options = leafletOptions(worldCopyJump = F, crs = epsg3006)) %&gt;%\n  setView(11.965053, 57.70451, 13) %&gt;%\n  addTiles(urlTemplate = tile_url,\n    attribution = tile_attrib,\n    options = tileOptions(minZoom = 0, maxZoom = 14, continuousWorld = T)) %&gt;%\n  addMarkers(11.965053, 57.70451)\n\n\n\n\n\n\n\n\nCode\nleaflet() %&gt;%\n  setView(11.965053, 57.70451, 16) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(11.965053, 57.70451)\n\n\n\n\n\n\n\n\n3 Displaying shapes with cusotm projections\n\n\nCode\nlibrary(sp)\nlibrary(albersusa)\n#&gt; Please note that 'maptools' will be retired during October 2023,\n#&gt; plan transition at your earliest convenience (see\n#&gt; https://r-spatial.org/r/2023/05/15/evolution4.html and earlier blogs\n#&gt; for guidance);some functionality will be moved to 'sp'.\n#&gt;  Checking rgeos availability: TRUE\n#&gt; Please note that rgdal will be retired during October 2023,\n#&gt; plan transition to sf/stars/terra functions using GDAL and PROJ\n#&gt; at your earliest convenience.\n#&gt; See https://r-spatial.org/r/2023/05/15/evolution4.html and https://github.com/r-spatial/evolution\n#&gt; rgdal: version: 1.6-7, (SVN revision 1203)\n#&gt; Geospatial Data Abstraction Library extensions to R successfully loaded\n#&gt; Loaded GDAL runtime: GDAL 3.6.2, released 2023/01/02\n#&gt; Path to GDAL shared files: H:/R_USER/R/win-library/4.3/rgdal/gdal\n#&gt;  GDAL does not use iconv for recoding strings.\n#&gt; GDAL binary built with GEOS: TRUE \n#&gt; Loaded PROJ runtime: Rel. 9.2.0, March 1st, 2023, [PJ_VERSION: 920]\n#&gt; Path to PROJ shared files: C:\\Program Files\\PostgreSQL\\12\\share\\contrib\\postgis-3.1\\proj\n#&gt; PROJ CDN enabled: FALSE\n#&gt; Linking to sp version:1.6-1\n#&gt; To mute warnings of possible GDAL/OSR exportToProj4() degradation,\n#&gt; use options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading sp or rgdal.\n#&gt; rgeos version: 0.6-3, (SVN revision 696)\n#&gt;  GEOS runtime version: 3.11.2-CAPI-1.17.2 \n#&gt;  Please note that rgeos will be retired during October 2023,\n#&gt; plan transition to sf or terra functions using GEOS at your earliest convenience.\n#&gt; See https://r-spatial.org/r/2023/05/15/evolution4.html for details.\n#&gt;  GEOS using OverlayNG\n#&gt;  Linking to sp version: 2.0-0 \n#&gt;  Polygon checking: TRUE\n\nspdf &lt;- rmapshaper::ms_simplify(usa_sf(), keep = 0.1)\n#&gt; old-style crs object detected; please recreate object with a recent sf::st_crs()\npal &lt;- colorNumeric(\"Blues\", domain = spdf$pop_2014)\nepsg2163 &lt;- leafletCRS(\n  crsClass = \"L.Proj.CRS\",\n  code = \"EPSG:2163\",\n  proj4def = \"+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\",\n  resolutions = 2^(16:7))\n\nleaflet(spdf, options = leafletOptions(crs = epsg2163)) %&gt;%\n  addPolygons(weight = 1, color = \"#444444\", opacity = 1,\n    fillColor = ~pal(pop_2014), fillOpacity = 0.7, smoothFactor = 0.5,\n    label = ~paste(name, pop_2014),\n    labelOptions = labelOptions(direction = \"auto\"))\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Leaflet for R",
      "Projections"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html",
    "title": "Spatial analysis with US Census data",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\nCode\nlibrary(here)\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(mapview)\n\ncur_dir &lt;- here()\noptions(tigris_use_cache = TRUE)",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#idenfifying-eometrices-within-a-metropolitan-area",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#idenfifying-eometrices-within-a-metropolitan-area",
    "title": "Spatial analysis with US Census data",
    "section": "1.1 Idenfifying eometrices within a metropolitan area",
    "text": "1.1 Idenfifying eometrices within a metropolitan area\ntigrisから2020年のデータを取得して処理する事例を試す.\n\n\nCode\nks_mo_tracts &lt;- \n    map_dfr(c(\"KS\", \"MO\"), \\(x) {\n        tracts(x, cb = TRUE, year = 2020)\n    }) |&gt; \n    st_transform(8528)\n\nkc_metro &lt;- \n    core_based_statistical_areas(cb = TRUE, year = 2020) |&gt; \n    filter(str_detect(NAME, \"Kansas City\")) |&gt; \n    st_transform(8528)\n\nggplot() + \n    geom_sf(data = ks_mo_tracts, fill = \"white\", color = \"grey\") + \n    geom_sf(data = kc_metro, fill = NA, color = \"red\") + \n    theme_void()",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#spatial-subsets",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#spatial-subsets",
    "title": "Spatial analysis with US Census data",
    "section": "1.2 Spatial Subsets",
    "text": "1.2 Spatial Subsets\nSpatial Subsettinghはデータのextentを使う. Spatial predicateが 定義されていることにより、空間部分集合はインデックス記法で記述することが可能である.\n\n\nCode\nkc_tracts &lt;- ks_mo_tracts[kc_metro, ]\n\nggplot() + \n    geom_sf(data = kc_tracts, fill = \"white\", color = \"grey\") + \n    geom_sf(data = kc_metro, fill =NA, color = \"red\") + \n    theme_void()\n\n\n\n\n\n\n\n\n\n上記で抽出対象となるのはinterects演算によるものである.\n\n\n\n\n\n\nNote\n\n\n\nintersectsは交差判定だけなので高速で演算できる. 交差集合を算出するintersectionはそれなりに時間を要する.\n\n\n一般的にはCensusデータ分析において、 あたえた都市領域からデータを抽出する債にはst_withinを使うことが求められる。\n\n\nCode\n# 前述の記法と同じ内容\nkc_tracts_within &lt;- \n    ks_mo_tracts |&gt; \n    st_filter(kc_metro, .predicate = st_within) \n\n\nggplot() + \n    geom_sf(data = kc_tracts_within, fill = \"white\", color = \"grey\") + \n    geom_sf(data = kc_metro, fill =NA, color = \"red\") + \n    theme_void()",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#point-in-polygon",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#point-in-polygon",
    "title": "Spatial analysis with US Census data",
    "section": "2.1 Point in polygon",
    "text": "2.1 Point in polygon\n\n\nCode\nlibrary(mapview)\n\ngainesville_patients &lt;- tibble(\n  patient_id = 1:10,\n  longitude = c(-82.308131, -82.311972, -82.361748, -82.374377, \n                -82.38177, -82.259461, -82.367436, -82.404031, \n                -82.43289, -82.461844),\n  latitude = c(29.645933, 29.655195, 29.621759, 29.653576, \n               29.677201, 29.674923, 29.71099, 29.711587, \n               29.648227, 29.624037)\n)\n\n\n上記のデータをst_as_sfで地理空間データに変換する. これならポイントのテーブルデータを簡単にGISデータに変換することが可能ですね.\n\n\nCode\ngainesville_sf &lt;- \n    gainesville_patients |&gt; \n    st_as_sf(\n        coords = c(\"longitude\", \"latitude\"), \n        crs = 4326\n    ) |&gt; \n    st_transform(6440)\n\ngainesville_sf\n#&gt; Simple feature collection with 10 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 797379.2 ymin: 70862.57 xmax: 816865.2 ymax: 80741.87\n#&gt; Projected CRS: NAD83(2011) / Florida North\n#&gt; # A tibble: 10 × 2\n#&gt;    patient_id            geometry\n#&gt;  *      &lt;int&gt;         &lt;POINT [m]&gt;\n#&gt;  1          1 (812216.7 73640.25)\n#&gt;  2          2 (811825.2 74659.57)\n#&gt;  3          3 (807076.4 70862.57)\n#&gt;  4          4 (805787.7 74365.85)\n#&gt;  5          5  (805023.4 76970.8)\n#&gt;  6          6 (816865.2 76944.63)\n#&gt;  7          7 (806340.6 80741.36)\n#&gt;  8          8   (802799 80741.87)\n#&gt;  9          9 (800134.3 73668.88)\n#&gt; 10         10 (797379.2 70937.49)\n\n\nspatial datasetに変換できたので、 地図上に載せることが可能となる.\n\n\nCode\nmapview(\n    gainesville_sf, \n    col.regions = \"red\", \n    legend = FALSE\n)\n\n\n\n\n\n\nポイントが準備できたので、healthデータを準備する.\n\n\nCode\nalachua_insurance &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"DP03_0096P\",\n  state = \"FL\",\n  county = \"Alachua\",\n  year = 2019,\n  geometry = TRUE\n) %&gt;%\n  select(GEOID, pct_insured = estimate, \n         pct_insured_moe = moe) %&gt;%\n  st_transform(6440)\n#&gt; Getting data from the 2015-2019 5-year ACS\n#&gt; Warning: • You have not set a Census API key. Users without a key are limited to 500\n#&gt; queries per day and may experience performance limitations.\n#&gt; ℹ For best results, get a Census API key at\n#&gt; http://api.census.gov/data/key_signup.html and then supply the key to the\n#&gt; `census_api_key()` function to use it throughout your tidycensus session.\n#&gt; This warning is displayed once per session.\n#&gt; Using the ACS Data Profile\n\n\n上記で得られたデータを紐付ける前に、一度地図上で空間関係を確認する.\n\n\nCode\nmapview(\n    alachua_insurance, \n    zcol = \"pct_insured\", \n    layer.name = \"% with health &lt;br/&gt;insurance\"\n) + \n    mapview(\n        gainesville_sf, \n        col.regions = \"red\", \n        legend = FALSE\n    )\n\n\n\n\n\n\nmap上ならば関係性が明らかである. これをデータとして紐付ける.\n\n\nCode\npatients_joined &lt;- \n    st_join(\n        gainesville_sf, \n        alachua_insurance\n    )\n\npatients_joined\n#&gt; Simple feature collection with 10 features and 4 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 797379.2 ymin: 70862.57 xmax: 816865.2 ymax: 80741.87\n#&gt; Projected CRS: NAD83(2011) / Florida North\n#&gt; # A tibble: 10 × 5\n#&gt;    patient_id            geometry GEOID       pct_insured pct_insured_moe\n#&gt;  *      &lt;int&gt;         &lt;POINT [m]&gt; &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n#&gt;  1          1 (812216.7 73640.25) 12001000700        81.6             7  \n#&gt;  2          2 (811825.2 74659.57) 12001000500        91               5.1\n#&gt;  3          3 (807076.4 70862.57) 12001001515        85.2             6.2\n#&gt;  4          4 (805787.7 74365.85) 12001001603        88.3             5.1\n#&gt;  5          5  (805023.4 76970.8) 12001001100        96.2             2.7\n#&gt;  6          6 (816865.2 76944.63) 12001001902        86               5.9\n#&gt;  7          7 (806340.6 80741.36) 12001001803        92.3             4  \n#&gt;  8          8   (802799 80741.87) 12001001813        97.9             1.4\n#&gt;  9          9 (800134.3 73668.88) 12001002207        95.7             2.4\n#&gt; 10         10 (797379.2 70937.49) 12001002205        96.5             1.6",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#spatial-joins-and-group-wise-spatial-analysis",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#spatial-joins-and-group-wise-spatial-analysis",
    "title": "Spatial analysis with US Census data",
    "section": "2.2 Spatial joins and group-wise spatial analysis",
    "text": "2.2 Spatial joins and group-wise spatial analysis\nポリゴン対ポリゴンの空間結合を行う. ポイント対ポリゴンについては明確であるが、新しい概念であるこの結合では 使用する空間述語には十分に注意すること。\n\n\nCode\ntx_cbsa &lt;- get_acs(\n  geography = \"cbsa\",\n  variables = \"B01003_001\",\n  year = 2019,\n  survey = \"acs1\",\n  geometry = TRUE\n) %&gt;%\n  filter(str_detect(NAME, \"TX\")) %&gt;%\n  slice_max(estimate, n = 4) %&gt;%\n  st_transform(6579)\n#&gt; Getting data from the 2019 1-year ACS\n#&gt; The 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\n\n\nCode\npct_hispanic &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"DP05_0071P\",\n  state = \"TX\",\n  year = 2019,\n  geometry = TRUE\n) %&gt;%\n  st_transform(6579)\n#&gt; Getting data from the 2015-2019 5-year ACS\n#&gt; Using the ACS Data Profile\n\n\n上記のデータを結合する. left = FALSEにすることで内部欠尾久になる. つまり, 返値は4つの都市圏に含まれる場合のみが返される.\n\n\nCode\nhispanic_by_metro &lt;- st_join(\n    pct_hispanic, \n    tx_cbsa, \n    join = st_within , \n    suffix = c(\"_tracs\", \"_metro\"), \n    left = FALSE\n)\n\nhispanic_by_metro\n#&gt; Simple feature collection with 3189 features and 10 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1538292 ymin: 7167200 xmax: 2045975 ymax: 7705975\n#&gt; Projected CRS: NAD83(2011) / Texas Centric Albers Equal Area\n#&gt; First 10 features:\n#&gt;    GEOID_tracs                                 NAME_tracs variable_tracs\n#&gt; 1  48113019204  Census Tract 192.04, Dallas County, Texas     DP05_0071P\n#&gt; 3  48029190601  Census Tract 1906.01, Bexar County, Texas     DP05_0071P\n#&gt; 8  48201311900    Census Tract 3119, Harris County, Texas     DP05_0071P\n#&gt; 9  48113015500     Census Tract 155, Dallas County, Texas     DP05_0071P\n#&gt; 11 48439102000   Census Tract 1020, Tarrant County, Texas     DP05_0071P\n#&gt; 13 48201450200    Census Tract 4502, Harris County, Texas     DP05_0071P\n#&gt; 14 48201450400    Census Tract 4504, Harris County, Texas     DP05_0071P\n#&gt; 22 48157670300 Census Tract 6703, Fort Bend County, Texas     DP05_0071P\n#&gt; 25 48201554502 Census Tract 5545.02, Harris County, Texas     DP05_0071P\n#&gt; 27 48121020503  Census Tract 205.03, Denton County, Texas     DP05_0071P\n#&gt;    estimate_tracs moe_tracs GEOID_metro\n#&gt; 1            58.1       5.7       19100\n#&gt; 3            86.2       6.7       41700\n#&gt; 8            84.9       5.8       26420\n#&gt; 9            56.6       7.5       19100\n#&gt; 11           15.7       6.4       19100\n#&gt; 13           10.7       3.9       26420\n#&gt; 14           26.8      13.1       26420\n#&gt; 22           27.0       5.8       26420\n#&gt; 25           13.4       3.0       26420\n#&gt; 27           35.3       8.7       19100\n#&gt;                                         NAME_metro variable_metro\n#&gt; 1       Dallas-Fort Worth-Arlington, TX Metro Area     B01003_001\n#&gt; 3         San Antonio-New Braunfels, TX Metro Area     B01003_001\n#&gt; 8  Houston-The Woodlands-Sugar Land, TX Metro Area     B01003_001\n#&gt; 9       Dallas-Fort Worth-Arlington, TX Metro Area     B01003_001\n#&gt; 11      Dallas-Fort Worth-Arlington, TX Metro Area     B01003_001\n#&gt; 13 Houston-The Woodlands-Sugar Land, TX Metro Area     B01003_001\n#&gt; 14 Houston-The Woodlands-Sugar Land, TX Metro Area     B01003_001\n#&gt; 22 Houston-The Woodlands-Sugar Land, TX Metro Area     B01003_001\n#&gt; 25 Houston-The Woodlands-Sugar Land, TX Metro Area     B01003_001\n#&gt; 27      Dallas-Fort Worth-Arlington, TX Metro Area     B01003_001\n#&gt;    estimate_metro moe_metro                       geometry\n#&gt; 1         7573136        NA MULTIPOLYGON (((1801563 765...\n#&gt; 3         2550960        NA MULTIPOLYGON (((1642736 726...\n#&gt; 8         7066140        NA MULTIPOLYGON (((1950592 729...\n#&gt; 9         7573136        NA MULTIPOLYGON (((1778712 763...\n#&gt; 11        7573136        NA MULTIPOLYGON (((1746016 762...\n#&gt; 13        7066140        NA MULTIPOLYGON (((1925521 730...\n#&gt; 14        7066140        NA MULTIPOLYGON (((1922292 730...\n#&gt; 22        7066140        NA MULTIPOLYGON (((1935603 728...\n#&gt; 25        7066140        NA MULTIPOLYGON (((1918552 732...\n#&gt; 27        7573136        NA MULTIPOLYGON (((1766859 768...\n\n\n\n\nCode\nhispanic_by_metro %&gt;%\n  mutate(NAME_metro = str_replace(\n      NAME_metro, \n      \", TX Metro Area\", \n      \"\")) %&gt;%\n  ggplot() + \n  geom_density(\n      aes(x = estimate_tracs), \n      color = \"navy\", \n      fill = \"navy\", \n      alpha = 0.4) + \n  theme_minimal() + \n  facet_wrap(~NAME_metro) + \n  labs(title = \"Distribution of Hispanic/Latino population by Census tract\",\n       subtitle = \"Largest metropolitan areas in Texas\",\n       y = \"Kernel density estimate\",\n       x = \"Percent Hispanic/Latino in Census tract\")\n#&gt; Warning: Removed 9 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\n\n\nグループWiseなデータ分析を通じて、 より大きな地物としてrolled upされることになる。\n\n\nCode\nmedian_by_metro &lt;- \n  hispanic_by_metro %&gt;%\n  group_by(NAME_metro) %&gt;%\n  summarize(median_hispanic = median(estimate_tracs, na.rm = TRUE))\n\nmedian_by_metro\n#&gt; Simple feature collection with 4 features and 2 fields\n#&gt; Geometry type: GEOMETRY\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1538292 ymin: 7167200 xmax: 2045975 ymax: 7705975\n#&gt; Projected CRS: NAD83(2011) / Texas Centric Albers Equal Area\n#&gt; # A tibble: 4 × 3\n#&gt;   NAME_metro                           median_hispanic                  geometry\n#&gt;   &lt;chr&gt;                                          &lt;dbl&gt;            &lt;GEOMETRY [m]&gt;\n#&gt; 1 Austin-Round Rock-Georgetown, TX Me…            25.9 POLYGON ((1700741 730245…\n#&gt; 2 Dallas-Fort Worth-Arlington, TX Met…            22.6 POLYGON ((1737530 756507…\n#&gt; 3 Houston-The Woodlands-Sugar Land, T…            32.4 MULTIPOLYGON (((1901162 …\n#&gt; 4 San Antonio-New Braunfels, TX Metro…            53.5 POLYGON ((1619499 717051…\n\n\n\n\nCode\nplot(median_by_metro |&gt; slice(1) |&gt; pull(geometry))",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#area-weighted-areal-interpolation",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#area-weighted-areal-interpolation",
    "title": "Spatial analysis with US Census data",
    "section": "3.1 Area-weighted areal interpolation",
    "text": "3.1 Area-weighted areal interpolation\nst_interploate_awを使うことで, Area-weightd areal interpolationを 実施することが可能となる. つまり、「面積加重の面内補間」である。\nwfh15の値をwfh_20の大きさに補間する処理をしている.\n\n\nCode\nwfh_interpolate_aw &lt;- st_interpolate_aw(\n  wfh_15,\n  wfh_20,\n  extensive = TRUE\n) %&gt;%\n  mutate(GEOID = wfh_20$GEOID)\n\n\n\n\nCode\nmapview(wfh_interpolate_aw)\n\n\n\n\n\n\n上記の処理方法に対して、Census blocksを利用した例を考える. これにはst_interploate_pwを使う。これにより面積以外の情報を考慮した 回帰をおこなうことが可能となる. ここれでは人口を表す国勢調査ブロックを 重みとした例を計算する\ntidycensusのなかにもinterpolate_pwが実装されている。\n\n\nCode\nmaricopa_blocks &lt;- blocks(\n  state = \"AZ\",\n  county = \"Maricopa\",\n  year = 2020\n)\n\nwfh_interpolate_pw &lt;- interpolate_pw(\n  wfh_15,\n  wfh_20,\n  to_id = \"GEOID\",\n  extensive = TRUE, \n  weights = maricopa_blocks,\n  weight_column = \"POP20\",\n  crs = 26949\n)\n\n\nデータの形状が2020に統一されたので、left_joinにより データの結合が可能となる.\n\n\nCode\nlibrary(mapboxapi)\n#&gt; Usage of the Mapbox APIs is governed by the Mapbox Terms of Service.\n#&gt; Please visit https://www.mapbox.com/legal/tos/ for more information.\n\nwfh_shift &lt;- \n    wfh_20 %&gt;%\n    left_join(\n        st_drop_geometry(wfh_interpolate_pw), \n        by = \"GEOID\",\n        suffix = c(\"_2020\", \"_2015\")) %&gt;%\n    mutate(wfh_shift = estimate_2020 - estimate_2015)\n\n# maricopa_basemap &lt;- layer_static_mapbox(\n#   location = wfh_shift,\n#   style_id = \"dark-v9\",\n#   username = \"mapbox\"\n# )\n\nggplot() + \n  # maricopa_basemap + \n  geom_sf(\n      data = wfh_shift, \n      aes(fill = wfh_shift), \n      color = NA, \n      alpha = 0.8) + \n  scale_fill_distiller(palette = \"PuOr\", direction = -1) + \n  labs(fill = \"Shift, 2011-2015 to\\n2016-2020 ACS\",\n       title = \"Change in work-from-home population\",\n       subtitle = \"Maricopa County, Arizona\") + \n  theme_void()\n\n\n\n\n\n\n\n\n\n一応、ブロック形状を面的補間したデータについて数値を確認する.\n\n\nCode\nwfh_interpolate_pw$estimate |&gt; sum()\n#&gt; [1] 105836\n\n\n\n\nCode\nwfh_15$estimate |&gt; sum()\n#&gt; [1] 105836",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#catchment-areas",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#catchment-areas",
    "title": "Spatial analysis with US Census data",
    "section": "4.1 Catchment areas",
    "text": "4.1 Catchment areas\n到達圏分析も行うことができる.\n\n\nCode\niowa_methodist &lt;- filter(ia_trauma, ID == \"0009850308\")\n\nbuf5km &lt;- st_buffer(iowa_methodist, dist = 5000) \n\n\n```{r}\niso10min &lt;- mb_isochrone(\n  iowa_methodist, \n  time = 10, \n  profile = \"driving-traffic\"\n)\nwrite_rds(iso10min, here(cur_dir, \"output\", \"ch07_iso10min.rds\"))\n\n```\n\n\nCode\nlibrary(leaflet)\nlibrary(leafsync)\niso10min &lt;- read_rds(here(cur_dir, \"output\", \"ch07_iso10min.rds\"))\nhospital_icon &lt;- makeAwesomeIcon(icon = \"ios-medical\", \n                                 markerColor = \"red\",\n                                 library = \"ion\")\n\n# The Leaflet package requires data be in CRS 4326\nmap1 &lt;- leaflet() %&gt;% \n  addTiles() %&gt;%\n  addPolygons(data = st_transform(buf5km, 4326)) %&gt;% \n  addAwesomeMarkers(data = st_transform(iowa_methodist, 4326),\n                    icon = hospital_icon)\n\nmap2 &lt;- leaflet() %&gt;% \n  addTiles() %&gt;%\n  addPolygons(data = iso10min) %&gt;% \n  addAwesomeMarkers(data = st_transform(iowa_methodist, 4326),\n                    icon = hospital_icon)\n\nsync(map1, map2)",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#computing-demographic-estimates-for-zones-with-areal-interpolation",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#computing-demographic-estimates-for-zones-with-areal-interpolation",
    "title": "Spatial analysis with US Census data",
    "section": "4.2 Computing demographic estimates for zones with areal interpolation",
    "text": "4.2 Computing demographic estimates for zones with areal interpolation\n\n\nCode\npolk_poverty &lt;- get_acs(\n  geography = \"block group\",\n  variables = c(poverty_denom = \"B17010_001\",\n                poverty_num = \"B17010_002\"),\n  state = \"IA\",\n  county = \"Polk\",\n  geometry = TRUE,\n  output = \"wide\",\n  year = 2020\n) %&gt;%\n  select(poverty_denomE, poverty_numE) %&gt;%\n  st_transform(26975)\n#&gt; Getting data from the 2016-2020 5-year ACS\n\n\n\n\nCode\nlibrary(glue)\n\npolk_blocks &lt;- blocks(\n  state = \"IA\",\n  county = \"Polk\",\n  year = 2020\n)\n\nbuffer_pov &lt;- interpolate_pw(\n  from = polk_poverty, \n  to = buf5km,\n  extensive = TRUE,\n  weights = polk_blocks,\n  weight_column = \"POP20\",\n  crs = 26975\n) %&gt;%\n  mutate(pct_poverty = 100 * (poverty_numE / poverty_denomE))\n\niso_pov &lt;- interpolate_pw(\n  from = polk_poverty, \n  to = iso10min,\n  extensive = TRUE,\n  weights = polk_blocks,\n  weight_column = \"POP20\",\n  crs = 26975\n) %&gt;%\n  mutate(pct_poverty = 100 * (poverty_numE / poverty_denomE))",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#understanding-spatial-neighborhoods",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#understanding-spatial-neighborhoods",
    "title": "Spatial analysis with US Census data",
    "section": "6.1 Understanding spatial neighborhoods",
    "text": "6.1 Understanding spatial neighborhoods\n隣接は空間データ分析において非常に重要な概念である. 探索空間データ分析に向いているのはspdepである.\n隣接の概念には次の３つがある.\n\n6.1.1 距離に基づく隣接性（Proximity-based neighbors）：\n隣接するフィーチャーは、何らかの距離の基準に基づいて識別されます。隣接するものとして定義されるのは、与えられた距離のしきい値内にあるもの（例：与えられたフィーチャーから2km以内のすべてのフィーチャー）や、k-最近傍法（例：与えられたフィーチャーに最も近い8つのフィーチャー）です。\n\n\n6.1.2 グラフに基づく隣接性（Graph-based neighbors）：\n隣接関係はネットワーク関係（例：通りのネットワーク沿い）を通じて定義されます。\n\n\n6.1.3 接触に基づく隣接性（Contiguity-based neighbors）：\n地理的なフィーチャーが多角形の場合に使用されます。接触に基づく空間関係のオプションには、クイーンケースの隣接性が含まれ、少なくとも1つの頂点を共有するすべての多角形が隣接とみなされます。また、ルークケースの隣接性もあり、多角形が少なくとも1つの線分を共有する必要があります。",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#example",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#example",
    "title": "Spatial analysis with US Census data",
    "section": "6.2 Example",
    "text": "6.2 Example\n「queen’s case」」とは２つの多角形が少なくとも１つの頂点を共有している場合に、それらを隣接と見なす方法である。つまり、クイーンケースでは多角形が辺を共有しちえるだけでなく、各（頂点）のみで触れている場合でも隣接しているとみなす。\n\n\nCode\nneighbors &lt;- poly2nb(dfw_tracts, queen = TRUE)\n\nsummary(neighbors)\n#&gt; Neighbour list object:\n#&gt; Number of regions: 1699 \n#&gt; Number of nonzero links: 10930 \n#&gt; Percentage nonzero weights: 0.378646 \n#&gt; Average number of links: 6.433196 \n#&gt; Link number distribution:\n#&gt; \n#&gt;   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17 \n#&gt;   8  52 172 305 396 343 218 112  44  29  11   5   2   1   1 \n#&gt; 8 least connected regions:\n#&gt; 59 438 470 763 1163 1305 1365 1524 with 2 links\n#&gt; 1 most connected region:\n#&gt; 1101 with 17 links\n\n\n\n\nCode\ndfw_coords &lt;- dfw_tracts %&gt;%\n  st_centroid() %&gt;%\n  st_coordinates()\n#&gt; Warning: st_centroid assumes attributes are constant over geometries\n\nplot(dfw_tracts$geometry)\nplot(neighbors, \n     coords = dfw_coords, \n     add = TRUE, \n     col = \"blue\", \n     points = FALSE)",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#generating-the-spatial-weights-matrix",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#generating-the-spatial-weights-matrix",
    "title": "Spatial analysis with US Census data",
    "section": "6.3 Generating the spatial weights matrix",
    "text": "6.3 Generating the spatial weights matrix\nneighbors listをspatial weightsにすることで、空間データ解析が行えるようになる。 nb2listwを使うｔこで対応することができる. style = wを指定すると行に正則となる.\n\n\nCode\nweights &lt;- nb2listw(neighbors, style = \"W\")\n\nweights$weights[[1]]\n#&gt; [1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#moralns-i",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#moralns-i",
    "title": "Spatial analysis with US Census data",
    "section": "7.1 Moralns I",
    "text": "7.1 Moralns I\n\n\nCode\ndfw_tracts$lag_estimate &lt;- lag.listw(weights, dfw_tracts$estimate)\n\n\n\n\nCode\nggplot(dfw_tracts, aes(x = estimate, y = lag_estimate)) + \n  geom_point(alpha = 0.3) + \n  geom_abline(color = \"red\") + \n  theme_minimal() + \n  labs(title = \"Median age by Census tract, Dallas-Fort Worth TX\",\n       x = \"Median age\",\n       y = \"Spatial lag, median age\", \n       caption = \"Data source: 2016-2020 ACS via the tidycensus R package.\\nSpatial relationships based on queens-case polygon contiguity.\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nmoran.test(dfw_tracts$estimate, weights)\n#&gt; \n#&gt;  Moran I test under randomisation\n#&gt; \n#&gt; data:  dfw_tracts$estimate  \n#&gt; weights: weights    \n#&gt; \n#&gt; Moran I statistic standard deviate = 21.275, p-value &lt; 2.2e-16\n#&gt; alternative hypothesis: greater\n#&gt; sample estimates:\n#&gt; Moran I statistic       Expectation          Variance \n#&gt;      0.2926713016     -0.0005889282      0.0001900099\n\n\nモーランテストの帰無仮説は空間に対してランダムな状態にあるということである。ポジティブのときには空間的なクラスターがあるということになる。\n正規化した分散共分散行列と同じであるので平均に対して正の値が集合しているや、負の値が集合しているということが判定できる。\n\n\n\n\n\n\nNote\n\n\n\nThe Moran’s I statistic of 0.292 is positive, and the small p-value suggests that we reject the null hypothesis of spatial randomness in our dataset. (See Section 8.2.4 for additional discussion of p-values). As the statistic is positive, it suggests that our data are spatially clustered; a negative statistic would suggest spatial uniformity. In a practical sense, this means that Census tracts with older populations tend to be located near one another, and Census tracts with younger populations also tend to be found in the same areas.\n\n\n\n\nCode\n# For Gi*, re-compute the weights with `include.self()`\nlocalg_weights &lt;- nb2listw(include.self(neighbors))\n\ndfw_tracts$localG &lt;- spdep::localG(dfw_tracts$estimate, localg_weights)\n\nggplot(dfw_tracts) + \n  geom_sf(aes(fill = as.numeric(localG)), color = NA) + \n  scale_fill_distiller(palette = \"RdYlBu\") + \n  theme_void() + \n  labs(fill = \"Local Gi* statistic\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndfw_tracts &lt;- dfw_tracts %&gt;%\n  mutate(hotspot = case_when(\n    localG &gt;= 2.576 ~ \"High cluster\",\n    localG &lt;= -2.576 ~ \"Low cluster\",\n    TRUE ~ \"Not significant\"\n  ))\n\nggplot(dfw_tracts) + \n  geom_sf(aes(fill = hotspot), color = \"grey90\", size = 0.1) + \n  scale_fill_manual(values = c(\"red\", \"blue\", \"grey\")) + \n  theme_void()",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#lisa",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#lisa",
    "title": "Spatial analysis with US Census data",
    "section": "7.2 LISA",
    "text": "7.2 LISA\n\\[\nI_i = z_i \\sum_{j} w_{ij}z_j\n\\]\n\n\nCode\nset.seed(1983)\n\ndfw_tracts$scaled_estimate &lt;- as.numeric(scale(dfw_tracts$estimate))\n\ndfw_lisa &lt;- localmoran_perm(\n  dfw_tracts$scaled_estimate, \n  weights, \n  nsim = 999L, \n  alternative = \"two.sided\"\n) %&gt;%\n  as_tibble() %&gt;%\n  set_names(c(\"local_i\", \"exp_i\", \"var_i\", \"z_i\", \"p_i\",\n              \"p_i_sim\", \"pi_sim_folded\", \"skewness\", \"kurtosis\"))\n\ndfw_lisa_df &lt;- dfw_tracts %&gt;%\n  select(GEOID, scaled_estimate) %&gt;%\n  mutate(lagged_estimate = lag.listw(weights, scaled_estimate)) %&gt;%\n  bind_cols(dfw_lisa)\n\n\n\n\nCode\ndfw_lisa_clusters &lt;- dfw_lisa_df %&gt;%\n  mutate(lisa_cluster = case_when(\n    p_i &gt;= 0.05 ~ \"Not significant\",\n    scaled_estimate &gt; 0 & local_i &gt; 0 ~ \"High-high\",\n    scaled_estimate &gt; 0 & local_i &lt; 0 ~ \"High-low\",\n    scaled_estimate &lt; 0 & local_i &gt; 0 ~ \"Low-low\",\n    scaled_estimate &lt; 0 & local_i &lt; 0 ~ \"Low-high\"\n  ))\n\n\n\n\nCode\ncolor_values &lt;- c(`High-high` = \"red\", \n                  `High-low` = \"pink\", \n                  `Low-low` = \"blue\", \n                  `Low-high` = \"lightblue\", \n                  `Not significant` = \"white\")\n\nggplot(dfw_lisa_clusters, aes(x = scaled_estimate, \n                              y = lagged_estimate,\n                              fill = lisa_cluster)) + \n  geom_point(color = \"black\", shape = 21, size = 2) + \n  theme_minimal() + \n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  geom_vline(xintercept = 0, linetype = \"dashed\") + \n  scale_fill_manual(values = color_values) + \n  labs(x = \"Median age (z-score)\",\n       y = \"Spatial lag of median age (z-score)\",\n       fill = \"Cluster type\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(dfw_lisa_clusters, aes(fill = lisa_cluster)) + \n  geom_sf(size = 0.1) + \n  theme_void() + \n  scale_fill_manual(values = color_values) + \n  labs(fill = \"Cluster type\")",
    "crumbs": [
      "R",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"ggplot2\", \"readr\", \"tibble\", \"tidyr\", \"forcats\", \n  \"stringr\",\n  \"lubridate\", \"here\", \"systemfonts\", \"magick\", \n  \"scales\", \"grid\",\n  \"grDevices\", \"colorspace\", \"viridis\", \n  \"RColorBrewer\", \"rcartocolor\",\n  \"scico\", \"ggsci\", \"ggthemes\", \"nord\", \n  \"MetBrewer\", \"ggrepel\",\n  \"ggforce\", \"ggtext\", \"ggdist\", \"ggbeeswarm\", \n  \"gghalves\", \"patchwork\", \n  \"palmerpenguins\", \"rnaturalearth\", \"sf\", \"rmapshaper\", \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#the-data",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#the-data",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "1.1 the data",
    "text": "1.1 the data\n\n\nCode\nbikes &lt;- \n    read_csv(\n        here(\n            cur_dir, \n            \"ggplot2-course-data\", \n            \"london-bikes-custom.csv\"),\n        col_types = \"Dcfffilllddddc\"\n    )\n\nbikes$season &lt;- forcats::fct_inorder(bikes$season)\n\n\nbikes |&gt; \n    head() |&gt; \n    rmarkdown::paged_table()",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#the-group",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#the-group",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "1.2 the group",
    "text": "1.2 the group\n\n\nCode\nbikes |&gt; \n    ggplot(aes(x = temp_feel, y = count)) + \n    geom_point(aes(color = season), alpah = .5) +\n    geom_smooth(\n        aes(group = day_night), \n        method = \"lm\"\n    )\n#&gt; Warning in geom_point(aes(color = season), alpah = 0.5): Ignoring unknown\n#&gt; parameters: `alpah`\n#&gt; `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#stat-geom",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#stat-geom",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "1.3 stat, geom",
    "text": "1.3 stat, geom\n\n\n\n\nCode\nggplot(bikes, aes(x = season)) + \n    stat_count(geom = \"bar\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(bikes, aes(x = season)) + \n    geom_bar(stat = \"count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(bikes, aes(x = date, y = temp_feel)) + \n    stat_identity(geom = \"point\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(bikes, aes(x = date, y = temp_feel)) + \n    geom_point(stat = \"identity\")",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#statisitical-summarys",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#statisitical-summarys",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "1.4 Statisitical Summarys",
    "text": "1.4 Statisitical Summarys\n\n\nCode\nggplot(\n    bikes, \n    aes(x = season , y = temp_feel)\n) + \n    geom_boxplot() + \n    stat_summary(\n        fun = mean, \n        geom = \"point\", \n        color = \"#28a87d\", \n        size = 3\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes, \n    aes(x = season, y = temp_feel)\n  ) +\n  stat_summary(\n    fun = mean, \n    fun.max = function(y) mean(y) + sd(y), \n    fun.min = function(y) mean(y) - sd(y) \n  )",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#how-to-work-with-aspect-ratios",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#how-to-work-with-aspect-ratios",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "3.1 How to Work with Aspect Ratios",
    "text": "3.1 How to Work with Aspect Ratios\nRStudioのViewerは、実際にファイルへ出力したときと見た目が異なる。 これに対して１度保存してから確認するということもあるが、chunkのセッティングを調整することもできる. もしくはcamcorderを使うことになる。動かないのでとりあえず無視する.\n\n\nCode\nlibrary(camcorder)\n\n\ngg_record(\n    dir = here(cur_dir, \"output\", \"temp_plots\"), \n    device = \"pdf\", \n    width  =  297, \n    height = 210, \n    units  = \"mm\"\n)\n\ng",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch03_data_communication.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch03_data_communication.html",
    "title": "Data Communication",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\n\n\n\n\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"ggplot2\", \"readr\", \"tibble\", \"tidyr\", \"forcats\", \n  \"stringr\",\n  \"lubridate\", \"here\", \"systemfonts\", \"magick\", \n  \"scales\", \"grid\",\n  \"grDevices\", \"colorspace\", \"viridis\", \n  \"RColorBrewer\", \"rcartocolor\",\n  \"scico\", \"ggsci\", \"ggthemes\", \"nord\", \n  \"MetBrewer\", \"ggrepel\",\n  \"ggforce\", \"ggtext\", \"ggdist\", \"ggbeeswarm\", \n  \"gghalves\", \"patchwork\", \n  \"palmerpenguins\", \"rnaturalearth\", \"sf\", \"rmapshaper\", \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 note\n\nOur data is never a perfect reflectionof the real world.\n\n\nonly a subset\ncollected by humans\ncollected by machines\n\n\nThe best use of data is to teach us what isn’t true\n\n\ndont’ formulate a single statement\nconfront yourself with a falsifiable universal statement\n\nデータヴィジュアライゼーションの参考サイトは次です。実務で１番参考になるのは１番上の、From Data to Vizというサイトです。このサイトでは、R、Python、D3、Reactのコードも記載されており、実務ですぐ使えとなります。\n\nfrom Data to Viz\nData Viz Project\nvisualizationuniverse\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Data Communication"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html",
    "title": "Working with Colors",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggfittext\",\n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Working with Colors"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#rcartocolor",
    "href": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#rcartocolor",
    "title": "Working with Colors",
    "section": "2.1 rcartocolor",
    "text": "2.1 rcartocolor\n\n\nCode\n\n# install.packages(\"rcartocolor\")\n\nggplot(\n    bikes, \n    aes(x = day_night, y = count, \n        fill = season)\n  ) +\n  geom_boxplot() +\n  rcartocolor::scale_fill_carto_d(\n    palette = \"Vivid\" \n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nrcartocolor::display_carto_all(colorblind_friendly = TRUE)",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Working with Colors"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#scico-and-else",
    "href": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#scico-and-else",
    "title": "Working with Colors",
    "section": "2.2 scico and else",
    "text": "2.2 scico and else\n\n\nCode\n# install.packages(\"scico\")\n\nggplot(\n    bikes, \n    aes(x = day_night, y = count, \n        fill = season)\n  ) +\n  geom_boxplot() +\n  scico::scale_fill_scico_d(\n    palette = \"hawaii\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggsci\")\nggplot(\n    bikes, \n    aes(x = day_night, y = count, \n        fill = season)\n  ) +\n  geom_boxplot() +\n  ggsci::scale_fill_npg()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggthemes\")\nggplot(\n    bikes, \n    aes(x = day_night, y = count, \n        fill = season)\n  ) +\n  geom_boxplot() +\n  ggthemes::scale_fill_gdocs()",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Working with Colors"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#emulate-cvd",
    "href": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#emulate-cvd",
    "title": "Working with Colors",
    "section": "5.1 Emulate CVD",
    "text": "5.1 Emulate CVD\n色盲タイプ別の色チェックということだと思う。\n\n\nCode\ndeut &lt;- \n  colorspace::deutan(\n    viridis::turbo(\n      n = 100, direction = -1\n    )\n  )\n\nggplot(\n    bikes, \n    aes(x = temp_feel, y = count,\n        color = temp_feel)\n  ) +\n  geom_point() +\n  scale_color_gradientn(\n    colors = deut\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ng &lt;- \n  ggplot(\n    bikes, \n    aes(x = day_night, y = count, \n        fill = season)\n  ) +\n  geom_boxplot() +\n  scale_fill_manual(\n    values = carto_custom\n  )\n\n# devtools::install_github(\n#   \"clauswilke/colorblindr\"\n# )\n\ncolorblindr::cvd_grid(g)",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Working with Colors"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/index.html",
    "href": "contents/website/graphic_design_with_ggplot2/index.html",
    "title": "Graphic Design with ggplot2",
    "section": "",
    "text": "1 はじめに\n\nrstudioによるggplotのワークショップである\nかなり細かいところまでやっており非常に情報量が多い\nChatGPTの世界であるがやはりハンズオン形式で勉強したい\nサイト\nパッケージだけでなくフォントのインストールも忘れずに\n\n\n\nCode\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\n\n\n\n\nCode\npackages &lt;- c(\n  \"ggplot2\", \"readr\", \"tibble\", \"tidyr\", \"forcats\", \"stringr\",\n  \"lubridate\", \"here\", \"systemfonts\", \"magick\", \"scales\", \"grid\",\n  \"grDevices\", \"colorspace\", \"viridis\", \"RColorBrewer\", \"rcartocolor\",\n  \"scico\", \"ggsci\", \"ggthemes\", \"nord\", \"MetBrewer\", \"ggrepel\",\n  \"ggforce\", \"ggtext\", \"ggdist\", \"ggbeeswarm\", \"gghalves\", \"patchwork\", \n  \"palmerpenguins\", \"rnaturalearth\", \"sf\", \"rmapshaper\", \"devtools\"\n)\n\ninstall.packages(setdiff(packages, rownames(installed.packages())))  \n\n## install {colorblindr} and requirements2\nremotes::install_github(\"wilkelab/cowplot\")\nremotes::install_github(\"clauswilke/colorblindr\")\nremotes::install_git(\"https://git.sr.ht/~hrbrmstr/albersusa\")\n\n\n\n\n2 ggplot2 examples\n\n\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"ggplot2\", \"readr\", \"tibble\", \"tidyr\", \"forcats\", \n  \"stringr\",\n  \"lubridate\", \"here\", \"systemfonts\", \"magick\", \n  \"scales\", \"grid\",\n  \"grDevices\", \"colorspace\", \"viridis\", \n  \"RColorBrewer\", \"rcartocolor\",\n  \"scico\", \"ggsci\", \"ggthemes\", \"nord\", \n  \"MetBrewer\", \"ggrepel\",\n  \"ggforce\", \"ggtext\", \"ggdist\", \"ggbeeswarm\", \n  \"gghalves\", \"patchwork\", \n  \"palmerpenguins\", \"rnaturalearth\", \"sf\", \"rmapshaper\", \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\n\n\nCode\n# フォントのデータベースを作成\nfont_import(prompt = FALSE)\n\n\n\n\nCode\n\n# フォントのデータベースをロード\nloadfonts(device = \"win\")\n\n\n\n\nCode\nbikes &lt;- read_csv(here::here(cur_dir,\"ggplot2-course-data/london-bikes-custom.csv\"))\n#&gt; Rows: 1454 Columns: 14\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr   (3): day_night, season, weather_type\n#&gt; dbl  (10): year, month, count, is_workday, is_weekend, is_holiday, temp, tem...\n#&gt; date  (1): date\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nbikes\n#&gt; # A tibble: 1,454 × 14\n#&gt;    date       day_night  year month season count is_workday is_weekend\n#&gt;    &lt;date&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1 2015-01-04 day        2015     1 winter  6830          0          1\n#&gt;  2 2015-01-04 night      2015     1 winter  2404          0          1\n#&gt;  3 2015-01-05 day        2015     1 winter 14763          1          0\n#&gt;  4 2015-01-05 night      2015     1 winter  5609          1          0\n#&gt;  5 2015-01-06 day        2015     1 winter 14501          1          0\n#&gt;  6 2015-01-06 night      2015     1 winter  6112          1          0\n#&gt;  7 2015-01-07 day        2015     1 winter 16358          1          0\n#&gt;  8 2015-01-07 night      2015     1 winter  4706          1          0\n#&gt;  9 2015-01-08 day        2015     1 winter  9971          1          0\n#&gt; 10 2015-01-08 night      2015     1 winter  5630          1          0\n#&gt; # ℹ 1,444 more rows\n#&gt; # ℹ 6 more variables: is_holiday &lt;dbl&gt;, temp &lt;dbl&gt;, temp_feel &lt;dbl&gt;,\n#&gt; #   humidity &lt;dbl&gt;, wind_speed &lt;dbl&gt;, weather_type &lt;chr&gt;\n\n\n\n\nCode\ncodes &lt;- c(\n    \"0\" = \"Workday\", \n    \"1\" = \"Weekend or Holiday\"\n)\n\nggplot(bikes, aes(temp_feel, count)) +\n    geom_point(\n        color = \"black\", \n        fill = \"white\", \n        shape = 21, \n        size = 2.8\n    ) + \n    geom_point(\n        color = \"white\", \n        size = 2.2\n    ) + \n    geom_point(\n        aes(color = forcats::fct_relabel(season, str_to_title)), \n        size = 2.2, \n        alpha = .55\n    ) + \n    facet_grid(\n        day_night ~ is_workday, \n        scales = \"free_y\", \n        space = \"free_y\", \n        labeller = labeller(\n            day_night = stringr::str_to_title, \n            is_workday = codes\n        )\n    ) + \n    scale_x_continuous(\n        expand = c(.02, .02), \n        breaks = 0:6 * 5, \n        labels = \\(x) paste0(x, \"°C\")\n    ) + \n    scale_y_continuous(\n        expand = c(.1, .1), \n        limits = c(0, NA), \n        breaks = 0:5 * 10000, \n        labels = scales::comma_format()\n    ) + \n    scale_color_manual(\n        values = c(\"#3c89d9\", \"#1ec99b\", \"#F7B01B\", \"#a26e7c\"), \n        name = NULL,\n        guide = guide_legend(override.aes = list(size = 5))\n\n    ) +\n    labs(\n      x = \"Feels-Like Temperature\", y = NULL,\n      caption = \"Data: TfL (Transport for London), Jan 2015 — Dec 2016\",\n      title = \"Reported bike rents versus feels-like temperature in London per time of day, period, and season.\"\n    ) +\n    theme_light(\n        base_size = 14, \n        base_family = \"Calibri\"\n    ) + \n    theme(\n        plot.title.position = \"plot\", \n        plot.caption.position = \"plot\", \n        plot.title = element_text(face = \"bold\", size = rel(.9)) , \n        # axis.text = element_text(family = \"Tabular\"), \n        axis.title.x = element_text(hjust = 0, color = \"grey30\", margin = margin(t = 12)), \n        strip.text = element_text(face = \"bold\", size = rel(1)), \n        strip.text.y.right = element_text(face = \"bold\", size = rel(.7)), \n        panel.grid.major.x = element_blank(), \n        panel.grid.minor = element_blank(), \n        panel.spacing = unit(1.2, \"lines\"), \n        legend.position = \"top\", \n        legend.text = element_text(size = rel(1)), \n        panel.spacing.x = unit(.1, \"lines\"), \n        panel.spacing.y = unit(.1, \"lines\"),\n        \n        legend.key = element_rect(color = \"#f8f8f8\", fill = \"#f8f8f8\"), \n        legend.background = element_rect(color = \"#f8f8f8f8\", fill = \"#f8f8f8f8\"), \n        plot.background = element_rect(color = \"#f8f8f8\", fill = \"#f8f8f8\")\n        \n    )\n#&gt; Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n#&gt; not found in Windows font database\n\n#&gt; Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n#&gt; not found in Windows font database\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n#&gt; Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n#&gt; not found in Windows font database\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n#&gt; Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n#&gt; not found in Windows font database\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n#&gt; Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n#&gt; font family not found in Windows font database\n\n#&gt; Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n#&gt; font family not found in Windows font database\n\n#&gt; Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n#&gt; font family not found in Windows font database\n\n#&gt; Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n#&gt; font family not found in Windows font database\n\n#&gt; Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n#&gt; font family not found in Windows font database\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Graphic Design with ggplot2",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/GGally/working.html",
    "href": "contents/libs/GGally/working.html",
    "title": "GGally",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/GGally\")\nCode\n library(ComplexHeatmap)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "libs",
      "GGally"
    ]
  },
  {
    "objectID": "contents/libs/GGally/working.html#ggparcoord",
    "href": "contents/libs/GGally/working.html#ggparcoord",
    "title": "GGally",
    "section": "2.1 ggparcoord",
    "text": "2.1 ggparcoord\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           showPoints = TRUE) +\n           scale_color_brewer(palette = \"Set2\") \n\n\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           splineFactor = TRUE) +\n           scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\nスケールを色々と変更することが可能である。\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           scale = \"robust\") +\n           scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           scale = \"uniminmax\") +\n           scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           scale = \"globalminmax\") +\n           scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           scale = \"center\") +\n           scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           scale = \"centerObs\") +\n           scale_color_brewer(palette = \"Set2\")",
    "crumbs": [
      "libs",
      "GGally"
    ]
  },
  {
    "objectID": "contents/libs/ggfittext/working.html",
    "href": "contents/libs/ggfittext/working.html",
    "title": "ggfittext",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/ggfit\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggfittext\",\n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "libs",
      "ggfittext"
    ]
  },
  {
    "objectID": "contents/libs/ggfittext/working.html#fitting-text-inside-a-box",
    "href": "contents/libs/ggfittext/working.html#fitting-text-inside-a-box",
    "title": "ggfittext",
    "section": "2.1 Fitting text inside a box",
    "text": "2.1 Fitting text inside a box\n\n\nCode\n\nggplot(animals, aes(x = type, y = flies, label = animal)) +\n  geom_tile(fill = \"white\", colour = \"black\") +\n  geom_fit_text()",
    "crumbs": [
      "libs",
      "ggfittext"
    ]
  },
  {
    "objectID": "contents/libs/ggfittext/working.html#reflowing-text",
    "href": "contents/libs/ggfittext/working.html#reflowing-text",
    "title": "ggfittext",
    "section": "2.2 Reflowing text",
    "text": "2.2 Reflowing text\n\n\nCode\nggplot(animals, aes(x = type, y = flies, label = animal)) +\n  geom_tile(fill = \"white\", colour = \"black\") +\n  geom_fit_text(reflow = TRUE)",
    "crumbs": [
      "libs",
      "ggfittext"
    ]
  },
  {
    "objectID": "contents/libs/ggfittext/working.html#growing-text",
    "href": "contents/libs/ggfittext/working.html#growing-text",
    "title": "ggfittext",
    "section": "2.3 Growing text",
    "text": "2.3 Growing text\n\n\nCode\n\nggplot(animals, aes(x = type, y = flies, label = animal)) +\n  geom_tile(fill = \"white\", colour = \"black\") +\n  geom_fit_text(reflow = TRUE, grow = TRUE)",
    "crumbs": [
      "libs",
      "ggfittext"
    ]
  },
  {
    "objectID": "contents/libs/ggfittext/working.html#placing-text",
    "href": "contents/libs/ggfittext/working.html#placing-text",
    "title": "ggfittext",
    "section": "2.4 Placing text",
    "text": "2.4 Placing text\n\n\nCode\nggplot(animals, aes(x = type, y = flies, label = animal)) +\n  geom_tile(fill = \"white\", colour = \"black\") +\n  geom_fit_text(place = \"topleft\", reflow = TRUE)",
    "crumbs": [
      "libs",
      "ggfittext"
    ]
  },
  {
    "objectID": "contents/libs/ggfittext/working.html#bar-plots",
    "href": "contents/libs/ggfittext/working.html#bar-plots",
    "title": "ggfittext",
    "section": "2.5 Bar plots",
    "text": "2.5 Bar plots\ngeom_bar_textを使うだけで簡単に作成することができてしまう。\n\n\nCode\n\nggplot(altitudes, aes(x = craft, y = altitude, label = altitude)) +\n  geom_col() +\n  geom_bar_text()\n\n\n\n\n\n\n\n\n\n\n\nCode\n\nggplot(beverages, aes(x = beverage, y = proportion, label = ingredient,\n                    fill = ingredient)) +\n  geom_col(position = \"stack\") +\n  geom_bar_text(position = \"stack\", reflow = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(beverages, aes(x = beverage, y = proportion, label = ingredient,\n                    fill = ingredient)) +\n  geom_col(position = \"dodge\") +\n  geom_bar_text(position = \"dodge\", grow = TRUE, reflow = TRUE, \n                place = \"left\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nリッチなテクストを配置することも可能である。\n\n\nCode\nggplot(animals_rich, aes(x = type, y = flies, label = animal)) +\n  geom_tile(fill = \"white\", colour = \"black\") +\n  geom_fit_text(reflow = TRUE, grow = TRUE, rich = TRUE)",
    "crumbs": [
      "libs",
      "ggfittext"
    ]
  },
  {
    "objectID": "contents/libs/ggfittext/working.html#specifying-the-box-limits",
    "href": "contents/libs/ggfittext/working.html#specifying-the-box-limits",
    "title": "ggfittext",
    "section": "2.6 Specifying the box limits",
    "text": "2.6 Specifying the box limits\n\n\nCode\nggplot(presidential, aes(ymin = start, ymax = end, x = party, label = name)) +\n  geom_fit_text(grow = TRUE) +\n  geom_errorbar(alpha = 0.5)",
    "crumbs": [
      "libs",
      "ggfittext"
    ]
  },
  {
    "objectID": "contents/libs/ggfittext/working.html#experimental-feaure-text-inpolar-coordinates",
    "href": "contents/libs/ggfittext/working.html#experimental-feaure-text-inpolar-coordinates",
    "title": "ggfittext",
    "section": "2.7 Experimental feaure: text inpolar coordinates",
    "text": "2.7 Experimental feaure: text inpolar coordinates\n\n\nCode\np &lt;- ggplot(gold, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, \n                 fill = linenumber, label = line)) +\n  coord_polar() +\n  geom_rect() +\n  scale_fill_gradient(low = \"#fee391\", high = \"#238443\")\n\np + geom_fit_text(min.size = 0, grow = TRUE)",
    "crumbs": [
      "libs",
      "ggfittext"
    ]
  },
  {
    "objectID": "contents/libs/ggfittext/working.html#other-useful-arguments",
    "href": "contents/libs/ggfittext/working.html#other-useful-arguments",
    "title": "ggfittext",
    "section": "2.8 Other useful arguments",
    "text": "2.8 Other useful arguments\ncontrastパラメータは自動でテキストの色を反転させることができる。\n\n\nCode\n\nggplot(animals, aes(x = type, y = flies, fill = mass, label = animal)) +\n  geom_tile() +\n  geom_fit_text(reflow = TRUE, grow = TRUE, contrast = TRUE)",
    "crumbs": [
      "libs",
      "ggfittext"
    ]
  },
  {
    "objectID": "contents/libs/ggtext/working.html",
    "href": "contents/libs/ggtext/working.html",
    "title": "ggtext",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/ggtext\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "libs",
      "ggtext"
    ]
  },
  {
    "objectID": "contents/libs/ggtext/working.html#markdown-in-theme-elements",
    "href": "contents/libs/ggtext/working.html#markdown-in-theme-elements",
    "title": "ggtext",
    "section": "2.1 Markdown in theme elements",
    "text": "2.1 Markdown in theme elements\nthemeでelementを指定することで、html要素を使うことが可能となる。 たとえば&lt;br&gt;で改行したり、iタグでCSSを当てたりすることができる.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(glue)\n\ndata &lt;- tibble(\n  bactname = c(\"Staphylococcaceae\", \"Moraxella\", \"Streptococcus\", \"Acinetobacter\"),\n  OTUname = c(\"OTU 1\", \"OTU 2\", \"OTU 3\", \"OTU 4\"),\n  value = c(-0.5, 0.5, 2, 3)\n)\n\ndata %&gt;% mutate(\n  color = c(\"#009E73\", \"#D55E00\", \"#0072B2\", \"#000000\"),\n  name = glue(\"&lt;i style='color:{color}'&gt;{bactname}&lt;/i&gt; ({OTUname})\"),\n  name = fct_reorder(name, value)\n)  %&gt;%\n  ggplot(aes(value, name, fill = color)) + \n  geom_col(alpha = 0.5) + \n  scale_fill_identity() +\n  labs(caption = \"Example posted on **stackoverflow.com**&lt;br&gt;(using made-up data)\") +\n  theme(\n    axis.text.y = element_markdown(),\n    plot.caption = element_markdown(lineheight = 1.2)\n  )\n\n\n\n\n\n\n\n\n\nimgタグを使うことも可能である。\n\n\nCode\nlabels &lt;- c(\n  setosa = \"&lt;img src='unnamed-chunk-6-1.png'\n    width='100' /&gt;&lt;br&gt;*I. setosa*\",\n  virginica = \"&lt;img src='unnamed-chunk-4-1.png'\n    width='100' /&gt;&lt;br&gt;*I. virginica*\",\n  versicolor = \"&lt;img src='unnamed-chunk-5-1.png'\n    width='100' /&gt;&lt;br&gt;*I. versicolor*\"\n)\n\nggplot(iris, aes(Species, Sepal.Width)) +\n  geom_boxplot() +\n  scale_x_discrete(\n    name = NULL,\n    labels = labels\n  ) +\n  theme(\n    axis.text.x = element_markdown(color = \"black\", size = 11)\n  )\n\n\n\n\n\n\n\n\n\ngeom_textbox_simpleなどでは、直接マークダウン要素を使うことが可能である。\n\n\nCode\nggplot(mtcars, aes(disp, mpg)) + \n  geom_point() +\n  labs(\n    title = \"&lt;b&gt;Fuel economy vs. engine displacement&lt;/b&gt;&lt;br&gt;\n    &lt;span style = 'font-size:10pt'&gt;Lorem ipsum *dolor sit amet,*\n    consectetur adipiscing elit, **sed do eiusmod tempor incididunt** ut\n    labore et dolore magna aliqua. &lt;span style = 'color:red;'&gt;Ut enim\n    ad minim veniam,&lt;/span&gt; quis nostrud exercitation ullamco laboris nisi\n    ut aliquip ex ea commodo consequat.&lt;/span&gt;\",\n    x = \"displacement (in&lt;sup&gt;3&lt;/sup&gt;)\",\n    y = \"Miles per gallon (mpg)&lt;br&gt;&lt;span style = 'font-size:8pt'&gt;A measure of\n    the car's fuel efficiency.&lt;/span&gt;\"\n  ) +\n  theme(\n    plot.title.position = \"plot\",\n    plot.title = element_textbox_simple(\n      size = 13,\n      lineheight = 1,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"cornsilk\"\n    ),\n    axis.title.x = element_textbox_simple(\n      width = NULL,\n      padding = margin(4, 4, 4, 4),\n      margin = margin(4, 0, 0, 0),\n      linetype = 1,\n      r = grid::unit(8, \"pt\"),\n      fill = \"azure1\"\n    ),\n    axis.title.y = element_textbox_simple(\n      hjust = 0,\n      orientation = \"left-rotated\",\n      minwidth = unit(1, \"in\"),\n      maxwidth = unit(2, \"in\"),\n      padding = margin(4, 4, 2, 4),\n      margin = margin(0, 0, 2, 0),\n      fill = \"lightsteelblue1\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nfacetのラベルにも使うことが可能である。\n\n\nCode\n\nlibrary(cowplot)\n\nggplot(mpg, aes(cty, hwy)) + \n  geom_point() +\n  facet_wrap(~class) +\n  theme_half_open(12) +\n  background_grid() +\n  theme(\n    strip.background = element_blank(),\n    strip.text = element_textbox(\n      size = 12,\n      color = \"white\", fill = \"#5D729D\", box.color = \"#4A618C\",\n      halign = 0.5, linetype = 1, r = unit(5, \"pt\"), width = unit(1, \"npc\"),\n      padding = margin(2, 0, 1, 0), margin = margin(3, 3, 3, 3)\n    )\n  )\n\n\n\n\n\n\n\n\n\nCode\n#&gt; Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n#&gt; Please use the `linewidth` argument instead.\n#&gt; Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n#&gt; Please use the `linewidth` argument instead.",
    "crumbs": [
      "libs",
      "ggtext"
    ]
  },
  {
    "objectID": "contents/libs/ggtext/working.html#geoms",
    "href": "contents/libs/ggtext/working.html#geoms",
    "title": "ggtext",
    "section": "2.2 Geoms",
    "text": "2.2 Geoms\ngeom_richtextを使うとテキストボックスの加工が色々と可能となる。\n\n\nCode\ndf &lt;- tibble(\n  label = c(\n    \"Some text **in bold.**\",\n    \"Linebreaks&lt;br&gt;Linebreaks&lt;br&gt;Linebreaks\",\n    \"*x*&lt;sup&gt;2&lt;/sup&gt; + 5*x* + *C*&lt;sub&gt;*i*&lt;/sub&gt;\",\n    \"Some &lt;span style='color:blue'&gt;blue text **in bold.**&lt;/span&gt;&lt;br&gt;And *italics text.*&lt;br&gt;\n    And some &lt;span style='font-size:18pt; color:black'&gt;large&lt;/span&gt; text.\"\n  ),\n  x = c(.2, .1, .5, .9),\n  y = c(.8, .4, .1, .5),\n  hjust = c(0.5, 0, 0, 1),\n  vjust = c(0.5, 1, 0, 0.5),\n  angle = c(0, 0, 45, -45),\n  color = c(\"black\", \"blue\", \"black\", \"red\"),\n  fill = c(\"cornsilk\", \"white\", \"lightblue1\", \"white\")\n)\n\n\nggplot(df) +\n  aes(\n    x, y, label = label, angle = angle, color = color, fill = fill,\n    hjust = hjust, vjust = vjust\n  ) +\n  geom_richtext() +\n  geom_point(color = \"black\", size = 2) +\n  scale_color_identity() +\n  scale_fill_identity() +\n  xlim(0, 1) + ylim(0, 1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df) +\n  aes(\n    x, y, label = label, angle = angle, color = color,\n    hjust = hjust, vjust = vjust\n  ) +\n  geom_richtext(\n    fill = NA, label.color = NA, # remove background and outline\n    label.padding = grid::unit(rep(0, 4), \"pt\") # remove padding\n  ) +\n  geom_point(color = \"black\", size = 2) +\n  scale_color_identity() +\n  xlim(0, 1) + ylim(0, 1)\n\n\n\n\n\n\n\n\n\ngeom_textboxは、word wrapさせることも可能である。ただし、回転角度についてはサポートされれていない。\n\n\nCode\n\ndf &lt;- tibble(\n  label = rep(\"Lorem ipsum dolor **sit amet,** consectetur adipiscing elit,\n    sed do *eiusmod tempor incididunt* ut labore et dolore magna\n    aliqua.\", 2),\n  x = c(0, .6),\n  y = c(1, .6),\n  hjust = c(0, 0),\n  vjust = c(1, 0),\n  orientation = c(\"upright\", \"right-rotated\"),\n  color = c(\"black\", \"blue\"),\n  fill = c(\"cornsilk\", \"white\")\n)\n\nggplot(df) +\n  aes(\n    x, y, label = label, color = color, fill = fill,\n    hjust = hjust, vjust = vjust,\n    orientation = orientation\n  ) +\n  geom_textbox(width = unit(0.4, \"npc\")) +\n  geom_point(color = \"black\", size = 2) +\n  scale_discrete_identity(aesthetics = c(\"color\", \"fill\", \"orientation\")) +\n  xlim(0, 1) + ylim(0, 1)",
    "crumbs": [
      "libs",
      "ggtext"
    ]
  },
  {
    "objectID": "contents/libs/igraph/r_interface.html",
    "href": "contents/libs/igraph/r_interface.html",
    "title": "igraph",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/igraph\")\n\n\n\n\nCode\nbox::use(\n  igraph[...],\n  ggplot2[...],\n  cowplot[...], \n  showtext[showtext_auto], \n  sysfonts[font_add_google],\n)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 Create a graph\n\n\nCode\ng &lt;- make_empty_graph()\n\ng\n#&gt; IGRAPH b3eb4e8 D--- 0 0 -- \n#&gt; + edges from b3eb4e8:\n\n\n\nmake_graph系はエッジからグラフを構築する\n10個の頂点と2のエッジからなるグラフは次である\n\nedgeは1to2, 1to5の2つである\n\n\n\n\nCode\ng &lt;- make_graph(\n  edges = c(1, 2,\n            1, 5), \n  n = 10, \n  directed = FALSE\n)\ng\n#&gt; IGRAPH b3f59b2 U--- 10 2 -- \n#&gt; + edges from b3f59b2:\n#&gt; [1] 1--2 1--5\n\n\n\n\nCode\nplot(g)\n\n\n\n\n\n\n\n\n\n特殊な式でエッジを指定することも可能である。 ~からはじめることで次のようにエッジを指定できる\n\n\nCode\n# ノードはエッジと重複していても大丈夫である\nmake_graph(\n  ~ 1--2, 1--5, 3, 4, 5\n) |&gt; plot()\n\n\n\n\n\n\n\n\n\nグラフの名前を使ってグラフを構築することも可能である。 たとえば、Zacharyというグラフは次のように構築できる\n\n\nCode\nmake_graph(\"Zachary\") |&gt; plot()\n\n\n\n\n\n\n\n\n\nより詳しい詳細はこの後のチュートリアルに追記される。\n\n\n2 Vertex and edge IDs\nigraphの頂点とエッジはそれぞれ数値IDを有してる。 頂点のIDは必ず1からはじまる。また数値IDは連続しています。 たとえばサブグラフが構築されたときには、 サブグラフの中で頂点番号が振り直されます。 *Pythonとかは0からはじまるので注意すること。\n\n\n3 Adding/deleting vertices and edges\n\n\nCode\ng &lt;- make_empty_graph()\ng &lt;- add_vertices(g, 3) # 頂点を3つ追加する\nplot(g)\n\n\n\n\n\n\n\n\n\nもう一度頂点を追加してみる。頂点が３つ追加されて、６つになっていることがわかる。\n\n\nCode\ng &lt;- add_vertices(g, 3)\nplot(g)\n\n\n\n\n\n\n\n\n\n次はエッジを追加する。存在しない頂点番号を指定するとエラーになる。\n\n\nCode\ng &lt;- add_edges(g, edges = c(1, 3, 1, 6, 3, 4))\nplot(g)\n\n\n\n\n\n\n\n\n\n次の記法でも追加することが出来る。 ただしadd_edgesを使う方が効率的である。\n\n\nCode\ng + edges(c(2, 5))\n#&gt; IGRAPH b45a58d D--- 6 4 -- \n#&gt; + edges from b45a58d:\n#&gt; [1] 1-&gt;3 1-&gt;6 3-&gt;4 2-&gt;5\n\n\nより複雑な追加をしてみる.\n\n\nCode\ng &lt;- \n  make_graph(\"Zachary\") |&gt; \n  add_edges(edges = c(1, 34)) |&gt; \n  add_vertices(6) |&gt; \n  add_edges(edges = c(38, 39, 39, 40, 40, 38, 40, 37))\n\ng\n#&gt; IGRAPH b460e3c U--- 40 83 -- Zachary\n#&gt; + attr: name (g/c)\n#&gt; + edges from b460e3c:\n#&gt;  [1]  1-- 2  1-- 3  1-- 4  1-- 5  1-- 6  1-- 7  1-- 8  1-- 9  1--11  1--12\n#&gt; [11]  1--13  1--14  1--18  1--20  1--22  1--32  2-- 3  2-- 4  2-- 8  2--14\n#&gt; [21]  2--18  2--20  2--22  2--31  3-- 4  3-- 8  3--28  3--29  3--33  3--10\n#&gt; [31]  3-- 9  3--14  4-- 8  4--13  4--14  5-- 7  5--11  6-- 7  6--11  6--17\n#&gt; [41]  7--17  9--31  9--33  9--34 10--34 14--34 15--33 15--34 16--33 16--34\n#&gt; [51] 19--33 19--34 20--34 21--33 21--34 23--33 23--34 24--26 24--28 24--33\n#&gt; [61] 24--34 24--30 25--26 25--28 25--32 26--32 27--30 27--34 28--34 29--32\n#&gt; [71] 29--34 30--33 30--34 31--33 31--34 32--33 32--34 33--34  1--34 38--39\n#&gt; + ... omitted several edges\n\n\nVertexのIDは常に1からはじまり連続的である。 Vertexがrenumberedされたとき、エッジはrenumberedされない。 しかし、エッジの頂点はrenumberedされる。\ndelete_verticesとdelete_edgesを使い操作してみる。\n\n\nCode\n# エッジ番号\nget.edge.ids(g, c(1, 34))\n#&gt; [1] 79\n\n\n\n\nCode\n# delete\ng &lt;- delete_edges(g, 79)\nget.edge.ids(g, c(1, 34)) # return 0\n#&gt; [1] 0\n\n\n例題として次のグラフを考える。リングを作成して、1と10のエッジを削除する。\n\n\nCode\ng &lt;- make_ring(10) %&gt;% delete_edges(\"10|1\")\nplot(g)\n\n\n\n\n\n\n\n\n\n同様に色々試す。\n\n\nCode\ng &lt;- make_ring(5)\ng &lt;- delete_edges(g, get.edge.ids(g, c(1,5, 4,5)))\nplot(g)\n\n\n\n\n\n\n\n\n\nリテラル表現のグラフ構造である。\n\n\nCode\ng1 &lt;- graph_from_literal(A-B:C:I, B-A:C:D, C-A:B:E:H, D-B:E:F,\n                         E-C:D:F:H, F-D:E:G, G-F:H, H-C:E:G:I,\n                         I-A:H)\nplot(g1)\n\n\n\n\n\n\n\n\n\nグラフがchordalであるのかを判定し、足りないエッジを調べる。\n\n\nCode\nis_chordal(g1, fillin=TRUE)\n#&gt; $chordal\n#&gt; [1] FALSE\n#&gt; \n#&gt; $fillin\n#&gt;  [1] 2 6 8 7 5 7 2 7 6 1 7 1\n#&gt; \n#&gt; $newgraph\n#&gt; NULL\n\n\n足りないエッジを追加する。\n\n\nCode\nchordal_graph &lt;- add_edges(g1, is_chordal(g1, fillin=TRUE)$fillin)\nplot(chordal_graph)\n\n\n\n\n\n\n\n\n\n\n\n4 Constucting graphs\n\n\n5 Setting and retrieving graph\n\n\n6 Structural properties\n\n\n7 Querying the graph\n\n\n8 Treating a graph as an adjacency\n\n\n9 Layouts and plotting\n\n\n10 igraph and the outside world\n\n\n11 where to go next\n\n\n12 Session info\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "libs",
      "igraph"
    ]
  },
  {
    "objectID": "contents/libs/treemapify/working.html",
    "href": "contents/libs/treemapify/working.html",
    "title": "treemapify",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/treemapify\")\nCode\nlibrary(treemapify)\nlibrary(showtext)\nlibrary(dplyr)\nlibrary(ggplot2)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "libs",
      "treemapify"
    ]
  },
  {
    "objectID": "contents/libs/treemapify/working.html#sample-data",
    "href": "contents/libs/treemapify/working.html#sample-data",
    "title": "treemapify",
    "section": "2.1 sample data",
    "text": "2.1 sample data\n\n\nCode\ngroup &lt;- paste(\"Group\", 1:9)\nsubgroup &lt;- c(\"A\", \"C\", \"B\", \"A\", \"A\",\n              \"C\", \"C\", \"B\", \"B\")\nvalue &lt;- c(7, 25, 50, 5, 16,\n           18, 30, 12, 41)\n\ndf &lt;- data.frame(group, subgroup, value) \ndf\n#&gt;     group subgroup value\n#&gt; 1 Group 1        A     7\n#&gt; 2 Group 2        C    25\n#&gt; 3 Group 3        B    50\n#&gt; 4 Group 4        A     5\n#&gt; 5 Group 5        A    16\n#&gt; 6 Group 6        C    18\n#&gt; 7 Group 7        C    30\n#&gt; 8 Group 8        B    12\n#&gt; 9 Group 9        B    41",
    "crumbs": [
      "libs",
      "treemapify"
    ]
  },
  {
    "objectID": "contents/libs/treemapify/working.html#fill-by-categorical",
    "href": "contents/libs/treemapify/working.html#fill-by-categorical",
    "title": "treemapify",
    "section": "2.2 Fill by categorical",
    "text": "2.2 Fill by categorical\n\n\nCode\nggplot(df, aes(area = value, fill = group)) +\n  geom_treemap()",
    "crumbs": [
      "libs",
      "treemapify"
    ]
  },
  {
    "objectID": "contents/libs/treemapify/working.html#fill-by-the-numerical-variable",
    "href": "contents/libs/treemapify/working.html#fill-by-the-numerical-variable",
    "title": "treemapify",
    "section": "2.3 Fill by the numerical variable",
    "text": "2.3 Fill by the numerical variable\n\n\nCode\n\nggplot(df, aes(area = value, fill = value)) +\n  geom_treemap()",
    "crumbs": [
      "libs",
      "treemapify"
    ]
  },
  {
    "objectID": "contents/libs/treemapify/working.html#with-label",
    "href": "contents/libs/treemapify/working.html#with-label",
    "title": "treemapify",
    "section": "2.4 with label",
    "text": "2.4 with label\n\n\nCode\nggplot(df, aes(area = value, fill = group, label = value)) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"white\",\n                    place = \"centre\",\n                    size = 15)",
    "crumbs": [
      "libs",
      "treemapify"
    ]
  },
  {
    "objectID": "contents/libs/treemapify/working.html#label-grow",
    "href": "contents/libs/treemapify/working.html#label-grow",
    "title": "treemapify",
    "section": "2.5 label grow",
    "text": "2.5 label grow\n\n\nCode\nggplot(df, aes(area = value, fill = value, label = group)) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"white\",\n                    place = \"centre\",\n                    size = 15,\n                    grow = TRUE)",
    "crumbs": [
      "libs",
      "treemapify"
    ]
  },
  {
    "objectID": "contents/libs/treemapify/working.html#sub-group",
    "href": "contents/libs/treemapify/working.html#sub-group",
    "title": "treemapify",
    "section": "2.6 sub group",
    "text": "2.6 sub group\n\n\nCode\nggplot(df, aes(area = value, fill = value,\n               label = group, subgroup = subgroup)) +\n  geom_treemap() +\n  geom_treemap_subgroup_border(colour = \"white\", size = 5) +\n  geom_treemap_subgroup_text(place = \"centre\", grow = TRUE,\n                             alpha = 0.25, colour = \"black\",\n                             fontface = \"italic\") +\n  geom_treemap_text(colour = \"white\", place = \"centre\",\n                    size = 15, grow = TRUE)",
    "crumbs": [
      "libs",
      "treemapify"
    ]
  },
  {
    "objectID": "contents/libs/treemapify/working.html#color-customize",
    "href": "contents/libs/treemapify/working.html#color-customize",
    "title": "treemapify",
    "section": "2.7 color customize",
    "text": "2.7 color customize\n\n\nCode\nggplot(df, aes(area = value, fill = group, label = value)) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"white\",\n                    place = \"centre\",\n                    size = 15) +\n  scale_fill_brewer(palette = \"Blues\")",
    "crumbs": [
      "libs",
      "treemapify"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch02_統計の基礎知識.html#section",
    "href": "contents/books/01_Rによる実証分析_2e/ch02_統計の基礎知識.html#section",
    "title": "ch02 統計の基礎知識",
    "section": "3.1 2.1",
    "text": "3.1 2.1\n\n\nCode\nx &lt;- runif(100)\nprint(mean(x))\n#&gt; [1] 0.5322551\nprint(var(x))\n#&gt; [1] 0.08880894\nprint(sd(x))\n#&gt; [1] 0.2980083\n\n\n\n\nCode\nparams &lt;- tibble(\n    n_sample =  c(1:1000), \n    n_simulate = 1000\n)\nresult &lt;- \n    params |&gt; \n    mutate(res = map2(n_sample, n_simulate, \\(x, y) replicate(y, mean(runif(x))))) |&gt; \n    mutate(mean = map_dbl(res, mean), sd = map_dbl(res, sd))\n\nresult |&gt; \n    ggplot(aes(x = n_sample, y = mean)) + \n    geom_line() + \n    scale_x_log10() + \n    scale_y_continuous(\n        breaks = 0:10 * .1, \n        limits = c(0, 1)) + \n    geom_ribbon(\n        aes(ymin = mean - sd, ymax = mean + sd), fill = \"pink\", alpha = .5) + \n    theme(\n        panel.grid.minor = element_blank()\n    )",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch02 統計の基礎知識"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch02_統計の基礎知識.html#section-1",
    "href": "contents/books/01_Rによる実証分析_2e/ch02_統計の基礎知識.html#section-1",
    "title": "ch02 統計の基礎知識",
    "section": "3.2 2.2",
    "text": "3.2 2.2\n\n\nCode\nlibrary(correlation)\n\ndf &lt;- tibble(\n    x = runif(100), \n    y = rnorm(100, 0, 1), \n    z = 1.3 * x - .7 * y\n)\n\nuv &lt;- \n    combn(syms(c(\"x\", \"y\", \"z\")), 2, simplify = FALSE)\n\ngraphs &lt;-\n    uv |&gt; \n    map(\\(x) ggplot(df, aes(!!x[[1]], !!x[[2]])) + geom_point()) |&gt; \n    list_modify(ncol = 1)\n\ndo.call(grid.arrange, graphs)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf |&gt; \n    correlation()\n#&gt; # Correlation Matrix (pearson-method)\n#&gt; \n#&gt; Parameter1 | Parameter2 |     r |         95% CI |  t(98) |         p\n#&gt; ---------------------------------------------------------------------\n#&gt; x          |          y |  0.05 | [-0.15,  0.25] |   0.52 | 0.603    \n#&gt; x          |          z |  0.43 | [ 0.25,  0.58] |   4.69 | &lt; .001***\n#&gt; y          |          z | -0.88 | [-0.92, -0.83] | -18.34 | &lt; .001***\n#&gt; \n#&gt; p-value adjustment method: Holm (1979)\n#&gt; Observations: 100\n\n\n標本サイズと相関係数の関係を確認する。\n\n\nCode\nlibrary(correlation)\n\nparams &lt;- tibble(\n    n_sample =  c(10:1000), \n    n_simulate = 100\n)\nresult &lt;- \n    params |&gt; \n    mutate(res1 = map2(\n        n_sample, \n        n_simulate, \n        \\(x, y) replicate(\n            y, {\n                xx = runif(x)\n                yy = rnorm(x, 0, 1)\n                zz = 1.3 * xx - .7 * yy\n                c(\"xy\" = cor(xx, yy), \"yz\" = cor(yy, zz), \"zx\" = cor(zz, xx))\n            }))) |&gt; \n    mutate(res1 = map(res1, \\(x) as_tibble(t(x)))) |&gt; \n    mutate(res1 = map(res1, \\(x) summarise(x, across(everything(), .fns = list(mean = mean, sd = sd))))) |&gt; \n    unnest(res1)\n\n\n\n\nCode\ncols &lt;- c(\"xy\", \"yz\", \"zx\")\ngraphs &lt;- \n    map(cols, \\(x) {\n        u &lt;- sym(glue::glue(\"{x}_mean\"))\n        v &lt;- sym(glue::glue(\"{x}_sd\"))\n        result |&gt; \n            ggplot(aes(x = n_sample, y = !!u)) + \n            geom_path() + \n            geom_ribbon(\n                aes(ymin = !!u - !!v, ymax = !!u + !!v), fill = \"pink\", alpha = .5)\n    }) |&gt; \n    list_modify(ncol = 1)\n\ndo.call(grid.arrange, graphs)",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch02 統計の基礎知識"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch04_回帰分析の基礎.html#気温と電力使用量",
    "href": "contents/books/01_Rによる実証分析_2e/ch04_回帰分析の基礎.html#気温と電力使用量",
    "title": "ch04 回帰分析の基礎",
    "section": "2.1 気温と電力使用量",
    "text": "2.1 気温と電力使用量\n\n\nCode\npath &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap03/temperature.csv\")\ntempdata  &lt;- \n    read_csv(path, show_col_types = FALSE) |&gt; \n    filter(between(strftime(date), strftime(\"2014/8/1\"), strftime(\"2014/8/31\")))\nhead(tempdata)\n#&gt; # A tibble: 6 × 5\n#&gt;   date     time    elec  prec  temp\n#&gt;   &lt;chr&gt;    &lt;time&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2014/8/1 00:00   3193     0  27.9\n#&gt; 2 2014/8/1 01:00   2960     0  27.9\n#&gt; 3 2014/8/1 02:00   2807     0  27.1\n#&gt; 4 2014/8/1 03:00   2748     0  26.8\n#&gt; 5 2014/8/1 04:00   2735     0  26.9\n#&gt; 6 2014/8/1 05:00   2736     0  27.3\n\n\n\n\nCode\nsummaries &lt;- tempdata |&gt; skim() |&gt; partition()\n\nmap(summaries, paged_table)\n#&gt; $character\n#&gt;   skim_variable n_missing complete_rate min max empty n_unique whitespace\n#&gt; 1          date         0             1   8   9     0       31          0\n#&gt; \n#&gt; $difftime\n#&gt;   skim_variable n_missing complete_rate    min        max     median n_unique\n#&gt; 1          time         0             1 0 secs 82800 secs 41400 secs       24\n#&gt; \n#&gt; $numeric\n#&gt;   skim_variable n_missing complete_rate        mean          sd     p0      p25\n#&gt; 1          elec         0             1 3398.486559 714.3446713 2213.0 2823.500\n#&gt; 2          prec         0             1    0.141129   0.9451167    0.0    0.000\n#&gt; 3          temp         0             1   27.668414   3.5454533   19.8   25.675\n#&gt;      p50      p75   p100  hist\n#&gt; 1 3342.5 3871.500 4980.0 ▆▇▇▃▃\n#&gt; 2    0.0    0.000   17.0 ▇▁▁▁▁\n#&gt; 3   28.0   30.125   35.5 ▃▂▇▅▂\n\n\nまずはデータをプロットしてみる。\n\n\nCode\ntempdata |&gt; \n    filter(between(strftime(date), strftime(\"2014/8/1\"), strftime(\"2014/8/31\"))) |&gt; \n    ggplot(aes(x = time, y = temp)) +\n    geom_point() + \n    xlab(\"時刻\") + \n    ylab(\"気温\") + \n    ggtitle(\"2014年8月における時刻と気温の関係\")\n\n\n\n\n\n\n\n\n\nノンパラメトリック回帰として時刻ごとの平均値を求める。\n\n\nCode\ntempdata |&gt; \n    group_by(time) |&gt; \n    summarise(across(c(temp), .fns = list(mean = mean, sd = sd, max = max, min = min)))\n#&gt; # A tibble: 24 × 5\n#&gt;    time   temp_mean temp_sd temp_max temp_min\n#&gt;    &lt;time&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 00:00       26.4    2.75     29.3     20.1\n#&gt;  2 01:00       26.1    2.73     29.3     19.8\n#&gt;  3 02:00       25.9    2.69     28.7     19.8\n#&gt;  4 03:00       25.8    2.61     28.3     20.2\n#&gt;  5 04:00       25.7    2.67     28.5     19.9\n#&gt;  6 05:00       25.8    2.76     28.6     20.1\n#&gt;  7 06:00       26.4    3.10     29.8     19.9\n#&gt;  8 07:00       27.3    3.41     31.1     20.2\n#&gt;  9 08:00       28.1    3.54     32.1     20.5\n#&gt; 10 09:00       29.0    3.80     33.5     20.9\n#&gt; # ℹ 14 more rows\n\n\n\n\nCode\ntempdata |&gt; \n    filter(between(strftime(date), strftime(\"2014/8/1\"), strftime(\"2014/8/31\"))) |&gt; \n    ggplot(aes(x = time, y = temp)) + \n    geom_point() + \n    stat_summary(geom = \"line\", fun = \"mean\") + \n    xlab(\"時間\") + \n    ylab(\"気温\") + \n    coord_cartesian()\n\n\n\n\n\n\n\n\n\n相関係数を見てみると、電力と気温には強い正の関係があることがわかる。\n\n\nCode\nlibrary(correlation)\ncorrelation(tempdata)\n#&gt; # Correlation Matrix (pearson-method)\n#&gt; \n#&gt; Parameter1 | Parameter2 |     r |         95% CI | t(742) |         p\n#&gt; ---------------------------------------------------------------------\n#&gt; elec       |       prec | -0.08 | [-0.15, -0.01] |  -2.20 | 0.028*   \n#&gt; elec       |       temp |  0.72 | [ 0.68,  0.75] |  28.25 | &lt; .001***\n#&gt; prec       |       temp | -0.16 | [-0.23, -0.09] |  -4.31 | &lt; .001***\n#&gt; \n#&gt; p-value adjustment method: Holm (1979)\n#&gt; Observations: 744\n\n\n\n\nCode\ntempdata |&gt; \n    ggplot(aes(temp, elec)) + \n    geom_point()\n\n\n\n\n\n\n\n\n\nノンパラメトリック回帰として、気温ごとの平均値を算出する。\n\n\nCode\ntempdata |&gt; \n    transmute(temp = round(temp), elec, time = hour(time)) |&gt; \n    ggplot(aes(x = temp, y = elec)) +\n    geom_point(aes(color = time)) + \n    stat_summary(geom= \"line\", fun = \"mean\", color = \"red\") + \n    theme(panel.grid.minor = element_blank()) + \n    scale_color_viridis_b(\n        breaks = c(0, 6, 12, 18, 24), \n        limits = c(0, 24)\n    )",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch04 回帰分析の基礎"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch06_相関関係と因果関係.html#疑似相関",
    "href": "contents/books/01_Rによる実証分析_2e/ch06_相関関係と因果関係.html#疑似相関",
    "title": "ch06 相関関係と因果関係",
    "section": "2.1 疑似相関",
    "text": "2.1 疑似相関\n下記のグラフを見ると、ゲーム時間が少ないほど、得点が高いという負の相関が見られる。\nただし本当にこれを因果関係とみるのかは別の問題である。なぜなら、家庭環境などの交絡情報が含まれている可能性が高いためである。 また、実際には点そのものではなく、勉強時間などへの影響をみるべきである。そうでないと、 ゲームの時間を減らしただけで点があがるのかということになる。\nつまり、家庭環境や勉強時間を条件付けた上で、ゲームの時間がテストの点に与える影響を見る必要がある。\n\n\nCode\npath &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap06/video_game.csv\")\nvideodata &lt;- read_csv(path, show_col_types = FALSE)\nvideodata\n#&gt; # A tibble: 500 × 2\n#&gt;    grade hours\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1    81   0  \n#&gt;  2    47   2.4\n#&gt;  3    59   2  \n#&gt;  4    47   2.2\n#&gt;  5    22   2.3\n#&gt;  6    49   1.7\n#&gt;  7    78   0  \n#&gt;  8    59   0  \n#&gt;  9    42   2.2\n#&gt; 10    43   1.6\n#&gt; # ℹ 490 more rows\n\n\n\n\nCode\nvideodata |&gt; \n    ggplot(aes(hours, grade)) + \n    geom_point()",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch06 相関関係と因果関係"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch06_相関関係と因果関係.html#同時性",
    "href": "contents/books/01_Rによる実証分析_2e/ch06_相関関係と因果関係.html#同時性",
    "title": "ch06 相関関係と因果関係",
    "section": "2.2 同時性",
    "text": "2.2 同時性\n同時性とは「２つの事柄について、お互いがお互いの原因であり同時に結果である」という状態である。 需要と供給で言えば、ニーズが高まれば多く供給されるが、多く供給されることでニーズが減るという、関係性にある。",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch06 相関関係と因果関係"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch08_ランダム化実験.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch08_ランダム化実験.html",
    "title": "ch08 ランダム化実験",
    "section": "",
    "text": "1 Setup\n\n\n2 はじめに\nまず本当に独立な説明変数であったら、その変数が含まれているのかどうか係数に影響しないことを確認する。 下記から変数にバイアスが生じていないことが確認できる。\n\n\nCode\nn  &lt;- 100\nbeta0 &lt;- 0.5\nbeta1 &lt;- 1\nbeta2 &lt;- 2\n\nresult &lt;- replicate(300, {\n    x1 &lt;- rnorm(n, sd = .8)\n    x2 &lt;- rnorm(n, sd = 1.5) + 3\n    y &lt;- beta0 + beta1 * x1 + beta2 * x2 + rnorm(n)\n    coef(lm(y ~ x1))[2]\n})\n\nhist(result)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsummary(result)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; -0.2883  0.7491  1.0277  1.0248  1.3080  2.6383\n\n\nただし、定数項は別なので注意すること。\n\n\nCode\nn  &lt;- 100\nbeta0 &lt;- 0.5\nbeta1 &lt;- 1\nbeta2 &lt;- 2\n\nresult &lt;- replicate(300, {\n    x1 &lt;- rnorm(n, sd = .8)\n    x2 &lt;- rnorm(n, sd = 1.5) + 3\n    y &lt;- beta0 + beta1 * x1 + beta2 * x2 + rnorm(n)\n    coef(lm(y ~ x1))[1]\n})\n\nhist(result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch08 ランダム化実験"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch10_回帰不連続デザイン.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch10_回帰不連続デザイン.html",
    "title": "ch10 回帰不連続デザイン",
    "section": "",
    "text": "1 Setup\n\n\n2 はじめに\n「テストの点数が60未満を対象に強制した補講」の学習に与える効果を考える。 ランダムなトリートメントではないためこれまでの比較は困難であるが、 60点前後の人達については実際にはランダムに近い状態にある。 ここではその情報を使って処理を行う。\nもう少し詳しくいうとある変数\\(Z_i\\)がわかればトリートメントの状態が 判断できるというデータセットについて考える。\n「ぎりぎり補講にならなかった学生」と「ぎりぎり補講になった学生」の比較なら、 補講の効果を判断しやすいはずという考えである。\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch10 回帰不連続デザイン"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch12_パネルデータ.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch12_パネルデータ.html",
    "title": "ch12 パネルデータ",
    "section": "",
    "text": "1 Setup\n\n\n2 固定効果モデル\n1997年から2019年の47都道府県別の失業率と自殺死亡率のデータを使う.\n\npref: 都道府県\nyear: 年度\nsuicide: 自殺率\nunemp: 完全失業率\n\n上記のデータに対して、失業などの経済的な不安定が心身の 疲弊を招き、最悪の場合には自殺に至るという考える。\n\n\nCode\npath_to_file &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap12/prefecture.csv\")\nprefdata &lt;- read_csv(path_to_file, show_col_types = FALSE)\nprefdata |&gt; \n    head() |&gt; \n    paged_table()\n\n\n\n  \n\n\n\n\n\nCode\nprefdata |&gt; \n    lm(suicide ~ -1 + unemp + pref, data = _) |&gt; \n    tidy() |&gt; \n    select(term, estimate) |&gt; \n    filter(term == \"unemp\")\n#&gt; # A tibble: 1 × 2\n#&gt;   term  estimate\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;\n#&gt; 1 unemp     3.07\n\n\n上記の結果は失業率が１％上昇すると、１０万人あたりの自殺者が３人増えるという結果である。\n次に、被説明変数と説明変数からそれぞれの都道府県別平均を引いた変数を作成して、 within推定を実施する。\n\n\nCode\nprefdata |&gt; \n    group_by(pref) |&gt; \n    mutate(\n        suicidebar = mean(suicide), \n        unempbar = mean(unemp), \n        suicide2 = suicide - suicidebar, \n        unemp2 = unemp - unempbar\n    ) |&gt; \n    lm(suicide2 ~ -1 + unemp2, data = _) |&gt; \n    tidy() |&gt; \n    select(term, estimate)\n#&gt; # A tibble: 1 × 2\n#&gt;   term   estimate\n#&gt;   &lt;chr&gt;     &lt;dbl&gt;\n#&gt; 1 unemp2     3.07\n\n\n上記の結果は、都道府県について固定したモデルである。 ここでさらに、時間について固定した、２方向固定効果モデルについて検討する。\n\n\nCode\nlibrary(fixest)\nprefdata |&gt; \n    feols(suicide ~ unemp | pref + year)\n#&gt; OLS estimation, Dep. Var.: suicide\n#&gt; Observations: 1,081 \n#&gt; Fixed-effects: pref: 47,  year: 23\n#&gt; Standard-errors: Clustered (pref) \n#&gt;       Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; unemp 0.770868   0.298724 2.58054 0.013121 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.76097     Adj. R2: 0.8622  \n#&gt;                 Within R2: 0.022751\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch12 パネルデータ"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch01_導入.html",
    "href": "contents/books/02_因果推論ミックステープ/ch01_導入.html",
    "title": "ch01 導入",
    "section": "",
    "text": "1 はじめに\n\n本書は因果推論のプログラム的な部分を補間する内容である\n扱っている内容\n\n潜在アウトカムモデル\n実験デザイン\nマッチング\n操作変数法\n回帰不連続デザイン\nパネルデータ\nDAG：非巡回的有向グラフ\n\n扱っていない内容\n\n合成コントロール\nグラフィカルモデル\n\n\n\n\n2 最適化はすべてを内生化する\n\n\n\n\n\n\nCaution\n\n\n\n「因果を発見した」というあらゆる主張には、その正当化のために事前の知識が必要不可欠である。\n\n\n\n\n\n\n\n\nCaution\n\n\n\n信頼できる、価値のある研究をするためには、特定の意図した結果を求めるのではなく、 方法論に則って正しく研究をおこなうことが、より重要でえある。\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch01 導入"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html",
    "href": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html",
    "title": "ch03 非巡回的有向グラフ",
    "section": "",
    "text": "DAG記法では因果関係は一方向にしか進まない\nDAGでは需要供給のような同時性を扱うことはできない\n因果関係には2つの生じ方がある\n\n直接働く場合\n第三の変数を媒介する場合\n\nこれをバックドアパスという\n第三の変数が未観測のときバックドアが開いてるという\nバックドアがある状態では因果不明である\nこのような第三因子を交絡因子ともいう\n\n\nDAGはデータが関係あるということを示している\n\n\n\nバックドアとなる変数が観測されているときに、正しく推定ができるのかを確認してみる。次の例は、xがyに直接影響を与える場合と、xがzを経由してyに影響を与えることがわかる。両方とも観測できていれば、それぞれがyに与える影響を正しく推定出来てることがわかる。\n\n\nCode\ndataset &lt;- tibble(\n    x = rnorm(100), \n    z = 2 * x + rnorm(100), \n    y = 1 + x + 3 * z + rnorm(100) \n) \n\nfit &lt;- lm(y ~ x + z, data = dataset)\n\nfit |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x + z, data = dataset)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1.85428 -0.74562 -0.00954  0.82648  2.44393 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  1.04295    0.10084  10.343  &lt; 2e-16 ***\n#&gt; x            1.38382    0.20241   6.837 7.22e-10 ***\n#&gt; z            2.84427    0.09043  31.452  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.006 on 97 degrees of freedom\n#&gt; Multiple R-squared:  0.9829, Adjusted R-squared:  0.9825 \n#&gt; F-statistic:  2781 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nxがzを経由してyに影響を与えている分もあるので、zを見込まないとバイアスが入ることがわかる。\n\n\nCode\ndataset &lt;- tibble(\n    x = rnorm(100), \n    z = 2 * x + rnorm(100), \n    y = 1 + x + 3 * z + rnorm(100) \n) \n\nfit &lt;- lm(y ~ x, data = dataset)\n\nfit |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, data = dataset)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -7.2801 -1.5175  0.1067  1.8525 10.1467 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.1068     0.3101   3.569 0.000557 ***\n#&gt; x             7.0884     0.3114  22.764  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.096 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.841,  Adjusted R-squared:  0.8393 \n#&gt; F-statistic: 518.2 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nバックドアパスが開いているときに、回帰分析をすると、 推定値の符号を反転させるほどの深刻な問題が引き起こされる。 このためデータ分析の際には、バックドアパスを閉じることにする。\nバックドアを閉じる方法は次の２つである。\n\n交絡因子を条件付ける(マッチングなど)\n合流点を作りバックドアを閉じる\n\nすべてのバックドアパスが閉じているときに、そのリサーチデザインはバックドア基準を満たすということになる。\nバックドアの合流点があるときに、その合流点を制御することで、バックドアパスが閉じられることになる。アウトカムとなる変数について、関心がある変数以外の合流点があるときにはその合流点を制御するだけで、バックドア基準を満たすことができる。　\n合流点バイアスの考え方は非常に難しいわ。\n\n\nCode\nlibrary(stargazer)\n\ntb &lt;- tibble(\n    female  = ifelse(runif(10000) &gt; .5, 1, 0), \n    ability = rnorm(10000), \n    discrimination = female, \n    occupation = 1 + 2 * ability + 0 * female - 2 * discrimination  + rnorm(10000), \n    wage       = 1 - 1 * discrimination  + 1 * occupation + 2 * ability + rnorm(10000)\n)\n\nlm_1 &lt;- lm(wage ~ female, tb)\nlm_2 &lt;- lm(wage ~ female + occupation, tb)\nlm_3 &lt;- lm(wage ~ female + occupation + ability, tb)\n\n\n# warningが出るが問題ない\n# chunkで実行する場合にだけwarningが発生してくる\nstargazer(lm_1, lm_2, lm_3, type = \"text\", omit.stat = \"all\", column.labels = c(\"a\", \"b\", \"c\"))\n#&gt; \n#&gt; ========================================\n#&gt;                 Dependent variable:     \n#&gt;            -----------------------------\n#&gt;                        wage             \n#&gt;                a         b         c    \n#&gt;               (1)       (2)       (3)   \n#&gt; ----------------------------------------\n#&gt; female     -2.970***  0.611*** -0.992***\n#&gt;             (0.085)   (0.030)   (0.028) \n#&gt;                                         \n#&gt; occupation            1.798*** 0.990*** \n#&gt;                       (0.006)   (0.010) \n#&gt;                                         \n#&gt; ability                        2.025*** \n#&gt;                                 (0.022) \n#&gt;                                         \n#&gt; Constant    2.013***  0.222*** 1.011*** \n#&gt;             (0.060)   (0.020)   (0.017) \n#&gt;                                         \n#&gt; ========================================\n#&gt; ========================================\n#&gt; Note:        *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nDAGにおいて合流点をコントロールすると、つまり今回の場合には職業をコントロールすると、 差別と賃金の間に、関係全体を歪めるほど強力活誤ったバックドアパスが 開いてしまう。職業と能力をコントロールした場合にのみ、 賃金に対するジェンダーの直接的な因果効果を分離することができるのである。\nバックドアとDAGのノート",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch03 非巡回的有向グラフ"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html#シミュレーション1",
    "href": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html#シミュレーション1",
    "title": "ch03 非巡回的有向グラフ",
    "section": "",
    "text": "バックドアとなる変数が観測されているときに、正しく推定ができるのかを確認してみる。次の例は、xがyに直接影響を与える場合と、xがzを経由してyに影響を与えることがわかる。両方とも観測できていれば、それぞれがyに与える影響を正しく推定出来てることがわかる。\n\n\nCode\ndataset &lt;- tibble(\n    x = rnorm(100), \n    z = 2 * x + rnorm(100), \n    y = 1 + x + 3 * z + rnorm(100) \n) \n\nfit &lt;- lm(y ~ x + z, data = dataset)\n\nfit |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x + z, data = dataset)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1.85428 -0.74562 -0.00954  0.82648  2.44393 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  1.04295    0.10084  10.343  &lt; 2e-16 ***\n#&gt; x            1.38382    0.20241   6.837 7.22e-10 ***\n#&gt; z            2.84427    0.09043  31.452  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.006 on 97 degrees of freedom\n#&gt; Multiple R-squared:  0.9829, Adjusted R-squared:  0.9825 \n#&gt; F-statistic:  2781 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nxがzを経由してyに影響を与えている分もあるので、zを見込まないとバイアスが入ることがわかる。\n\n\nCode\ndataset &lt;- tibble(\n    x = rnorm(100), \n    z = 2 * x + rnorm(100), \n    y = 1 + x + 3 * z + rnorm(100) \n) \n\nfit &lt;- lm(y ~ x, data = dataset)\n\nfit |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, data = dataset)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -7.2801 -1.5175  0.1067  1.8525 10.1467 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.1068     0.3101   3.569 0.000557 ***\n#&gt; x             7.0884     0.3114  22.764  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.096 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.841,  Adjusted R-squared:  0.8393 \n#&gt; F-statistic: 518.2 on 1 and 98 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch03 非巡回的有向グラフ"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html#バックドア基準",
    "href": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html#バックドア基準",
    "title": "ch03 非巡回的有向グラフ",
    "section": "",
    "text": "バックドアパスが開いているときに、回帰分析をすると、 推定値の符号を反転させるほどの深刻な問題が引き起こされる。 このためデータ分析の際には、バックドアパスを閉じることにする。\nバックドアを閉じる方法は次の２つである。\n\n交絡因子を条件付ける(マッチングなど)\n合流点を作りバックドアを閉じる\n\nすべてのバックドアパスが閉じているときに、そのリサーチデザインはバックドア基準を満たすということになる。\nバックドアの合流点があるときに、その合流点を制御することで、バックドアパスが閉じられることになる。アウトカムとなる変数について、関心がある変数以外の合流点があるときにはその合流点を制御するだけで、バックドア基準を満たすことができる。　\n合流点バイアスの考え方は非常に難しいわ。\n\n\nCode\nlibrary(stargazer)\n\ntb &lt;- tibble(\n    female  = ifelse(runif(10000) &gt; .5, 1, 0), \n    ability = rnorm(10000), \n    discrimination = female, \n    occupation = 1 + 2 * ability + 0 * female - 2 * discrimination  + rnorm(10000), \n    wage       = 1 - 1 * discrimination  + 1 * occupation + 2 * ability + rnorm(10000)\n)\n\nlm_1 &lt;- lm(wage ~ female, tb)\nlm_2 &lt;- lm(wage ~ female + occupation, tb)\nlm_3 &lt;- lm(wage ~ female + occupation + ability, tb)\n\n\n# warningが出るが問題ない\n# chunkで実行する場合にだけwarningが発生してくる\nstargazer(lm_1, lm_2, lm_3, type = \"text\", omit.stat = \"all\", column.labels = c(\"a\", \"b\", \"c\"))\n#&gt; \n#&gt; ========================================\n#&gt;                 Dependent variable:     \n#&gt;            -----------------------------\n#&gt;                        wage             \n#&gt;                a         b         c    \n#&gt;               (1)       (2)       (3)   \n#&gt; ----------------------------------------\n#&gt; female     -2.970***  0.611*** -0.992***\n#&gt;             (0.085)   (0.030)   (0.028) \n#&gt;                                         \n#&gt; occupation            1.798*** 0.990*** \n#&gt;                       (0.006)   (0.010) \n#&gt;                                         \n#&gt; ability                        2.025*** \n#&gt;                                 (0.022) \n#&gt;                                         \n#&gt; Constant    2.013***  0.222*** 1.011*** \n#&gt;             (0.060)   (0.020)   (0.017) \n#&gt;                                         \n#&gt; ========================================\n#&gt; ========================================\n#&gt; Note:        *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nDAGにおいて合流点をコントロールすると、つまり今回の場合には職業をコントロールすると、 差別と賃金の間に、関係全体を歪めるほど強力活誤ったバックドアパスが 開いてしまう。職業と能力をコントロールした場合にのみ、 賃金に対するジェンダーの直接的な因果効果を分離することができるのである。\nバックドアとDAGのノート",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch03 非巡回的有向グラフ"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch05_マッチングと層別化.html",
    "href": "contents/books/02_因果推論ミックステープ/ch05_マッチングと層別化.html",
    "title": "ch05 マッチングと層別化",
    "section": "",
    "text": "Warning\n\n\n\n全然理解出来ていないので、もう一度読み込むこと。 考え方はわかったが、数式の部分でわかっていない。 誤植がありそうなので別の図書を見た方がよいかもしれない。",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch05 マッチングと層別化"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch05_マッチングと層別化.html#識別のための仮定",
    "href": "contents/books/02_因果推論ミックステープ/ch05_マッチングと層別化.html#識別のための仮定",
    "title": "ch05 マッチングと層別化",
    "section": "1.1 識別のための仮定",
    "text": "1.1 識別のための仮定\n交絡因子があるときに因果効果を推定するためには、 CIAが成り立つこと、処置の確率が各層で0より大きく1より小さいこと。",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch05 マッチングと層別化"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch07_操作変数.html",
    "href": "contents/books/02_因果推論ミックステープ/ch07_操作変数.html",
    "title": "ch07 操作変数",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch07 操作変数"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch09_差分の差デザイン.html",
    "href": "contents/books/02_因果推論ミックステープ/ch09_差分の差デザイン.html",
    "title": "ch09 差分の差デザイン",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch09 差分の差デザイン"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch11_結論.html",
    "href": "contents/books/02_因果推論ミックステープ/ch11_結論.html",
    "title": "ch11 結論",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch11 結論"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch01_本書の狙い.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch01_本書の狙い.html",
    "title": "ch01 本書のねらい",
    "section": "",
    "text": "1 シミュレーションでわかること\n「相関係数が0.5のときの２変量確率変数をN個観測したときに実際に得られる相関係数」 についてシミュレーションをおこなう。これを観ると、サンプルサイズが２５では、非常にバラツキが大きいことがわかる。\n\n\nCode\nrho &lt;- .5\nn &lt;- 25\niter &lt;- 1000\n\nr &lt;- rep(0, iter)\n\nset.seed(123)\nfor (i in 1:iter) {\n  Y1 &lt;- rnorm(n, 0, 1)\n  Y2 &lt;- Y1 * rho + rnorm(n, 0, (1 - rho^2)^.5)\n  r[i] &lt;- cor(Y1, Y2)\n}\n\n\nhist(r)\n\n\n\n\n\n\n\n\n\nサンプルサイズを増やすと推定値のバラツキが小さくなることがわかる。\n\n\nCode\nrho &lt;- .5\nn &lt;- 100\niter &lt;- 1000\n\nr &lt;- rep(0, iter)\n\nset.seed(123)\nfor (i in 1:iter) {\n  Y1 &lt;- rnorm(n, 0, 1)\n  Y2 &lt;- Y1 * rho + rnorm(n, 0, (1 - rho^2)^.5)\n  r[i] &lt;- cor(Y1, Y2)\n}\n\n\nhist(r)\n\n\n\n\n\n\n\n\n\nさらに検定もおこなう. ここでは無相関検定\\(\\rho = 0\\)をおこなう。 具体的には無相関検定をおこなってp値を求める。求めたp値が、0.05を下回る 回数の割合を求める.\nこの結果を見ると仮定が正しいときに、p値が0.05を下回る確率は5%であることがわかる。\n\n\nCode\nrho &lt;- .0\nn &lt;- 25\niter &lt;- 10000\n\nr &lt;- rep(0, iter)\n\nset.seed(123)\nfor (i in 1:iter) {\n  Y1 &lt;- rnorm(n, 0, 1)\n  Y2 &lt;- Y1 * rho + rnorm(n, 0, (1 - rho^2)^.5)\n  r[i] &lt;- cor.test(Y1, Y2)$p.value\n}\n\nifelse(r &lt;= .05, 1, 0) |&gt; mean()\n#&gt; [1] 0.0488\n\n\n検定が適切におこなわれれば、p値は一様分布にしたがう。\n\n\nCode\nhist(r)\n\n\n\n\n\n\n\n\n\nまた、正しく検定がおこわれればp値はサンプルサイズに依らない。\n\n\nCode\nrho &lt;- .0\nn &lt;- 2500\niter &lt;- 10000\n\nr &lt;- rep(0, iter)\n\nset.seed(123)\nfor (i in 1:iter) {\n  Y1 &lt;- rnorm(n, 0, 1)\n  Y2 &lt;- Y1 * rho + rnorm(n, 0, (1 - rho^2)^.5)\n  r[i] &lt;- cor.test(Y1, Y2)$p.value\n}\n\nifelse(r &lt;= .05, 1, 0) |&gt; mean()\n#&gt; [1] 0.0486\n\n\n誤った尾統計分析としてあるのは、検定において データの検定結果を見てから「も少しで有意になるからサンプルサイズをもう少し大きくしよう」とデータを足すことです。\nこれが問題となることをシミュレーションで確認する。このシミュレーションから、後からデータを追加することは、p値のバイアスを生むことがわかる。\n\n\nCode\n## 設定と準備\nrho &lt;- 0\nn &lt;- 25\niter &lt;- 10000\nalpha &lt;- 0.05\n\n# 結果を格納するオブジェクト\np &lt;- rep(0, iter)\n\n## シミュレーション\nset.seed(123)\nfor (i in 1:iter) {\n  # 最初のデータ\n  Y1 &lt;- rnorm(n, 0, 1)\n  Y2 &lt;- Y1 * rho + rnorm(n, 0, (1 - rho^2)^0.5)\n  p[i] &lt;- cor.test(Y1, Y2)$p.value\n  # データ追加\n  count &lt;- 0\n  ## p値が5％を下回るか、データが当初の倍になるまで増やし続ける\n  while (p[i] &gt;= alpha && count &lt; n * 2) {\n    # 有意ではなかったとき、それぞれの変数に1つデータを追加\n    Y1_add &lt;- rnorm(1, 0, 1)\n    Y1 &lt;- c(Y1, Y1_add)\n    Y2 &lt;- c(Y2, Y1_add * rho + rnorm(1, 0, (1 - rho^2)^0.5))\n    p[i] &lt;- cor.test(Y1, Y2)$p.value\n    count &lt;- count + 1\n  }\n}\n\nifelse(p &lt; .05, 1, 0) |&gt; mean()\n#&gt; [1] 0.1791\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch01 本書のねらい"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch03_乱数生成シミュレーションの基礎.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch03_乱数生成シミュレーションの基礎.html",
    "title": "ch03 乱数生成シミュレーションの基礎",
    "section": "",
    "text": "「仮にもっと多くのデータがあったなら正確にわかったかもしれないこと」を 推測するには、データの生成モデルを考える必用がある。 「どのようなエータが観測されいあｙすいかは、確率法則に従う」という仮定を 億ことで検討がおこなえる。 これは仮定であるので、本ッにそうであるかどうかはわからない。\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch03 乱数生成シミュレーションの基礎"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch05_統計検定の論理とエラー確率のコントロール - コピー (2).html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch05_統計検定の論理とエラー確率のコントロール - コピー (2).html",
    "title": "ch05 統計検定の論理",
    "section": "",
    "text": "検定はあらかじめ\\(\\alpha\\)と\\(\\beta\\)を設定しておき、 その確率にタイプⅠ、タイプⅡエラー 確率が抑えられるように手続きを設計することが重要である。\n重要なのは低い\\(\\alpha\\)と\\(\\beta\\)を設定することではなく、 推論のエラー確率が定めた\\(\\alpha\\)と\\(\\beta\\)を下回るように、 手続きを設定することである。",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch05 統計検定の論理"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch05_統計検定の論理とエラー確率のコントロール - コピー (2).html#t検定の等分散の仮定からの逸脱",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch05_統計検定の論理とエラー確率のコントロール - コピー (2).html#t検定の等分散の仮定からの逸脱",
    "title": "ch05 統計検定の論理",
    "section": "2.1 \\(t\\)検定の等分散の仮定からの逸脱",
    "text": "2.1 \\(t\\)検定の等分散の仮定からの逸脱\n等分散が満たされないときに、 エラー確率が制御できないことを確認する。\n\n\nCode\nn &lt;- c(40, 20) # サンプルサイズを変えた設定\nsigma1 &lt;- 1\nsigma2 &lt;- c(0.2, 0.5, 0.75, 1, 1.5, 2, 5) # 母標準偏差のパターン\np &lt;- length(sigma2)\niter &lt;- 10000 # シミュレーション回数\nalpha &lt;- 0.05 # 有意水準\nmu &lt;- c(0, 0) # 母平均が等しい設定にする\n# 結果を格納するオブジェクト\npvalue &lt;- array(NA, dim = c(p, iter))\n## シミュレーション\nset.seed(1234)\nfor (i in 1:p) {\n  for (j in 1:iter) {\n    Y1 &lt;- rnorm(n[1], mu[1], sigma1)\n    Y2 &lt;- rnorm(n[2], mu[2], sigma2[i])\n    result &lt;- t.test(Y1, Y2, var.equal = TRUE)\n    pvalue[i, j] &lt;- result$p.value\n  }\n}\n# 結果を格納するオブジェクト\ntype1error_ttest &lt;- rep(0, p)\n\n\nfor (i in 1:p) {\n  type1error_ttest[i] &lt;- mean(pvalue[i, ] &lt; alpha)\n}\n\ntype1error_ttest |&gt; \n  plot(\n    type = \"b\", \n    xaxt = \"n\", \n    ylim = c(0, .2), \n    xlab = \"郡2の母標準偏差\"\n  )\n\naxis(1, at = 1:p, labels = sigma2)\nabline(h = alpha, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\npvalue[6,] |&gt; hist()\n\n\n\n\n\n\n\n\n\n一方でWeltch検定は等分散を仮定していないので、 制御出来ていることがわかる。\n\n\nCode\nn &lt;- c(40, 20) # サンプルサイズをかえる\nsigma1 &lt;- 1\nsigma2 &lt;- c(.2, .5, .75, 1, 1.5, 2, 5) # 母標準偏差のパターン\np &lt;- length(sigma2)\niter &lt;- 10000\nalpah &lt;- .05\nmu &lt;- c(0, 0)\n\nset.seed(1234)\npvalue &lt;- array(NA, dim = c(p, iter))\nfor (i in 1:p) {\n  for (j in 1:iter) {\n    Y1 &lt;- rnorm(n[1], mu[1], sigma1)\n    Y2 &lt;- rnorm(n[2], mu[2], sigma2[i])\n    result &lt;- t.test(Y1, Y2, var.equal = FALSE)\n    pvalue[i, j] &lt;- result$p.value\n  }\n}\n\ntype1error_welch &lt;- rep(0, p)\nfor (i in 1:p) {\n  type1error_welch[i] &lt;- mean(pvalue[i, ] &lt; alpha)\n}\n\ntype1error_welch |&gt; \n  plot(\n    type = \"b\", \n    xaxt = \"n\", \n    ylim = c(0, .2), \n    xlab = \"群2の母標準偏差\"\n  )\naxis(1, at = 1:p, labels = sigma2)\nabline(h = alpha, lty = 2)",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch05 統計検定の論理"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch07_回帰分析とシミュレーション.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch07_回帰分析とシミュレーション.html",
    "title": "ch07 回帰分析",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch07 回帰分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html",
    "title": "基礎の基礎",
    "section": "",
    "text": "因果関係は，原因と結果の関係である．因果関係は目に見えないため，因果推論とは 因果を推し測って考えるということである． 統計的因果推論とは，そのような因果推論をデータに基づいて行うことである.\n因果関係には「効果をもたらした原因」と「原因がもたらす効果」がある． 前者を判定するにはフィールドワークなどの観察により 事象の背景知識を把握することが必要である． 一方で後者はデータにもとづいて定量的に行われることになる． つまり統計的因果推論では「原因Aが結果Bにもたらす効果」を取り扱うことになる．\nなお統計的因果推論では原因のことを「処置」という．\n\n\n\n本書ではRubinの潜在的結果変数の枠組みで統計的因果推論を行う.\n大切だと思うのは常に交絡因子が存在していることであり， 交絡因子について思慮続けることである. 性差が原因だとして，その性差とは具体的には何を表しているのだろうか？ その性差がなぜ結果Bに影響を与えるのだろうか？\n操作無くして因果なしの考えに基づけば，性差は操作ができないものである． つまりある個人が別の性別，性認識での結果は存在しない． 性差と結果の間になにか中間変数があるはずである． 本署の中では性差が治療結果の違いに影響を与えるのは，処置方針という中間変数が存在しているため，という事例が記載されていた.\n\n\n\nグレンジャー因果は予測に役立つという意味であり， 統計的因果推論とは異なるものであることに注意する．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "基礎の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#統計的因果推論とは",
    "href": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#統計的因果推論とは",
    "title": "基礎の基礎",
    "section": "",
    "text": "因果関係は，原因と結果の関係である．因果関係は目に見えないため，因果推論とは 因果を推し測って考えるということである． 統計的因果推論とは，そのような因果推論をデータに基づいて行うことである.\n因果関係には「効果をもたらした原因」と「原因がもたらす効果」がある． 前者を判定するにはフィールドワークなどの観察により 事象の背景知識を把握することが必要である． 一方で後者はデータにもとづいて定量的に行われることになる． つまり統計的因果推論では「原因Aが結果Bにもたらす効果」を取り扱うことになる．\nなお統計的因果推論では原因のことを「処置」という．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "基礎の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#反事実モデル",
    "href": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#反事実モデル",
    "title": "基礎の基礎",
    "section": "",
    "text": "本書ではRubinの潜在的結果変数の枠組みで統計的因果推論を行う.\n大切だと思うのは常に交絡因子が存在していることであり， 交絡因子について思慮続けることである. 性差が原因だとして，その性差とは具体的には何を表しているのだろうか？ その性差がなぜ結果Bに影響を与えるのだろうか？\n操作無くして因果なしの考えに基づけば，性差は操作ができないものである． つまりある個人が別の性別，性認識での結果は存在しない． 性差と結果の間になにか中間変数があるはずである． 本署の中では性差が治療結果の違いに影響を与えるのは，処置方針という中間変数が存在しているため，という事例が記載されていた.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "基礎の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#グレンジャー因果",
    "href": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#グレンジャー因果",
    "title": "基礎の基礎",
    "section": "",
    "text": "グレンジャー因果は予測に役立つという意味であり， 統計的因果推論とは異なるものであることに注意する．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "基礎の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html",
    "title": "重要な仮定",
    "section": "",
    "text": "処置の無作為割り付けによって平均処置効果を適切に推定できることを確認した． ここまでは分散分析をはじめとした従来の統計学と同様の論旨である．\nここからは統計的因果推論における重要な仮定と， それを踏まえたうえで無作為割り付けを実行できない観察研究にける統計的因果推論を説明する.\n\n\nStable Unit Treatment Value Assumption\n処置を受ける個体(unit)ごとに処置の値が安定的であるという仮定である． 具体的には次の二つの条件が成り立つ．\n\n相互干渉がない\n個体に対する隠れた処置がない\n\n相互干渉ある場合には相互干渉がなくなるまでunitの単位を変更する. 隠れた処置がない，とはつまり処置以外の属性がランダム化されているという理解で良さそうである． 補習講義を受けた場合でも先生が異なる場合には，A先生の補習講義を受けた，B先生の補習講義を 受けた，補習講義を受けていないなどとして着目した変数以外をランダム化することに努める．\n確率が相互に背反であるとは，集合に重なりがないことであるあため，次式が成立することを指す.\n\\[\nP(A|B)=0\n\\]\n確率が独立であるとは，ある情報で条件づけても確率が変化しないことである．\n\\[\nP(A|B)=P(A)\n\\]\n\n\n\n観測されたデータから母数が一意に満たされていない場合， 標本から母集団への推定ができないということになる． このとき識別性がないという． 特に，ここでは識別性=正値性＋独立性である．\n正値性の条件を課すとunitが処置を受けるかうけないかは， わからない状態，つまりどちらも可能性があるため次の式で表現される.\n\\[\n0&lt;\\textrm{Pr}(T=1) &lt;1\n\\]\n独立性とは処置の割り付けが潜在的結果変数に依存して行われてはいけない，ということである． つまりすべての変量と独立になっており，実験研究ではこの条件が満たされやすい.\n\\[\n{Y(1), Y(0)} \\perp T\n\\]\n改めて観測値\\(Y_i\\)は次の式で表される.\n\\[\nY_i = (1-T_i)Y_i(0) + T_iY_i(1)\n\\]\n\\(Y_i(0), Y_i(1)\\)は潜在的結果変数の組であるため， 個体\\(i\\)に対してはどちらか一方しか観測されない. よって次式により処置効果を求めることは通常不可能である.\n\\[\n\\tau_{ATE}=E[Y_i(1)-Y_i(0)]=E[Y_i(1)]-E[Y_i(0)]\n\\]\n一方でそれぞれの処置で条件付けた場合には平均値の算出が可能であるため， 次式で示したナイーブな推定量を使うことが可能である. しかしこの値は本来推定したいものではない．本来推定したいのは，処置の平均処置効果，処置群の平均処置効果のどちらかである．\n\\[\nE[Y_i|T_i=1]-E[Y_i|T_i=0]\n\\]\n処置の割り付けを表す\\(T_i\\)が潜在的結果変数の組に依存していなければ \\(T_i\\)と\\({Y_i(0), Y_i(1)}\\)とは独立である. 無作為割り付けができる場合には\\(T_I\\)は\\({Y_i(0), Y_i(1)}\\)とも無関係となる. 無作為割り付けの場合には独立性の仮定が満たされることから，常識は\\(T_{ATE}\\)に変換できる.\n\\[\nE[Y_i|T_i=1]=E[Y_i(1)|T_i=1]=E[Y_i(1)]\\\\\nE[Y_i|T_i=0]=E[Y_i(0)|T_i=0]=E[Y_i(0)]\n\\]\n上記から無作為割り付けが出来ていれば，処置平均効果の差分を求めることで，処置効果を推定することが可能となることがわかる. つまり，処置の割り付けの有無をいかに結果変数と独立させるのかが重要である.\n\n\n\n条件BとCを与えたときのAの確率が，条件Cだけを与えたときのAの確率と 一致することを条件付き独立性という．これは\\(A \\perp B|C\\)と各．\n\\[\n\\textrm{Pr}(A|B,C)=\\textrm{Pr}(A|C)\n\\]\nこれは「独立ならば条件付き独立だる」とは限らない．\n統計的因果推論の立場から重要な点は，たとえデータ全体で独立でなかったとしても， 共変量に条件付けた場合には独立とみなし得るという点である．\n\n\n\n観察研究において適切な統計的因果推論を行うためには， 処置群と統制群をいかにして比較可能な状態にするのかが重要である． すなわち比較可能な２つの集団を用意することが出来る.\n共変量とは結果偏す\\(Y_i\\)に影響する変数の中で， 処置の影響を受けていない変数である．この式を\n\\[\nY_i(1), Y_i(0) \\perp T_i|X\n\\]\nまた条件付き正確性は次である.\n\\[\n0 &lt; \\textrm{Pr}(T=1|X) &lt; 1\n\\] 共変量を考慮することで，割り付けと結果変数が独立になる， つまり割り付けによる効果を推定することが可能となる. これは共変量が同じ個体では割り付け確率が同じになる，ということを意味している．\n・・・ということは，共変量が同じであるという条件を加えることで， 推定値を求めることが出来るようになる.\n共変量による条件づけで平均処置効果を算出できるとは，次式の関係があることを指す. \\[\nE[Y_i|T_i=1, X]=E[Y_i(1)|T_i=1, X]=E[Y_i(1)|X]\\\\\nE[Y_i|T_i=0, X]=E[Y_i(0)|T_i=0, X]=E[Y_i(0)|X]\n\\]\nここでは無視可能な割り付けによる算出例を見てみる.\n\n\nCode\npath   &lt;- \"./causality/data03.csv\"\ndata03 &lt;- read_csv(\n    path, \n    locale = locale(encoding = \"UTF-8\"),\n    show_col_types = FALSE\n)\nsummary(data03)\n#&gt;        x1              y3              t1           y0t             y1t       \n#&gt;  Min.   :70.00   Min.   :63.00   Min.   :0.0   Min.   :62.00   Min.   :71.00  \n#&gt;  1st Qu.:73.75   1st Qu.:73.75   1st Qu.:0.0   1st Qu.:66.50   1st Qu.:75.50  \n#&gt;  Median :80.00   Median :77.00   Median :0.5   Median :71.00   Median :81.50  \n#&gt;  Mean   :80.00   Mean   :77.25   Mean   :0.5   Mean   :72.20   Mean   :82.00  \n#&gt;  3rd Qu.:86.25   3rd Qu.:82.00   3rd Qu.:1.0   3rd Qu.:78.75   3rd Qu.:88.75  \n#&gt;  Max.   :90.00   Max.   :91.00   Max.   :1.0   Max.   :82.00   Max.   :92.00\n\n\n\n\nCode\nprint(head(data03))\n#&gt; # A tibble: 6 × 5\n#&gt;      x1    y3    t1   y0t   y1t\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    70    74     1    62    74\n#&gt; 2    70    63     0    63    74\n#&gt; 3    70    73     1    62    73\n#&gt; 4    70    71     1    65    71\n#&gt; 5    70    74     1    63    74\n#&gt; 6    75    67     0    67    77\n\n\n\n\nCode\nwith(data03, {\n    mean(y3[t1==1])-mean(y3[t1==0])\n})\n#&gt; [1] 3.3\n\n\n\n\nCode\n# 真値\nwith(data03, {\n    mean(y1t)-mean(y0t)\n})\n#&gt; [1] 9.8\n\n\n上記の数値の違いからわかることは 「無視可能な割り付け」とはナイーブな推定量によって文字どおりに 「割り付けを無視して解析してよい」ということを意味していない．\n\n\n\n\n\nCode\nmodel1 &lt;- lm(y3 ~ x1 + t1, data = data03)\nsummary(model1)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ x1 + t1, data = data03)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1.84950 -0.54042  0.07711  0.38619  1.18781 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -2.12562    2.27423  -0.935    0.363    \n#&gt; x1           0.93085    0.02704  34.422  &lt; 2e-16 ***\n#&gt; t1           9.81592    0.42757  22.957 3.11e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.8573 on 17 degrees of freedom\n#&gt; Multiple R-squared:  0.9867, Adjusted R-squared:  0.9851 \n#&gt; F-statistic: 629.5 on 2 and 17 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nconfint(model1, level = .95)\n#&gt;                  2.5 %     97.5 %\n#&gt; (Intercept) -6.9238192  2.6725754\n#&gt; x1           0.8737921  0.9878995\n#&gt; t1           8.9138219 10.7180189\n\n\n無視可能な割り付けであれば，共変量による条件づけることで解析が行える. グラフィカルモデルで言えば共変量と処置変数",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#sutva",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#sutva",
    "title": "重要な仮定",
    "section": "",
    "text": "Stable Unit Treatment Value Assumption\n処置を受ける個体(unit)ごとに処置の値が安定的であるという仮定である． 具体的には次の二つの条件が成り立つ．\n\n相互干渉がない\n個体に対する隠れた処置がない\n\n相互干渉ある場合には相互干渉がなくなるまでunitの単位を変更する. 隠れた処置がない，とはつまり処置以外の属性がランダム化されているという理解で良さそうである． 補習講義を受けた場合でも先生が異なる場合には，A先生の補習講義を受けた，B先生の補習講義を 受けた，補習講義を受けていないなどとして着目した変数以外をランダム化することに努める．\n確率が相互に背反であるとは，集合に重なりがないことであるあため，次式が成立することを指す.\n\\[\nP(A|B)=0\n\\]\n確率が独立であるとは，ある情報で条件づけても確率が変化しないことである．\n\\[\nP(A|B)=P(A)\n\\]",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#識別性の条件",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#識別性の条件",
    "title": "重要な仮定",
    "section": "",
    "text": "観測されたデータから母数が一意に満たされていない場合， 標本から母集団への推定ができないということになる． このとき識別性がないという． 特に，ここでは識別性=正値性＋独立性である．\n正値性の条件を課すとunitが処置を受けるかうけないかは， わからない状態，つまりどちらも可能性があるため次の式で表現される.\n\\[\n0&lt;\\textrm{Pr}(T=1) &lt;1\n\\]\n独立性とは処置の割り付けが潜在的結果変数に依存して行われてはいけない，ということである． つまりすべての変量と独立になっており，実験研究ではこの条件が満たされやすい.\n\\[\n{Y(1), Y(0)} \\perp T\n\\]\n改めて観測値\\(Y_i\\)は次の式で表される.\n\\[\nY_i = (1-T_i)Y_i(0) + T_iY_i(1)\n\\]\n\\(Y_i(0), Y_i(1)\\)は潜在的結果変数の組であるため， 個体\\(i\\)に対してはどちらか一方しか観測されない. よって次式により処置効果を求めることは通常不可能である.\n\\[\n\\tau_{ATE}=E[Y_i(1)-Y_i(0)]=E[Y_i(1)]-E[Y_i(0)]\n\\]\n一方でそれぞれの処置で条件付けた場合には平均値の算出が可能であるため， 次式で示したナイーブな推定量を使うことが可能である. しかしこの値は本来推定したいものではない．本来推定したいのは，処置の平均処置効果，処置群の平均処置効果のどちらかである．\n\\[\nE[Y_i|T_i=1]-E[Y_i|T_i=0]\n\\]\n処置の割り付けを表す\\(T_i\\)が潜在的結果変数の組に依存していなければ \\(T_i\\)と\\({Y_i(0), Y_i(1)}\\)とは独立である. 無作為割り付けができる場合には\\(T_I\\)は\\({Y_i(0), Y_i(1)}\\)とも無関係となる. 無作為割り付けの場合には独立性の仮定が満たされることから，常識は\\(T_{ATE}\\)に変換できる.\n\\[\nE[Y_i|T_i=1]=E[Y_i(1)|T_i=1]=E[Y_i(1)]\\\\\nE[Y_i|T_i=0]=E[Y_i(0)|T_i=0]=E[Y_i(0)]\n\\]\n上記から無作為割り付けが出来ていれば，処置平均効果の差分を求めることで，処置効果を推定することが可能となることがわかる. つまり，処置の割り付けの有無をいかに結果変数と独立させるのかが重要である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#独立性と条件付き独立性シンプソンのパラドックス",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#独立性と条件付き独立性シンプソンのパラドックス",
    "title": "重要な仮定",
    "section": "",
    "text": "条件BとCを与えたときのAの確率が，条件Cだけを与えたときのAの確率と 一致することを条件付き独立性という．これは\\(A \\perp B|C\\)と各．\n\\[\n\\textrm{Pr}(A|B,C)=\\textrm{Pr}(A|C)\n\\]\nこれは「独立ならば条件付き独立だる」とは限らない．\n統計的因果推論の立場から重要な点は，たとえデータ全体で独立でなかったとしても， 共変量に条件付けた場合には独立とみなし得るという点である．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#共変量の役割",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#共変量の役割",
    "title": "重要な仮定",
    "section": "",
    "text": "観察研究において適切な統計的因果推論を行うためには， 処置群と統制群をいかにして比較可能な状態にするのかが重要である． すなわち比較可能な２つの集団を用意することが出来る.\n共変量とは結果偏す\\(Y_i\\)に影響する変数の中で， 処置の影響を受けていない変数である．この式を\n\\[\nY_i(1), Y_i(0) \\perp T_i|X\n\\]\nまた条件付き正確性は次である.\n\\[\n0 &lt; \\textrm{Pr}(T=1|X) &lt; 1\n\\] 共変量を考慮することで，割り付けと結果変数が独立になる， つまり割り付けによる効果を推定することが可能となる. これは共変量が同じ個体では割り付け確率が同じになる，ということを意味している．\n・・・ということは，共変量が同じであるという条件を加えることで， 推定値を求めることが出来るようになる.\n共変量による条件づけで平均処置効果を算出できるとは，次式の関係があることを指す. \\[\nE[Y_i|T_i=1, X]=E[Y_i(1)|T_i=1, X]=E[Y_i(1)|X]\\\\\nE[Y_i|T_i=0, X]=E[Y_i(0)|T_i=0, X]=E[Y_i(0)|X]\n\\]\nここでは無視可能な割り付けによる算出例を見てみる.\n\n\nCode\npath   &lt;- \"./causality/data03.csv\"\ndata03 &lt;- read_csv(\n    path, \n    locale = locale(encoding = \"UTF-8\"),\n    show_col_types = FALSE\n)\nsummary(data03)\n#&gt;        x1              y3              t1           y0t             y1t       \n#&gt;  Min.   :70.00   Min.   :63.00   Min.   :0.0   Min.   :62.00   Min.   :71.00  \n#&gt;  1st Qu.:73.75   1st Qu.:73.75   1st Qu.:0.0   1st Qu.:66.50   1st Qu.:75.50  \n#&gt;  Median :80.00   Median :77.00   Median :0.5   Median :71.00   Median :81.50  \n#&gt;  Mean   :80.00   Mean   :77.25   Mean   :0.5   Mean   :72.20   Mean   :82.00  \n#&gt;  3rd Qu.:86.25   3rd Qu.:82.00   3rd Qu.:1.0   3rd Qu.:78.75   3rd Qu.:88.75  \n#&gt;  Max.   :90.00   Max.   :91.00   Max.   :1.0   Max.   :82.00   Max.   :92.00\n\n\n\n\nCode\nprint(head(data03))\n#&gt; # A tibble: 6 × 5\n#&gt;      x1    y3    t1   y0t   y1t\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    70    74     1    62    74\n#&gt; 2    70    63     0    63    74\n#&gt; 3    70    73     1    62    73\n#&gt; 4    70    71     1    65    71\n#&gt; 5    70    74     1    63    74\n#&gt; 6    75    67     0    67    77\n\n\n\n\nCode\nwith(data03, {\n    mean(y3[t1==1])-mean(y3[t1==0])\n})\n#&gt; [1] 3.3\n\n\n\n\nCode\n# 真値\nwith(data03, {\n    mean(y1t)-mean(y0t)\n})\n#&gt; [1] 9.8\n\n\n上記の数値の違いからわかることは 「無視可能な割り付け」とはナイーブな推定量によって文字どおりに 「割り付けを無視して解析してよい」ということを意味していない．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#回帰分析と共分散分析",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#回帰分析と共分散分析",
    "title": "重要な仮定",
    "section": "",
    "text": "Code\nmodel1 &lt;- lm(y3 ~ x1 + t1, data = data03)\nsummary(model1)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ x1 + t1, data = data03)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1.84950 -0.54042  0.07711  0.38619  1.18781 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -2.12562    2.27423  -0.935    0.363    \n#&gt; x1           0.93085    0.02704  34.422  &lt; 2e-16 ***\n#&gt; t1           9.81592    0.42757  22.957 3.11e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.8573 on 17 degrees of freedom\n#&gt; Multiple R-squared:  0.9867, Adjusted R-squared:  0.9851 \n#&gt; F-statistic: 629.5 on 2 and 17 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nconfint(model1, level = .95)\n#&gt;                  2.5 %     97.5 %\n#&gt; (Intercept) -6.9238192  2.6725754\n#&gt; x1           0.8737921  0.9878995\n#&gt; t1           8.9138219 10.7180189\n\n\n無視可能な割り付けであれば，共変量による条件づけることで解析が行える. グラフィカルモデルで言えば共変量と処置変数",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/105_回帰分析の基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/105_回帰分析の基礎.html",
    "title": "回帰分析の基礎",
    "section": "",
    "text": "1 回帰分析の基礎\n共変量Xによって条件付けることで無視可能な割り付けとみなせるとき， 回帰分析によって平均処置効果を推定することができるた． このような条件が満たされない場合には回帰係数は必ずしも因果関係を表さないことに注意する．\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "回帰分析の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html",
    "title": "最小二乗法の仮定と診断1",
    "section": "",
    "text": "最小二乗法による重回帰モデルでは６つの仮定が置かれている． ここでは次に示す３つの仮定について解説する.\n\n誤差項の平均値がゼロ\nパラメータの線形性\n誤差項の条件付き期待値がゼロ\n\n\n\n切片項があれば誤差項がゼロでなくて， 切片項に平均値分を持たせることが出来るので， 通常はこの仮定を満たすことが出来る.\nつまり，回帰係数の不偏性には影響しない.\n\n\n\n最小二乗法による重回帰モデルのパラメータ推定が， 不偏性を持つために必要な条件となる.\n例えば次の二つの式を考える．このうち，１つ目の式は対数変換を 行おうことで適切にモデル化することができる．一方で， 二つ目の式にはそのような変換が存在しない． これはパラメータの線形性と変数の非線形性という少し解釈が難しい話題である.\n\\[\n\\begin{align}\nY_i &= \\beta_0X_1^{\\beta_1}X_2^{\\beta_2}\\exp{\\epsilon_i}\\\\\nY_i &= \\beta_0X_1^{\\beta_1}X_2^{\\beta_2} + \\epsilon_i\n\\end{align}\n\\]\n．．．とはいいつつもデータの非線形性はよいが， パラメータについてはいつも線形であると理解しておけばよさそう.\n\n\n共変量が多変量になる場合には他の共変量を統制した場合の効果， つまり偏回帰係数に興味がある． これは二変量の散布図では適切な関数系を示すことが出来ないため， 成分プラス残差プロットを私用することが推奨されている.\n\n\n\n\n下記の式で表される条件である．これはつまり，統制すべき交絡因子が十分に モデルに含まれていることを指している． しかし，そのことを診断する方法はない.\n\\[\nE[\\epsilon_i|X]=E[\\epsilon_i]=0\n\\]\n統計的因果論の立場で考えると， 観測された共変量の値が同じ個体同士では，処置の割り付けは 無作為になっていると考えて良いという仮定を指す．\n\\[\n\\text{Pr}(T_i|Y_i(1), Y_i(0),X) = \\text{Pr}(T_i|X)\n\\]\nこの仮定を満たすことを考えるためには，できるだけ 多くの変数をモデルに取り入れて必要な共変量を取りこぼす可能性を下げることである． このとき検討する項目は次の２点である.\n\n不要な変数を取り込んだことの影響\n因果関係の間に位置する変数の取り扱い\n\n\n\n説明変数に「多重共線性」が生じていなければ不偏性には問題ない． ただしパラメータの標準誤差が大きくなる.\n\n\nCode\nbeta0 &lt;- 1.\nbeta1 &lt;- 1.5\nbeta2 &lt;- 1.2\ndata &lt;- tibble(\n    x1 = rnorm(n = 100, mean = 1), \n    x2 = rnorm(n = 100, mean = 3), \n    x3 = rnorm(100),\n    y  = 1 + beta1 * x1 + beta2 * x2  + rnorm(100)\n)\n\nmodels &lt;- \n    tibble(\n        formula = c(\n            \"y ~ x1\", \n            \"y ~ x1 + x2\", \n            \"y ~ x1 + x2 + x3\"\n        )\n    ) |&gt; \n    mutate(\n        reg = map(formula, ~ lm(., data = data))\n    )\n\n\n余計な変数を入れても不偏性には影響しない.\n\n\nCode\nmodels$reg \n#&gt; [[1]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;       4.634        1.409  \n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2  \n#&gt;      0.9523       1.5342       1.1703  \n#&gt; \n#&gt; \n#&gt; [[3]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2           x3  \n#&gt;    0.952686     1.533826     1.170225    -0.001641\n\n\nただしパラメータ推定の分散は大きくなる\n\n\nCode\nmodels$reg |&gt; lapply(confint)\n#&gt; [[1]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 4.214445 5.054522\n#&gt; x1          1.132469 1.685788\n#&gt; \n#&gt; [[2]]\n#&gt;                 2.5 %   97.5 %\n#&gt; (Intercept) 0.3004943 1.604135\n#&gt; x1          1.3589493 1.709550\n#&gt; x2          0.9808976 1.359608\n#&gt; \n#&gt; [[3]]\n#&gt;                  2.5 %    97.5 %\n#&gt; (Intercept)  0.2962068 1.6091652\n#&gt; x1           1.3519278 1.7157247\n#&gt; x2           0.9798373 1.3606120\n#&gt; x3          -0.1762091 0.1729263\n\n\n\n\n\n「因果関係の間に位置する変数」の問題を考える。 結論的にいえばそのような変数を含めてはならない.\n\n\nCode\ndata &lt;- tibble(\n    x1 = rnorm(n = 100, mean = 1), \n    x2 = 1 + 1.5 * x1 + rnorm(n = 100), \n    y  = 1 + 1.2 * x1 + 1.6 * x2  + rnorm(100)\n)\n\nmodels &lt;- \n    tibble(\n        formula = c(\n            \"y ~ x1\", \n            \"y ~ x1 + x2\"\n        )\n    ) |&gt; \n    mutate(\n        reg = map(formula, ~ lm(., data = data))\n    )\n\n\n上記のサンプルデータの場合、x1がyに与える影響は \\(1.2 + 1.5 * 1.6 = 3.6\\)なので中間変数であるx2を含まない方がよい推定である ことがわかる.\n\n\nCode\nmodels$reg\n#&gt; [[1]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;       2.682        3.463  \n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2  \n#&gt;      0.8675       1.4104       1.5806\n\n\n\n\nCode\nmodels$reg |&gt; lapply(confint)\n#&gt; [[1]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 2.109008 3.255472\n#&gt; x1          3.085248 3.840094\n#&gt; \n#&gt; [[2]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 0.488317 1.246698\n#&gt; x1          1.085086 1.735793\n#&gt; x2          1.382953 1.778201",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断1"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#誤差項の期待値ゼロ",
    "href": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#誤差項の期待値ゼロ",
    "title": "最小二乗法の仮定と診断1",
    "section": "",
    "text": "切片項があれば誤差項がゼロでなくて， 切片項に平均値分を持たせることが出来るので， 通常はこの仮定を満たすことが出来る.\nつまり，回帰係数の不偏性には影響しない.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断1"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#パラメータにおける線形性",
    "href": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#パラメータにおける線形性",
    "title": "最小二乗法の仮定と診断1",
    "section": "",
    "text": "最小二乗法による重回帰モデルのパラメータ推定が， 不偏性を持つために必要な条件となる.\n例えば次の二つの式を考える．このうち，１つ目の式は対数変換を 行おうことで適切にモデル化することができる．一方で， 二つ目の式にはそのような変換が存在しない． これはパラメータの線形性と変数の非線形性という少し解釈が難しい話題である.\n\\[\n\\begin{align}\nY_i &= \\beta_0X_1^{\\beta_1}X_2^{\\beta_2}\\exp{\\epsilon_i}\\\\\nY_i &= \\beta_0X_1^{\\beta_1}X_2^{\\beta_2} + \\epsilon_i\n\\end{align}\n\\]\n．．．とはいいつつもデータの非線形性はよいが， パラメータについてはいつも線形であると理解しておけばよさそう.\n\n\n共変量が多変量になる場合には他の共変量を統制した場合の効果， つまり偏回帰係数に興味がある． これは二変量の散布図では適切な関数系を示すことが出来ないため， 成分プラス残差プロットを私用することが推奨されている.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断1"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#誤差項の条件付き期待値ゼロ",
    "href": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#誤差項の条件付き期待値ゼロ",
    "title": "最小二乗法の仮定と診断1",
    "section": "",
    "text": "下記の式で表される条件である．これはつまり，統制すべき交絡因子が十分に モデルに含まれていることを指している． しかし，そのことを診断する方法はない.\n\\[\nE[\\epsilon_i|X]=E[\\epsilon_i]=0\n\\]\n統計的因果論の立場で考えると， 観測された共変量の値が同じ個体同士では，処置の割り付けは 無作為になっていると考えて良いという仮定を指す．\n\\[\n\\text{Pr}(T_i|Y_i(1), Y_i(0),X) = \\text{Pr}(T_i|X)\n\\]\nこの仮定を満たすことを考えるためには，できるだけ 多くの変数をモデルに取り入れて必要な共変量を取りこぼす可能性を下げることである． このとき検討する項目は次の２点である.\n\n不要な変数を取り込んだことの影響\n因果関係の間に位置する変数の取り扱い\n\n\n\n説明変数に「多重共線性」が生じていなければ不偏性には問題ない． ただしパラメータの標準誤差が大きくなる.\n\n\nCode\nbeta0 &lt;- 1.\nbeta1 &lt;- 1.5\nbeta2 &lt;- 1.2\ndata &lt;- tibble(\n    x1 = rnorm(n = 100, mean = 1), \n    x2 = rnorm(n = 100, mean = 3), \n    x3 = rnorm(100),\n    y  = 1 + beta1 * x1 + beta2 * x2  + rnorm(100)\n)\n\nmodels &lt;- \n    tibble(\n        formula = c(\n            \"y ~ x1\", \n            \"y ~ x1 + x2\", \n            \"y ~ x1 + x2 + x3\"\n        )\n    ) |&gt; \n    mutate(\n        reg = map(formula, ~ lm(., data = data))\n    )\n\n\n余計な変数を入れても不偏性には影響しない.\n\n\nCode\nmodels$reg \n#&gt; [[1]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;       4.634        1.409  \n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2  \n#&gt;      0.9523       1.5342       1.1703  \n#&gt; \n#&gt; \n#&gt; [[3]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2           x3  \n#&gt;    0.952686     1.533826     1.170225    -0.001641\n\n\nただしパラメータ推定の分散は大きくなる\n\n\nCode\nmodels$reg |&gt; lapply(confint)\n#&gt; [[1]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 4.214445 5.054522\n#&gt; x1          1.132469 1.685788\n#&gt; \n#&gt; [[2]]\n#&gt;                 2.5 %   97.5 %\n#&gt; (Intercept) 0.3004943 1.604135\n#&gt; x1          1.3589493 1.709550\n#&gt; x2          0.9808976 1.359608\n#&gt; \n#&gt; [[3]]\n#&gt;                  2.5 %    97.5 %\n#&gt; (Intercept)  0.2962068 1.6091652\n#&gt; x1           1.3519278 1.7157247\n#&gt; x2           0.9798373 1.3606120\n#&gt; x3          -0.1762091 0.1729263\n\n\n\n\n\n「因果関係の間に位置する変数」の問題を考える。 結論的にいえばそのような変数を含めてはならない.\n\n\nCode\ndata &lt;- tibble(\n    x1 = rnorm(n = 100, mean = 1), \n    x2 = 1 + 1.5 * x1 + rnorm(n = 100), \n    y  = 1 + 1.2 * x1 + 1.6 * x2  + rnorm(100)\n)\n\nmodels &lt;- \n    tibble(\n        formula = c(\n            \"y ~ x1\", \n            \"y ~ x1 + x2\"\n        )\n    ) |&gt; \n    mutate(\n        reg = map(formula, ~ lm(., data = data))\n    )\n\n\n上記のサンプルデータの場合、x1がyに与える影響は \\(1.2 + 1.5 * 1.6 = 3.6\\)なので中間変数であるx2を含まない方がよい推定である ことがわかる.\n\n\nCode\nmodels$reg\n#&gt; [[1]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;       2.682        3.463  \n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2  \n#&gt;      0.8675       1.4104       1.5806\n\n\n\n\nCode\nmodels$reg |&gt; lapply(confint)\n#&gt; [[1]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 2.109008 3.255472\n#&gt; x1          3.085248 3.840094\n#&gt; \n#&gt; [[2]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 0.488317 1.246698\n#&gt; x1          1.085086 1.735793\n#&gt; x2          1.382953 1.778201",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断1"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "共分散分析のモデル次式に示す． ダミー変数を含む重回帰モデルであり， 共変量によるデータの偏りを調整したうえで処置の影響を算出する有力なモデルである．\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1}+\\beta_2T_i+\\epsilon_i\n\\]\n\\[\nT_i =\n\\begin{cases}\n1 \\\\\n0\n\\end{cases}\n\\]\n重回帰モデルであるため共分散分析では，これまでの仮定を満たす必要がある． さらに処置変数によらず共変量が目的変数に与える影響が同じ， つまり\\(\\beta_1\\)が共通という仮定がある.\nこの仮定が満たされない場合には，交互作用項を見込んだモデルを作成する必要がある.\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1}+\\beta_2T_i+\\beta_3X_{i1}T_i+\\epsilon_i\n\\]\nこのモデルを使うことより説明変数が目的変数に与える影響は， \\(\\beta_3\\)だけ処置群間で異なることがわかる.\n\\[\nY_i = \\beta_0 + (\\beta_1+\\beta_3T_i)X_{i1}+\\beta_2T_i+\\epsilon_i\n\\]\nこのようなモデルが必要になるのは, 説明変数で条件づけたときに， 処置の割り付け確率が異なるということが影響している． これにより潜在的結果変数と処置が独立でなくなる.\n\n\n\n\\[\nY_i(T_i=0) = \\beta_0 + \\beta_1X_{i}+\\epsilon_i\n\\]\n\\[\nY_i(T_i=1) = (\\beta_0 + \\beta_2) + (\\beta_1+\\beta_3)X_{i}+\\epsilon_i\n\\]\n\n\n個体別効果は潜在的結果変数\\(Y_i(1)\\)と\\(Y_i(0)\\)の差であるから， これは次式で表されることになる.\n\\[\n\\tau_i=Y_i(1)-Y_i(0)=\\beta_2+\\beta_3X_i\n\\]\nまた実際に推定すべきは個別効果の期待値である． つまり，\\(\\beta_2\\)に\\(\\beta_3\\)と\\(X_i\\)の平均値の積を加えた値である. これは実際に計算してみると共分散分析で求めることが可能となる.\n\\[\n\\tau_{ATE}=E[Y_i(1)-Y_i(0)]\\\\\n\\tau_{ATE}=\\beta_2+\\beta_3E[X_i]\n\\]\n\n\n\n\\(\\tau_{\\text{ATE}}\\)の標準誤差は lmの結果からは求めることが出来ないので注意． 詳細は省略するが\\(var(\\tau_{\\text{ATE}})\\)を求める必要がある.\n\n\n\n\n\n結果変数には影響するが，処置変数とは関連のない共変量は，モデルに入れても偏りに悪影響はない．ただし，モデルの説明量を改善できるため，そのような変数が存在する場合には積極的に活用すること.\n結果変数に影響を与えており，処置変数とも関連のある共変量は偏りに悪影響を及ぼすため，モデルにふくめなければならない．このような変数のことを「交絡因子」と呼ぶ\n処置変数とは関連あるが，結果変数に影響のない共変量は，標準誤差を大きくするため含めない方が望ましい．\n処置変数と結果変数の因果のパスの間に存在する中間辺陬はモデルに入れてはならない\n２以上の共変量間に強い多重共線性があってもそのままモデルに入れてよい\nあまり重要でない共変量はまとめて誤差のとして扱うのが現実的な場合もある.\nもし処置群と統制群とで回帰の傾きが平行でないと考えられるならば，モデルに交互作用項を入れる必要がある\n\n\n\n\n共変量が多変量のときに作業が膨大になる. 平均処置効果，つまり処置があった場合となかった場合の「差」を推定することは可能であるが， 処置群の平均処置効果を推定することが出来ない．\n\n\n\n傾向スコアとは共変量\\(X\\)が与えられたとき， 処置に割り付けられる確率である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#共分散分析の仮定",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#共分散分析の仮定",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "共分散分析のモデル次式に示す． ダミー変数を含む重回帰モデルであり， 共変量によるデータの偏りを調整したうえで処置の影響を算出する有力なモデルである．\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1}+\\beta_2T_i+\\epsilon_i\n\\]\n\\[\nT_i =\n\\begin{cases}\n1 \\\\\n0\n\\end{cases}\n\\]\n重回帰モデルであるため共分散分析では，これまでの仮定を満たす必要がある． さらに処置変数によらず共変量が目的変数に与える影響が同じ， つまり\\(\\beta_1\\)が共通という仮定がある.\nこの仮定が満たされない場合には，交互作用項を見込んだモデルを作成する必要がある.\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1}+\\beta_2T_i+\\beta_3X_{i1}T_i+\\epsilon_i\n\\]\nこのモデルを使うことより説明変数が目的変数に与える影響は， \\(\\beta_3\\)だけ処置群間で異なることがわかる.\n\\[\nY_i = \\beta_0 + (\\beta_1+\\beta_3T_i)X_{i1}+\\beta_2T_i+\\epsilon_i\n\\]\nこのようなモデルが必要になるのは, 説明変数で条件づけたときに， 処置の割り付け確率が異なるということが影響している． これにより潜在的結果変数と処置が独立でなくなる.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#交互作用項のある共分散分析-1",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#交互作用項のある共分散分析-1",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "\\[\nY_i(T_i=0) = \\beta_0 + \\beta_1X_{i}+\\epsilon_i\n\\]\n\\[\nY_i(T_i=1) = (\\beta_0 + \\beta_2) + (\\beta_1+\\beta_3)X_{i}+\\epsilon_i\n\\]\n\n\n個体別効果は潜在的結果変数\\(Y_i(1)\\)と\\(Y_i(0)\\)の差であるから， これは次式で表されることになる.\n\\[\n\\tau_i=Y_i(1)-Y_i(0)=\\beta_2+\\beta_3X_i\n\\]\nまた実際に推定すべきは個別効果の期待値である． つまり，\\(\\beta_2\\)に\\(\\beta_3\\)と\\(X_i\\)の平均値の積を加えた値である. これは実際に計算してみると共分散分析で求めることが可能となる.\n\\[\n\\tau_{ATE}=E[Y_i(1)-Y_i(0)]\\\\\n\\tau_{ATE}=\\beta_2+\\beta_3E[X_i]\n\\]\n\n\n\n\\(\\tau_{\\text{ATE}}\\)の標準誤差は lmの結果からは求めることが出来ないので注意． 詳細は省略するが\\(var(\\tau_{\\text{ATE}})\\)を求める必要がある.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#統制すべき共変量に関するまとめ",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#統制すべき共変量に関するまとめ",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "結果変数には影響するが，処置変数とは関連のない共変量は，モデルに入れても偏りに悪影響はない．ただし，モデルの説明量を改善できるため，そのような変数が存在する場合には積極的に活用すること.\n結果変数に影響を与えており，処置変数とも関連のある共変量は偏りに悪影響を及ぼすため，モデルにふくめなければならない．このような変数のことを「交絡因子」と呼ぶ\n処置変数とは関連あるが，結果変数に影響のない共変量は，標準誤差を大きくするため含めない方が望ましい．\n処置変数と結果変数の因果のパスの間に存在する中間辺陬はモデルに入れてはならない\n２以上の共変量間に強い多重共線性があってもそのままモデルに入れてよい\nあまり重要でない共変量はまとめて誤差のとして扱うのが現実的な場合もある.\nもし処置群と統制群とで回帰の傾きが平行でないと考えられるならば，モデルに交互作用項を入れる必要がある",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#共分散分析の限界",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#共分散分析の限界",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "共変量が多変量のときに作業が膨大になる. 平均処置効果，つまり処置があった場合となかった場合の「差」を推定することは可能であるが， 処置群の平均処置効果を推定することが出来ない．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#傾向スコアと共分散分析の優劣",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#傾向スコアと共分散分析の優劣",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "傾向スコアとは共変量\\(X\\)が与えられたとき， 処置に割り付けられる確率である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "よく似たデザインは，文化や政治体制，教育，社会保障などが似た 二つの国を対象にして，ある変数を比較することである． しかし，この方法は完全に一致することはないので，厳密にはどの変数が影響しているのかを 判断することは難しい．\n\n\n\n同じ個体に対して，ある処置を行った場合，行っていない場合の二つの結果を得るのは不可能である．\nこのため実験研究では，無作為に処置の割り付けをした処置群と統制群の二つの群間で， 結果を比較する方法を開発してきた．これにより平均処置効果(ATE)を推定することが可能となった.\n観察研究では無作為の割り付けが困難であるため， 共変量を使ったマッチング方法が開発されて， 事実上同じ個体であると扱うことで因果推論を行うことが出来た．\n交絡因子の検証を行うことで，処置効果を推定することが可能となる． マッチング後にこのような考察を行うには， 繰り返しになるが「未確認の交絡因子がない」という仮定である\n\n\n\n統計的因果推論では，平均処置効果(ATE)，処置群の平均処置効果(ATT)が主な推定対象である.\n\\[\n\\begin{align}\n\\tau_{ATE}&=E[Y_i(1)-Y_i(0)]=E[Y_i(1)]-E[Y_i(0)] \\\\\n\\tau_{ATT}&=E[Y_i(1)-Y_i(0)|T_i=1]=E[Y_i(1)|T_i=1]-E[Y_i(0)|T_i=1]\n\\end{align}\n\\]\nマッチングにおける推定対象は，ATTだけである． なぜなら，処置群における個体に対して，対照群からマッチングする候補を選んでくるため， マッチング後のデータは処置群の個体を中心として構成されているからである．\nまた共分散分析ではATEを推定することは出来るが，ATTを推定することができない． ATTを推定する場合には傾向スコアマッチング，ATEを推定する場合には傾向スコアによる 層化解析，傾向スコアによる重み付け方法を用いる．\n\n\n\n使用するデータは多変量対数正規分布である． このデータの説明変数はテプリッツ行列を相関係数行列して設定している. テプリッツ行列は対称行列であり，斜め成分の値が一致である．\n\n\nCode\ndata11 &lt;- read_csv(\"./causality/data11.csv\", show_col_types = FALSE)\ncor((select(data11, starts_with(\"x\"))))\n#&gt;             x1         x2         x3        x4         x5          x6\n#&gt; x1  1.00000000 0.49291889 0.23515110 0.1042767 0.01814199 -0.01702065\n#&gt; x2  0.49291889 1.00000000 0.50415434 0.2381513 0.08718005  0.02007256\n#&gt; x3  0.23515110 0.50415434 1.00000000 0.5155681 0.21899658  0.09432731\n#&gt; x4  0.10427668 0.23815132 0.51556809 1.0000000 0.49573573  0.28363283\n#&gt; x5  0.01814199 0.08718005 0.21899658 0.4957357 1.00000000  0.53941181\n#&gt; x6 -0.01702065 0.02007256 0.09432731 0.2836328 0.53941181  1.00000000\n\n\n\n\nCode\n# 設定した相関係数行列\ntoeplitz(0.5^(0:5))\n#&gt;         [,1]   [,2]  [,3]  [,4]   [,5]    [,6]\n#&gt; [1,] 1.00000 0.5000 0.250 0.125 0.0625 0.03125\n#&gt; [2,] 0.50000 1.0000 0.500 0.250 0.1250 0.06250\n#&gt; [3,] 0.25000 0.5000 1.000 0.500 0.2500 0.12500\n#&gt; [4,] 0.12500 0.2500 0.500 1.000 0.5000 0.25000\n#&gt; [5,] 0.06250 0.1250 0.250 0.500 1.0000 0.50000\n#&gt; [6,] 0.03125 0.0625 0.125 0.250 0.5000 1.00000\n\n\n\n\nCode\n#ATE\nwith(data11, {\n    print(mean(y1t) - mean(y0t))\n})\n#&gt; [1] 3.755947\n\n\n\n\nCode\n#ATT\nwith(data11, {\n    print(mean(y1t[t1==1]) - mean(y0t[t1==1]))\n})\n#&gt; [1] 2.888651\n\n\n\n\n\nナイーブな比較では\\(t_1\\)の係数が15となっており，ATT，ATEとどちらもも大きく離れていることがわかる．\n\n\nCode\n# ナイーブ\nlm( y3 ~ t1, data = data11) |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1, data = data11)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -62.190 -13.494  -1.613  15.272  69.376 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  26.2678     0.9065   28.98   &lt;2e-16 ***\n#&gt; t1           15.8099     1.4534   10.88   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 22.41 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.106,  Adjusted R-squared:  0.1051 \n#&gt; F-statistic: 118.3 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n共分散分析の結果ではATT値とは当然異なるが， ATE値を見ても異なることがわかる． これは全ての交差項が含まれていないため，正しくモデリングできていないことによる． （もともとのデータで共変量の傾きが処置群と統制群で共通でないものを作成している）．\n\n\nCode\n# 共分散分析\nlm( y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11) |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -18.9525  -3.0228  -0.1244   3.1150  17.4160 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.0519     0.3086   3.409 0.000678 ***\n#&gt; t1            3.4741     0.3266  10.637  &lt; 2e-16 ***\n#&gt; x1            1.3928     0.1841   7.566 8.77e-14 ***\n#&gt; x2           -0.2920     0.1942  -1.504 0.132947    \n#&gt; x3            0.4540     0.1943   2.337 0.019663 *  \n#&gt; x4            7.4736     0.2046  36.531  &lt; 2e-16 ***\n#&gt; x5            9.6599     0.1967  49.101  &lt; 2e-16 ***\n#&gt; x6           11.1501     0.1873  59.545  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4.836 on 992 degrees of freedom\n#&gt; Multiple R-squared:  0.9586, Adjusted R-squared:  0.9583 \n#&gt; F-statistic:  3282 on 7 and 992 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n個体をペアと見なす傾向マッチングでは， 復元抽出であるか，非復元抽出であるのかを決める必要がある. とりあえず復元抽出の方が情報を抽出しやすいということを覚えておく．\n\n\n\n傾向スコアの算出はロジスティックス回帰である必要はない． 個体間の距離が\\(|e_j-e_i|\\)がスカラーで算出するだけでよい．\nマッチング方法も様々ある.\n\n\n\n\n\nCode\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nm.out1 &lt;- matchit(\n    t1 ~ x1 + x2 + x3 + x4 + x5 + x6, \n    data = data11, \n    replace = TRUE, \n    distance = \"glm\", \n    method = \"nearest\"\n)\n\n# マッチング後のデータを抽出\nm.data1 &lt;- match.data(m.out1)\nmodel1  &lt;- lm(y3 ~ t1, data = m.data1, weights = weights)\nmodel2  &lt;- lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = m.data1, weights = weights)\n\n\n\n\nCode\nm.data1\n#&gt; # A tibble: 618 × 12\n#&gt;       y0t   y1t     y3    t1     x1     x2      x3    x4      x5     x6 distance\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 61.6   64.7  64.7       1  0.640 -1.20  -0.0539 1.67   2.11    2.47     0.618\n#&gt;  2 -0.275 -1.97 -0.275     0 -0.473  0.747  0.961  0.956  0.223  -1.06     0.210\n#&gt;  3 46.8   48.3  48.3       1  0.846 -0.855  0.159  1.05   1.29    2.05     0.515\n#&gt;  4 31.0   34.6  34.6       1  0.680 -0.106  0.304  0.362  1.66    0.992    0.427\n#&gt;  5 53.3   54.3  54.3       1  1.70   2.02   2.23   2.88   1.58    1.22     0.481\n#&gt;  6 23.6   20.6  23.6       0 -0.752 -0.567  0.440  1.56   0.686   0.263    0.348\n#&gt;  7 49.5   56.3  49.5       0  2.13   2.11   1.72   1.66   1.27    1.94     0.467\n#&gt;  8 54.5   44.1  54.5       0  1.40  -0.291  1.67   1.58   0.982   2.05     0.549\n#&gt;  9 15.1   30.3  30.3       1  1.48   2.36   0.775  0.918 -0.0192  0.808    0.245\n#&gt; 10 24.4   42.3  42.3       1  0.357  1.47  -0.0248 0.435  1.79    0.733    0.341\n#&gt; # ℹ 608 more rows\n#&gt; # ℹ 1 more variable: weights &lt;dbl&gt;\n\n\n\n\nCode\nmodel1 |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1, data = m.data1, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -75.717 -14.327  -1.654  13.604  83.700 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   39.484      1.464  26.968   &lt;2e-16 ***\n#&gt; t1             2.594      1.845   1.406     0.16    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 22.16 on 616 degrees of freedom\n#&gt; Multiple R-squared:  0.003197,   Adjusted R-squared:  0.001579 \n#&gt; F-statistic: 1.976 on 1 and 616 DF,  p-value: 0.1603\n\n\n\n\nCode\nmodel2 |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = m.data1, \n#&gt;     weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -19.7535  -2.9321  -0.1799   2.7255  19.9425 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.8485     0.5049   3.661 0.000273 ***\n#&gt; t1            2.9174     0.4026   7.247 1.29e-12 ***\n#&gt; x1            1.7283     0.2465   7.010 6.32e-12 ***\n#&gt; x2            1.0956     0.2480   4.417 1.18e-05 ***\n#&gt; x3           -2.1161     0.2437  -8.683  &lt; 2e-16 ***\n#&gt; x4            7.8469     0.2553  30.731  &lt; 2e-16 ***\n#&gt; x5            9.9916     0.2479  40.306  &lt; 2e-16 ***\n#&gt; x6           11.2766     0.2413  46.742  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4.83 on 610 degrees of freedom\n#&gt; Multiple R-squared:  0.9531, Adjusted R-squared:  0.9526 \n#&gt; F-statistic:  1771 on 7 and 610 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n一応作法があるので注意\n\n\n\nVar.Ratioの値が１に近いとバランスが取れている．\n\n\nCode\nsummary(m.out1)\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, \n#&gt;     method = \"nearest\", distance = \"glm\", replace = TRUE)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.3584          0.6022     0.9897    0.1755\n#&gt; x1              0.9655        0.9919         -0.0297     0.7966    0.0210\n#&gt; x2              0.9596        0.9810         -0.0217     0.8841    0.0200\n#&gt; x3              1.1451        0.9162          0.2196     1.0303    0.0616\n#&gt; x4              1.2246        0.8986          0.3429     0.9352    0.1017\n#&gt; x5              1.2874        0.7913          0.4963     1.0065    0.1461\n#&gt; x6              1.2896        0.8382          0.4860     0.9166    0.1376\n#&gt;          eCDF Max\n#&gt; distance   0.3232\n#&gt; x1         0.0540\n#&gt; x2         0.0524\n#&gt; x3         0.1002\n#&gt; x4         0.1797\n#&gt; x5         0.2387\n#&gt; x6         0.2212\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.4369          0.0012     1.0028    0.0014\n#&gt; x1              0.9655        0.9759         -0.0117     0.7990    0.0241\n#&gt; x2              0.9596        0.9644         -0.0049     0.8047    0.0273\n#&gt; x3              1.1451        1.1281          0.0162     0.9851    0.0100\n#&gt; x4              1.2246        1.2465         -0.0231     0.8135    0.0241\n#&gt; x5              1.2874        1.2614          0.0260     1.0968    0.0121\n#&gt; x6              1.2896        1.3208         -0.0336     0.9959    0.0120\n#&gt;          eCDF Max Std. Pair Dist.\n#&gt; distance   0.0129          0.0076\n#&gt; x1         0.0668          1.2125\n#&gt; x2         0.0771          1.2002\n#&gt; x3         0.0386          1.0269\n#&gt; x4         0.0566          0.9933\n#&gt; x5         0.0540          0.6018\n#&gt; x6         0.0488          0.6997\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All            611.       389\n#&gt; Matched (ESS)  167.21     389\n#&gt; Matched        229.       389\n#&gt; Unmatched      382.         0\n#&gt; Discarded        0.         0\n\n\nラブプロットという図を書いてすべてが0.1以下になるように， 共変量や高次項などを調整していく. ラブプロットを記述するライブラリも存在しているが，ここでは スクラッチで実装する．これは後に再利用するためである．\n\n\nCode\ndiffa &lt;- abs(summary(m.out1)$sum.all[,3])\ndiffb &lt;- abs(summary(m.out1)$sum.matched[,3])\ndiff1 &lt;- rev(diffa)\ndiff2 &lt;- rev(diffb)\n\nmaxx    &lt;- max(diff1, diff2)\nlabels0 &lt;- rownames(summary(m.out1)$sum.all)\nlabels1 &lt;- rev(labels0)\n\ndotchart(diff1, xlim = c(0, maxx), labels = c(labels1))\nabline(v = .0, col = 8)\nabline(v = .1, col = 8)\nabline(v = .05, lty = 2, col = 8)\n\npar(new = TRUE)\ndotchart(diff2, xlim = c(0, maxx), labels = c(labels1), \n         pch = 16, xlab = \"Absolute Standardized Mean Difference\")",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#比較政治学におけるよく似たシスステムデザイン",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#比較政治学におけるよく似たシスステムデザイン",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "よく似たデザインは，文化や政治体制，教育，社会保障などが似た 二つの国を対象にして，ある変数を比較することである． しかし，この方法は完全に一致することはないので，厳密にはどの変数が影響しているのかを 判断することは難しい．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#統計的因果推論におけるマッチング",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#統計的因果推論におけるマッチング",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "同じ個体に対して，ある処置を行った場合，行っていない場合の二つの結果を得るのは不可能である．\nこのため実験研究では，無作為に処置の割り付けをした処置群と統制群の二つの群間で， 結果を比較する方法を開発してきた．これにより平均処置効果(ATE)を推定することが可能となった.\n観察研究では無作為の割り付けが困難であるため， 共変量を使ったマッチング方法が開発されて， 事実上同じ個体であると扱うことで因果推論を行うことが出来た．\n交絡因子の検証を行うことで，処置効果を推定することが可能となる． マッチング後にこのような考察を行うには， 繰り返しになるが「未確認の交絡因子がない」という仮定である",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#推定対象",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#推定対象",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "統計的因果推論では，平均処置効果(ATE)，処置群の平均処置効果(ATT)が主な推定対象である.\n\\[\n\\begin{align}\n\\tau_{ATE}&=E[Y_i(1)-Y_i(0)]=E[Y_i(1)]-E[Y_i(0)] \\\\\n\\tau_{ATT}&=E[Y_i(1)-Y_i(0)|T_i=1]=E[Y_i(1)|T_i=1]-E[Y_i(0)|T_i=1]\n\\end{align}\n\\]\nマッチングにおける推定対象は，ATTだけである． なぜなら，処置群における個体に対して，対照群からマッチングする候補を選んでくるため， マッチング後のデータは処置群の個体を中心として構成されているからである．\nまた共分散分析ではATEを推定することは出来るが，ATTを推定することができない． ATTを推定する場合には傾向スコアマッチング，ATEを推定する場合には傾向スコアによる 層化解析，傾向スコアによる重み付け方法を用いる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#使用するデータ",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#使用するデータ",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "使用するデータは多変量対数正規分布である． このデータの説明変数はテプリッツ行列を相関係数行列して設定している. テプリッツ行列は対称行列であり，斜め成分の値が一致である．\n\n\nCode\ndata11 &lt;- read_csv(\"./causality/data11.csv\", show_col_types = FALSE)\ncor((select(data11, starts_with(\"x\"))))\n#&gt;             x1         x2         x3        x4         x5          x6\n#&gt; x1  1.00000000 0.49291889 0.23515110 0.1042767 0.01814199 -0.01702065\n#&gt; x2  0.49291889 1.00000000 0.50415434 0.2381513 0.08718005  0.02007256\n#&gt; x3  0.23515110 0.50415434 1.00000000 0.5155681 0.21899658  0.09432731\n#&gt; x4  0.10427668 0.23815132 0.51556809 1.0000000 0.49573573  0.28363283\n#&gt; x5  0.01814199 0.08718005 0.21899658 0.4957357 1.00000000  0.53941181\n#&gt; x6 -0.01702065 0.02007256 0.09432731 0.2836328 0.53941181  1.00000000\n\n\n\n\nCode\n# 設定した相関係数行列\ntoeplitz(0.5^(0:5))\n#&gt;         [,1]   [,2]  [,3]  [,4]   [,5]    [,6]\n#&gt; [1,] 1.00000 0.5000 0.250 0.125 0.0625 0.03125\n#&gt; [2,] 0.50000 1.0000 0.500 0.250 0.1250 0.06250\n#&gt; [3,] 0.25000 0.5000 1.000 0.500 0.2500 0.12500\n#&gt; [4,] 0.12500 0.2500 0.500 1.000 0.5000 0.25000\n#&gt; [5,] 0.06250 0.1250 0.250 0.500 1.0000 0.50000\n#&gt; [6,] 0.03125 0.0625 0.125 0.250 0.5000 1.00000\n\n\n\n\nCode\n#ATE\nwith(data11, {\n    print(mean(y1t) - mean(y0t))\n})\n#&gt; [1] 3.755947\n\n\n\n\nCode\n#ATT\nwith(data11, {\n    print(mean(y1t[t1==1]) - mean(y0t[t1==1]))\n})\n#&gt; [1] 2.888651",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#ナイーブな比較と共分散分析",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#ナイーブな比較と共分散分析",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "ナイーブな比較では\\(t_1\\)の係数が15となっており，ATT，ATEとどちらもも大きく離れていることがわかる．\n\n\nCode\n# ナイーブ\nlm( y3 ~ t1, data = data11) |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1, data = data11)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -62.190 -13.494  -1.613  15.272  69.376 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  26.2678     0.9065   28.98   &lt;2e-16 ***\n#&gt; t1           15.8099     1.4534   10.88   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 22.41 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.106,  Adjusted R-squared:  0.1051 \n#&gt; F-statistic: 118.3 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n共分散分析の結果ではATT値とは当然異なるが， ATE値を見ても異なることがわかる． これは全ての交差項が含まれていないため，正しくモデリングできていないことによる． （もともとのデータで共変量の傾きが処置群と統制群で共通でないものを作成している）．\n\n\nCode\n# 共分散分析\nlm( y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11) |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -18.9525  -3.0228  -0.1244   3.1150  17.4160 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.0519     0.3086   3.409 0.000678 ***\n#&gt; t1            3.4741     0.3266  10.637  &lt; 2e-16 ***\n#&gt; x1            1.3928     0.1841   7.566 8.77e-14 ***\n#&gt; x2           -0.2920     0.1942  -1.504 0.132947    \n#&gt; x3            0.4540     0.1943   2.337 0.019663 *  \n#&gt; x4            7.4736     0.2046  36.531  &lt; 2e-16 ***\n#&gt; x5            9.6599     0.1967  49.101  &lt; 2e-16 ***\n#&gt; x6           11.1501     0.1873  59.545  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4.836 on 992 degrees of freedom\n#&gt; Multiple R-squared:  0.9586, Adjusted R-squared:  0.9583 \n#&gt; F-statistic:  3282 on 7 and 992 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#復元によるマッチングと非復元によるマッチング",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#復元によるマッチングと非復元によるマッチング",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "個体をペアと見なす傾向マッチングでは， 復元抽出であるか，非復元抽出であるのかを決める必要がある. とりあえず復元抽出の方が情報を抽出しやすいということを覚えておく．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#距離",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#距離",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "傾向スコアの算出はロジスティックス回帰である必要はない． 個体間の距離が\\(|e_j-e_i|\\)がスカラーで算出するだけでよい．\nマッチング方法も様々ある.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#rによる復元抽出の傾向スコアマッチングattの推定",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#rによる復元抽出の傾向スコアマッチングattの推定",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "Code\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nm.out1 &lt;- matchit(\n    t1 ~ x1 + x2 + x3 + x4 + x5 + x6, \n    data = data11, \n    replace = TRUE, \n    distance = \"glm\", \n    method = \"nearest\"\n)\n\n# マッチング後のデータを抽出\nm.data1 &lt;- match.data(m.out1)\nmodel1  &lt;- lm(y3 ~ t1, data = m.data1, weights = weights)\nmodel2  &lt;- lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = m.data1, weights = weights)\n\n\n\n\nCode\nm.data1\n#&gt; # A tibble: 618 × 12\n#&gt;       y0t   y1t     y3    t1     x1     x2      x3    x4      x5     x6 distance\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 61.6   64.7  64.7       1  0.640 -1.20  -0.0539 1.67   2.11    2.47     0.618\n#&gt;  2 -0.275 -1.97 -0.275     0 -0.473  0.747  0.961  0.956  0.223  -1.06     0.210\n#&gt;  3 46.8   48.3  48.3       1  0.846 -0.855  0.159  1.05   1.29    2.05     0.515\n#&gt;  4 31.0   34.6  34.6       1  0.680 -0.106  0.304  0.362  1.66    0.992    0.427\n#&gt;  5 53.3   54.3  54.3       1  1.70   2.02   2.23   2.88   1.58    1.22     0.481\n#&gt;  6 23.6   20.6  23.6       0 -0.752 -0.567  0.440  1.56   0.686   0.263    0.348\n#&gt;  7 49.5   56.3  49.5       0  2.13   2.11   1.72   1.66   1.27    1.94     0.467\n#&gt;  8 54.5   44.1  54.5       0  1.40  -0.291  1.67   1.58   0.982   2.05     0.549\n#&gt;  9 15.1   30.3  30.3       1  1.48   2.36   0.775  0.918 -0.0192  0.808    0.245\n#&gt; 10 24.4   42.3  42.3       1  0.357  1.47  -0.0248 0.435  1.79    0.733    0.341\n#&gt; # ℹ 608 more rows\n#&gt; # ℹ 1 more variable: weights &lt;dbl&gt;\n\n\n\n\nCode\nmodel1 |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1, data = m.data1, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -75.717 -14.327  -1.654  13.604  83.700 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   39.484      1.464  26.968   &lt;2e-16 ***\n#&gt; t1             2.594      1.845   1.406     0.16    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 22.16 on 616 degrees of freedom\n#&gt; Multiple R-squared:  0.003197,   Adjusted R-squared:  0.001579 \n#&gt; F-statistic: 1.976 on 1 and 616 DF,  p-value: 0.1603\n\n\n\n\nCode\nmodel2 |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = m.data1, \n#&gt;     weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -19.7535  -2.9321  -0.1799   2.7255  19.9425 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.8485     0.5049   3.661 0.000273 ***\n#&gt; t1            2.9174     0.4026   7.247 1.29e-12 ***\n#&gt; x1            1.7283     0.2465   7.010 6.32e-12 ***\n#&gt; x2            1.0956     0.2480   4.417 1.18e-05 ***\n#&gt; x3           -2.1161     0.2437  -8.683  &lt; 2e-16 ***\n#&gt; x4            7.8469     0.2553  30.731  &lt; 2e-16 ***\n#&gt; x5            9.9916     0.2479  40.306  &lt; 2e-16 ***\n#&gt; x6           11.2766     0.2413  46.742  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4.83 on 610 degrees of freedom\n#&gt; Multiple R-squared:  0.9531, Adjusted R-squared:  0.9526 \n#&gt; F-statistic:  1771 on 7 and 610 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#標準誤差",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#標準誤差",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "一応作法があるので注意",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#バランシングの評価",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#バランシングの評価",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "Var.Ratioの値が１に近いとバランスが取れている．\n\n\nCode\nsummary(m.out1)\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, \n#&gt;     method = \"nearest\", distance = \"glm\", replace = TRUE)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.3584          0.6022     0.9897    0.1755\n#&gt; x1              0.9655        0.9919         -0.0297     0.7966    0.0210\n#&gt; x2              0.9596        0.9810         -0.0217     0.8841    0.0200\n#&gt; x3              1.1451        0.9162          0.2196     1.0303    0.0616\n#&gt; x4              1.2246        0.8986          0.3429     0.9352    0.1017\n#&gt; x5              1.2874        0.7913          0.4963     1.0065    0.1461\n#&gt; x6              1.2896        0.8382          0.4860     0.9166    0.1376\n#&gt;          eCDF Max\n#&gt; distance   0.3232\n#&gt; x1         0.0540\n#&gt; x2         0.0524\n#&gt; x3         0.1002\n#&gt; x4         0.1797\n#&gt; x5         0.2387\n#&gt; x6         0.2212\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.4369          0.0012     1.0028    0.0014\n#&gt; x1              0.9655        0.9759         -0.0117     0.7990    0.0241\n#&gt; x2              0.9596        0.9644         -0.0049     0.8047    0.0273\n#&gt; x3              1.1451        1.1281          0.0162     0.9851    0.0100\n#&gt; x4              1.2246        1.2465         -0.0231     0.8135    0.0241\n#&gt; x5              1.2874        1.2614          0.0260     1.0968    0.0121\n#&gt; x6              1.2896        1.3208         -0.0336     0.9959    0.0120\n#&gt;          eCDF Max Std. Pair Dist.\n#&gt; distance   0.0129          0.0076\n#&gt; x1         0.0668          1.2125\n#&gt; x2         0.0771          1.2002\n#&gt; x3         0.0386          1.0269\n#&gt; x4         0.0566          0.9933\n#&gt; x5         0.0540          0.6018\n#&gt; x6         0.0488          0.6997\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All            611.       389\n#&gt; Matched (ESS)  167.21     389\n#&gt; Matched        229.       389\n#&gt; Unmatched      382.         0\n#&gt; Discarded        0.         0\n\n\nラブプロットという図を書いてすべてが0.1以下になるように， 共変量や高次項などを調整していく. ラブプロットを記述するライブラリも存在しているが，ここでは スクラッチで実装する．これは後に再利用するためである．\n\n\nCode\ndiffa &lt;- abs(summary(m.out1)$sum.all[,3])\ndiffb &lt;- abs(summary(m.out1)$sum.matched[,3])\ndiff1 &lt;- rev(diffa)\ndiff2 &lt;- rev(diffb)\n\nmaxx    &lt;- max(diff1, diff2)\nlabels0 &lt;- rownames(summary(m.out1)$sum.all)\nlabels1 &lt;- rev(labels0)\n\ndotchart(diff1, xlim = c(0, maxx), labels = c(labels1))\nabline(v = .0, col = 8)\nabline(v = .1, col = 8)\nabline(v = .05, lty = 2, col = 8)\n\npar(new = TRUE)\ndotchart(diff2, xlim = c(0, maxx), labels = c(labels1), \n         pch = 16, xlab = \"Absolute Standardized Mean Difference\")",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html",
    "title": "操作変数法の基礎",
    "section": "",
    "text": "これまでは共変量を調整することで交絡を無視することが出来た． 操作変数法では一定の仮定のもとで共変量による調整が出来ない場合， 効かない場合でも信頼のおける因果推論手法となる.\n操作変数法は，二段階最小二乗法，同時方程式モデリング，内生成とったキーワードでも 取り扱われている．\n\n\n\\(Y\\)を結果変数，\\(X\\)が観測さえれる共変量, \\(U\\)を観測されない共変量，とするときこれらの変数の関係は 次の図で表される構造を想定する. この構造において\\(X\\)から\\(Y\\)への効果を推定したいとする.\nなおこの構造は\\(U-&gt;X-&gt;Y\\)という部分で\\(U\\)が交絡しており， \\(X-&gt;U-&gt;Y\\)という原因\\(X\\)の中間変数\\(U\\)があるわけでないことに注意する. この構造は「アイスの売り上げと水難事故の相関における気温」にあたる．\n\n\nCode\ngrViz(\"\ndigraph dot {\n\ngraph [layout = dot, rankdir = TR]\n\nnode [shape = circle,\n      style = filled,\n      color = grey]\n    Y X U\n\nedge [color = black]\n    U -&gt; {Y X}\n    X -&gt; {Y}\n{rank = max; X; Y}\n\n}\")\n\n\n\n\n\n\nモデル式としては次となる. もし\\(U\\)が観測されるならば， これまでと同じように\\(U\\)をモデルに加えることで， 共分散分析や傾向スコアモデリングにより交絡を取り除くことが出来る． (\\(U\\)で\\(Y\\)と\\(X\\)を条件づけることで適切な解析が行える)\n\\[\nY= \\beta_0 + \\beta_1X + \\beta_2U + \\epsilon\n\\] しかし，ここでは\\(U\\)は観測されない共変量であることしており， 除外変数による偏りが不可避に発生しそうである． このような状況に対して観測できる共変量\\(Z\\)があり次のような構造にあると考える.\n\n\nCode\ngrViz(\"\ndigraph dot {\n\ngraph [layout = dot, rankdir = TR]\n\nnode [shape = circle,\n      style = filled,\n      color = grey]\n    Y X U Z\n\nedge [color = black]\n    U -&gt; {Y X}\n    X -&gt; {Y}\n    Z -&gt; {X}\n{rank = max; X; Y; Z}\n\n}\")\n\n\n\n\n\n\nこのような\\(U\\)に依存しておらず\\(X\\)に影響を与える変数を操作変数と呼ぶ. 操作変数を利用することで観測できない交絡因子\\(U\\)の影響を除いた， \\(X\\)から\\(Y\\)への影響を推定することが出来る.\n\n\n\n内生変数とはモデルの依存関係によって解の定まる変数である． 外生変数とは，モデルの依存関係によらず値が決まる変数である.\n結果変数と中間変数は内生変数である．共変量は外生変数である． 処理の割り付けが無作為でおこなわれているならば処置変数は外生変数である． しかし観察研究では処置変数は内生変数である\n\n\n\n結局のところ操作変数として適切な変数を見つけることは容易でない． 「結局は操作変数が解析者による何らかの介入や突発的な災害や事故の前後といった 変数以外では操作変数法の解析結果はあまり信頼されていない」と言われてる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数法の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#操作変数法のイメージ",
    "href": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#操作変数法のイメージ",
    "title": "操作変数法の基礎",
    "section": "",
    "text": "\\(Y\\)を結果変数，\\(X\\)が観測さえれる共変量, \\(U\\)を観測されない共変量，とするときこれらの変数の関係は 次の図で表される構造を想定する. この構造において\\(X\\)から\\(Y\\)への効果を推定したいとする.\nなおこの構造は\\(U-&gt;X-&gt;Y\\)という部分で\\(U\\)が交絡しており， \\(X-&gt;U-&gt;Y\\)という原因\\(X\\)の中間変数\\(U\\)があるわけでないことに注意する. この構造は「アイスの売り上げと水難事故の相関における気温」にあたる．\n\n\nCode\ngrViz(\"\ndigraph dot {\n\ngraph [layout = dot, rankdir = TR]\n\nnode [shape = circle,\n      style = filled,\n      color = grey]\n    Y X U\n\nedge [color = black]\n    U -&gt; {Y X}\n    X -&gt; {Y}\n{rank = max; X; Y}\n\n}\")\n\n\n\n\n\n\nモデル式としては次となる. もし\\(U\\)が観測されるならば， これまでと同じように\\(U\\)をモデルに加えることで， 共分散分析や傾向スコアモデリングにより交絡を取り除くことが出来る． (\\(U\\)で\\(Y\\)と\\(X\\)を条件づけることで適切な解析が行える)\n\\[\nY= \\beta_0 + \\beta_1X + \\beta_2U + \\epsilon\n\\] しかし，ここでは\\(U\\)は観測されない共変量であることしており， 除外変数による偏りが不可避に発生しそうである． このような状況に対して観測できる共変量\\(Z\\)があり次のような構造にあると考える.\n\n\nCode\ngrViz(\"\ndigraph dot {\n\ngraph [layout = dot, rankdir = TR]\n\nnode [shape = circle,\n      style = filled,\n      color = grey]\n    Y X U Z\n\nedge [color = black]\n    U -&gt; {Y X}\n    X -&gt; {Y}\n    Z -&gt; {X}\n{rank = max; X; Y; Z}\n\n}\")\n\n\n\n\n\n\nこのような\\(U\\)に依存しておらず\\(X\\)に影響を与える変数を操作変数と呼ぶ. 操作変数を利用することで観測できない交絡因子\\(U\\)の影響を除いた， \\(X\\)から\\(Y\\)への影響を推定することが出来る.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数法の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#内生変数と外生変数",
    "href": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#内生変数と外生変数",
    "title": "操作変数法の基礎",
    "section": "",
    "text": "内生変数とはモデルの依存関係によって解の定まる変数である． 外生変数とは，モデルの依存関係によらず値が決まる変数である.\n結果変数と中間変数は内生変数である．共変量は外生変数である． 処理の割り付けが無作為でおこなわれているならば処置変数は外生変数である． しかし観察研究では処置変数は内生変数である",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数法の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#よくない操作変数",
    "href": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#よくない操作変数",
    "title": "操作変数法の基礎",
    "section": "",
    "text": "結局のところ操作変数として適切な変数を見つけることは容易でない． 「結局は操作変数が解析者による何らかの介入や突発的な災害や事故の前後といった 変数以外では操作変数法の解析結果はあまり信頼されていない」と言われてる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数法の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html",
    "title": "回帰不連続デザイン",
    "section": "",
    "text": "傾向スコアでは処置群と統制群の間で重なりがあると仮定している． これは処理の割り付けが確率的である場合であることを意味する． 処理の割り付けが確定的な場合，たとえば入学試験で60以下の場合に受ける補習講義の ような場合にはその効果を傾向スコアで測ることは出来ない．\n回帰不連続デザインはこの場合に使える準実験である． つまり，同じグループ内で全員が一斉に何らかの介入を受けている状況を想定している． ある条件を満たしたときの介入がない状況と， ある状況を満たしていないときの介入がない状況に関する情報がない．\n血圧が高い人に処方する薬があるとする．このとき， ナイーブに推定すると薬を処方されると血圧が高まるという結果になってしまう． これは血圧が高いが薬が処方されていない人がいないという，交絡による．\n\n\nひとことでいうと，共分散分析をおこなうと外挿することになる． 血圧が高いが薬が処方されていない人については， 薬が処方されていない人から推定することになる. 共分散分析による外挿モデルでは，線形モデルを仮定することになるが， 線形モデルの妥当性に関する情報はデータから得ることが出来ない．\n\n\n\n上記の話しは「外挿は必要」であるが「外挿をしたくない」という話しである． そこでデータ全体における平均処置効果であるATEの推定を諦めて， 閾値\\(c\\)前後のみで比較することを考える.\n\\[\nE[Y_i(1)-Y_i(0)|X_i=c]=E[Y_i(1)|X_i=c]-E[Y_i(0)|X_i=c]\n\\]\n式の意味で言えば，別々に回帰モデルをつくって説明変数が\\(c\\)のときの値を比較する， という流れになる． 実際には\\(c\\pm h\\)の範囲を抽出して共分散分析をおこなうことになる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "回帰不連続デザイン"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html#平均処置効果と共分散分析",
    "href": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html#平均処置効果と共分散分析",
    "title": "回帰不連続デザイン",
    "section": "",
    "text": "ひとことでいうと，共分散分析をおこなうと外挿することになる． 血圧が高いが薬が処方されていない人については， 薬が処方されていない人から推定することになる. 共分散分析による外挿モデルでは，線形モデルを仮定することになるが， 線形モデルの妥当性に関する情報はデータから得ることが出来ない．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "回帰不連続デザイン"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html#閾値における局所的な平均処置効果",
    "href": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html#閾値における局所的な平均処置効果",
    "title": "回帰不連続デザイン",
    "section": "",
    "text": "上記の話しは「外挿は必要」であるが「外挿をしたくない」という話しである． そこでデータ全体における平均処置効果であるATEの推定を諦めて， 閾値\\(c\\)前後のみで比較することを考える.\n\\[\nE[Y_i(1)-Y_i(0)|X_i=c]=E[Y_i(1)|X_i=c]-E[Y_i(0)|X_i=c]\n\\]\n式の意味で言えば，別々に回帰モデルをつくって説明変数が\\(c\\)のときの値を比較する， という流れになる． 実際には\\(c\\pm h\\)の範囲を抽出して共分散分析をおこなうことになる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "回帰不連続デザイン"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/index.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/index.html",
    "title": "統計的因果推論の理論と実際",
    "section": "",
    "text": "本稿は共立出版の統計的因果推論の理論と実装に関する勉強ノートです. レベルとしては大学レベルの入門統計学を学習したものとされています. 正誤表があるので内容には注意して読むこと．\n\n前半部分の回帰分析までは理解できたと思うが， 特に回帰不連続デザインの実装部分からよくわかっていないので， 改めて勉強する必要がある．\nまずは処置群の平均処置効果を知りたいときには 「傾向スコアマッチング」をおこない， 平均処置効果を知りたい場合には「傾向スコアからの層別解析」を おこなうことを知っておくことが重要となる． 共分散分析，傾向スコア，操作変数法，回帰不連続デザインなどは 基本的な考え方はわかった． いずれにしてもデータといかに向き合うのかは重要と言える.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/index.html#概要",
    "href": "contents/books/05_統計的因果推論の理論と実際/index.html#概要",
    "title": "統計的因果推論の理論と実際",
    "section": "",
    "text": "本稿は共立出版の統計的因果推論の理論と実装に関する勉強ノートです. レベルとしては大学レベルの入門統計学を学習したものとされています. 正誤表があるので内容には注意して読むこと．\n\n前半部分の回帰分析までは理解できたと思うが， 特に回帰不連続デザインの実装部分からよくわかっていないので， 改めて勉強する必要がある．\nまずは処置群の平均処置効果を知りたいときには 「傾向スコアマッチング」をおこない， 平均処置効果を知りたい場合には「傾向スコアからの層別解析」を おこなうことを知っておくことが重要となる． 共分散分析，傾向スコア，操作変数法，回帰不連続デザインなどは 基本的な考え方はわかった． いずれにしてもデータといかに向き合うのかは重要と言える.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/test/R2/renv.html",
    "href": "contents/test/R2/renv.html",
    "title": "renv",
    "section": "",
    "text": "1 hereでrenvはどのように指定すればいいか\n\nプロジェクトファイルがあるフォルダで指定するときはそのフォルダになる\nプロジェクトファイルがないフォルダで指定するときはルートフォルダになる\n正確には１番近い親フォルダになっているみたい\nなのでこの場合のhereの挙動には注意する\n\n\n\nCode\nhere::here()\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite\"\ncur_dir &lt;- here::here(\"contents\", \"test\", \"R\")\nprint(cur_dir)\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\n# renv::init(cur_dir)\n\nprint(getwd())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R2\"\nprint(here::here())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite\"\n\n\n\n\nCode\nlibrary(here)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Test",
      "R2",
      "renv"
    ]
  },
  {
    "objectID": "contents/test/bash/quick_sample.html",
    "href": "contents/test/bash/quick_sample.html",
    "title": "Quick Sample",
    "section": "",
    "text": "engine: knitrを追加することで動作\nカレントディレクトリはこのファイルがある場所\nルートディレクトリになっていないことに注意\n\n\n\nCode\nls -la | cat\n#&gt; total 0\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 22:06 .\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 21:52 ..\n#&gt; -rwxrwxrwx 1 asozan asozan  376 Sep 18 22:06 quick_sample.qmd\n#&gt; -rwxrwxrwx 1 asozan asozan  402 Sep 18 22:06 quick_sample.rmarkdown\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 22:06 quick_sample_cache\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 21:48 日本語フォルダ\n\n\n\n\nCode\nls -la\n#&gt; total 0\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 22:06 .\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 21:52 ..\n#&gt; -rwxrwxrwx 1 asozan asozan  376 Sep 18 22:06 quick_sample.qmd\n#&gt; -rwxrwxrwx 1 asozan asozan  402 Sep 18 22:06 quick_sample.rmarkdown\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 22:06 quick_sample_cache\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 21:48 日本語フォルダ\n\n\n\n\nCode\ncat 日本語フォルダ/日本語ファイル.txt\n#&gt; \n#&gt; 日本語テキストです\n\n\npwdで確認するとWSLでマウントしたときのディレクトリが カレントディレクトリになっていることがわかる。\n\n\nCode\npwd\n#&gt; /mnt/h/Dropbox/R/Workspace/RTipsSite/contents/test/bash\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Test",
      "Bash",
      "Quick Sample"
    ]
  },
  {
    "objectID": "contents/test/R/sub_directory/renv.html",
    "href": "contents/test/R/sub_directory/renv.html",
    "title": "renv",
    "section": "",
    "text": "1 renvはどのように指定すればいいか\n\nプロジェクトファイルがあるフォルダで指定するときはそのフォルダになる\nプロジェクトファイルがないフォルダで指定するときはルートフォルダになる\n正確には１番近い親フォルダになっているみたい\nなのでこの場合のhereの挙動には注意する\n\n\n\nCode\nhere::here()\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\ncur_dir &lt;- here::here(\"contents\", \"test\", \"R\")\nprint(cur_dir)\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R/contents/test/R\"\n# renv::init(cur_dir)\n\nprint(getwd())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R/sub_directory\"\nprint(here::here())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\n\n\n\n\nCode\nlibrary(here)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Test",
      "R",
      "Sub Directory",
      "renv"
    ]
  }
]