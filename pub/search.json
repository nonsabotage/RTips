[
  {
    "objectID": "contents/sql/duckdb/documentaion/guides_python.html",
    "href": "contents/sql/duckdb/documentaion/guides_python.html",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "Code\nimport duckdb\nimport pandas as pd\n\n\n%load_ext sql\nconn = duckdb.connect(\"penguins.db\")\n%sql conn --alias duckdb\n\n\nDeploy Panel apps for free on Ploomber Cloud! Learn more: https://ploomber.io/s/signup\n\n\n\n\n\n\n\nCode\nimport duckdb\nimport pandas as pd\n\n%load_ext sql\n%config SqlMagic.autopandas = True\n%config SqlMagic.feedback = False\n%config SqlMagic.displaycon = False\n\n\nThe sql extension is already loaded. To reload it, use:\n  %reload_ext sql\n\n\n\n\nCode\n%reload_ext sql\n\n\n\n\n\npicture 0",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guides_python.html#connecting-to-duckdb-natively",
    "href": "contents/sql/duckdb/documentaion/guides_python.html#connecting-to-duckdb-natively",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "Code\nimport duckdb\nimport pandas as pd\n\n\n%load_ext sql\nconn = duckdb.connect(\"penguins.db\")\n%sql conn --alias duckdb\n\n\nDeploy Panel apps for free on Ploomber Cloud! Learn more: https://ploomber.io/s/signup",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guides_python.html#connecting-to-duckdb-via-sqlalchemy-using-duckdb_engine",
    "href": "contents/sql/duckdb/documentaion/guides_python.html#connecting-to-duckdb-via-sqlalchemy-using-duckdb_engine",
    "title": "Jupyter Notebooks",
    "section": "",
    "text": "Code\nimport duckdb\nimport pandas as pd\n\n%load_ext sql\n%config SqlMagic.autopandas = True\n%config SqlMagic.feedback = False\n%config SqlMagic.displaycon = False\n\n\nThe sql extension is already loaded. To reload it, use:\n  %reload_ext sql\n\n\n\n\nCode\n%reload_ext sql\n\n\n\n\n\npicture 0",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guides_python.html#querying-pandas-dataframes",
    "href": "contents/sql/duckdb/documentaion/guides_python.html#querying-pandas-dataframes",
    "title": "Jupyter Notebooks",
    "section": "2.1 Querying Pandas Dataframes",
    "text": "2.1 Querying Pandas Dataframes\n\n\nCode\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nGentoo\nBiscoe\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\nGentoo\nBiscoe\n46.8\n14.3\n215.0\n4850.0\nFemale\n\n\n341\nGentoo\nBiscoe\n50.4\n15.7\n222.0\n5750.0\nMale\n\n\n342\nGentoo\nBiscoe\n45.2\n14.8\n212.0\n5200.0\nFemale\n\n\n343\nGentoo\nBiscoe\n49.9\n16.1\n213.0\n5400.0\nMale\n\n\n\n\n344 rows × 7 columns\n\n\n\n\n\nCode\npenguins.to_csv(\"./penguins.csv\", index = False)\n\n\n\n\nCode\n%%sql --save penguins\nSELECT species, sex, SUM(bill_length_mm)\nFROM penguins\nGROUP BY species, sex;\n\n\nRunning query in 'duckdb'\n\n\n\n\n\nspecies\nsex\nsum(bill_length_mm)\n\n\n\n\nGentoo\nNone\n182.5\n\n\nChinstrap\nFemale\n1583.5\n\n\nAdelie\nFemale\n2719.7999999999993\n\n\nGentoo\nMale\n3017.9000000000005\n\n\nChinstrap\nMale\n1737.2\n\n\nAdelie\nMale\n2948.4999999999973\n\n\nGentoo\nFemale\n2642.7\n\n\nAdelie\nNone\n189.2\n\n\n\n\n\n\n\nCode\n%%sql\nfrom penguins;\n\n\nYour query is using the following snippets: penguins. The query is not a SELECT type query and as snippets only work with SELECT queries, CTE generation is disabled\n\n\nRunning query in 'duckdb'\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\nAdelie\nTorgersen\nNone\nNone\nNone\nNone\nNone\n\n\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nMale\n\n\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nFemale\n\n\nAdelie\nTorgersen\n39.2\n19.6\n195.0\n4675.0\nMale\n\n\nAdelie\nTorgersen\n34.1\n18.1\n193.0\n3475.0\nNone\n\n\nAdelie\nTorgersen\n42.0\n20.2\n190.0\n4250.0\nNone\n\n\n\n\nTruncated to displaylimit of 10.\n\n\n\n\nCode\n%%sql\nSELECT \n    COALESCE(species, 'ALL') as species\n    , COALESCE(sex, 'ALL') as sex\n    , SUM(bill_length_mm) as sum_bill_legth_mm\n    , COUNT(*) as total\n    , COUNT(bill_length_mm) as cnt_bill_length_mm\n    , AVG(bill_length_mm) as avg_bill_length_mm\n    , sum_bill_legth_mm / total\n    , sum_bill_legth_mm / cnt_bill_length_mm\nFROM (\n    SELECT  \n    COALESCE(species, 'NA') as species\n    , COALESCE(sex, 'NA') as sex\n    , bill_length_mm\n    FROM read_csv('penguins.csv')\n)\nGROUP BY ROLLUP(species, sex)\nORDER BY species, sex\n\n\nRunning query in 'duckdb'\n\n\n\n\n\nspecies\nsex\nsum_bill_legth_mm\ntotal\ncnt_bill_length_mm\navg_bill_length_mm\n(sum_bill_legth_mm / total)\n(sum_bill_legth_mm / cnt_bill_length_mm)\n\n\n\n\nALL\nALL\n15021.300000000005\n344\n342\n43.92192982456142\n43.66656976744187\n43.92192982456142\n\n\nAdelie\nALL\n5857.500000000003\n152\n151\n38.79139072847684\n38.536184210526336\n38.79139072847684\n\n\nAdelie\nFemale\n2719.7999999999993\n73\n73\n37.25753424657533\n37.25753424657533\n37.25753424657533\n\n\nAdelie\nMale\n2948.4999999999973\n73\n73\n40.39041095890407\n40.39041095890407\n40.39041095890407\n\n\nAdelie\nNA\n189.2\n6\n5\n37.839999999999996\n31.53333333333333\n37.839999999999996\n\n\nChinstrap\nALL\n3320.7000000000003\n68\n68\n48.83382352941177\n48.83382352941177\n48.83382352941177\n\n\nChinstrap\nFemale\n1583.5\n34\n34\n46.5735294117647\n46.5735294117647\n46.5735294117647\n\n\nChinstrap\nMale\n1737.2\n34\n34\n51.09411764705882\n51.09411764705882\n51.09411764705882\n\n\nGentoo\nALL\n5843.0999999999985\n124\n123\n47.504878048780476\n47.121774193548376\n47.504878048780476\n\n\nGentoo\nFemale\n2642.7\n58\n58\n45.563793103448276\n45.563793103448276\n45.563793103448276\n\n\n\n\nTruncated to displaylimit of 10.\n\n\n\n\nCode\n%%sql\n\nselect sex, count(*)\n\nfrom read_csv ('penguins.csv')\n\nwhere species = 'Adelie'\n\ngroup by sex\n\n\n\nRunning query in 'duckdb'\n\n\n\n\n\nsex\ncount_star()\n\n\n\n\nMale\n73\n\n\nNone\n6\n\n\nFemale\n73",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guides_python.html#install-and-load-duckdb-httpsfs-extensions",
    "href": "contents/sql/duckdb/documentaion/guides_python.html#install-and-load-duckdb-httpsfs-extensions",
    "title": "Jupyter Notebooks",
    "section": "3.1 Install and load DuckDB httpsfs Extensions",
    "text": "3.1 Install and load DuckDB httpsfs Extensions\n\n\nCode\n%%sql\n\nINSTALL httpfs;\nLOAD httpfs;\n\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\n\n\npicture 1\n\n\n\n\nCode\n%%sql --save short_trips --no-execute\nSELECT *\nFROM 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet'\nWHERE trip_distance &lt; 6.3\n\n\nSkipping execution...\n\n\n\n\nCode\n%sqlplot histogram --table short_trips --column trip_distance --bins 10 --with short_trips",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guides_python.html#null-handling",
    "href": "contents/sql/duckdb/documentaion/guides_python.html#null-handling",
    "title": "Jupyter Notebooks",
    "section": "4.1 NULL Handling",
    "text": "4.1 NULL Handling\n\n\nCode\nimport duckdb\nfrom duckdb.typing import *\n\ndef dont_intercept_null(x):\n    return 5\n\nduckdb.create_function(\"dont_intercept\", dont_intercept_null, [BIGINT], BIGINT)\nres = duckdb.sql(\"SELECT dont_intercept(NULL)\").fetchall()\nprint(res)\n# [(None,)]\n\nduckdb.remove_function(\"dont_intercept\")\nduckdb.create_function(\"dont_intercept\", dont_intercept_null, [BIGINT], BIGINT, null_handling=\"special\")\nres = duckdb.sql(\"SELECT dont_intercept(NULL)\").fetchall()\nprint(res)\n# [(5,)]\n\n\n[(None,)]\n[(5,)]",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guides_python.html#exception-handling",
    "href": "contents/sql/duckdb/documentaion/guides_python.html#exception-handling",
    "title": "Jupyter Notebooks",
    "section": "4.2 Exception Handling",
    "text": "4.2 Exception Handling\n\n\nCode\nimport duckdb\nfrom duckdb.typing import *\n\ndef will_throw():\n    raise ValueError(\"ERROR\")\n\nduckdb.create_function(\"throws\", will_throw, [], BIGINT)\ntry:\n    res = duckdb.sql(\"SELECT throws()\").fetchall()\nexcept duckdb.InvalidInputException as e:\n    print(e)\n\nduckdb.create_function(\"doesnt_throw\", will_throw, [], BIGINT, exception_handling=\"return_null\")\nres = duckdb.sql(\"SELECT doesnt_throw()\").fetchall()\nprint(res)\n# [(None,)]\n\n\nInvalid Input Error: Python exception occurred while executing the UDF: ValueError: ERROR\n\nAt:\n  C:\\Users\\suzuk\\AppData\\Local\\Temp\\ipykernel_2440\\2917477593.py(5): will_throw\n  C:\\Users\\suzuk\\AppData\\Local\\Temp\\ipykernel_2440\\2917477593.py(9): &lt;module&gt;\n  c:\\pyenv\\py312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3526): run_code\n  c:\\pyenv\\py312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3466): run_ast_nodes\n  c:\\pyenv\\py312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3284): run_cell_async\n  c:\\pyenv\\py312\\Lib\\site-packages\\IPython\\core\\async_helpers.py(129): _pseudo_sync_runner\n  c:\\pyenv\\py312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3079): _run_cell\n  c:\\pyenv\\py312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py(3024): run_cell\n  c:\\pyenv\\py312\\Lib\\site-packages\\ipykernel\\zmqshell.py(546): run_cell\n  c:\\pyenv\\py312\\Lib\\site-packages\\ipykernel\\ipkernel.py(422): do_execute\n  c:\\pyenv\\py312\\Lib\\site-packages\\ipykernel\\kernelbase.py(740): execute_request\n  c:\\pyenv\\py312\\Lib\\site-packages\\ipykernel\\kernelbase.py(412): dispatch_shell\n  c:\\pyenv\\py312\\Lib\\site-packages\\ipykernel\\kernelbase.py(505): process_one\n  c:\\pyenv\\py312\\Lib\\site-packages\\ipykernel\\kernelbase.py(516): dispatch_queue\n  C:\\Program Files\\Python312\\Lib\\asyncio\\events.py(84): _run\n  C:\\Program Files\\Python312\\Lib\\asyncio\\base_events.py(1951): _run_once\n  C:\\Program Files\\Python312\\Lib\\asyncio\\base_events.py(618): run_forever\n  c:\\pyenv\\py312\\Lib\\site-packages\\tornado\\platform\\asyncio.py(195): start\n  c:\\pyenv\\py312\\Lib\\site-packages\\ipykernel\\kernelapp.py(736): start\n  c:\\pyenv\\py312\\Lib\\site-packages\\traitlets\\config\\application.py(1053): launch_instance\n  c:\\pyenv\\py312\\Lib\\site-packages\\ipykernel_launcher.py(17): &lt;module&gt;\n  &lt;frozen runpy&gt;(88): _run_code\n  &lt;frozen runpy&gt;(198): _run_module_as_main\n\n[(None,)]",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guides_python.html#side-effects",
    "href": "contents/sql/duckdb/documentaion/guides_python.html#side-effects",
    "title": "Jupyter Notebooks",
    "section": "4.3 Side Effects",
    "text": "4.3 Side Effects\nグローバルのcountが使われているのがわかる。\n\n\nCode\ndef count() -&gt; int:\n    old = count.counter;\n    count.counter += 1\n    return old\n\ncount.counter = 0\n\n\n\n\nCode\ncon = duckdb.connect()\ncon.create_function(\"my_counter\", count, side_effects = False)\nres = con.sql(\"SELECT my_counter() FROM range(10)\").fetchall()\nprint(res)\n# [(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n\n\n[(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n\n\n\n\nCode\ncon.create_function(\"my_counter3\", count, side_effects = True)\nres = con.sql(\"SELECT my_counter3() FROM range(10)\").fetchall()\nprint(res)\n\n\n[(1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (9,), (10,)]",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Jupyter Notebooks"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap07_.html",
    "href": "contents/sql/duckdb/sql-bigdata/chap07_.html",
    "title": "データ活用の精度を高めるための分析術",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\nCode\n\ncur_dir &lt;- here::here(\"contents/sql/duckdb/sql-bigdata\")\n\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nlibrary(here)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ活用の精度を高めるための分析術"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap07_.html#マスターデータの重複",
    "href": "contents/sql/duckdb/sql-bigdata/chap07_.html#マスターデータの重複",
    "title": "データ活用の精度を高めるための分析術",
    "section": "2.1 マスターデータの重複",
    "text": "2.1 マスターデータの重複\n\n\nCode\nselect\n  count(1) as total_num\n  , count(distinct id) as key_num\nfrom \n  mst_categories\n;\n\n\n\n1 records\n\n\ntotal_num\nkey_num\n\n\n\n\n8\n7\n\n\n\n\n\n\n2.1.1 キーが重複しているレコードを確認する\n\n\nCode\nselect\n  id\n  , count(*) as record_num\n  , string_agg(name, ',') as name_list\n  , string_agg(stamp, ',') as stamp_list\nfrom \n  mst_categories\ngroup by id\nhaving count(*) &gt; 1\n\n\n\n1 records\n\n\nid\nrecord_num\nname_list\nstamp_list\n\n\n\n\n6\n2\nfood,cooking\n2016-01-01 10:00:00,2016-02-01 10:00:00\n\n\n\n\n\nもとのレコードを保持したまま処理する。\n\n\nCode\nwith\nmst_categories_with_key_num as (\n  select\n    *\n    , count(1) over(partition by id) as key_num\n  from mst_categories\n)\nselect \n  * \nfrom\n  mst_categories_with_key_num\nwhere\n  key_num &gt; 1;;\n\n\n\n2 records\n\n\nid\nname\nstamp\nkey_num\n\n\n\n\n6\nfood\n2016-01-01 10:00:00\n2\n\n\n6\ncooking\n2016-02-01 10:00:00\n2",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ活用の精度を高めるための分析術"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap07_.html#ログの重複を検出する",
    "href": "contents/sql/duckdb/sql-bigdata/chap07_.html#ログの重複を検出する",
    "title": "データ活用の精度を高めるための分析術",
    "section": "2.2 ログの重複を検出する",
    "text": "2.2 ログの重複を検出する\nログはマスタと異なり、正常なデータ取得でもデータが重複してしまう倍ｇあある。\n\n2.2.1 サンプルデータ\n\n\nCode\nselect * from dup_action_log;\n\n\n\nDisplaying records 1 - 10\n\n\nstamp\nsession\nuser_id\naction\nproducts\n\n\n\n\n2016-11-03 18:00:00\n989004ea\nU001\nclick\nD001\n\n\n2016-11-03 19:00:00\n47db0370\nU002\nclick\nD002\n\n\n2016-11-03 20:00:00\n1cf7678e\nU003\nclick\nA001\n\n\n2016-11-03 21:00:00\n5eb2e107\nU004\nclick\nA001\n\n\n2016-11-03 21:00:00\nfe05e1d8\nU004\nclick\nD001\n\n\n2016-11-04 18:00:00\n87b5725f\nU001\nclick\nD001\n\n\n2016-11-04 19:00:00\neee2bb21\nU005\nclick\nA001\n\n\n2016-11-04 20:00:00\n5d5b0997\nU006\nclick\nD001\n\n\n2016-11-04 21:00:00\n111f2996\nU007\nclick\nD002\n\n\n2016-11-04 22:00:00\n3efe001c\nU008\nclick\nA001\n\n\n\n\n\n\n\n2.2.2 重複データを各員する\n\n\nCode\nselect\n  user_id\n  , products\n  , string_agg(session, ',') as session_list\n  , string_agg(stamp, ',') as stamp_list\nfrom dup_action_log\ngroup by user_id, products\nhaving count(*) &gt; 1;\n\n\n\n2 records\n\n\n\n\n\n\n\n\nuser_id\nproducts\nsession_list\nstamp_list\n\n\n\n\nU008\nA001\n3efe001c,3efe001c\n2016-11-04 22:00:00,2016-11-04 22:00:10\n\n\nU001\nD001\n989004ea,87b5725f\n2016-11-03 18:00:00,2016-11-04 18:00:00\n\n\n\n\n\n\n\n2.2.3 重複データを削除する\nたとえば１番古いデータを残す。\n\n\nCode\nselect \n  session\n  , user_id\n  , action\n  , products\n  , min(stamp) as stamp\nfrom \n  dup_action_log\ngroup by \n  session, user_id, action, products\n;\n\n\n\nDisplaying records 1 - 10\n\n\nsession\nuser_id\naction\nproducts\nstamp\n\n\n\n\n47db0370\nU002\nclick\nD002\n2016-11-03 19:00:00\n\n\n1cf7678e\nU003\nclick\nA001\n2016-11-03 20:00:00\n\n\nfe05e1d8\nU004\nclick\nD001\n2016-11-03 21:00:00\n\n\neee2bb21\nU005\nclick\nA001\n2016-11-04 19:00:00\n\n\n3efe001c\nU008\nclick\nA001\n2016-11-04 22:00:00\n\n\n989004ea\nU001\nclick\nD001\n2016-11-03 18:00:00\n\n\n87b5725f\nU001\nclick\nD001\n2016-11-04 18:00:00\n\n\n111f2996\nU007\nclick\nD002\n2016-11-04 21:00:00\n\n\n5d5b0997\nU006\nclick\nD001\n2016-11-04 20:00:00\n\n\n5eb2e107\nU004\nclick\nA001\n2016-11-03 21:00:00\n\n\n\n\n\n前回のアクションからの経過時間で同一を判定する。\n\n\nCode\nwith\ndup_action_log_with_lag_seconds as (\n  select\n    user_id\n    , action\n    , products\n    , stamp\n    , extract(\n        epoch from stamp::timestamp - \n        lag(stamp::timestamp)\n        over(partition by user_id, action, products order by stamp)) as lag_seconds\n  from dup_action_log\n)\n-- 30分以内の同一アクションを重複として除外する\nselect\n  user_id\n  , action\n  , products\n  , stamp\nfrom dup_action_log_with_lag_seconds\nwhere (lag_seconds is null or lag_seconds &gt;= 30 * 60)\norder by stamp;\n\n\n\nDisplaying records 1 - 10\n\n\nuser_id\naction\nproducts\nstamp\n\n\n\n\nU001\nclick\nD001\n2016-11-03 18:00:00\n\n\nU002\nclick\nD002\n2016-11-03 19:00:00\n\n\nU003\nclick\nA001\n2016-11-03 20:00:00\n\n\nU004\nclick\nA001\n2016-11-03 21:00:00\n\n\nU004\nclick\nD001\n2016-11-03 21:00:00\n\n\nU001\nclick\nD001\n2016-11-04 18:00:00\n\n\nU005\nclick\nA001\n2016-11-04 19:00:00\n\n\nU006\nclick\nD001\n2016-11-04 20:00:00\n\n\nU007\nclick\nD002\n2016-11-04 21:00:00\n\n\nU008\nclick\nA001\n2016-11-04 22:00:00\n\n\n\n\n\n\n\n2.2.4 変更されたマスタデータをすべて抽出する\nNULLを含めた比較がis distinct fromを使うことで行える。 つまり、どちらかがNULLだと演算が出来ないがわかる。\n\n\nCode\nselect\n    coalesce(new_mst.product_id, old_mst.product_id) as product_id\n  , coalesce(new_mst.name,       old_mst.name) as name\n  , coalesce(new_mst.price,      old_mst.price) as price\n  , coalesce(new_mst.updated_at, old_mst.updated_at) as updated_at\n  , case\n      when old_mst.updated_at is null then 'added'\n      when new_mst.updated_at is null then 'deleted'\n      when new_mst.updated_at &lt;&gt; old_mst.updated_at then 'updated'\n    end as status\nfrom \n  mst_products_20170101 as new_mst\n  full outer join\n    mst_products_20161201 as old_mst\n  on \n    new_mst.product_id = old_mst.product_id\nwhere\n  -- null\n  new_mst.updated_at is distinct from old_mst.updated_at\n\n\n\n3 records\n\n\nproduct_id\nname\nprice\nupdated_at\nstatus\n\n\n\n\nC001\nCCA\n500\n2016-12-04 18:00:00\nupdated\n\n\nB001\nBBB\n500\n2016-11-03 20:00:00\ndeleted\n\n\nD002\nDAD\n500\n2016-12-04 19:00:00\nadded",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ活用の精度を高めるための分析術"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap05_.html",
    "href": "contents/sql/duckdb/sql-bigdata/chap05_.html",
    "title": "ユーザーを把握するためのデータ抽出",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\nCode\n\ncur_dir &lt;- here::here(\"contents/sql/duckdb/sql-bigdata\")\n\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nlibrary(here)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "ユーザーを把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap05_.html#サンプルデータ",
    "href": "contents/sql/duckdb/sql-bigdata/chap05_.html#サンプルデータ",
    "title": "ユーザーを把握するためのデータ抽出",
    "section": "2.1 サンプルデータ",
    "text": "2.1 サンプルデータ\n\n\nCode\nSELECT * FROM mst_users;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\nuser_id\nsex\nbirth_date\nregister_date\nregister_device\nwithdraw_date\n\n\n\n\nU001\nM\n1977-06-17\n2016-10-01\npc\nNA\n\n\nU002\nF\n1953-06-12\n2016-10-01\nsp\n2016-10-10\n\n\nU003\nM\n1965-01-06\n2016-10-01\npc\nNA\n\n\nU004\nF\n1954-05-21\n2016-10-05\npc\nNA\n\n\nU005\nM\n1987-11-23\n2016-10-05\nsp\nNA\n\n\nU006\nF\n1950-01-21\n2016-10-10\npc\n2016-10-10\n\n\nU007\nF\n1950-07-18\n2016-10-10\napp\nNA\n\n\nU008\nF\n2006-12-09\n2016-10-10\nsp\nNA\n\n\nU009\nM\n2004-10-23\n2016-10-15\npc\nNA\n\n\nU010\nF\n1987-03-18\n2016-10-16\npc\nNA\n\n\n\n\n\n\n\nCode\nSELECT * FROM action_log;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\nsession\nuser_id\naction\ncategory\nproducts\namount\nstamp\n\n\n\n\n989004ea\nU001\npurchase\ndrama\nD001,D002\n2000\n2016-10-01 18:10:00\n\n\n989004ea\nU001\nview\nNA\nNA\nNA\n2016-10-01 18:00:00\n\n\n989004ea\nU001\nfavorite\ndrama\nD001\nNA\n2016-10-01 18:00:00\n\n\n989004ea\nU001\nreview\ndrama\nD001\nNA\n2016-10-01 18:00:00\n\n\n989004ea\nU001\nadd_cart\ndrama\nD001\nNA\n2016-10-01 18:00:00\n\n\n989004ea\nU001\nadd_cart\ndrama\nD001\nNA\n2016-10-01 18:00:00\n\n\n989004ea\nU001\nadd_cart\ndrama\nD001\nNA\n2016-10-01 18:00:00\n\n\n989004ea\nU001\nadd_cart\ndrama\nD001\nNA\n2016-10-01 18:00:00\n\n\n989004ea\nU001\nadd_cart\ndrama\nD001\nNA\n2016-10-01 18:00:00\n\n\n989004ea\nU001\nadd_cart\ndrama\nD002\nNA\n2016-10-01 18:01:00",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "ユーザーを把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap05_.html#ユーザーのアクション数を集計",
    "href": "contents/sql/duckdb/sql-bigdata/chap05_.html#ユーザーのアクション数を集計",
    "title": "ユーザーを把握するためのデータ抽出",
    "section": "2.2 ユーザーのアクション数を集計",
    "text": "2.2 ユーザーのアクション数を集計\n\n\nCode\nWITH \nstats as (\n  SELECT COUNT(DISTINCT session) as total_uu\n  FROM action_log\n)\n\nSELECT \n  l.action\n  , count(distinct l.session) as action_uu\n  , count(1) as action_count\n  , s.total_uu\n  , 100. * count(distinct l.session) / s.total_uu as usage_rate\n  , 1. * count(1) / count(distinct l.session) as count_per_user\nFROM \n  action_log as l, \n  stats as s\nGROUP BY\n  l.action, s.total_uu\n;\n\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\naction\naction_uu\naction_count\ntotal_uu\nusage_rate\ncount_per_user\n\n\n\n\nfavorite\n1\n1\n4\n25\n1.000000\n\n\nreview\n1\n1\n4\n25\n1.000000\n\n\nadd_cart\n3\n12\n4\n75\n4.000000\n\n\npurchase\n3\n5\n4\n75\n1.666667\n\n\nview\n1\n1\n4\n25\n1.000000\n\n\n\n\n\n\n2.2.1 ログイン状態を分けて判定する\n\n\nCode\nWITH\naction_log_with_status as (\n  SELECT\n    session\n    , user_id\n    , action\n    , case when coalesce(user_id, '') &lt;&gt; '' THEN 'login' ELSE 'guest' END\n      as login_status\n  FROM action_log\n)\nSELECT * \nFROM action_log_with_status\n;\n\n\n\nDisplaying records 1 - 10\n\n\nsession\nuser_id\naction\nlogin_status\n\n\n\n\n989004ea\nU001\npurchase\nlogin\n\n\n989004ea\nU001\nview\nlogin\n\n\n989004ea\nU001\nfavorite\nlogin\n\n\n989004ea\nU001\nreview\nlogin\n\n\n989004ea\nU001\nadd_cart\nlogin\n\n\n989004ea\nU001\nadd_cart\nlogin\n\n\n989004ea\nU001\nadd_cart\nlogin\n\n\n989004ea\nU001\nadd_cart\nlogin\n\n\n989004ea\nU001\nadd_cart\nlogin\n\n\n989004ea\nU001\nadd_cart\nlogin\n\n\n\n\n\nROLLUPを使って合計・小計を求める。\n\n\nCode\nWITH\naction_log_with_status as (\n  SELECT\n    session\n    , user_id\n    , action\n    , case when coalesce(user_id, '') &lt;&gt; '' THEN 'login' ELSE 'guest' END\n      as login_status\n  FROM action_log\n)\nSELECT\n  coalesce(action, 'all') as action\n  , coalesce(login_status, 'all') as login_status\n  , count(distinct session) as action_uu\n  , count(1) as action_count\nFROM \n  action_log_with_status\nGROUP BY\n  rollup(action, login_status)\n\n\n\nDisplaying records 1 - 10\n\n\naction\nlogin_status\naction_uu\naction_count\n\n\n\n\nreview\nlogin\n1\n1\n\n\nadd_cart\nlogin\n3\n12\n\n\nall\nall\n4\n20\n\n\npurchase\nall\n3\n5\n\n\nfavorite\nall\n1\n1\n\n\nview\nall\n1\n1\n\n\nreview\nall\n1\n1\n\n\nadd_cart\nall\n3\n12\n\n\npurchase\nlogin\n3\n5\n\n\nview\nlogin\n1\n1",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "ユーザーを把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap05_.html#年齢別区分を集計",
    "href": "contents/sql/duckdb/sql-bigdata/chap05_.html#年齢別区分を集計",
    "title": "ユーザーを把握するためのデータ抽出",
    "section": "2.3 年齢別区分を集計",
    "text": "2.3 年齢別区分を集計\n\n\nCode\nWITH\nmst_users_with_int_birth_date as (\n  select \n     *\n    , 20170101 as int_specific_date\n    , cast(replace(substring(birth_date, 1, 10), '-', '') as integer) as int_birth_date\n  from \n    mst_users\n)\n, mst_users_with_age as (\n  select\n    * \n    , floor((int_specific_date - int_birth_date) / 10000) as age\n  from \n    mst_users_with_int_birth_date\n)\nselect \n  user_id, sex, birth_date, age\nfrom \n  mst_users_with_age\n;\n\n\n\nDisplaying records 1 - 10\n\n\nuser_id\nsex\nbirth_date\nage\n\n\n\n\nU001\nM\n1977-06-17\n39\n\n\nU002\nF\n1953-06-12\n63\n\n\nU003\nM\n1965-01-06\n51\n\n\nU004\nF\n1954-05-21\n62\n\n\nU005\nM\n1987-11-23\n29\n\n\nU006\nF\n1950-01-21\n66\n\n\nU007\nF\n1950-07-18\n66\n\n\nU008\nF\n2006-12-09\n10\n\n\nU009\nM\n2004-10-23\n12\n\n\nU010\nF\n1987-03-18\n29\n\n\n\n\n\n現在時刻から年齢を算出する。ageを使えば求められるが、interval型なので秒数に変換されて変えされる。 コマンドラインからだと正しく閲覧できる。\n\n\nCode\nselect\n localtimestamp::date a\n , birth_date::date b\n , datediff('year', birth_date::date, localtimestamp::date) c\n , datediff('year', birth_date::date, '2024-06-17'::date) d\n , age('2024-06-18'::timestamp, birth_date::timestamp) e\nfrom \n  mst_users\n;\n\n\n\nDisplaying records 1 - 10\n\n\na\nb\nc\nd\ne\n\n\n\n\n2024-03-31\n1977-06-17\n47\n47\n1461974400 secs\n\n\n2024-03-31\n1953-06-12\n71\n71\n2208902400 secs\n\n\n2024-03-31\n1965-01-06\n59\n59\n1849132800 secs\n\n\n2024-03-31\n1954-05-21\n70\n70\n2179699200 secs\n\n\n2024-03-31\n1987-11-23\n37\n37\n1137456000 secs\n\n\n2024-03-31\n1950-01-21\n74\n74\n2314483200 secs\n\n\n2024-03-31\n1950-07-18\n74\n74\n2299104000 secs\n\n\n2024-03-31\n2006-12-09\n18\n18\n545097600 secs\n\n\n2024-03-31\n2004-10-23\n20\n20\n611366400 secs\n\n\n2024-03-31\n1987-03-18\n37\n37\n1158624000 secs",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "ユーザーを把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap05_.html#ベン図でユーザーのアクションを集計する",
    "href": "contents/sql/duckdb/sql-bigdata/chap05_.html#ベン図でユーザーのアクションを集計する",
    "title": "ユーザーを把握するためのデータ抽出",
    "section": "2.4 ベン図でユーザーのアクションを集計する",
    "text": "2.4 ベン図でユーザーのアクションを集計する\nユーザーごとに各種機能の利用状況を調べる。\n\n\nCode\nWITH\nuser_action_flag as (\n  select\n    user_id\n    , sign(sum(case when action='purchase' then 1 else 0 end)) as has_purchase\n    , sign(sum(case when action='review' then 1 else 0 end)) as has_review\n    , sign(sum(case when action='favorite' then 1 else 0 end)) as has_favorite\n  from\n    action_log\n  group by\n    user_id\n)\nselect * \nfrom user_action_flag;\n\n\n\n2 records\n\n\nuser_id\nhas_purchase\nhas_review\nhas_favorite\n\n\n\n\nU001\n1\n1\n1\n\n\nU002\n1\n0\n0\n\n\n\n\n\nCUBE句を使うとすべての組合せに対して、集計をおこなうことができる。 パッと見だけどもとからの水準+NULLで組合せを生成しているように見える。 つまり、ROLLUPと機能は似ている。 NULLのところが小計なりなんなりになっている。\n\n\nCode\nwith\nuser_action_flag as (\n  select\n    user_id\n    , sign(sum(case when action='purchase' then 1 else 0 end)) as has_purchase\n    , sign(sum(case when action='review' then 1 else 0 end)) as has_review\n    , sign(sum(case when action='favorite' then 1 else 0 end)) as has_favorite\n  from\n    action_log\n  group by\n    user_id\n), action_venn_diagram as (\n  select\n      has_purchase\n    , has_review\n    , has_favorite\n    , count(1) as users\n  from \n    user_action_flag\n  group by\n    cube(has_purchase, has_review, has_favorite)\n)\n\nselect * \nfrom action_venn_diagram;\n\n\n\nDisplaying records 1 - 10\n\n\nhas_purchase\nhas_review\nhas_favorite\nusers\n\n\n\n\nNA\n1\n1\n1\n\n\nNA\n0\n0\n1\n\n\n1\nNA\nNA\n2\n\n\n1\nNA\n0\n1\n\n\n1\n1\nNA\n1\n\n\n1\n0\nNA\n1\n\n\nNA\nNA\nNA\n2\n\n\n1\n1\n1\n1\n\n\n1\n0\n0\n1\n\n\nNA\nNA\n0\n1",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "ユーザーを把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap05_.html#デシル分析でユーザーを10段階のグループに分ける",
    "href": "contents/sql/duckdb/sql-bigdata/chap05_.html#デシル分析でユーザーを10段階のグループに分ける",
    "title": "ユーザーを把握するためのデータ抽出",
    "section": "2.5 デシル分析でユーザーを10段階のグループに分ける",
    "text": "2.5 デシル分析でユーザーを10段階のグループに分ける\n\n\nCode\nwith\nuser_purchase_amount as (\n  select\n    user_id\n    , sum(amount) as purchase_amount\n  from \n    action_log\n  where\n    action = 'purchase'\n  group by\n    user_id\n), \nusers_with_decile as (\n  select \n    user_id\n    , purchase_amount\n    , ntile(10) over(order by purchase_amount desc) as decile\n  from \n    user_purchase_amount\n)\nselect * \nfrom users_with_decile;\n\n\n\n2 records\n\n\nuser_id\npurchase_amount\ndecile\n\n\n\n\nU001\n4000\n1\n\n\nU002\n3000\n2",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "ユーザーを把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap05_.html#rfm分析",
    "href": "contents/sql/duckdb/sql-bigdata/chap05_.html#rfm分析",
    "title": "ユーザーを把握するためのデータ抽出",
    "section": "2.6 RFM分析",
    "text": "2.6 RFM分析\nRFM分析はユーザーのグループ化の考え方である。 R:最新購入日、F:購入回数、M:購入金額合計の３つである。\n\n\nCode\nwith\npurchase_log as (\n  select\n    user_id\n    , amount\n    , substring(stamp, 1, 10) as dt\n  from \n    action_log\n  where\n    action = 'purchase'\n)\n, user_rfm as (\n  select\n    user_id\n    , max(dt) as recent_date\n    , current_date - max(dt::date) as recency\n    , count(dt) as frequency\n    , sum(amount) as monetary\n  from \n    purchase_log\n  group by\n    user_id\n)\nselect * \nfrom user_rfm;\n\n\n\n2 records\n\n\nuser_id\nrecent_date\nrecency\nfrequency\nmonetary\n\n\n\n\nU001\n2016-11-01\n2707\n2\n4000\n\n\nU002\n2016-12-01\n2677\n3\n3000",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "ユーザーを把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap05_.html#サンプルデータ-1",
    "href": "contents/sql/duckdb/sql-bigdata/chap05_.html#サンプルデータ-1",
    "title": "ユーザーを把握するためのデータ抽出",
    "section": "3.1 サンプルデータ",
    "text": "3.1 サンプルデータ\n\n\nCode\nselect * from mst_users;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\nuser_id\nsex\nbirth_date\nregister_date\nregister_device\nwithdraw_date\n\n\n\n\nU001\nM\n1977-06-17\n2016-10-01\npc\nNA\n\n\nU002\nF\n1953-06-12\n2016-10-01\nsp\n2016-10-10\n\n\nU003\nM\n1965-01-06\n2016-10-01\npc\nNA\n\n\nU004\nF\n1954-05-21\n2016-10-05\npc\nNA\n\n\nU005\nM\n1987-11-23\n2016-10-05\nsp\nNA\n\n\nU006\nF\n1950-01-21\n2016-10-10\npc\n2016-10-10\n\n\nU007\nF\n1950-07-18\n2016-10-10\napp\nNA\n\n\nU008\nF\n2006-12-09\n2016-10-10\nsp\nNA\n\n\nU009\nM\n2004-10-23\n2016-10-15\npc\nNA\n\n\nU010\nF\n1987-03-18\n2016-10-16\npc\nNA",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "ユーザーを把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap05_.html#登録数の推移と傾向",
    "href": "contents/sql/duckdb/sql-bigdata/chap05_.html#登録数の推移と傾向",
    "title": "ユーザーを把握するためのデータ抽出",
    "section": "3.2 登録数の推移と傾向",
    "text": "3.2 登録数の推移と傾向\n\n\nCode\nselect\n  register_date\n  , count(distinct user_id) as register_count\nfrom \n  mst_users\ngroup by \n  register_date\norder by \n  register_date\n;\n\n\n\n5 records\n\n\nregister_date\nregister_count\n\n\n\n\n2016-10-01\n3\n\n\n2016-10-05\n2\n\n\n2016-10-10\n3\n\n\n2016-10-15\n1\n\n\n2016-10-16\n1",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "ユーザーを把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap05_.html#デバイスごとの登録数",
    "href": "contents/sql/duckdb/sql-bigdata/chap05_.html#デバイスごとの登録数",
    "title": "ユーザーを把握するためのデータ抽出",
    "section": "3.3 デバイスごとの登録数",
    "text": "3.3 デバイスごとの登録数\n\n\nCode\nwith\nmst_users_with_year_month as (\n  select \n    *\n  , substring(register_date, 1, 7) as year_month\n  from \n    mst_users\n)\n, stats as (\n  select \n    year_month\n    , register_device\n    , count(*) as cnt\n  from\n    mst_users_with_year_month\n  group by\n    year_month, register_device\n)\nselect *\nfrom (\n  pivot stats\n  on register_device\n  using sum(cnt)\n);\n\n\n\n1 records\n\n\nyear_month\napp\npc\nsp\n\n\n\n\n2016-10\n1\n6\n3",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "ユーザーを把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap05_.html#継続率と定着率を評価する",
    "href": "contents/sql/duckdb/sql-bigdata/chap05_.html#継続率と定着率を評価する",
    "title": "ユーザーを把握するためのデータ抽出",
    "section": "3.4 継続率と定着率を評価する",
    "text": "3.4 継続率と定着率を評価する\nある日に登録した人達がN日後に継続しているか、定着率は継続率のうち７日目の指標である。\nまずユーザーごとに登録日とアクションの日付を出す。\n\n\nCode\nwith \naction_log_with_mst_users as (\n  select\n    u.user_id\n    , u.register_date\n    , cast(a.stamp as date) as action_date\n    , max(cast(a.stamp as date)) over() as latest_date\n    \n    -- 登録日の１日後の日付での計算\n    , cast(u.register_date::date + '1 day'::interval as date)\n      as next_day_1\n  from\n    mst_users as u\n    left outer join action_log as a\n    on u.user_id = a.user_id\n)\nselect * \nfrom action_log_with_mst_users\norder by register_date;\n\n\n\nDisplaying records 1 - 10\n\n\nuser_id\nregister_date\naction_date\nlatest_date\nnext_day_1\n\n\n\n\nU001\n2016-10-01\n2016-10-01\n2016-12-01\n2016-10-02\n\n\nU001\n2016-10-01\n2016-10-01\n2016-12-01\n2016-10-02\n\n\nU001\n2016-10-01\n2016-10-01\n2016-12-01\n2016-10-02\n\n\nU001\n2016-10-01\n2016-10-01\n2016-12-01\n2016-10-02\n\n\nU001\n2016-10-01\n2016-10-01\n2016-12-01\n2016-10-02\n\n\nU001\n2016-10-01\n2016-10-01\n2016-12-01\n2016-10-02\n\n\nU001\n2016-10-01\n2016-10-01\n2016-12-01\n2016-10-02\n\n\nU001\n2016-10-01\n2016-10-01\n2016-12-01\n2016-10-02\n\n\nU001\n2016-10-01\n2016-10-01\n2016-12-01\n2016-10-02\n\n\nU001\n2016-10-01\n2016-10-01\n2016-12-01\n2016-10-02\n\n\n\n\n\n上記を集計する。\n\n\nCode\nwith \naction_log_with_mst_users as (\n  select\n    u.user_id\n    , u.register_date\n    , cast(a.stamp as date) as action_date\n    , max(cast(a.stamp as date)) over() as latest_date\n    \n    -- 登録日の１日後の日付での計算\n    , cast(u.register_date::date + '1 day'::interval as date)\n      as next_day_1\n  from\n    mst_users as u\n    left outer join action_log as a\n    on u.user_id = a.user_id\n)\n, user_action_flag as (\n  select\n    user_id\n    , register_date\n    , sign(\n      sum(\n        case when next_day_1 &lt;= latest_date then \n          case when next_day_1 = action_date then 1 \n          else 0 \n          end\n        end\n      )\n    ) as next_1_day_action\n  from \n    action_log_with_mst_users\n  group by\n    user_id, register_date\n)\nselect\n  register_date\n  , avg(100. * next_1_day_action) as repeat_rate_1_day\nfrom \n  user_action_flag\ngroup by \n  register_date\norder by\n  register_date\n;\n\n\n\n5 records\n\n\nregister_date\nrepeat_rate_1_day\n\n\n\n\n2016-10-01\n0\n\n\n2016-10-05\n0\n\n\n2016-10-10\n0\n\n\n2016-10-15\n0\n\n\n2016-10-16\n0",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "ユーザーを把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html",
    "title": "データ加工のためのSQL",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\nCode\n\ncur_dir &lt;- here::here(\"contents/sql/duckdb/sql-bigdata\")\n\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nlibrary(here)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#コード値をラベルに置き換える",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#コード値をラベルに置き換える",
    "title": "データ加工のためのSQL",
    "section": "2.1 コード値をラベルに置き換える",
    "text": "2.1 コード値をラベルに置き換える\n\n\nCode\nSELECT\n  user_id\n  , CASE \n      WHEN register_device = 1 THEN 'PC'\n      WHEN register_device = 2 THEN 'SP'\n      WHEN register_device = 3 THEN 'アプリ'\n    END device_name\nFROM mst_users;\n\n\n\n3 records\n\n\nuser_id\ndevice_name\n\n\n\n\nU001\nPC\n\n\nU002\nSP\n\n\nU003\nアプリ",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#urlから要素を取り出す",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#urlから要素を取り出す",
    "title": "データ加工のためのSQL",
    "section": "2.2 URLから要素を取り出す",
    "text": "2.2 URLから要素を取り出す\n\n\nCode\nSELECT\n  *\nFROM access_log;\n\n\n\n3 records\n\n\n\n\n\n\n\nstamp\nreferrer\nurl\n\n\n\n\n2016-08-26 12:02:00\nhttp://www.other.com/path1/index.php?k1=v1&k2=v2#Ref1\nhttp://www.example.com/video/detail?id=001\n\n\n2016-08-26 12:02:01\nhttp://www.other.net/path1/index.php?k1=v1&k2=v2#Ref1\nhttp://www.example.com/video#ref\n\n\n2016-08-26 12:02:01\nhttps://www.other.com/\nhttp://www.example.com/book/detail?id=002\n\n\n\n\n\n正規表現を使うことが出来る。\n\n\nCode\nSELECT\n  stamp\n  , regexp_extract(referrer , 'https?://([^/]*)') referrer_host\nFROM access_log;\n\n\n\n3 records\n\n\nstamp\nreferrer_host\n\n\n\n\n2016-08-26 12:02:00\nhttp://www.other.com\n\n\n2016-08-26 12:02:01\nhttp://www.other.net\n\n\n2016-08-26 12:02:01\nhttps://www.other.com\n\n\n\n\n\n\n2.2.1 URLからパスやクエリパラメータを取り出す\nインデックスで取り出す必要がある。\n\n\nCode\nSELECT \n  stamp\n  , url\n  , regexp_extract(url, '//[^/]+(?P&lt;name&gt;[^?#]+)', 1) path\n  , regexp_extract(url, 'id=([^&]*)', 1) id\nFROM access_log;\n\n\n\n3 records\n\n\n\n\n\n\n\n\nstamp\nurl\npath\nid\n\n\n\n\n2016-08-26 12:02:00\nhttp://www.example.com/video/detail?id=001\n/video/detail\n001\n\n\n2016-08-26 12:02:01\nhttp://www.example.com/video#ref\n/video\n\n\n\n2016-08-26 12:02:01\nhttp://www.example.com/book/detail?id=002\n/book/detail\n002",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#文字列を配列に分解する",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#文字列を配列に分解する",
    "title": "データ加工のためのSQL",
    "section": "2.3 文字列を配列に分解する",
    "text": "2.3 文字列を配列に分解する\nregex_extractでホストと以下の部分を取得して, split_partで分割をおこなう。\n\n\nCode\nSELECT \n  stamp\n  , url\n  , regexp_extract(url, '//[^/]+([^?#]+)', 1) as A\n  , split_part(regexp_extract(url, '//[^/]+([^?#]+)', 1), '/', 2) path1\n  , split_part(regexp_extract(url, '//[^/]+([^?#]+)', 1), '/', 3) path2\nFROM \n  access_log;\n\n\n\n3 records\n\n\n\n\n\n\n\n\n\nstamp\nurl\nA\npath1\npath2\n\n\n\n\n2016-08-26 12:02:00\nhttp://www.example.com/video/detail?id=001\n/video/detail\nvideo\ndetail\n\n\n2016-08-26 12:02:01\nhttp://www.example.com/video#ref\n/video\nvideo\n\n\n\n2016-08-26 12:02:01\nhttp://www.example.com/book/detail?id=002\n/book/detail\nbook\ndetail\n\n\n\n\n\n非効率に見えるので次でもいいと思う。\n\n\nCode\nWITH path_extracted AS (\n  SELECT\n    stamp,\n    url,\n    regexp_extract(url, '//[^/]+([^?#]+)', 1) AS extracted_path\n  FROM\n    access_log\n)\nSELECT\n  stamp,\n  url,\n  extracted_path AS A,\n  split_part(extracted_path, '/', 2) AS path1,\n  split_part(extracted_path, '/', 3) AS path2\nFROM\n  path_extracted;\n\n\n\n3 records\n\n\n\n\n\n\n\n\n\nstamp\nurl\nA\npath1\npath2\n\n\n\n\n2016-08-26 12:02:00\nhttp://www.example.com/video/detail?id=001\n/video/detail\nvideo\ndetail\n\n\n2016-08-26 12:02:01\nhttp://www.example.com/video#ref\n/video\nvideo\n\n\n\n2016-08-26 12:02:01\nhttp://www.example.com/book/detail?id=002\n/book/detail\nbook\ndetail",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#日付やタイムスタンプを扱う",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#日付やタイムスタンプを扱う",
    "title": "データ加工のためのSQL",
    "section": "2.4 日付やタイムスタンプを扱う",
    "text": "2.4 日付やタイムスタンプを扱う\nINSTALL icu;\nLOAD icu;\nタイムゾーンの設定はAisa/TokyoだけどTodayなどを使うとUTCの時間が取得される。 現地の時間が欲しければローカルタイムを使うこと。\n\n\nCode\nSELECT * FROM duckdb_settings() WHERE name = 'TimeZone';\n\n\n\n1 records\n\n\nname\nvalue\ndescription\ninput_type\n\n\n\n\nTimeZone\nAsia/Tokyo\nThe current time zone\nVARCHAR\n\n\n\n\n\nタイムゾーンの変更をおこなう。\n\n\nCode\nSET TimeZone='Asia/Tokyo';\n\n\n\n\nCode\nSELECT \n  CURRENT_DATE as dt\n  , CURRENT_TIMESTAMP as stamp\n  , LOCALTIMESTAMP as lstamp\n  , now() as now\n  , today() as tdy\n;\n\n\n\n1 records\n\n\n\n\n\n\n\n\n\ndt\nstamp\nlstamp\nnow\ntdy\n\n\n\n\n2024-03-31\n2024-03-31 06:33:39\n2024-03-31 15:33:39\n2024-03-31 06:33:39\n2024-03-31\n\n\n\n\n\n\n2.4.1 文字列の日付や時刻データを変換する\nCASTのなかのAsは必要なことに注意する。\n\n\nCode\nSELECT\n    -- ■ PostgreSQL, Hive, Redshift, BigQuery, SparkSQLのすべてで、\n    --   「CAST(value AS type)」の形式が利用できる\n    CAST('2016-01-30' AS date) AS dt\n  , CAST('2016-01-30 12:00:00' AS timestamp) AS stamp;\n\n\n\n1 records\n\n\ndt\nstamp\n\n\n\n\n2016-01-30\n2016-01-30 12:00:00\n\n\n\n\n\n\n\nCode\nWITH tmp as (SELECT\n    -- ■ PostgreSQL, Hive, Redshift, BigQuery, SparkSQLのすべてで、\n    --   「CAST(value AS type)」の形式が利用できる\n    strptime('2016-01-30', '%Y-%m-%d') AS dt\n  , strptime('2016-01-30 12:00:00', '%Y-%m-%d %H:%M:%S') AS stamp\n) \nSELECT *\nFROM tmp;\n\n\n\n1 records\n\n\ndt\nstamp\n\n\n\n\n2016-01-30\n2016-01-30 12:00:00\n\n\n\n\n\n\n\nCode\nWITH tmp as (SELECT\n    -- ■ PostgreSQL, Hive, Redshift, BigQuery, SparkSQLのすべてで、\n    --   「CAST(value AS type)」の形式が利用できる\n    '2016-01-30'::date AS dt\n  , '2016-01-30 12:00:00'::timestamp AS stamp\n) \nSELECT *\nFROM tmp;\n\n\n\n1 records\n\n\ndt\nstamp\n\n\n\n\n2016-01-30\n2016-01-30 12:00:00\n\n\n\n\n\n\n\n2.4.2 日付/時刻から特定のフィールドを取り出す\n\n\nCode\nSELECT\n  stamp\n  , EXTRACT(YEAR  FROM stamp) AS year\n  , EXTRACT(MONTH FROM stamp) AS month\n  , EXTRACT(DAY   FROM stamp) AS day\n  , EXTRACT(HOUR  FROM stamp) AS hour\nFROM  (SELECT CAST('2016-01-30 12:00:00' AS timestamp) as stamp) AS t ;\n\n\n\n1 records\n\n\nstamp\nyear\nmonth\nday\nhour\n\n\n\n\n2016-01-30 12:00:00\n2016\n1\n30\n12",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#欠損値をデフォルト値に置き換える",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#欠損値をデフォルト値に置き換える",
    "title": "データ加工のためのSQL",
    "section": "2.5 欠損値をデフォルト値に置き換える",
    "text": "2.5 欠損値をデフォルト値に置き換える\n\n\nCode\nSELECT\n  purchase_id\n  , amount\n  , coupon\n  , amount - coupon discount_amount1\n  , amount - COALESCE(coupon, 0) discount_amount2\nFROM \n  purchase_log_with_coupon\n;\n\n\n\n3 records\n\n\npurchase_id\namount\ncoupon\ndiscount_amount1\ndiscount_amount2\n\n\n\n\n10001\n3280\nNA\nNA\n3280\n\n\n10002\n4650\n500\n4150\n4150\n\n\n10003\n3870\nNA\nNA\n3870",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#文字列を連結する",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#文字列を連結する",
    "title": "データ加工のためのSQL",
    "section": "3.1 文字列を連結する",
    "text": "3.1 文字列を連結する\n\n\nCode\nSELECT\n  user_id\n  , CONCAT(pref_name, city_name) pref_city\n  , pref_name || city_name pref_city2\n  , CONCAT_WS('/', pref_name, city_name) pref_city3\nFROM\n  mst_user_location\n;\n\n\n\n3 records\n\n\nuser_id\npref_city\npref_city2\npref_city3\n\n\n\n\nU001\n東京都千代田区\n東京都千代田区\n東京都/千代田区\n\n\nU002\n東京都渋谷区\n東京都渋谷区\n東京都/渋谷区\n\n\nU003\n千葉県八千代区\n千葉県八千代区\n千葉県/八千代区",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#複数の値を比較する",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#複数の値を比較する",
    "title": "データ加工のためのSQL",
    "section": "3.2 複数の値を比較する",
    "text": "3.2 複数の値を比較する\n横持ちのデータを比較することを考える。\n\n\nCode\nSELECT\n  year\n  , q1\n  , q2\n  , CASE\n      WHEN q1 &lt; q2 THEN '+'\n      WHEN q1 = q2 THEN ' '\n      ELSE '-'\n    END judge_q1_q2\n  , q2-q1 diff_q2_q1\n  , SIGN(q2-q1) sign_q2_q1\nFROM\n  quarterly_sales\nORDER BY\n  year\n;\n\n\n\n3 records\n\n\nyear\nq1\nq2\njudge_q1_q2\ndiff_q2_q1\nsign_q2_q1\n\n\n\n\n2015\n82000\n83000\n+\n1000\n1\n\n\n2016\n85000\n85000\n\n0\n0\n\n\n2017\n92000\n81000\n-\n-11000\n-1\n\n\n\n\n\n\n3.2.1 年間の最大値をを見つける\n\n\nCode\nSELECT\n  year\n  , greatest(q1, q2, q3, q4) greatest_sales\n  , least(q1, q2, q3, q4) least_sales\nFROM\n  quarterly_sales\nORDER BY\n  year\n;\n\n\n\n3 records\n\n\nyear\ngreatest_sales\nleast_sales\n\n\n\n\n2015\n83000\n78000\n\n\n2016\n85000\n80000\n\n\n2017\n92000\n81000\n\n\n\n\n\n\n\n3.2.2 年間の平均四半期売上を計算する\n\n\nCode\nSELECT\n  year\n  , (q1 + q2 + q3 + q4) / 4 average\nFROM\n  quarterly_sales\nORDER BY\n  year\n;\n\n\n\n3 records\n\n\nyear\naverage\n\n\n\n\n2015\n81500\n\n\n2016\n82750\n\n\n2017\nNA\n\n\n\n\n\nこれはだるいのだが・・・？縦持ちに変換できれば出来そうだけど。\n次は行ごとにNULLでないカラムでの平均値を求める。\n\n\nCode\nSELECT\n  year\n  , (COALESCE(q1, 0) + COALESCE(q2, 0) + COALESCE(q3, 0) + COALESCE(q4, 0)) \n    / (SIGN(COALESCE(q1, 0)) + SIGN(COALESCE(q2, 0)) + SIGN(COALESCE(q3, 0)) + SIGN(COALESCE(q4, 0))) average\nFROM\n  quarterly_sales\nORDER BY\n  year\n;\n\n\n\n3 records\n\n\nyear\naverage\n\n\n\n\n2015\n81500\n\n\n2016\n82750\n\n\n2017\n86500\n\n\n\n\n\nduckdbだとPIVOTとUNPIVOTがあるので簡単におこなうことができる。\n\n\nCode\nUNPIVOT quarterly_sales\nON q1, q2, q3, q4\nINTO \n  NAME quarter\n  VALUE sales;\n\n\n\nDisplaying records 1 - 10\n\n\nyear\nquarter\nsales\n\n\n\n\n2015\nq1\n82000\n\n\n2015\nq2\n83000\n\n\n2015\nq3\n78000\n\n\n2015\nq4\n83000\n\n\n2016\nq1\n85000\n\n\n2016\nq2\n85000\n\n\n2016\nq3\n80000\n\n\n2016\nq4\n81000\n\n\n2017\nq1\n92000\n\n\n2017\nq2\n81000\n\n\n\n\n\n正規表現とかを使うことも可能である。\n\n\nCode\nSELECT \n  COLUMNS(*)\nFROM (\n  UNPIVOT quarterly_sales\n  ON COLUMNS('^q\\d')\n  INTO \n    NAME quarter\n    VALUE sales\n);\n\n\n\nDisplaying records 1 - 10\n\n\nyear\nquarter\nsales\n\n\n\n\n2015\nq1\n82000\n\n\n2015\nq2\n83000\n\n\n2015\nq3\n78000\n\n\n2015\nq4\n83000\n\n\n2016\nq1\n85000\n\n\n2016\nq2\n85000\n\n\n2016\nq3\n80000\n\n\n2016\nq4\n81000\n\n\n2017\nq1\n92000\n\n\n2017\nq2\n81000",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#つの値の比率を計算する",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#つの値の比率を計算する",
    "title": "データ加工のためのSQL",
    "section": "3.3 2つの値の比率を計算する",
    "text": "3.3 2つの値の比率を計算する\n\n\nCode\nSELECT * FROM advertising_stats;\n\n\n\n6 records\n\n\ndt\nad_id\nimpressions\nclicks\n\n\n\n\n2017-04-01\n001\n100000\n3000\n\n\n2017-04-01\n002\n120000\n1200\n\n\n2017-04-01\n003\n500000\n10000\n\n\n2017-04-02\n001\n0\n0\n\n\n2017-04-02\n002\n130000\n1400\n\n\n2017-04-02\n003\n620000\n15000\n\n\n\n\n\n\n3.3.1 整数型のデータの除算\nCTR(Click Through Rate)を計算する。CTRの定義は「クリック数 / インプレッション数」である。 整数型同士の乗算は事前に実数型に変換しておく必要がある。\n\n\nCode\nSELECT\n  dt\n  , ad_id\n  , clicks / impressions ctr\n  , 100. * clicks / impressions ctr_as_percent\n  , 100  * clicks / impressions ctr_as_percent2\nFROM\n  advertising_stats\nWHERE\n  dt = '2017-04-01'\nORDER BY\n  dt, ad_id\n;\n\n\n\n3 records\n\n\ndt\nad_id\nctr\nctr_as_percent\nctr_as_percent2\n\n\n\n\n2017-04-01\n001\n0.03\n3\n3\n\n\n2017-04-01\n002\n0.01\n1\n1\n\n\n2017-04-01\n003\n0.02\n2\n2\n\n\n\n\n\n\n\n3.3.2 0除算を避ける\n0除算を回避するにはCASE分を用いて分母の数が０かどうかを判定すること、 または、NULLを伝搬させることがある。NULLIF(x, default)は0のときに置き換える演算である。\n\n\nCode\nSELECT\n  dt\n  , ad_id\n  , CASE\n      WHEN impressions &gt; 0 THEN 100. * clicks / impressions\n      END ctr_as_percent_by_case\n  , 100. * clicks / NULLIF(impressions, 0) ctr_as_percent_by_null\nFROM \n  advertising_stats\nORDER BY\n  dt, ad_id\n;\n\n\n\n6 records\n\n\ndt\nad_id\nctr_as_percent_by_case\nctr_as_percent_by_null\n\n\n\n\n2017-04-01\n001\n3.000000\n3.000000\n\n\n2017-04-01\n002\n1.000000\n1.000000\n\n\n2017-04-01\n003\n2.000000\n2.000000\n\n\n2017-04-02\n001\nNA\nNA\n\n\n2017-04-02\n002\n1.076923\n1.076923\n\n\n2017-04-02\n003\n2.419355\n2.419355",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#つの値の距離を計算する",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#つの値の距離を計算する",
    "title": "データ加工のためのSQL",
    "section": "3.4 2つの値の距離を計算する",
    "text": "3.4 2つの値の距離を計算する\n\n3.4.1 RMS\n\n\nCode\nSELECT\n  abs(x1-x2) abs\n  , sqrt(power(x1-x2, 2)) rms\nFROM location_1d;\n\n\n\n5 records\n\n\nabs\nrms\n\n\n\n\n5\n5\n\n\n5\n5\n\n\n6\n6\n\n\n0\n0\n\n\n1\n1\n\n\n\n\n\n\n\nCode\nSELECT\n  x1\n  , AVG(x1) OVER()\n  , x1 - AVG(x1) over()\n  , POWER(x1 - AVG(x1) over(), 2)\nFROM location_1d;\n\n\n\n5 records\n\n\n\n\n\n\n\n\nx1\navg(x1) OVER ()\n(x1 - avg(x1) OVER ())\npower((x1 - avg(x1) OVER ()), 2)\n\n\n\n\n5\n3.2\n1.8\n3.24\n\n\n10\n3.2\n6.8\n46.24\n\n\n-2\n3.2\n-5.2\n27.04\n\n\n3\n3.2\n-0.2\n0.04\n\n\n0\n3.2\n-3.2\n10.24\n\n\n\n\n\n\n\n3.4.2 xy平面上で2点間のユークリッド距離を計算する\nなし\n\n\n3.4.3 arrayやlistを持ちいた距離\nDBI経由だと計算できないが、コマンドラインからだと直接演算することが可能である。 どうやらarrayは0.10から追加されて、その後に\n#| connection: con\n\nDROP TABLE IF EXISTS x;\nDROP TABLE IF EXISTS y;\nCREATE TABLE x (i INT, v FLOAT[3]);\nCREATE TABLE y (i INT, v FLOAT[3]);\nINSERT INTO x VALUES (1, array_value(1.0::FLOAT, 2.0::FLOAT, 3.0::FLOAT));\nINSERT INTO y VALUES (1, array_value(2.0::FLOAT, 3.0::FLOAT, 4.0::FLOAT));\n-- compute cross product\nSELECT array_cross_product(x.v, y.v)\nFROM x, y\nWHERE x.i = y.i;\nリストは使えるのでリスで計算すればいいのかもしれない？？？\n\n\nCode\nselect list_value(1,2,3)\n\n\n\n1 records\n\n\nlist_value(1, 2, 3)\n\n\n\n\n1, 2, 3",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#日付時刻を計算する",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#日付時刻を計算する",
    "title": "データ加工のためのSQL",
    "section": "3.5 日付・時刻を計算する",
    "text": "3.5 日付・時刻を計算する\n2つの日付・時刻データの差分を計算したり、１時間後の時刻を計算するなど、の手法をみる。\n\n\nCode\nSELECT\n  *\nFROM mst_users_with_birthday;\n\n\n\n3 records\n\n\nuser_id\nregister_stamp\nbirth_date\n\n\n\n\nU001\n2016-02-28 10:00:00\n2000-02-29\n\n\nU002\n2016-02-29 10:00:00\n2000-02-29\n\n\nU003\n2016-03-01 10:00:00\n2000-02-29\n\n\n\n\n\n\n3.5.1 日付・時刻データと定数の足し算・引き算\n\n\nCode\nSELECT\n  user_id\n  , register_stamp::timestamp register_stamp\n  , register_stamp::timestamp + '1 hour'::interval after_1_hour\n  , register_stamp::timestamp - '30 minutes'::interval before_30_minutes\n  , register_stamp::date register_date\n  , (register_stamp::date + '1 day'::interval)::date AS after_1_day\n  , (register_stamp::date - '1 month'::interval)::date AS before_1_month\n  , date_add(register_stamp::date, '1 week'::interval) as after_1_week\n  , date_trunc('month', register_stamp::date) as trunc_month\nFROM mst_users_with_birthday;\n\n\n\n3 records\n\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\nregister_stamp\nafter_1_hour\nbefore_30_minutes\nregister_date\nafter_1_day\nbefore_1_month\nafter_1_week\ntrunc_month\n\n\n\n\nU001\n2016-02-28 10:00:00\n2016-02-28 11:00:00\n2016-02-28 09:30:00\n2016-02-28\n2016-02-29\n2016-01-28\n2016-03-06\n2016-02-01\n\n\nU002\n2016-02-29 10:00:00\n2016-02-29 11:00:00\n2016-02-29 09:30:00\n2016-02-29\n2016-03-01\n2016-01-29\n2016-03-07\n2016-02-01\n\n\nU003\n2016-03-01 10:00:00\n2016-03-01 11:00:00\n2016-03-01 09:30:00\n2016-03-01\n2016-03-02\n2016-02-01\n2016-03-08\n2016-03-01\n\n\n\n\n\n\n\n3.5.2 日付データ同士の差分を計算する\n\n\nCode\nSELECT\n  user_id\n  , CURRENT_DATE today\n  , register_stamp::date register_date\n  , CURRENT_DATE - register_stamp::date diff_days\nFROM mst_users_with_birthday;\n\n\n\n3 records\n\n\nuser_id\ntoday\nregister_date\ndiff_days\n\n\n\n\nU001\n2024-03-31\n2016-02-28\n2954\n\n\nU002\n2024-03-31\n2016-02-29\n2953\n\n\nU003\n2024-03-31\n2016-03-01\n2952\n\n\n\n\n\n\n\n3.5.3 ユーザーの生年月日から年齢を計算する\n\n\nCode\nSELECT\n  user_id\n  , CURRENT_DATE today\n  , register_stamp::date register_date\n  , birth_date::date birth_date\n  , age(birth_date::date) age\n  , age(register_stamp::date, birth_date::date) age2\n  , EXTRACT(YEAR FROM age(birth_date::date)) current_age\n  , EXTRACT(YEAR FROM age(register_stamp::date, birth_date::date)) register_age\nFROM mst_users_with_birthday;\n\n\n\n3 records\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\ntoday\nregister_date\nbirth_date\nage\nage2\ncurrent_age\nregister_age\n\n\n\n\nU001\n2024-03-31\n2016-02-28\n2000-02-29\n749284420 secs\n497491200 secs\n24\n15\n\n\nU002\n2024-03-31\n2016-02-29\n2000-02-29\n749284420 secs\n497664000 secs\n24\n16\n\n\nU003\n2024-03-31\n2016-03-01\n2000-02-29\n749284420 secs\n497750400 secs\n24\n16",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#ipアドレスを扱う",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#ipアドレスを扱う",
    "title": "データ加工のためのSQL",
    "section": "3.6 IPアドレスを扱う",
    "text": "3.6 IPアドレスを扱う\nduckdbにはinet型はないみたいである。\n#| connection: con\nSELECT\n  CAST('172.16.4.46' as inet) &lt; cast('172.17.49.127' as inet ) as lt;\n\n3.6.1 整数値や文字列としてIPアドレスを扱う\nBIT型が存在しているらしいがここでは使うことができなかった。ｓｓ\n\n\nCode\nSELECT\n  ip\n  , CAST(split_part(ip, '.',  1) AS integer) part_1\n  , CAST(split_part(ip, '.',  2) AS integer) part_2\n  , CAST(split_part(ip, '.',  3) AS integer) part_3\n  , CAST(split_part(ip, '.',  4) AS integer) part_4\n  \n  -- ０パディング\n  , lpad(split_part(ip, '.',  1), 3, '0') part_1\n  , lpad(split_part(ip, '.',  2), 3, '0') part_2\n  , lpad(split_part(ip, '.',  3), 3, '0') part_3\n  , lpad(split_part(ip, '.',  4), 3, '0') part_4\n  \n  -- , CAST(split_part(ip, '.',  1) AS integer) bit_part_1\n  -- , CAST(split_part(ip, '.',  2) AS integer)::BIT bit_part_2\n  -- , CAST(split_part(ip, '.',  3) AS integer)::BIT bit_part_3\n  -- , CAST(split_part(ip, '.',  4) AS integer)::BIT bit_part_4\nFROM (\n  SELECT '192.168.0.1' AS ip\n) as t\n\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\nip\npart_1\npart_2\npart_3\npart_4\npart_1\npart_2\npart_3\npart_4\n\n\n\n\n192.168.0.1\n192\n168\n0\n1\n192\n168\n000\n001",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#グループの特徴を捉える",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#グループの特徴を捉える",
    "title": "データ加工のためのSQL",
    "section": "4.1 グループの特徴を捉える。",
    "text": "4.1 グループの特徴を捉える。\n\n\nCode\nSELECT * FROM review;\n\n\n\n9 records\n\n\nuser_id\nproduct_id\nscore\n\n\n\n\nU001\nA001\n4\n\n\nU001\nA002\n5\n\n\nU001\nA003\n5\n\n\nU002\nA001\n3\n\n\nU002\nA002\n3\n\n\nU002\nA003\n4\n\n\nU003\nA001\n5\n\n\nU003\nA002\n4\n\n\nU003\nA003\n4\n\n\n\n\n\n\n4.1.1 テーブル全体の特徴量\n\n\nCode\nSELECT\n  COUNT(*) total_count\n  , COUNT(DISTINCT user_id) user_count\n  , COUNT(DISTINCT product_id) product_count\n  , SUM(score) sum\n  , AVG(score) avg\n  , MIN(score) min\n  , MAX(score) max\n  , LN(SUM(EXP(score))) log_sum_exp\nFROM \n  review\n;\n\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\ntotal_count\nuser_count\nproduct_count\nsum\navg\nmin\nmax\nlog_sum_exp\n\n\n\n\n9\n3\n3\n37\n4.111111\n3\n5\n6.556499\n\n\n\n\n\n\n\n4.1.2 グルーピングしたテーブル全体の特徴量\n\n\nCode\nSELECT\n  COUNT(*) total_count\n  , COUNT(DISTINCT user_id) user_count\n  , COUNT(DISTINCT product_id) product_count\n  , SUM(score) sum\n  , AVG(score) avg\n  , MIN(score) min\n  , MAX(score) max\n  , LN(SUM(EXP(score))) log_sum_exp\nFROM \n  review\nGROUP BY\n  user_id\n;\n\n\n\n3 records\n\n\n\n\n\n\n\n\n\n\n\n\ntotal_count\nuser_count\nproduct_count\nsum\navg\nmin\nmax\nlog_sum_exp\n\n\n\n\n3\n1\n3\n10\n3.333333\n3\n4\n4.551445\n\n\n3\n1\n3\n13\n4.333333\n4\n5\n5.551445\n\n\n3\n1\n3\n14\n4.666667\n4\n5\n5.861995\n\n\n\n\n\n\n\n4.1.3 集約関数を適用した値と集約前の値を同時に扱う\n\n\nCode\nSELECT\n  user_id\n  , product_id\n  , score\n  , AVG(score) OVER() avg_score\n  , AVG(score) OVER(PARTITION BY user_id) user_avg_score\n  , score - AVG(score) OVER(PARTITION BY user_id) user_avg_score_diff\nFROM \n  review\n;\n\n\n\n9 records\n\n\n\n\n\n\n\n\n\n\nuser_id\nproduct_id\nscore\navg_score\nuser_avg_score\nuser_avg_score_diff\n\n\n\n\nU002\nA001\n3\n4.111111\n3.333333\n-0.3333333\n\n\nU002\nA002\n3\n4.111111\n3.333333\n-0.3333333\n\n\nU002\nA003\n4\n4.111111\n3.333333\n0.6666667\n\n\nU003\nA001\n5\n4.111111\n4.333333\n0.6666667\n\n\nU003\nA002\n4\n4.111111\n4.333333\n-0.3333333\n\n\nU003\nA003\n4\n4.111111\n4.333333\n-0.3333333\n\n\nU001\nA001\n4\n4.111111\n4.666667\n-0.6666667\n\n\nU001\nA002\n5\n4.111111\n4.666667\n0.3333333\n\n\nU001\nA003\n5\n4.111111\n4.666667\n0.3333333",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#グループの中での順序を扱う",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#グループの中での順序を扱う",
    "title": "データ加工のためのSQL",
    "section": "4.2 グループの中での順序を扱う",
    "text": "4.2 グループの中での順序を扱う\n\n\nCode\nSELECT * FROM popular_products;\n\n\n\n8 records\n\n\nproduct_id\ncategory\nscore\n\n\n\n\nA001\naction\n94\n\n\nA002\naction\n81\n\n\nA003\naction\n78\n\n\nA004\naction\n64\n\n\nD001\ndrama\n90\n\n\nD002\ndrama\n82\n\n\nD003\ndrama\n78\n\n\nD004\ndrama\n58\n\n\n\n\n\n\n4.2.1 ORDER BYを使う\nウインドウ関数のなかで「ORDER BY」を使う。\n\n\nCode\nSELECT\n  product_id\n  \n  , score\n  \n  , ROW_NUMBER() OVER(ORDER BY score DESC) as row\n  \n  , RANK() OVER(ORDER BY score DESC) as rank\n  \n  , DENSE_RANK() OVER(ORDER BY score DESC) dense_rank\n  \n  , LAG(product_id) OVER(ORDER BY score DESC) lag1\n  \n  , LAG(product_id, 2) OVER(ORDER BY score DESC) lag2\n  \n  , LEAD(product_id) OVER(ORDER BY score DESC) lead1\n  \n  , LEAD(product_id, 2) OVER(ORDER BY score DESC) lead2\n\nFROM popular_products\n\nORDER BY row\n\n;\n\n\n\n8 records\n\n\nproduct_id\nscore\nrow\nrank\ndense_rank\nlag1\nlag2\nlead1\nlead2\n\n\n\n\nA001\n94\n1\n1\n1\nNA\nNA\nD001\nD002\n\n\nD001\n90\n2\n2\n2\nA001\nNA\nD002\nA002\n\n\nD002\n82\n3\n3\n3\nD001\nA001\nA002\nA003\n\n\nA002\n81\n4\n4\n4\nD002\nD001\nA003\nD003\n\n\nA003\n78\n5\n5\n5\nA002\nD002\nD003\nA004\n\n\nD003\n78\n6\n5\n5\nA003\nA002\nA004\nD004\n\n\nA004\n64\n7\n7\n6\nD003\nA003\nD004\nNA\n\n\nD004\n58\n8\n8\n7\nA004\nD003\nNA\nNA\n\n\n\n\n\n\n\n4.2.2 ORDER BYと集約組み合わせる\n\n\nCode\nSELECT\n  product_id\n  \n  , score\n  \n  , ROW_NUMBER() OVER(ORDER BY score DESC) as row\n  \n  , SUM(score) OVER(ORDER BY score DESC\n                    ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) cum_score\n                    \n  , AVG(score) OVER(ORDER BY score DESC \n                    ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) local_avg\n                    \n  -- ランキング上位の商品IDを取得する\n  , FIRST_VALUE(product_id) OVER(ORDER BY score DESC\n                                 ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as first_value\nFROM popular_products\nORDER BY row;\n\n\n\n8 records\n\n\nproduct_id\nscore\nrow\ncum_score\nlocal_avg\nfirst_value\n\n\n\n\nA001\n94\n1\n94\n92.00000\nA001\n\n\nD001\n90\n2\n184\n88.66667\nA001\n\n\nD002\n82\n3\n266\n84.33333\nA001\n\n\nA002\n81\n4\n347\n80.33333\nA001\n\n\nA003\n78\n5\n425\n79.00000\nA001\n\n\nD003\n78\n6\n503\n73.33333\nA001\n\n\nA004\n64\n7\n567\n66.66667\nA001\n\n\nD004\n58\n8\n625\n61.00000\nA001\n\n\n\n\n\n商品の集約をおこなおうことも可能である\n\n\nCode\nSELECT\n  product_id\n  \n  , ROW_NUMBER() OVER(ORDER BY score DESC) as row\n  \n  , list(product_id) \n      OVER(ORDER BY score DESC \n           ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) list\n  \nFROM popular_products\nWHERE category = 'action'\nORDER BY row;\n\n\n\n4 records\n\n\nproduct_id\nrow\nlist\n\n\n\n\nA001\n1\nA001, A002, A003, A004\n\n\nA002\n2\nA001, A002, A003, A004\n\n\nA003\n3\nA001, A002, A003, A004\n\n\nA004\n4\nA001, A002, A003, A004\n\n\n\n\n\nDuckdDBだと配列で距離を計算できる。version 0.10以降なのに注意する。\n\n\n4.2.3 PARTITION BY と ORDER BYを組み合わせる\n\n\nCode\nSELECT\n  category\n  , product_id\n  , score\n  , ROW_NUMBER() \n    OVER(\n        PARTITION BY category\n        ORDER BY score DESC) as row\nFROM popular_products\nWHERE category = 'action'\nORDER BY row;\n\n\n\n4 records\n\n\ncategory\nproduct_id\nscore\nrow\n\n\n\n\naction\nA001\n94\n1\n\n\naction\nA002\n81\n2\n\n\naction\nA003\n78\n3\n\n\naction\nA004\n64\n4\n\n\n\n\n\nカテゴリごとに最上位ランクの商品を取り出す。\n\n\nCode\nSELECT DISTINCT \n  category\n  , FIRST_VALUE(product_id)\n      OVER(PARTITION BY category ORDER BY score DESC\n           ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n      product_id\nFROM popular_products;\n\n\n\n2 records\n\n\ncategory\nproduct_id\n\n\n\n\ndrama\nD001\n\n\naction\nA001",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#縦持ちのデータを横持ちに変換する",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#縦持ちのデータを横持ちに変換する",
    "title": "データ加工のためのSQL",
    "section": "4.3 縦持ちのデータを横持ちに変換する",
    "text": "4.3 縦持ちのデータを横持ちに変換する\nduckdbはPIVOTを使えるので問題ない。他のSQLおこなう方法について示す。\n\n\nCode\nSELECT * FROM daily_kpi;\n\n\n\n6 records\n\n\ndt\nindicator\nval\n\n\n\n\n2017-01-01\nimpressions\n1800\n\n\n2017-01-01\nsessions\n500\n\n\n2017-01-01\nusers\n200\n\n\n2017-01-02\nimpressions\n2000\n\n\n2017-01-02\nsessions\n700\n\n\n2017-01-02\nusers\n250\n\n\n\n\n\n\n\nCode\nSELECT\n  dt\n  , MAX(CASE WHEN indicator = 'impressions' THEN val END) impressions\n  , MAX(CASE WHEN indicator = 'sessions' THEN val END) sessions\n  , MAX(CASE WHEN indicator = 'users' THEN val END) users\nFROM daily_kpi\nGROUP BY dt\nORDER BY dt;\n\n\n\n2 records\n\n\ndt\nimpressions\nsessions\nusers\n\n\n\n\n2017-01-01\n1800\n500\n200\n\n\n2017-01-02\n2000\n700\n250\n\n\n\n\n\n\n4.3.1 行を区切りの文字列に集約する\n\n\nCode\nSELECT * FROM purchase_detail_log;\n\n\n\n6 records\n\n\npurchase_id\nproduct_id\nprice\n\n\n\n\n100001\nA001\n300\n\n\n100001\nA002\n400\n\n\n100001\nA003\n200\n\n\n100002\nD001\n500\n\n\n100002\nD002\n300\n\n\n100003\nA001\n300\n\n\n\n\n\n\n\nCode\nSELECT\n  purchase_id\n  , string_agg(product_id, ',') prudct_ids\nFROM purchase_detail_log\nGROUP BY purchase_id\nORDER BY purchase_id\n\n\n\n3 records\n\n\npurchase_id\nprudct_ids\n\n\n\n\n100001\nA001,A002,A003\n\n\n100002\nD001,D002\n\n\n100003\nA001\n\n\n\n\n\n\n\nCode\nSELECT DISTINCT * FROM duckdb_functions() WHERE function_name LIKE '%nest' ORDER BY function_name;\n\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatabase_name\ndatabase_oid\nschema_name\nfunction_name\nfunction_type\ndescription\ncomment\nreturn_type\nparameters\nparameter_types\nvarargs\nmacro_definition\nhas_side_effects\ninternal\nfunction_oid\nexample\nstability\n\n\n\n\nsystem\n0\nmain\nunnest\ntable\nNA\nNA\nNA\ncol0\nTABLE\nNA\nNA\nNA\nTRUE\n84\nNA\nNA",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#横持ちデータを縦持ちデータに変換する",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#横持ちデータを縦持ちデータに変換する",
    "title": "データ加工のためのSQL",
    "section": "4.4 横持ちデータを縦持ちデータに変換する",
    "text": "4.4 横持ちデータを縦持ちデータに変換する\n組合せを作成しておいて、1つ1つ設定することになる。\nCROSS JOINをしているがつまりは直積集合であるのでFROM A, Bでも大丈夫ななず。\n\n\nCode\nselect\n    q.year\n    , case\n        when p.idx = 1 then 'q1'\n        when p.idx = 2 then 'q2'\n        when p.idx = 3 then 'q3'\n        when p.idx = 4 then 'q4'\n    end as name\n    , case\n        when p.idx = 1 then q.q1\n        when p.idx = 2 then q.q2\n        when p.idx = 3 then q.q3\n        when p.idx = 4 then q.q4\n    end as value\nfrom\n    quarterly_sales as q\n    cross join (\n        select 1 as idx\n        union all select 2 idx\n        union all select 3 idx\n        union all select 4 idx\n    ) as p\n;\n\n\n\nDisplaying records 1 - 10\n\n\nyear\nname\nvalue\n\n\n\n\n2015\nq1\n82000\n\n\n2016\nq1\n85000\n\n\n2017\nq1\n92000\n\n\n2015\nq2\n83000\n\n\n2016\nq2\n85000\n\n\n2017\nq2\n81000\n\n\n2015\nq3\n78000\n\n\n2016\nq3\n80000\n\n\n2017\nq3\nNA\n\n\n2015\nq4\n83000\n\n\n\n\n\n\n4.4.1 任意長の配列を行に展開する\n\n\nCode\nWITH NESTED AS (\n  SELECT\n    purchase_id\n    , string_agg(product_id, ',') prudct_ids\n  FROM purchase_detail_log\n  GROUP BY purchase_id\n  ORDER BY purchase_id\n)\nSELECT\n  purchase_id\n  , UNNEST(string_split(prudct_ids, ',')) product_id\nFROM NESTED;\n\n\n\n6 records\n\n\npurchase_id\nproduct_id\n\n\n\n\n100001\nA001\n\n\n100001\nA002\n\n\n100001\nA003\n\n\n100002\nD001\n\n\n100002\nD002\n\n\n100003\nA001",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#縦に並べる",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#縦に並べる",
    "title": "データ加工のためのSQL",
    "section": "5.1 縦に並べる",
    "text": "5.1 縦に並べる\nUNIONかUNION ALL句を使う。\n結合するときには同じカラム構造にしておく必要がある。\n\n\nCode\nSELECT 'app1' AS app_name, user_id, name, email FROM app1_mst_users\nUNION ALL\nSELECT 'app2' AS app_name, user_id, name, NULL AS email FROM app2_mst_users;\n\n\n\n4 records\n\n\napp_name\nuser_id\nname\nemail\n\n\n\n\napp1\nU001\nSato\nsato@example.com\n\n\napp1\nU002\nSuzuki\nsuzuki@example.com\n\n\napp2\nU001\nIto\nNA\n\n\napp2\nU002\nTanaka\nNA",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#複数のテーブルを横に並べる",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#複数のテーブルを横に並べる",
    "title": "データ加工のためのSQL",
    "section": "5.2 複数のテーブルを横に並べる",
    "text": "5.2 複数のテーブルを横に並べる\nJOIN句を使う。一般に単にJOINとした場合にはinner JOINがおこなわれる。\n\n\nCode\nSELECT\n  m.category_id\n  , m.name\n  , s.sales\n  , r.product_id sales_product\nFROM\n  mst_categories as m\n  JOIN\n    category_sales as s\n    ON m.category_id = s.category_id\n  JOIN\n    product_sale_ranking as r\n    ON m.category_id = r.category_id\n;\n\n\n\n6 records\n\n\ncategory_id\nname\nsales\nsales_product\n\n\n\n\n1\ndvd\n850000\nD001\n\n\n1\ndvd\n850000\nD002\n\n\n1\ndvd\n850000\nD003\n\n\n2\ncd\n500000\nC001\n\n\n2\ncd\n500000\nC002\n\n\n2\ncd\n500000\nC003\n\n\n\n\n\n相関サブクエリが使える場合には、JOINをも居て複数のテーブルの値を横に並べることが可能である。\n\n\nCode\nSELECT\n  m.category_id\n  , m.name\n  ---相関サブクエリを使いカテゴリー別の売上額を取得\n  , (SELECT s.sales FROM category_sales as s WHERE m.category_id = s.category_id) sales\n  , (SELECT r.product_id FROM product_sale_ranking r WHERE m.category_id = r.category_id\n     ORDER BY sales DESC LIMIT 1) top_sale_product\nFROM\n  mst_categories as m\n;\n\n\n\n3 records\n\n\ncategory_id\nname\nsales\ntop_sale_product\n\n\n\n\n1\ndvd\n850000\nD001\n\n\n2\ncd\n500000\nC001\n\n\n3\nbook\nNA\nNA",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#条件のフラグを0-1で表現する",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#条件のフラグを0-1で表現する",
    "title": "データ加工のためのSQL",
    "section": "5.3 条件のフラグを0, 1で表現する",
    "text": "5.3 条件のフラグを0, 1で表現する\nCASE式で場合分けをするのが、SIGN関数で0と1に変換する方法がある。\n\n\nCode\nSELECT * FROM mst_users_with_card_number;\n\n\n\n3 records\n\n\nuser_id\ncard_number\n\n\n\n\nU001\n1234-xxxx-xxxx-xxxx\n\n\nU002\nNA\n\n\nU003\n5678-xxxx-xxxx-xxxx\n\n\n\n\n\n\n\nCode\nSELECT * FROM purchase_log;\n\n\n\n5 records\n\n\npurchase_id\nuser_id\namount\nstamp\n\n\n\n\n10001\nU001\n200\n2017-01-30 10:00:00\n\n\n10002\nU001\n500\n2017-02-10 10:00:00\n\n\n10003\nU001\n200\n2017-02-12 10:00:00\n\n\n10004\nU002\n800\n2017-03-01 10:00:00\n\n\n10005\nU002\n400\n2017-03-02 10:00:00\n\n\n\n\n\n\n\nCode\nSELECT\n  m.user_id\n  , m.card_number\n  , COUNT(p.user_id) as purchase_count\n  , CASE WHEN m.card_number IS NOT NULL THEN 1 ELSE 0 END as has_card\n  , SIGN(COUNT(p.user_id)) as has_purchased\nFROM \n  mst_users_with_card_number as m\n  LEFT JOIN\n    purchase_log as p\n    ON m.user_id = p.user_id\nGROUP BY m.user_id, m.card_number\n\n\n\n3 records\n\n\nuser_id\ncard_number\npurchase_count\nhas_card\nhas_purchased\n\n\n\n\nU003\n5678-xxxx-xxxx-xxxx\n0\n1\n0\n\n\nU002\nNA\n2\n0\n1\n\n\nU001\n1234-xxxx-xxxx-xxxx\n3\n1\n1",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#計算したテーブルに名前を付けて再利用する",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#計算したテーブルに名前を付けて再利用する",
    "title": "データ加工のためのSQL",
    "section": "5.4 計算したテーブルに名前を付けて再利用する",
    "text": "5.4 計算したテーブルに名前を付けて再利用する\nSQL99におけるCTE(Common Table Expression)を用いると１つのクエリの中で使える 一時的なテーブルに名前を付けて利用できるため、クエリの可読性が大きく向上する。\n\n\nCode\nSELECT * FROM product_sales;\n\n\n\nDisplaying records 1 - 10\n\n\ncategory_name\nproduct_id\nsales\n\n\n\n\ndvd\nD001\n50000\n\n\ndvd\nD002\n20000\n\n\ndvd\nD003\n10000\n\n\ncd\nC001\n30000\n\n\ncd\nC002\n20000\n\n\ncd\nC003\n10000\n\n\nbook\nB001\n20000\n\n\nbook\nB002\n15000\n\n\nbook\nB003\n10000\n\n\nbook\nB004\n5000\n\n\n\n\n\nカテゴリーごとにランキングを付ける。そして、同じ順位の商品を横並びにする。\n\n\nCode\nWITH\n\nproduct_sale_ranking as (\n  SELECT\n    category_name\n    , product_id\n    , sales\n    , ROW_NUMBER() OVER(PARTITION BY category_name ORDER BY sales DESC) rank\n  FROM \n    product_sales\n)\n, mst_rank as (\n  SELECT DISTINCT rank\n  FROM product_sale_ranking\n)\nSELECT\n  m.rank\n  , r1.product_id as dvd\n  , r1.sales as dvd_sales\n  , r2.product_id as cd\n  , r2.sales as cd_sales\n  , r3.product_id as book\n  , r3.sales as book_sales\nFROM\n  mst_rank as m\n  LEFT JOIN\n    product_sale_ranking r1\n    on m.rank = r1.rank\n    AND r1.category_name = 'dvd'\n  LEFT JOIN\n    product_sale_ranking r2\n    ON m.rank = r2.rank\n    AND r2.category_name = 'cd'\n  LEFT JOIN\n    product_sale_ranking as r3\n    ON m.rank = r3.rank\n    AND r3.category_name = 'book'\nORDER BY m.rank\n\n;\n\n\n\n4 records\n\n\nrank\ndvd\ndvd_sales\ncd\ncd_sales\nbook\nbook_sales\n\n\n\n\n1\nD001\n50000\nC001\n30000\nB001\n20000\n\n\n2\nD002\n20000\nC002\n20000\nB002\n15000\n\n\n3\nD003\n10000\nC003\n10000\nB003\n10000\n\n\n4\nNA\nNA\nNA\nNA\nB004\n5000",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap03_.html#擬似的なテーブルを作成する",
    "href": "contents/sql/duckdb/sql-bigdata/chap03_.html#擬似的なテーブルを作成する",
    "title": "データ加工のためのSQL",
    "section": "5.5 擬似的なテーブルを作成する",
    "text": "5.5 擬似的なテーブルを作成する\nダミーでテーブルを作成して、実際に集計を試す。\n\n5.5.1 任意のレコードを持つ\n\n\nCode\nWITH\nmst_devices as (\n            SELECT 1 as device_id, 'pc' as device_name\n  UNION ALL SELECT 2 as device_id, 'sp' as device_name\n  UNION ALL SELECT 3 as device_id, 'アプリ' as device_name\n)\nSELECT * \nFROM mst_devices;\n\n\n\n3 records\n\n\ndevice_id\ndevice_name\n\n\n\n\n1\npc\n\n\n2\nsp\n\n\n3\nアプリ\n\n\n\n\n\nVALUESで作成することも可能である。 vALUESはINSERTとことなりまとめてロードが行える\n\n\nCode\nWITH\nmst_devices(device_id, device_name) AS(\n  VALUES\n    (1, 'PC')\n)\nSELECT * \nFROM mst_devices;\n\n\n\n1 records\n\n\ndevice_id\ndevice_name\n\n\n\n\n1\nPC\n\n\n\n\n\n配列型テーブル関数を用いた疑似テーブルの作成ができるミドルウェアもある。\n\n\n5.5.2 連番を用いてテーブルを作成する\n\n\nCode\nWITH \nseries as (\n  SELECT unnest(generate_series(1, 5)) idx\n)\nSELECT * \nFROM series;\n\n\n\n5 records\n\n\nidx\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "データ加工のためのSQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/mytips/01_.html",
    "href": "contents/sql/duckdb/mytips/01_.html",
    "title": "交差演算",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\n\n\n\n\nCode\ncur_dir &lt;- here::here(\"contents/sql/duckdb/mytips\")\n\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nlibrary(here)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 Setup\n\n\nCode\ncon &lt;- dbConnect(duckdb(here(cur_dir, \"data.db\")))\n\n\n\n\nCode\nINSTALL icu;\nINSTALL spatial;\nINSTALL httpfs;\nINSTALL json;\n\n\n\n\nCode\nLOAD icu;\nLOAD spatial;\nLOAD httpfs;\nLOAD json;\n\n\n\n\nCode\nSHOW ALL TABLES;\n\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\ndatabase\nschema\nname\ncolumn_names\ncolumn_types\ntemporary\n\n\n\n\ndata\nmain\nag\nMESH_ID , SHICODE , PTN_2015 , HITOKU2020, GASSAN2020, PTN_2020 , PT0_2020 , PT1_2020 , PT2_2020 , PT3_2020 , PT4_2020 , PT5_2020 , PT6_2020 , PT7_2020 , PT8_2020 , PT9_2020 , PT10_2020 , PT11_2020 , PT12_2020 , PT13_2020 , PT14_2020 , PT15_2020 , PT16_2020 , PT17_2020 , PT18_2020 , PT19_2020 , PTA_2020 , PTB_2020 , PTC_2020 , PTD_2020 , PTE_2020 , RTA_2020 , RTB_2020 , RTC_2020 , RTD_2020 , RTE_2020 , HITOKU2025, GASSAN2025, PTN_2025 , PT0_2025 , PT1_2025 , PT2_2025 , PT3_2025 , PT4_2025 , PT5_2025 , PT6_2025 , PT7_2025 , PT8_2025 , PT9_2025 , PT10_2025 , PT11_2025 , PT12_2025 , PT13_2025 , PT14_2025 , PT15_2025 , PT16_2025 , PT17_2025 , PT18_2025 , PT19_2025 , PTA_2025 , PTB_2025 , PTC_2025 , PTD_2025 , PTE_2025 , RTA_2025 , RTB_2025 , RTC_2025 , RTD_2025 , RTE_2025 , HITOKU2030, GASSAN2030, PTN_2030 , PT0_2030 , PT1_2030 , PT2_2030 , PT3_2030 , PT4_2030 , PT5_2030 , PT6_2030 , PT7_2030 , PT8_2030 , PT9_2030 , PT10_2030 , PT11_2030 , PT12_2030 , PT13_2030 , PT14_2030 , PT15_2030 , PT16_2030 , PT17_2030 , PT18_2030 , PT19_2030 , PTA_2030 , PTB_2030 , PTC_2030 , PTD_2030 , PTE_2030 , RTA_2030 , RTB_2030 , RTC_2030 , RTD_2030 , RTE_2030 , HITOKU2035, GASSAN2035, PTN_2035 , PT0_2035 , PT1_2035 , PT2_2035 , PT3_2035 , PT4_2035 , PT5_2035 , PT6_2035 , PT7_2035 , PT8_2035 , PT9_2035 , PT10_2035 , PT11_2035 , PT12_2035 , PT13_2035 , PT14_2035 , PT15_2035 , PT16_2035 , PT17_2035 , PT18_2035 , PT19_2035 , PTA_2035 , PTB_2035 , PTC_2035 , PTD_2035 , PTE_2035 , RTA_2035 , RTB_2035 , RTC_2035 , RTD_2035 , RTE_2035 , HITOKU2040, GASSAN2040, PTN_2040 , PT0_2040 , PT1_2040 , PT2_2040 , PT3_2040 , PT4_2040 , PT5_2040 , PT6_2040 , PT7_2040 , PT8_2040 , PT9_2040 , PT10_2040 , PT11_2040 , PT12_2040 , PT13_2040 , PT14_2040 , PT15_2040 , PT16_2040 , PT17_2040 , PT18_2040 , PT19_2040 , PTA_2040 , PTB_2040 , PTC_2040 , PTD_2040 , PTE_2040 , RTA_2040 , RTB_2040 , RTC_2040 , RTD_2040 , RTE_2040 , HITOKU2045, GASSAN2045, PTN_2045 , PT0_2045 , PT1_2045 , PT2_2045 , PT3_2045 , PT4_2045 , PT5_2045 , PT6_2045 , PT7_2045 , PT8_2045 , PT9_2045 , PT10_2045 , PT11_2045 , PT12_2045 , PT13_2045 , PT14_2045 , PT15_2045 , PT16_2045 , PT17_2045 , PT18_2045 , PT19_2045 , PTA_2045 , PTB_2045 , PTC_2045 , PTD_2045 , PTE_2045 , RTA_2045 , RTB_2045 , RTC_2045 , RTD_2045 , RTE_2045 , HITOKU2050, GASSAN2050, PTN_2050 , PT0_2050 , PT1_2050 , PT2_2050 , PT3_2050 , PT4_2050 , PT5_2050 , PT6_2050 , PT7_2050 , PT8_2050 , PT9_2050 , PT10_2050 , PT11_2050 , PT12_2050 , PT13_2050 , PT14_2050 , PT15_2050 , PT16_2050 , PT17_2050 , PT18_2050 , PT19_2050 , PTA_2050 , PTB_2050 , PTC_2050 , PTD_2050 , PTE_2050 , RTA_2050 , RTB_2050 , RTC_2050 , RTD_2050 , RTE_2050 , geom\nBIGINT , INTEGER , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , GEOMETRY\nFALSE\n\n\ndata\nmain\nag_pref_intersection\nN03_001 , N03_002 , N03_003 , N03_004 , N03_005 , N03_007 , MESH_ID , SHICODE , PTN_2015, geom\nVARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , BIGINT , INTEGER , DOUBLE , GEOMETRY\nFALSE\n\n\ndata\nmain\npref\nN03_001, N03_002, N03_003, N03_004, N03_005, N03_007, geom\nVARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , GEOMETRY\nFALSE\n\n\ndata\nmain\nta\nid , ng , N03_001 , N03_002 , N03_003 , N03_004 , N03_005 , N03_007 , MESH_ID , SHICODE , PTN_2015, geom\nUUID , INTEGER , VARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , BIGINT , INTEGER , DOUBLE , GEOMETRY\nFALSE\n\n\ndata\nmain\ntb\nid, ng\nUUID , INTEGER\nFALSE\n\n\n\n\n\n\n\nCode\nSELECT DISTINCT * FROM duckdb_functions() WHERE function_name LIKE 'st_%' ORDER BY function_name;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatabase_name\ndatabase_oid\nschema_name\nfunction_name\nfunction_type\ndescription\ncomment\ntags\nreturn_type\nparameters\nparameter_types\nvarargs\nmacro_definition\nhas_side_effects\ninternal\nfunction_oid\nexample\nstability\n\n\n\n\nsystem\n0\nmain\nstarts_with\nscalar\nReturns true if string begins with search_string\nNA\n\nBOOLEAN\nstring , search_string\nVARCHAR, VARCHAR\nNA\nNA\nFALSE\nTRUE\n936\nstarts_with(‘abc’,‘a’)\nCONSISTENT\n\n\nsystem\n0\nmain\nstats\nscalar\nReturns a string with statistics about the expression. Expression can be a column, constant, or SQL expression\nNA\n\nVARCHAR\nexpression\nANY\nNA\nNA\nTRUE\nTRUE\n938\nstats(5)\nVOLATILE\n\n\nsystem\n0\nmain\nstddev\naggregate\nReturns the sample standard deviation\nNA\n\nDOUBLE\nx\nDOUBLE\nNA\nNA\nFALSE\nTRUE\n940\nsqrt(var_samp(x))\nCONSISTENT\n\n\nsystem\n0\nmain\nstddev_pop\naggregate\nReturns the population standard deviation.\nNA\n\nDOUBLE\nx\nDOUBLE\nNA\nNA\nFALSE\nTRUE\n942\nsqrt(var_pop(x))\nCONSISTENT\n\n\nsystem\n0\nmain\nstddev_samp\naggregate\nReturns the sample standard deviation\nNA\n\nDOUBLE\nx\nDOUBLE\nNA\nNA\nFALSE\nTRUE\n944\nsqrt(var_samp(x))\nCONSISTENT\n\n\nsystem\n0\nmain\nstorage_info\npragma\nNA\nNA\n\nNA\ncol0\nVARCHAR\nNA\nNA\nNA\nTRUE\n304\nNA\nNA\n\n\nsystem\n0\nmain\nstr_split\nscalar\nSplits the string along the separator\nNA\n\nVARCHAR[]\nstring , separator\nVARCHAR, VARCHAR\nNA\nNA\nFALSE\nTRUE\n946\nstring_split(‘hello-world’, ‘-’)\nCONSISTENT\n\n\nsystem\n0\nmain\nstr_split_regex\nscalar\nSplits the string along the regex\nNA\n\nVARCHAR[]\nstring , separator, col2\nVARCHAR, VARCHAR, VARCHAR\nNA\nNA\nFALSE\nTRUE\n948\nstring_split_regex(‘hello␣world; 42’, ‘;?␣’)\nCONSISTENT\n\n\nsystem\n0\nmain\nstr_split_regex\nscalar\nSplits the string along the regex\nNA\n\nVARCHAR[]\nstring , separator\nVARCHAR, VARCHAR\nNA\nNA\nFALSE\nTRUE\n948\nstring_split_regex(‘hello␣world; 42’, ‘;?␣’)\nCONSISTENT\n\n\nsystem\n0\nmain\nstrftime\nscalar\nConverts timestamp to string according to the format string\nNA\n\nVARCHAR\ntext , format\nTIMESTAMP, VARCHAR\nNA\nNA\nFALSE\nTRUE\n950\nstrftime(timestamp ‘1992-01-01 20:38:40’, ‘%a, %-d %B %Y - %I:%M:%S %p’)\nCONSISTENT\n\n\n\n\n\n\n\n2 交差演算\n国土数値情報の行政区域データと農業地域データの交差集合を作成する。\n\n\nCode\nCREATE OR REPLACE TABLE pref AS SELECT * FROM './data/N03-20240101_GML/N03-20240101.shp';\nCREATE OR REPLACE TABLE ag AS (\n  SELECT * FROM './data/ag/1km_mesh_2018_01.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_02.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_03.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_04.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_05.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_06.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_07.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_08.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_09.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_10.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_11.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_12.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_13.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_14.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_15.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_16.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_17.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_18.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_19.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_20.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_21.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_22.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_23.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_24.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_25.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_26.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_27.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_28.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_29.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_30.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_31.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_32.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_33.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_34.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_35.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_36.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_37.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_38.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_39.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_40.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_41.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_42.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_43.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_44.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_45.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_46.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_47.shp'\n);\n\n\n\n\nCode\ndescribe pref;\n\n\n\n7 records\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\n\nN03_001\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nN03_002\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nN03_003\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nN03_004\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nN03_005\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nN03_007\nVARCHAR\nYES\nNA\nNA\nNA\n\n\ngeom\nGEOMETRY\nYES\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ndescribe ag;\n\n\n\nDisplaying records 1 - 10\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\n\nMESH_ID\nBIGINT\nYES\nNA\nNA\nNA\n\n\nSHICODE\nINTEGER\nYES\nNA\nNA\nNA\n\n\nPTN_2015\nDOUBLE\nYES\nNA\nNA\nNA\n\n\nHITOKU2020\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nGASSAN2020\nBIGINT\nYES\nNA\nNA\nNA\n\n\nPTN_2020\nDOUBLE\nYES\nNA\nNA\nNA\n\n\nPT0_2020\nDOUBLE\nYES\nNA\nNA\nNA\n\n\nPT1_2020\nDOUBLE\nYES\nNA\nNA\nNA\n\n\nPT2_2020\nDOUBLE\nYES\nNA\nNA\nNA\n\n\nPT3_2020\nDOUBLE\nYES\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\nselect count(*), ST_GeometryType(geom) as type  from pref group by ST_GeometryType(geom) ;\n\n\n\n1 records\n\n\ncount_star()\ntype\n\n\n\n\n124134\nPOLYGON\n\n\n\n\n\n\n\nCode\nselect \n  count(*) as cnt\n  , sum(cast(ST_IsRing(geom) as integer)) as has_ring \n  , ST_GeometryType(geom) as type\nfrom \n  ag \ngroup by \n  ST_GeometryType(geom) ;\n\n\n\n1 records\n\n\ncnt\nhas_ring\ntype\n\n\n\n\n178347\n0\nPOLYGON\n\n\n\n\n\n\n\nCode\nselect count(*), ST_GeometryType(geom) as type  from ag group by ST_GeometryType(geom) ;\n\n\n\n1 records\n\n\ncount_star()\ntype\n\n\n\n\n178347\nPOLYGON\n\n\n\n\n\n約10万どおしの演算であるが、数分で処理できる。すごい！\n\n\nCode\ncreate or replace table ag_pref_intersection as\nselect\n  COLUMNS(pref.* EXCLUDE(geom)), \n  ag.MESH_ID, \n  ag.SHICODE, \n  ag.PTN_2015,\n  st_intersection(pref.geom, ag.geom) as geom\nfrom \n  pref\n  inner join ag\n  on st_intersects(pref.geom, ag.geom)\n;\n\n\n\n\nCode\nprint(glue::glue(\"{difftime(Sys.time(), s, units = 'secs')} 秒\"))\n#&gt; 191.446138143539 秒\n\n\nデータ方を確認すると少し形状タイプがおかしいかも。\n\n\nCode\nselect count(*) as cnt\nfrom ag_pref_intersection ;\n\n\n\n1 records\n\n\ncnt\n\n\n\n\n265301\n\n\n\n\n\n次の結果をみるとどうやら混じっている。コレクションについて、QGISで具体的に確認したところ、行政境界とメッシュ境界がタッチしているところや一致しているところで発生する事象である。マルチポリゴンもそうであるが基本的にはsimplifyしてポリゴンだけ残せば大丈夫だと思う。\n\n\nCode\nselect \n  COUNT(*) as cnt\n  , COUNT_IF(ST_IsRing(geom)) as has_ring \n  , ST_GeometryType(geom) as type\nfrom \n  ag_pref_intersection \ngroup by \n  ST_GeometryType(geom) ;\n\n\n\n3 records\n\n\ncnt\nhas_ring\ntype\n\n\n\n\n256571\n0\nPOLYGON\n\n\n8715\n0\nMULTIPOLYGON\n\n\n15\n0\nGEOMETRYCOLLECTION\n\n\n\n\n\nどうやらWITH句でランダムを使うと変わってしまうらしい。またマルチポリゴンをポリゴンにキャストする方法は実装されてないようである。\n\n\nCode\nwith \nta as (\n  select \n    gen_random_uuid() as id\n    , ST_NGeometries(geom) as ng\n    , ROW_NUMBER() OVER() as row\n    , geom\n  from \n    ag_pref_intersection\n  where\n    ST_GeometryType(geom) = 'MULTIPOLYGON'\n  limit 2\n) \n, tb as (\n  select \n    id, ng, row\n  from \n    ta\n  limit 2\n)\n, ta_tb as (\n  select \n    *, \n    unnest(generate_series(1, ta.ng)) as idx\n  from\n   ta\n   inner join tb\n   on ta.row = tb.row\n)\n\nselect \n    *\nfrom\n  ta_tb\n;\n\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\nid\nng\nrow\ngeom\nid_1\nng_1\nrow_1\nidx\n\n\n\n\n3bd23c21-ce2c-4f1e-8f67-e1b7dff80191\n2\n1\n05, 04, 00, 00, 00, 00, 00, 00, cc, 0c, 03, 43, 99, 99, f6, 41, 00, 10, 03, 43, ab, aa, f6, 41, 05, 00, 00, 00, 02, 00, 00, 00, 02, 00, 00, 00, 01, 00, 00, 00, ab, 00, 00, 00, 00, 00, 00, 00, 18, c5, 4a, 83, fd, 61, 60, 40, d4, e6, 53, 55, 55, d5, 3e, 40, 97, 8a, 58, 87, fd, 61, 60, 40, cf, d1, 40, b2, 54, d5, 3e, 40, 8f, b4, 2b, 5f, fd, 61, 60, 40, 53, f0, 74, e4, 51, d5, 3e, 40, 51, 92, fc, 28, fd, 61, 60, 40, 78, 5f, 49, 7d, 4f, d5, 3e, 40, b1, 31, cb, 78, fd, 61, 60, 40, 3e, 96, 85, 49, 4d, d5, 3e, 40, 76, c1, 22, b7, fd, 61, 60, 40, 86, f8, db, 2c, 4b, d5, 3e, 40, ff, 76, 1b, cc, fd, 61, 60, 40, 23, 7e, 60, ef, 47, d5, 3e, 40, 21, 99, 94, d6, fd, 61, 60, 40, 9c, 67, 40, 4b, 44, d5, 3e, 40, e5, 19, 5c, bc, fd, 61, 60, 40, e4, 8f, 58, 41, 42, d5, 3e, 40, 3a, e7, c3, 8d, fd, 61, 60, 40, 63, b6, 18, c7, 3e, d5, 3e, 40, 5d, bd, a6, e1, fd, 61, 60, 40, 28, ed, 54, 93, 3c, d5, 3e, 40, 43, 16, 34, f0, fd, 61, 60, 40, ba, 0b, 2a, 30, 38, d5, 3e, 40, 31, 06, 69, d5, fd, 61, 60, 40, 03, ea, d2, 61, 33, d5, 3e, 40, b1, 59, 47, b8, fd, 61, 60, 40, 93, 52, 17, c3, 31, d5, 3e, 40, fe, 1d, d8, 91, fd, 61, 60, 40, 9a, b1, 21, 65, 2f, d5, 3e, 40, 98, e3, 9b, c1, fd, 61, 60, 40, e3, d9, 39, 5b, 2d, d5, 3e, 40, ff, 76, 1b, cc, fd, 61, 60, 40, cc, d1, 9c, 22, 2b, d5, 3e, 40, ff, c0, ce, ad, fd, 61, 60, 40, f8, dc, 04, 3d, 26, d5, 3e, 40, 0e, 30, 86, 81, fd, 61, 60, 40, 9a, 5a, fd, ab, 23, d5, 3e, 40, fc, 69, 6e, 48, fd, 61, 60, 40, 89, 67, 73, 16, 22, d5, 3e, 40, d0, 8a, b4, fc, fc, 61, 60, 40, c0, 97, 76, f5, 20, d5, 3e, 40, da, 1f, 79, fe, fc, 61, 60, 40, 91, 20, a6, 8e, 1f, d5, 3e, 40, d2, 3e, 1e, 46, fd, 61, 60, 40, 92, 9c, f8, dc, 1c, d5, 3e, 40, f5, 91, 5e, 4b, fd, 61, 60, 40, cf, 5d, 61, ad, 19, d5, 3e, 40, f4, ee, 67, 2f, fd, 61, 60, 40, 91, 6f, 59, ff, 13, d5, 3e, 40, 37, 1e, 34, 07, fd, 61, 60, 40, 38, 2a, 32, 98, 11, d5, 3e, 40, b6, bb, c5, cb, fc, 61, 60, 40, a5, 28, 2d, e2, 0f, d5, 3e, 40, f1, e1, ba, ab, fc, 61, 60, 40, b1, 45, 6d, 43, 0e, d5, 3e, 40, b6, 71, 12, ea, fc, 61, 60, 40, fa, 8a, 24, 30, 0c, d5, 3e, 40, fc, f7, 3e, 27, fd, 61, 60, 40, ad, 03, 8a, d2, 0a, d5, 3e, 40, ca, da, 20, 3f, fd, 61, 60, 40, 69, e3, cf, 8a, 04, d5, 3e, 40, 75, e5, 3c, 2e, fd, 61, 60, 40, 71, a1, 8d, 84, ff, d4, 3e, 40, 84, 54, f4, 01, fd, 61, 60, 40, 55, 0d, 16, dc, fc, d4, 3e, 40, 9c, ca, 9f, f8, fc, 61, 60, 40, 62, db, 5b, f7, fa, d4, 3e, 40, ec, fc, 99, 49, fd, 61, 60, 40, 4b, 05, 1a, 0e, f9, d4, 3e, 40, 63, fd, ed, 52, fd, 61, 60, 40, 2f, e8, 69, 32, f6, d4, 3e, 40, 5b, 3e, ca, 3c, fd, 61, 60, 40, 5c, c6, 01, 7f, ee, d4, 3e, 40, 49, 2e, ff, 21, fd, 61, 60, 40, ec, b7, 7e, 13, ed, d4, 3e, 40, 37, 68, e7, e8, fc, 61, 60, 40, d5, 75, a3, ed, ea, d4, 3e, 40, 7a, f2, d9, cf, fc, 61, 60, 40, 41, ae, dc, 24, e9, d4, 3e, 40, 8c, b8, f1, 08, fd, 61, 60, 40, ad, ac, d7, 6e, e7, d4, 3e, 40, d2, e3, f7, 36, fd, 61, 60, 40, f6, 85, f5, 1e, e5, d4, 3e, 40, 1e, 1a, b8, 31, fd, 61, 60, 40, 28, a6, 70, dc, e0, d4, 3e, 40, 9e, 6d, 96, 14, fd, 61, 60, 40, dc, 9a, 28, cd, dc, d4, 3e, 40, 02, aa, b5, b9, fc, 61, 60, 40, de, f9, db, 24, da, d4, 3e, 40, 24, a4, b2, 84, fc, 61, 60, 40, eb, 16, 1c, 86, d8, d4, 3e, 40, 3f, cc, e4, ef, fc, 61, 60, 40, 4b, 42, ce, 6d, d5, d4, 3e, 40, 2f, 5f, 10, f1, fc, 61, 60, 40, 81, 2d, 65, cf, d4, d4, 3e, 40, da, 1f, 79, fe, fc, 61, 60, 40, fc, d7, 03, f7, cd, d4, 3e, 40, 84, f9, cd, f2, fc, 61, 60, 40, 98, 7a, 27, b0, ca, d4, 3e, 40, 9c, 47, fd, a9, fc, 61, 60, 40, d5, 3b, 90, 80, c7, d4, 3e, 40, 68, 87, e8, a5, fc, 61, 60, 40, e2, ec, 36, a5, c5, d4, 3e, 40, 2f, ba, 36, 00, fd, 61, 60, 40, 13, c3, ba, 63, c4, d4, 3e, 40, 7b, 18, 73, 3a, fd, 61, 60, 40, 7f, 8f, 5a, 5e, c2, d4, 3e, 40, 0f, 06, 27, 6d, fd, 61, 60, 40, 98, c9, da, 20, bf, d4, 3e, 40, a0, 91, 4c, aa, fd, 61, 60, 40, 7d, 45, 1c, 8a, b9, d4, 3e, 40, f5, b9, da, 8a, fd, 61, 60, 40, cd, 92, 00, 35, b5, d4, 3e, 40, 6e, 85, a1, ef, fd, 61, 60, 40, da, af, 40, 96, b3, d4, 3e, 40, a2, 6d, 32, 33, fe, 61, 60, 40, 51, 94, 95, 70, b2, d4, 3e, 40, e8, 3d, 12, 52, fe, 61, 60, 40, b2, 7a, db, da, af, d4, 3e, 40, e8, 3d, 12, 52, fe, 61, 60, 40, 2b, 2a, 7d, 49, ac, d4, 3e, 40, d6, 2d, 47, 37, fe, 61, 60, 40, f6, d7, d7, 2c, aa, d4, 3e, 40, 55, 81, 25, 1a, fe, 61, 60, 40, c2, 0e, 6b, 43, a8, d4, 3e, 40, 46, 6d, 94, 55, fe, 61, 60, 40, 98, 72, 6f, 92, a7, d4, 3e, 40, f9, 03, 2a, 8b, fe, 61, 60, 40, d4, 9a, e6, 1d, a7, d4, 3e, 40, 8a, 1d, 20, a7, fe, 61, 60, 40, c9, 16, 98, 01, a6, d4, 3e, 40, 0b, ca, 41, c4, fe, 61, 60, 40, 2b, 2a, 36, 74, a0, d4, 3e, 40, ea, 4a, bf, d5, fe, 61, 60, 40, a4, f6, 76, d9, 9c, d4, 3e, 40, 6a, 9e, 9d, b8, fe, 61, 60, 40, ec, 1e, 8f, cf, 9a, d4, 3e, 40, ad, cd, 69, 90, fe, 61, 60, 40, 76, ac, fe, 7a, 98, d4, 3e, 40, 15, 04, e0, b6, fe, 61, 60, 40, 8f, fe, 92, b2, 97, d4, 3e, 40, 69, c6, 19, f8, fe, 61, 60, 40, 65, eb, cf, 34, 97, d4, 3e, 40, cb, 96, af, 42, ff, 61, 60, 40, f2, aa, 22, 6a, 8f, d4, 3e, 40, 7d, 2f, 28, 4d, ff, 61, 60, 40, c9, dc, cb, 69, 8e, d4, 3e, 40, 30, 43, 1b, 34, ff, 61, 60, 40, ee, 2e, 01, 0c, 8c, d4, 3e, 40, b8, 9f, d0, 0e, ff, 61, 60, 40, 5a, fb, a0, 06, 8a, d4, 3e, 40, 41, e3, 99, 02, ff, 61, 60, 40, c6, f9, 9b, 50, 88, d4, 3e, 40, cb, 96, af, 42, ff, 61, 60, 40, 79, 55, 62, fc, 86, d4, 3e, 40, da, d2, bc, 46, ff, 61, 60, 40, e7, 9d, 54, 45, 82, d4, 3e, 40, c9, 65, e8, 47, ff, 61, 60, 40, 6d, d6, cf, 51, 7d, d4, 3e, 40, c1, 8f, bb, 1f, ff, 61, 60, 40, 7f, e1, f5, 96, 7b, d4, 3e, 40, 27, 14, ab, d1, fe, 61, 60, 40, a4, 33, 2b, 39, 79, d4, 3e, 40, 73, 72, e7, 0b, ff, 61, 60, 40, fe, 88, 0b, b2, 78, d4, 3e, 40, 40, 55, c9, 23, ff, 61, 60, 40, c9, a2, ff, d1, 76, d4, 3e, 40, ea, 72, 3b, 15, ff, 61, 60, 40, 06, 2a, 2a, b5, 73, d4, 3e, 40, 95, 22, 31, f5, fe, 61, 60, 40, a3, e9, ec, 64, 70, d4, 3e, 40, ae, 26, ad, ca, fe, 61, 60, 40, e0, aa, 55, 35, 6d, d4, 3e, 40, c7, e6, 0b, a3, fe, 61, 60, 40, f3, 04, 76, c0, 6b, d4, 3e, 40, 91, 5b, 84, 43, fe, 61, 60, 40, 82, f6, f2, 54, 6a, d4, 3e, 40, f1, 2d, fd, 62, fe, 61, 60, 40, 17, 74, 4a, 59, 69, d4, 3e, 40, b5, d6, 40, 88, fe, 61, 60, 40, 60, 7f, c3, 58, 67, d4, 3e, 40, 70, f3, 0a, 67, fe, 61, 60, 40, f1, 9d, 98, f5, 62, d4, 3e, 40, 6f, 50, 14, 4b, fe, 61, 60, 40, c9, 68, 33, 3a, 5f, d4, 3e, 40, 9b, 3a, fc, 26, fe, 61, 60, 40, fa, 0c, 5c, a9, 5d, d4, 3e, 40, bc, 91, 02, d6, fd, 61, 60, 40, ee, d7, 07, d3, 5c, d4, 3e, 40, 6e, 02, ff, a0, fd, 61, 60, 40, c5, 09, b1, d2, 5b, d4, 3e, 40, 29, 47, 45, bf, fd, 61, 60, 40, 20, db, e3, 99, 58, d4, 3e, 40, 29, 30, 3c, ad, fd, 61, 60, 40, 6e, 5d, 7b, b0, 56, d4, 3e, 40, a7, cf, b0, 46, fd, 61, 60, 40, e7, 29, bc, 15, 53, d4, 3e, 40, 3f, cc, e4, ef, fc, 61, 60, 40, 8f, 9f, 28, 31, 51, d4, 3e, 40, a5, 83, 7e, 71, fc, 61, 60, 40, 71, 8f, 5e, 3a, 51, d4, 3e, 40, 14, ed, 2a, a4, fc, 61, 60, 40, c7, 85, bc, 4b, 4d, d4, 3e, 40, a5, 06, 21, c0, fc, 61, 60, 40, 09, b6, 60, 95, 4a, d4, 3e, 40, fa, d3, 88, 91, fc, 61, 60, 40, 23, a1, e6, 11, 47, d4, 3e, 40, f0, 16, 48, 50, fc, 61, 60, 40, d8, 5b, 60, 15, 43, d4, 3e, 40, d5, a4, 62, 03, fc, 61, 60, 40, 08, 6c, 22, c1, 41, d4, 3e, 40, 98, 25, 2a, e9, fb, 61, 60, 40, 5c, 7a, 94, 47, 40, d4, 3e, 40, c6, 5d, 27, 6f, fc, 61, 60, 40, b1, a2, 4d, a8, 3c, d4, 3e, 40, a3, ad, dd, 85, fc, 61, 60, 40, 59, 03, fe, 6a, 3a, d4, 3e, 40, d7, 6d, f2, 89, fc, 61, 60, 40, f6, a5, 21, 24, 37, d4, 3e, 40, c6, 5d, 27, 6f, fc, 61, 60, 40, bb, f9, fc, e6, 34, d4, 3e, 40, 9a, 7e, 6d, 23, fc, 61, 60, 40, c8, c7, 42, 02, 33, d4, 3e, 40, 11, 6e, 4e, ff, fb, 61, 60, 40, db, 21, 63, 8d, 31, d4, 3e, 40, 83, d3, 34, 88, fc, 61, 60, 40, 95, d4, 50, 3d, 2e, d4, 3e, 40, 60, 23, eb, 9e, fc, 61, 60, 40, 22, b1, 42, 69, 26, d4, 3e, 40, 72, c1, 86, 98, fc, 61, 60, 40, 5f, 55, 0c, 43, 23, d4, 3e, 40, e8, 0d, 71, 58, fc, 61, 60, 40, 84, c4, e0, db, 20, d4, 3e, 40, 33, a1, 3a, 37, fc, 61, 60, 40, 4a, 18, bc, 9e, 1e, d4, 3e, 40, 2c, 63, d6, 9a, fc, 61, 60, 40, 20, 67, 04, 95, 1d, d4, 3e, 40, 9c, fd, 49, c8, fc, 61, 60, 40, 6f, 38, 96, f1, 1b, d4, 3e, 40, 9c, fd, 49, c8, fc, 61, 60, 40, d8, 04, 67, dc, 13, d4, 3e, 40, c7, 5b, 44, 9a, fc, 61, 60, 40, 9e, 75, e1, 95, 11, d4, 3e, 40, 24, a4, b2, 84, fc, 61, 60, 40, c8, 53, f1, a7, 0f, d4, 3e, 40, 62, d7, 54, e8, fc, 61, 60, 40, c3, 78, 1c, f2, 0e, d4, 3e, 40, a7, cf, b0, 46, fd, 61, 60, 40, 5f, 38, df, a1, 0b, d4, 3e, 40, 98, 16, 46, 91, fd, 61, 60, 40, 2c, 45, ed, dc, 06, d4, 3e, 40, 7e, 6f, d3, 9f, fd, 61, 60, 40, 51, 97, 22, 7f, 04, d4, 3e, 40, b1, 8c, f1, 87, fd, 61, 60, 40, 24, d6, e2, 53, 00, d4, 3e, 40, a2, 6d, 32, 33, fe, 61, 60, 40, bf, fc, b3, be, ff, d3, 3e, 40, e1, 8d, 7e, 94, fe, 61, 60, 40, 07, 25, cc, b4, fd, d3, 3e, 40, 8c, 76, 63, e1, fe, 61, 60, 40, fd, 1c, d0, e6, f9, d3, 3e, 40, 73, 00, b8, ea, fe, 61, 60, 40, 7c, 92, 8a, b2, f6, d3, 3e, 40, 61, 62, 1c, f1, fe, 61, 60, 40, 1e, 2d, 22, 18, f4, d3, 3e, 40, 52, 4e, 8b, 2c, ff, 61, 60, 40, d1, d7, e2, 09, f3, d3, 3e, 40, 88, 8f, 5f, aa, ff, 61, 60, 40, b9, e7, 59, 46, f3, d3, 3e, 40, 6f, 41, 30, f3, ff, 61, 60, 40, 78, 16, 69, 54, f3, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, d9, bf, 4a, 23, f3, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 7d, 2c, db, 87, 8d, d3, 3e, 40, b6, 9d, 86, 0d, fe, 61, 60, 40, 5a, f8, 9c, e9, 95, d3, 3e, 40, 98, e3, 9b, c1, fd, 61, 60, 40, 6e, 8a, a1, 7e, 92, d3, 3e, 40, d5, 84, 40, 70, fd, 61, 60, 40, 7c, 7c, 0d, 84, 8e, d3, 3e, 40, 3d, 51, 35, 21, fd, 61, 60, 40, 02, 16, e7, 12, 88, d3, 3e, 40, 7c, c5, 2f, c9, fc, 61, 60, 40, e1, cb, be, d7, 82, d3, 3e, 40, 17, e0, 09, 5d, fc, 61, 60, 40, 36, 42, c5, 8c, 7e, d3, 3e, 40, 0a, 4c, 8b, 20, fc, 61, 60, 40, 08, 2c, 02, 61, 7c, d3, 3e, 40, d3, 34, e1, c4, fb, 61, 60, 40, 0a, 88, 0c, 55, 78, d3, 3e, 40, 1c, fa, b7, 63, fb, 61, 60, 40, 59, 2e, ca, 55, 74, d3, 3e, 40, df, 96, eb, 14, fb, 61, 60, 40, 9c, 49, b2, 46, 71, d3, 3e, 40, 6c, 17, 47, a3, fa, 61, 60, 40, 80, b4, 8d, ac, 6d, d3, 3e, 40, 03, be, b7, f6, f9, 61, 60, 40, ca, 7f, 76, f7, 68, d3, 3e, 40, 49, 9b, dd, 6c, f9, 61, 60, 40, 2b, f9, 75, 33, 65, d3, 3e, 40, 26, a9, 8e, 4d, f9, 61, 60, 40, 03, c7, 68, 94, 63, d3, 3e, 40, ff, 48, a1, c4, f8, 61, 60, 40, 95, cd, 7a, 03, 5c, d3, 3e, 40, 30, 38, 43, a4, f8, 61, 60, 40, 43, b9, 58, 44, 59, d3, 3e, 40, 59, 04, 85, 95, f8, 61, 60, 40, 80, 2d, 72, f9, 56, d3, 3e, 40, f1, 56, 7c, 94, f8, 61, 60, 40, 70, fe, fe, 4b, 54, d3, 3e, 40, 30, c3, 93, 9e, f8, 61, 60, 40, 2a, 6d, 7e, b7, 51, d3, 3e, 40, d9, c4, 64, d2, f8, 61, 60, 40, 08, 7e, da, fc, 4d, d3, 3e, 40, 4b, ee, 10, fc, f8, 61, 60, 40, 27, 30, 77, 0f, 4b, d3, 3e, 40, 42, 9d, e0, 05, fb, 61, 60, 40, 3c, 48, ad, 02, 45, d3, 3e, 40, 72, 8e, 65, ba, fa, 61, 60, 40, 4a, a5, 05, 53, 40, d3, 3e, 40, 27, e9, e3, 9f, fe, 61, 60, 40, 0f, f2, 63, 94, 33, d3, 3e, 40, 6e, ff, 2a, bd, ff, 61, 60, 40, 5c, b3, 83, b4, 40, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 1c, f6, 0d, 1a, 4a, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 01, ef, 6b, 8f, 3d, d3, 3e, 40, e4, 43, 32, df, ff, 61, 60, 40, f8, fd, 7a, 09, 3e, d3, 3e, 40, b2, 28, e7, e9, fe, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9a, 99, 99, 99, 99, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9a, 99, 99, 99, 99, 61, 60, 40, 55, 55, 55, 55, 55, d5, 3e, 40, ed, bb, 4a, 83, fd, 61, 60, 40, 55, 55, 55, 55, 55, d5, 3e, 40, 18, c5, 4a, 83, fd, 61, 60, 40, d4, e6, 53, 55, 55, d5, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, 06, 00, 00, 00, 00, 00, 00, 00, b2, 3d, 52, fb, ff, 61, 60, 40, 71, b6, 8b, f9, 51, d3, 3e, 40, 19, bd, 13, e6, ff, 61, 60, 40, 7d, 38, 2f, eb, 51, d3, 3e, 40, 9a, 6f, 00, da, ff, 61, 60, 40, fd, c6, 79, 2d, 58, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, e4, ec, 07, 2a, 5b, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 6b, cd, 17, 37, 50, d3, 3e, 40, b2, 3d, 52, fb, ff, 61, 60, 40, 71, b6, 8b, f9, 51, d3, 3e, 40\n8793c611-f562-4c3b-8ecd-d11cbae432ad\n2\n1\n1\n\n\n3bd23c21-ce2c-4f1e-8f67-e1b7dff80191\n2\n1\n05, 04, 00, 00, 00, 00, 00, 00, cc, 0c, 03, 43, 99, 99, f6, 41, 00, 10, 03, 43, ab, aa, f6, 41, 05, 00, 00, 00, 02, 00, 00, 00, 02, 00, 00, 00, 01, 00, 00, 00, ab, 00, 00, 00, 00, 00, 00, 00, 18, c5, 4a, 83, fd, 61, 60, 40, d4, e6, 53, 55, 55, d5, 3e, 40, 97, 8a, 58, 87, fd, 61, 60, 40, cf, d1, 40, b2, 54, d5, 3e, 40, 8f, b4, 2b, 5f, fd, 61, 60, 40, 53, f0, 74, e4, 51, d5, 3e, 40, 51, 92, fc, 28, fd, 61, 60, 40, 78, 5f, 49, 7d, 4f, d5, 3e, 40, b1, 31, cb, 78, fd, 61, 60, 40, 3e, 96, 85, 49, 4d, d5, 3e, 40, 76, c1, 22, b7, fd, 61, 60, 40, 86, f8, db, 2c, 4b, d5, 3e, 40, ff, 76, 1b, cc, fd, 61, 60, 40, 23, 7e, 60, ef, 47, d5, 3e, 40, 21, 99, 94, d6, fd, 61, 60, 40, 9c, 67, 40, 4b, 44, d5, 3e, 40, e5, 19, 5c, bc, fd, 61, 60, 40, e4, 8f, 58, 41, 42, d5, 3e, 40, 3a, e7, c3, 8d, fd, 61, 60, 40, 63, b6, 18, c7, 3e, d5, 3e, 40, 5d, bd, a6, e1, fd, 61, 60, 40, 28, ed, 54, 93, 3c, d5, 3e, 40, 43, 16, 34, f0, fd, 61, 60, 40, ba, 0b, 2a, 30, 38, d5, 3e, 40, 31, 06, 69, d5, fd, 61, 60, 40, 03, ea, d2, 61, 33, d5, 3e, 40, b1, 59, 47, b8, fd, 61, 60, 40, 93, 52, 17, c3, 31, d5, 3e, 40, fe, 1d, d8, 91, fd, 61, 60, 40, 9a, b1, 21, 65, 2f, d5, 3e, 40, 98, e3, 9b, c1, fd, 61, 60, 40, e3, d9, 39, 5b, 2d, d5, 3e, 40, ff, 76, 1b, cc, fd, 61, 60, 40, cc, d1, 9c, 22, 2b, d5, 3e, 40, ff, c0, ce, ad, fd, 61, 60, 40, f8, dc, 04, 3d, 26, d5, 3e, 40, 0e, 30, 86, 81, fd, 61, 60, 40, 9a, 5a, fd, ab, 23, d5, 3e, 40, fc, 69, 6e, 48, fd, 61, 60, 40, 89, 67, 73, 16, 22, d5, 3e, 40, d0, 8a, b4, fc, fc, 61, 60, 40, c0, 97, 76, f5, 20, d5, 3e, 40, da, 1f, 79, fe, fc, 61, 60, 40, 91, 20, a6, 8e, 1f, d5, 3e, 40, d2, 3e, 1e, 46, fd, 61, 60, 40, 92, 9c, f8, dc, 1c, d5, 3e, 40, f5, 91, 5e, 4b, fd, 61, 60, 40, cf, 5d, 61, ad, 19, d5, 3e, 40, f4, ee, 67, 2f, fd, 61, 60, 40, 91, 6f, 59, ff, 13, d5, 3e, 40, 37, 1e, 34, 07, fd, 61, 60, 40, 38, 2a, 32, 98, 11, d5, 3e, 40, b6, bb, c5, cb, fc, 61, 60, 40, a5, 28, 2d, e2, 0f, d5, 3e, 40, f1, e1, ba, ab, fc, 61, 60, 40, b1, 45, 6d, 43, 0e, d5, 3e, 40, b6, 71, 12, ea, fc, 61, 60, 40, fa, 8a, 24, 30, 0c, d5, 3e, 40, fc, f7, 3e, 27, fd, 61, 60, 40, ad, 03, 8a, d2, 0a, d5, 3e, 40, ca, da, 20, 3f, fd, 61, 60, 40, 69, e3, cf, 8a, 04, d5, 3e, 40, 75, e5, 3c, 2e, fd, 61, 60, 40, 71, a1, 8d, 84, ff, d4, 3e, 40, 84, 54, f4, 01, fd, 61, 60, 40, 55, 0d, 16, dc, fc, d4, 3e, 40, 9c, ca, 9f, f8, fc, 61, 60, 40, 62, db, 5b, f7, fa, d4, 3e, 40, ec, fc, 99, 49, fd, 61, 60, 40, 4b, 05, 1a, 0e, f9, d4, 3e, 40, 63, fd, ed, 52, fd, 61, 60, 40, 2f, e8, 69, 32, f6, d4, 3e, 40, 5b, 3e, ca, 3c, fd, 61, 60, 40, 5c, c6, 01, 7f, ee, d4, 3e, 40, 49, 2e, ff, 21, fd, 61, 60, 40, ec, b7, 7e, 13, ed, d4, 3e, 40, 37, 68, e7, e8, fc, 61, 60, 40, d5, 75, a3, ed, ea, d4, 3e, 40, 7a, f2, d9, cf, fc, 61, 60, 40, 41, ae, dc, 24, e9, d4, 3e, 40, 8c, b8, f1, 08, fd, 61, 60, 40, ad, ac, d7, 6e, e7, d4, 3e, 40, d2, e3, f7, 36, fd, 61, 60, 40, f6, 85, f5, 1e, e5, d4, 3e, 40, 1e, 1a, b8, 31, fd, 61, 60, 40, 28, a6, 70, dc, e0, d4, 3e, 40, 9e, 6d, 96, 14, fd, 61, 60, 40, dc, 9a, 28, cd, dc, d4, 3e, 40, 02, aa, b5, b9, fc, 61, 60, 40, de, f9, db, 24, da, d4, 3e, 40, 24, a4, b2, 84, fc, 61, 60, 40, eb, 16, 1c, 86, d8, d4, 3e, 40, 3f, cc, e4, ef, fc, 61, 60, 40, 4b, 42, ce, 6d, d5, d4, 3e, 40, 2f, 5f, 10, f1, fc, 61, 60, 40, 81, 2d, 65, cf, d4, d4, 3e, 40, da, 1f, 79, fe, fc, 61, 60, 40, fc, d7, 03, f7, cd, d4, 3e, 40, 84, f9, cd, f2, fc, 61, 60, 40, 98, 7a, 27, b0, ca, d4, 3e, 40, 9c, 47, fd, a9, fc, 61, 60, 40, d5, 3b, 90, 80, c7, d4, 3e, 40, 68, 87, e8, a5, fc, 61, 60, 40, e2, ec, 36, a5, c5, d4, 3e, 40, 2f, ba, 36, 00, fd, 61, 60, 40, 13, c3, ba, 63, c4, d4, 3e, 40, 7b, 18, 73, 3a, fd, 61, 60, 40, 7f, 8f, 5a, 5e, c2, d4, 3e, 40, 0f, 06, 27, 6d, fd, 61, 60, 40, 98, c9, da, 20, bf, d4, 3e, 40, a0, 91, 4c, aa, fd, 61, 60, 40, 7d, 45, 1c, 8a, b9, d4, 3e, 40, f5, b9, da, 8a, fd, 61, 60, 40, cd, 92, 00, 35, b5, d4, 3e, 40, 6e, 85, a1, ef, fd, 61, 60, 40, da, af, 40, 96, b3, d4, 3e, 40, a2, 6d, 32, 33, fe, 61, 60, 40, 51, 94, 95, 70, b2, d4, 3e, 40, e8, 3d, 12, 52, fe, 61, 60, 40, b2, 7a, db, da, af, d4, 3e, 40, e8, 3d, 12, 52, fe, 61, 60, 40, 2b, 2a, 7d, 49, ac, d4, 3e, 40, d6, 2d, 47, 37, fe, 61, 60, 40, f6, d7, d7, 2c, aa, d4, 3e, 40, 55, 81, 25, 1a, fe, 61, 60, 40, c2, 0e, 6b, 43, a8, d4, 3e, 40, 46, 6d, 94, 55, fe, 61, 60, 40, 98, 72, 6f, 92, a7, d4, 3e, 40, f9, 03, 2a, 8b, fe, 61, 60, 40, d4, 9a, e6, 1d, a7, d4, 3e, 40, 8a, 1d, 20, a7, fe, 61, 60, 40, c9, 16, 98, 01, a6, d4, 3e, 40, 0b, ca, 41, c4, fe, 61, 60, 40, 2b, 2a, 36, 74, a0, d4, 3e, 40, ea, 4a, bf, d5, fe, 61, 60, 40, a4, f6, 76, d9, 9c, d4, 3e, 40, 6a, 9e, 9d, b8, fe, 61, 60, 40, ec, 1e, 8f, cf, 9a, d4, 3e, 40, ad, cd, 69, 90, fe, 61, 60, 40, 76, ac, fe, 7a, 98, d4, 3e, 40, 15, 04, e0, b6, fe, 61, 60, 40, 8f, fe, 92, b2, 97, d4, 3e, 40, 69, c6, 19, f8, fe, 61, 60, 40, 65, eb, cf, 34, 97, d4, 3e, 40, cb, 96, af, 42, ff, 61, 60, 40, f2, aa, 22, 6a, 8f, d4, 3e, 40, 7d, 2f, 28, 4d, ff, 61, 60, 40, c9, dc, cb, 69, 8e, d4, 3e, 40, 30, 43, 1b, 34, ff, 61, 60, 40, ee, 2e, 01, 0c, 8c, d4, 3e, 40, b8, 9f, d0, 0e, ff, 61, 60, 40, 5a, fb, a0, 06, 8a, d4, 3e, 40, 41, e3, 99, 02, ff, 61, 60, 40, c6, f9, 9b, 50, 88, d4, 3e, 40, cb, 96, af, 42, ff, 61, 60, 40, 79, 55, 62, fc, 86, d4, 3e, 40, da, d2, bc, 46, ff, 61, 60, 40, e7, 9d, 54, 45, 82, d4, 3e, 40, c9, 65, e8, 47, ff, 61, 60, 40, 6d, d6, cf, 51, 7d, d4, 3e, 40, c1, 8f, bb, 1f, ff, 61, 60, 40, 7f, e1, f5, 96, 7b, d4, 3e, 40, 27, 14, ab, d1, fe, 61, 60, 40, a4, 33, 2b, 39, 79, d4, 3e, 40, 73, 72, e7, 0b, ff, 61, 60, 40, fe, 88, 0b, b2, 78, d4, 3e, 40, 40, 55, c9, 23, ff, 61, 60, 40, c9, a2, ff, d1, 76, d4, 3e, 40, ea, 72, 3b, 15, ff, 61, 60, 40, 06, 2a, 2a, b5, 73, d4, 3e, 40, 95, 22, 31, f5, fe, 61, 60, 40, a3, e9, ec, 64, 70, d4, 3e, 40, ae, 26, ad, ca, fe, 61, 60, 40, e0, aa, 55, 35, 6d, d4, 3e, 40, c7, e6, 0b, a3, fe, 61, 60, 40, f3, 04, 76, c0, 6b, d4, 3e, 40, 91, 5b, 84, 43, fe, 61, 60, 40, 82, f6, f2, 54, 6a, d4, 3e, 40, f1, 2d, fd, 62, fe, 61, 60, 40, 17, 74, 4a, 59, 69, d4, 3e, 40, b5, d6, 40, 88, fe, 61, 60, 40, 60, 7f, c3, 58, 67, d4, 3e, 40, 70, f3, 0a, 67, fe, 61, 60, 40, f1, 9d, 98, f5, 62, d4, 3e, 40, 6f, 50, 14, 4b, fe, 61, 60, 40, c9, 68, 33, 3a, 5f, d4, 3e, 40, 9b, 3a, fc, 26, fe, 61, 60, 40, fa, 0c, 5c, a9, 5d, d4, 3e, 40, bc, 91, 02, d6, fd, 61, 60, 40, ee, d7, 07, d3, 5c, d4, 3e, 40, 6e, 02, ff, a0, fd, 61, 60, 40, c5, 09, b1, d2, 5b, d4, 3e, 40, 29, 47, 45, bf, fd, 61, 60, 40, 20, db, e3, 99, 58, d4, 3e, 40, 29, 30, 3c, ad, fd, 61, 60, 40, 6e, 5d, 7b, b0, 56, d4, 3e, 40, a7, cf, b0, 46, fd, 61, 60, 40, e7, 29, bc, 15, 53, d4, 3e, 40, 3f, cc, e4, ef, fc, 61, 60, 40, 8f, 9f, 28, 31, 51, d4, 3e, 40, a5, 83, 7e, 71, fc, 61, 60, 40, 71, 8f, 5e, 3a, 51, d4, 3e, 40, 14, ed, 2a, a4, fc, 61, 60, 40, c7, 85, bc, 4b, 4d, d4, 3e, 40, a5, 06, 21, c0, fc, 61, 60, 40, 09, b6, 60, 95, 4a, d4, 3e, 40, fa, d3, 88, 91, fc, 61, 60, 40, 23, a1, e6, 11, 47, d4, 3e, 40, f0, 16, 48, 50, fc, 61, 60, 40, d8, 5b, 60, 15, 43, d4, 3e, 40, d5, a4, 62, 03, fc, 61, 60, 40, 08, 6c, 22, c1, 41, d4, 3e, 40, 98, 25, 2a, e9, fb, 61, 60, 40, 5c, 7a, 94, 47, 40, d4, 3e, 40, c6, 5d, 27, 6f, fc, 61, 60, 40, b1, a2, 4d, a8, 3c, d4, 3e, 40, a3, ad, dd, 85, fc, 61, 60, 40, 59, 03, fe, 6a, 3a, d4, 3e, 40, d7, 6d, f2, 89, fc, 61, 60, 40, f6, a5, 21, 24, 37, d4, 3e, 40, c6, 5d, 27, 6f, fc, 61, 60, 40, bb, f9, fc, e6, 34, d4, 3e, 40, 9a, 7e, 6d, 23, fc, 61, 60, 40, c8, c7, 42, 02, 33, d4, 3e, 40, 11, 6e, 4e, ff, fb, 61, 60, 40, db, 21, 63, 8d, 31, d4, 3e, 40, 83, d3, 34, 88, fc, 61, 60, 40, 95, d4, 50, 3d, 2e, d4, 3e, 40, 60, 23, eb, 9e, fc, 61, 60, 40, 22, b1, 42, 69, 26, d4, 3e, 40, 72, c1, 86, 98, fc, 61, 60, 40, 5f, 55, 0c, 43, 23, d4, 3e, 40, e8, 0d, 71, 58, fc, 61, 60, 40, 84, c4, e0, db, 20, d4, 3e, 40, 33, a1, 3a, 37, fc, 61, 60, 40, 4a, 18, bc, 9e, 1e, d4, 3e, 40, 2c, 63, d6, 9a, fc, 61, 60, 40, 20, 67, 04, 95, 1d, d4, 3e, 40, 9c, fd, 49, c8, fc, 61, 60, 40, 6f, 38, 96, f1, 1b, d4, 3e, 40, 9c, fd, 49, c8, fc, 61, 60, 40, d8, 04, 67, dc, 13, d4, 3e, 40, c7, 5b, 44, 9a, fc, 61, 60, 40, 9e, 75, e1, 95, 11, d4, 3e, 40, 24, a4, b2, 84, fc, 61, 60, 40, c8, 53, f1, a7, 0f, d4, 3e, 40, 62, d7, 54, e8, fc, 61, 60, 40, c3, 78, 1c, f2, 0e, d4, 3e, 40, a7, cf, b0, 46, fd, 61, 60, 40, 5f, 38, df, a1, 0b, d4, 3e, 40, 98, 16, 46, 91, fd, 61, 60, 40, 2c, 45, ed, dc, 06, d4, 3e, 40, 7e, 6f, d3, 9f, fd, 61, 60, 40, 51, 97, 22, 7f, 04, d4, 3e, 40, b1, 8c, f1, 87, fd, 61, 60, 40, 24, d6, e2, 53, 00, d4, 3e, 40, a2, 6d, 32, 33, fe, 61, 60, 40, bf, fc, b3, be, ff, d3, 3e, 40, e1, 8d, 7e, 94, fe, 61, 60, 40, 07, 25, cc, b4, fd, d3, 3e, 40, 8c, 76, 63, e1, fe, 61, 60, 40, fd, 1c, d0, e6, f9, d3, 3e, 40, 73, 00, b8, ea, fe, 61, 60, 40, 7c, 92, 8a, b2, f6, d3, 3e, 40, 61, 62, 1c, f1, fe, 61, 60, 40, 1e, 2d, 22, 18, f4, d3, 3e, 40, 52, 4e, 8b, 2c, ff, 61, 60, 40, d1, d7, e2, 09, f3, d3, 3e, 40, 88, 8f, 5f, aa, ff, 61, 60, 40, b9, e7, 59, 46, f3, d3, 3e, 40, 6f, 41, 30, f3, ff, 61, 60, 40, 78, 16, 69, 54, f3, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, d9, bf, 4a, 23, f3, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 7d, 2c, db, 87, 8d, d3, 3e, 40, b6, 9d, 86, 0d, fe, 61, 60, 40, 5a, f8, 9c, e9, 95, d3, 3e, 40, 98, e3, 9b, c1, fd, 61, 60, 40, 6e, 8a, a1, 7e, 92, d3, 3e, 40, d5, 84, 40, 70, fd, 61, 60, 40, 7c, 7c, 0d, 84, 8e, d3, 3e, 40, 3d, 51, 35, 21, fd, 61, 60, 40, 02, 16, e7, 12, 88, d3, 3e, 40, 7c, c5, 2f, c9, fc, 61, 60, 40, e1, cb, be, d7, 82, d3, 3e, 40, 17, e0, 09, 5d, fc, 61, 60, 40, 36, 42, c5, 8c, 7e, d3, 3e, 40, 0a, 4c, 8b, 20, fc, 61, 60, 40, 08, 2c, 02, 61, 7c, d3, 3e, 40, d3, 34, e1, c4, fb, 61, 60, 40, 0a, 88, 0c, 55, 78, d3, 3e, 40, 1c, fa, b7, 63, fb, 61, 60, 40, 59, 2e, ca, 55, 74, d3, 3e, 40, df, 96, eb, 14, fb, 61, 60, 40, 9c, 49, b2, 46, 71, d3, 3e, 40, 6c, 17, 47, a3, fa, 61, 60, 40, 80, b4, 8d, ac, 6d, d3, 3e, 40, 03, be, b7, f6, f9, 61, 60, 40, ca, 7f, 76, f7, 68, d3, 3e, 40, 49, 9b, dd, 6c, f9, 61, 60, 40, 2b, f9, 75, 33, 65, d3, 3e, 40, 26, a9, 8e, 4d, f9, 61, 60, 40, 03, c7, 68, 94, 63, d3, 3e, 40, ff, 48, a1, c4, f8, 61, 60, 40, 95, cd, 7a, 03, 5c, d3, 3e, 40, 30, 38, 43, a4, f8, 61, 60, 40, 43, b9, 58, 44, 59, d3, 3e, 40, 59, 04, 85, 95, f8, 61, 60, 40, 80, 2d, 72, f9, 56, d3, 3e, 40, f1, 56, 7c, 94, f8, 61, 60, 40, 70, fe, fe, 4b, 54, d3, 3e, 40, 30, c3, 93, 9e, f8, 61, 60, 40, 2a, 6d, 7e, b7, 51, d3, 3e, 40, d9, c4, 64, d2, f8, 61, 60, 40, 08, 7e, da, fc, 4d, d3, 3e, 40, 4b, ee, 10, fc, f8, 61, 60, 40, 27, 30, 77, 0f, 4b, d3, 3e, 40, 42, 9d, e0, 05, fb, 61, 60, 40, 3c, 48, ad, 02, 45, d3, 3e, 40, 72, 8e, 65, ba, fa, 61, 60, 40, 4a, a5, 05, 53, 40, d3, 3e, 40, 27, e9, e3, 9f, fe, 61, 60, 40, 0f, f2, 63, 94, 33, d3, 3e, 40, 6e, ff, 2a, bd, ff, 61, 60, 40, 5c, b3, 83, b4, 40, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 1c, f6, 0d, 1a, 4a, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 01, ef, 6b, 8f, 3d, d3, 3e, 40, e4, 43, 32, df, ff, 61, 60, 40, f8, fd, 7a, 09, 3e, d3, 3e, 40, b2, 28, e7, e9, fe, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9a, 99, 99, 99, 99, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9a, 99, 99, 99, 99, 61, 60, 40, 55, 55, 55, 55, 55, d5, 3e, 40, ed, bb, 4a, 83, fd, 61, 60, 40, 55, 55, 55, 55, 55, d5, 3e, 40, 18, c5, 4a, 83, fd, 61, 60, 40, d4, e6, 53, 55, 55, d5, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, 06, 00, 00, 00, 00, 00, 00, 00, b2, 3d, 52, fb, ff, 61, 60, 40, 71, b6, 8b, f9, 51, d3, 3e, 40, 19, bd, 13, e6, ff, 61, 60, 40, 7d, 38, 2f, eb, 51, d3, 3e, 40, 9a, 6f, 00, da, ff, 61, 60, 40, fd, c6, 79, 2d, 58, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, e4, ec, 07, 2a, 5b, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 6b, cd, 17, 37, 50, d3, 3e, 40, b2, 3d, 52, fb, ff, 61, 60, 40, 71, b6, 8b, f9, 51, d3, 3e, 40\n8793c611-f562-4c3b-8ecd-d11cbae432ad\n2\n1\n2\n\n\nb081e2be-6516-4385-a20a-61c2c04eaaa8\n3\n2\n05, 04, 00, 00, 00, 00, 00, 00, 99, 09, 03, 43, 88, 88, f6, 41, cd, 0c, 03, 43, 9a, 99, f6, 41, 05, 00, 00, 00, 03, 00, 00, 00, 02, 00, 00, 00, 01, 00, 00, 00, 2b, 00, 00, 00, 00, 00, 00, 00, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, a9, 3d, 8f, cb, 34, 61, 60, 40, 41, 9a, 6a, 93, 2e, d3, 3e, 40, 3a, d4, e2, 98, 34, 61, 60, 40, 90, 6b, fc, ef, 2c, d3, 3e, 40, 4a, 9c, dd, a6, 34, 61, 60, 40, 01, 13, 71, a0, 2b, d3, 3e, 40, cc, eb, f5, df, 34, 61, 60, 40, 73, 9d, 46, 5a, 2a, d3, 3e, 40, 98, 2b, e1, db, 34, 61, 60, 40, e2, 61, 8b, f1, 22, d3, 3e, 40, c1, b3, 3a, c2, 34, 61, 60, 40, e9, 37, 5d, 60, 20, d3, 3e, 40, 42, 38, e0, 9f, 34, 61, 60, 40, 61, ba, a6, c6, 1f, d3, 3e, 40, 9f, 0e, 1f, 69, 34, 61, 60, 40, f0, c8, c2, 51, 1e, d3, 3e, 40, b8, 29, a4, 50, 34, 61, 60, 40, 80, 31, 07, b3, 1c, d3, 3e, 40, da, a6, 43, 6a, 34, 61, 60, 40, 16, cc, fd, ad, 1b, d3, 3e, 40, 8f, 6e, a0, 9a, 34, 61, 60, 40, 9f, 14, 01, dc, 19, d3, 3e, 40, fe, 0a, f7, 9c, 34, 61, 60, 40, 32, 43, 8f, 8a, 12, d3, 3e, 40, 0e, c2, 7e, 7d, 34, 61, 60, 40, 93, bd, 3b, b8, 0f, d3, 3e, 40, c0, c0, 4b, 27, 34, 61, 60, 40, 58, 4b, 55, 68, 0d, d3, 3e, 40, c0, 0a, ff, 08, 34, 61, 60, 40, 47, cf, 92, 9f, 0b, d3, 3e, 40, 37, 22, 5c, 24, 34, 61, 60, 40, cc, 3c, c1, 17, 09, d3, 3e, 40, 84, 0e, 69, 3d, 34, 61, 60, 40, f6, 37, 70, 20, 07, d3, 3e, 40, f3, 4f, 99, 30, 34, 61, 60, 40, 64, b2, bd, b8, 02, d3, 3e, 40, bf, 34, 5e, 1d, 34, 61, 60, 40, 00, e9, 47, 35, ff, d2, 3e, 40, e1, 89, 81, f7, 33, 61, 60, 40, ae, 24, cd, ad, fd, d2, 3e, 40, cf, 51, 3a, 9d, 33, 61, 60, 40, 38, 01, 37, 9f, fb, d2, 3e, 40, 26, ea, 14, ca, 33, 61, 60, 40, 63, 19, 85, 9e, f9, d2, 3e, 40, 1d, 3c, 64, e1, 33, 61, 60, 40, 4c, 11, e8, 65, f7, d2, 3e, 40, 36, b2, 0f, d8, 33, 61, 60, 40, a1, cd, 07, 8a, f3, d2, 3e, 40, fa, 8d, fd, cc, 33, 61, 60, 40, d3, ed, 82, 47, ef, d2, 3e, 40, 79, e1, db, af, 33, 61, 60, 40, e0, 0a, c3, a8, ed, d2, 3e, 40, 12, 81, 06, 75, 33, 61, 60, 40, 8d, f7, 4d, db, eb, d2, 3e, 40, 23, ee, da, 73, 33, 61, 60, 40, dc, 5c, 46, fb, e9, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, cb, 4c, 1d, 6f, e8, d2, 3e, 40, ce, 7b, 99, b1, 33, 61, 60, 40, 7f, 90, cf, a5, e4, d2, 3e, 40, ad, a1, f0, b3, 33, 61, 60, 40, 17, 9a, 32, 0a, de, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 66, 88, 63, 5d, dc, d2, 3e, 40, 9a, 93, 08, 6e, 33, 61, 60, 40, 90, a0, b1, 5c, da, d2, 3e, 40, 1a, e5, 03, 7c, 33, 61, 60, 40, fc, 9e, ac, a6, d8, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, 92, 1c, 04, ab, d7, d2, 3e, 40, 79, 86, b5, a0, 33, 61, 60, 40, 60, 73, 09, e5, cf, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 0d, 92, ef, 66, ce, d2, 3e, 40, 23, 93, b4, 64, 33, 61, 60, 40, 61, 0c, fb, 29, cd, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9d, e9, 32, f0, 34, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, 06, 00, 00, 00, 00, 00, 00, 00, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, d6, 5c, f4, 69, 33, 61, 60, 40, 05, f1, 89, 8e, c7, d2, 3e, 40, de, f3, 9b, 40, 33, 61, 60, 40, 56, ba, c0, 87, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 89, b2, 46, 02, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, a5, 00, 00, 00, 00, 00, 00, 00, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40, cf, b3, 0b, 66, 39, 61, 60, 40, 66, da, b7, c0, 89, d2, 3e, 40, 15, e3, 73, 5c, 3a, 61, 60, 40, 3e, 88, b3, 0e, 86, d2, 3e, 40, 21, bd, ab, 95, 3e, 61, 60, 40, 68, ea, 29, fd, 7a, d2, 3e, 40, 3d, da, af, 70, 3f, 61, 60, 40, f7, 0d, 02, e1, 79, d2, 3e, 40, 25, 48, 0f, bd, 41, 61, 60, 40, a1, 7e, 6b, b5, 74, d2, 3e, 40, df, 70, 70, 00, 45, 61, 60, 40, 91, 61, 5c, 44, 70, d2, 3e, 40, b0, 5b, 13, f1, 47, 61, 60, 40, 3f, 6b, 86, 6d, 6e, d2, 3e, 40, 14, b1, ca, 7d, 49, 61, 60, 40, 39, a5, 6d, 10, 6e, d2, 3e, 40, c3, d2, 8b, 9d, 4a, 61, 60, 40, 2d, 21, 1f, f4, 6c, d2, 3e, 40, ee, fd, 87, a0, 4c, 61, 60, 40, c8, 96, ea, a4, 6c, d2, 3e, 40, 7f, 61, 31, 9e, 4c, 61, 60, 40, fd, 2d, fc, 3e, 6e, d2, 3e, 40, 6e, 79, e2, c2, 4c, 61, 60, 40, 72, 07, 9b, 4e, 73, d2, 3e, 40, 8a, 1c, 8f, 0a, 4d, 61, 60, 40, 04, 70, ae, bf, 77, d2, 3e, 40, fb, 28, 32, 59, 4d, 61, 60, 40, d4, e8, 24, 47, 79, d2, 3e, 40, 8c, 6a, a4, b4, 4d, 61, 60, 40, 26, fc, 99, 14, 7b, d2, 3e, 40, 85, 37, 6e, a8, 4d, 61, 60, 40, 5b, 39, 83, d8, 7c, d2, 3e, 40, 3f, d9, bd, aa, 4d, 61, 60, 40, b9, d0, 46, c2, 7f, d2, 3e, 40, fd, a7, 0e, fe, 4d, 61, 60, 40, e6, ae, 25, e4, 83, d2, 3e, 40, 8e, 8e, 5a, 4a, 4e, 61, 60, 40, 6d, e2, e4, 7e, 87, d2, 3e, 40, d4, 14, 87, 87, 4e, 61, 60, 40, b4, ab, 49, 1d, 88, d2, 3e, 40, 56, 8c, 1b, 00, 4f, 61, 60, 40, 43, 04, d5, 6c, 89, d2, 3e, 40, de, 9e, 1d, f9, 4e, 61, 60, 40, 76, 6e, 8e, fe, 8d, d2, 3e, 40, b7, df, 53, 99, 4f, 61, 60, 40, 84, d5, 0c, 34, 95, d2, 3e, 40, b7, 07, d0, d8, 4f, 61, 60, 40, 71, 7b, ec, a8, 96, d2, 3e, 40, 4a, 7a, 09, 2f, 50, 61, 60, 40, b9, 0a, 13, 5a, 97, d2, 3e, 40, 06, 18, 93, 87, 50, 61, 60, 40, 36, 53, 75, 1d, 97, d2, 3e, 40, 32, cf, d0, 93, 50, 61, 60, 40, ec, 91, 6b, e2, 9b, d2, 3e, 40, 44, df, 9b, ae, 50, 61, 60, 40, 80, e2, 6a, de, 9d, d2, 3e, 40, 23, 88, 95, ff, 50, 61, 60, 40, 42, 04, 63, 17, a1, d2, 3e, 40, e2, b1, 0c, 62, 51, 61, 60, 40, 3b, 19, d5, 4f, a3, d2, 3e, 40, 6c, 8d, 9e, e1, 51, 61, 60, 40, 05, cc, 32, 7a, a4, d2, 3e, 40, fd, 73, ea, 2d, 52, 61, 60, 40, 70, 1c, 80, 26, a5, d2, 3e, 40, 63, ac, 43, 29, 52, 61, 60, 40, a4, 51, 86, 4c, a7, d2, 3e, 40, 1f, 7f, 5a, 26, 52, 61, 60, 40, 85, a8, ca, 10, aa, d2, 3e, 40, d4, eb, 90, 47, 52, 61, 60, 40, d2, 2f, 65, 6e, ab, d2, 3e, 40, 18, 41, f6, 89, 52, 61, 60, 40, b3, 4c, 6b, 45, ae, d2, 3e, 40, a4, a8, 75, 13, 53, 61, 60, 40, a4, 6e, 6c, 18, b3, d2, 3e, 40, f2, 4e, 82, 5a, 53, 61, 60, 40, 51, c2, 05, 06, b4, d2, 3e, 40, d1, 52, a2, ba, 53, 61, 60, 40, d9, 71, 17, ef, b4, d2, 3e, 40, 15, 25, 65, ae, 53, 61, 60, 40, f6, b6, 94, 51, b7, d2, 3e, 40, 49, 9b, c6, d0, 53, 61, 60, 40, 8f, b0, 0d, b4, b9, d2, 3e, 40, a1, 8c, e4, 37, 54, 61, 60, 40, 3f, 46, 8a, 12, be, d2, 3e, 40, d5, 2a, c2, 99, 54, 61, 60, 40, c1, 3c, 69, 83, c1, d2, 3e, 40, f0, 41, 81, d7, 54, 61, 60, 40, f0, 47, a0, ad, c2, d2, 3e, 40, 02, 30, 15, 50, 55, 61, 60, 40, dd, ed, 7f, 22, c4, d2, 3e, 40, 25, 83, 55, 55, 55, 61, 60, 40, d1, a6, 18, 57, c4, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 0c, f9, b6, 4c, c5, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 17, 60, 66, 72, c6, d2, 3e, 40, 14, 9b, 06, 7a, 55, 61, 60, 40, 16, e4, 13, 24, c9, d2, 3e, 40, 39, 47, 8a, b9, 55, 61, 60, 40, 74, b5, 15, fb, cb, d2, 3e, 40, ec, 38, 46, fe, 55, 61, 60, 40, 78, 31, 37, 59, cf, d2, 3e, 40, c3, 7d, 42, 48, 56, 61, 60, 40, 12, bf, 16, 7f, d1, d2, 3e, 40, 67, 00, 47, b9, 56, 61, 60, 40, 95, 76, b4, bb, d1, d2, 3e, 40, 0a, 2a, 08, f0, 56, 61, 60, 40, 9a, bd, 22, ae, d2, d2, 3e, 40, 34, 55, 58, f2, 56, 61, 60, 40, 7b, da, 28, 85, d5, d2, 3e, 40, 60, 67, bc, 0d, 57, 61, 60, 40, b5, 86, 4d, c2, d7, d2, 3e, 40, 0d, 4e, be, 85, 57, 61, 60, 40, 01, 92, 95, d1, db, d2, 3e, 40, e3, 37, 94, c0, 57, 61, 60, 40, d0, 0a, 0c, 59, dd, d2, 3e, 40, 28, 8d, f9, 02, 58, 61, 60, 40, f9, d8, 62, 59, de, d2, 3e, 40, ab, 90, 7b, 85, 58, 61, 60, 40, be, b0, eb, cd, de, d2, 3e, 40, 44, 80, 9e, c9, 58, 61, 60, 40, e7, 12, a9, 91, df, d2, 3e, 40, d7, 6f, 35, d1, 58, 61, 60, 40, 2d, f4, 21, a5, e2, d2, 3e, 40, 72, 46, 6c, 2e, 59, 61, 60, 40, 26, 01, b1, 3f, e5, d2, 3e, 40, 6b, 08, 08, 92, 59, 61, 60, 40, a1, 1c, bb, fa, e7, d2, 3e, 40, 8e, de, ea, e5, 59, 61, 60, 40, 59, d7, 03, 0e, ea, d2, 3e, 40, f8, df, d3, 67, 5a, 61, 60, 40, ed, f5, a7, ba, eb, d2, 3e, 40, bd, 3c, 81, d6, 5a, 61, 60, 40, f2, 64, e3, 33, ec, d2, 3e, 40, ef, 26, f5, ee, 5a, 61, 60, 40, a4, b0, f0, cd, ed, d2, 3e, 40, 60, 0b, 1c, fe, 5a, 61, 60, 40, 14, 0e, 6e, 7f, ef, d2, 3e, 40, ea, e6, ad, 7d, 5b, 61, 60, 40, 54, b2, 06, 69, f2, d2, 3e, 40, 88, 80, 64, b6, 5b, 61, 60, 40, b3, 6e, 9d, 2e, f4, d2, 3e, 40, c9, ff, 05, fe, 57, 61, 60, 40, ec, 63, 7a, cc, 04, d3, 3e, 40, 8f, 4e, c0, 23, 58, 61, 60, 40, 74, 0a, 5a, 97, 06, d3, 3e, 40, 9e, 1a, 6f, f8, 5b, 61, 60, 40, c6, 88, bb, af, 07, d3, 3e, 40, 7e, 5b, a8, e3, 5e, 61, 60, 40, 26, 9a, df, e7, fa, d2, 3e, 40, ec, a3, 0c, 82, 62, 61, 60, 40, 01, e7, 92, 98, 0a, d3, 3e, 40, cd, 7c, 20, dc, 61, 61, 60, 40, 66, 5b, a5, 72, 15, d3, 3e, 40, 42, 02, c5, 16, 62, 61, 60, 40, 87, 88, 24, 45, 27, d3, 3e, 40, 06, 40, 51, e3, 62, 61, 60, 40, 81, df, aa, de, 26, d3, 3e, 40, 9a, a7, 17, d5, 62, 61, 60, 40, c5, 50, 14, 09, 22, d3, 3e, 40, 14, 98, ca, b2, 62, 61, 60, 40, a7, 23, ab, 1b, 22, d3, 3e, 40, 38, 1e, b5, 87, 62, 61, 60, 40, f5, 44, ee, 21, 15, d3, 3e, 40, 77, 15, ea, 9a, 6d, 61, 60, 40, 92, b5, fd, 60, 1d, d3, 3e, 40, 96, 1c, 3d, 9f, 6d, 61, 60, 40, 58, 1b, ec, 18, 1a, d3, 3e, 40, 2d, 0e, 0e, c6, 6d, 61, 60, 40, fc, 94, 51, cf, fc, d2, 3e, 40, ab, d7, 04, 83, 71, 61, 60, 40, 0b, d2, 6b, 37, ea, d2, 3e, 40, be, 5e, 85, 87, 75, 61, 60, 40, 13, 1a, 35, 1d, 09, d3, 3e, 40, 4e, a0, c2, f0, 75, 61, 60, 40, 85, fa, 3a, c9, 06, d3, 3e, 40, 83, cc, 3d, 40, 71, 61, 60, 40, 15, 21, 38, d8, e2, d2, 3e, 40, 77, 39, 27, 21, 71, 61, 60, 40, 7b, 57, 1e, 53, e0, d2, 3e, 40, fc, 1f, 90, e6, 70, 61, 60, 40, ca, 0a, 64, c7, dd, d2, 3e, 40, 96, ea, b6, cf, 70, 61, 60, 40, ee, 06, 18, 30, dd, d2, 3e, 40, e2, 78, 1d, f5, 70, 61, 60, 40, 95, c2, 4c, 73, dc, d2, 3e, 40, be, 50, a4, 21, 71, 61, 60, 40, d1, 60, 8d, 92, db, d2, 3e, 40, aa, f3, 70, 4e, 71, 61, 60, 40, a7, 3c, b5, 58, dc, d2, 3e, 40, 5e, 10, f4, c4, 71, 61, 60, 40, 70, 04, 7e, 94, de, d2, 3e, 40, 08, 4c, 1c, 83, 72, 61, 60, 40, f1, 85, 91, aa, e2, d2, 3e, 40, b3, 48, 8a, fd, 72, 61, 60, 40, f6, a8, d9, b2, e5, d2, 3e, 40, d4, 73, 93, 97, 73, 61, 60, 40, a0, e4, d6, f0, e9, d2, 3e, 40, 82, bb, 86, f5, 73, 61, 60, 40, 45, b0, eb, c3, ec, d2, 3e, 40, 07, 07, 0e, 77, 74, 61, 60, 40, 73, 99, a7, 2e, f1, d2, 3e, 40, d8, 77, 27, 9a, 75, 61, 60, 40, 5c, 2e, 48, 1e, fb, d2, 3e, 40, a2, 6b, 57, cf, 76, 61, 60, 40, 2d, 55, b3, 18, 06, d3, 3e, 40, 67, 12, b8, 1f, 77, 61, 60, 40, 67, 1f, 75, 85, 08, d3, 3e, 40, f2, dc, 0b, 64, 77, 61, 60, 40, ef, bb, 75, 40, 0a, d3, 3e, 40, 6e, f9, 22, 83, 77, 61, 60, 40, 65, 06, db, 9c, 0b, d3, 3e, 40, f1, a3, 96, bd, 77, 61, 60, 40, 8e, 1a, 4b, 0c, 0d, d3, 3e, 40, 43, 35, d4, 11, 78, 61, 60, 40, 46, b6, f8, b6, 0e, d3, 3e, 40, 37, 69, 48, 4c, 78, 61, 60, 40, 0a, 54, f2, f6, 0f, d3, 3e, 40, 83, 94, da, b6, 78, 61, 60, 40, 8c, 21, f9, 7d, 11, d3, 3e, 40, 0f, ff, d9, 24, 79, 61, 60, 40, 32, 2f, d1, 6a, 12, d3, 3e, 40, 52, 98, f2, 7f, 79, 61, 60, 40, c7, 69, 67, 1c, 13, d3, 3e, 40, 79, 42, c8, dc, 79, 61, 60, 40, 25, f4, f3, d9, 13, d3, 3e, 40, 3e, 60, bb, 07, 7a, 61, 60, 40, 7f, 4a, 81, 44, 14, d3, 3e, 40, ed, 66, 46, 3f, 7a, 61, 60, 40, fb, 94, 8e, 32, 15, d3, 3e, 40, f1, 0a, 0f, 5d, 7a, 61, 60, 40, 87, 5f, b4, 3e, 1a, d3, 3e, 40, 70, 83, 60, a9, 7c, 61, 60, 40, be, 5c, af, dc, 17, d3, 3e, 40, 26, 51, bd, a2, 7c, 61, 60, 40, 5a, 6e, 15, 36, 16, d3, 3e, 40, ea, b2, 98, d8, 7c, 61, 60, 40, 7e, b9, c3, e4, 15, d3, 3e, 40, c0, fc, f7, db, 7c, 61, 60, 40, ee, 8b, 0c, f1, 16, d3, 3e, 40, 75, 03, da, 5d, 7d, 61, 60, 40, 1f, 58, b1, 9f, 15, d3, 3e, 40, ee, 07, 16, 69, 7d, 61, 60, 40, 38, b9, 00, b0, 11, d3, 3e, 40, 4f, f1, 01, 7f, 7d, 61, 60, 40, 27, 02, b1, 79, 10, d3, 3e, 40, e4, 3c, 5c, a5, 7d, 61, 60, 40, 93, 27, 2a, ca, 0e, d3, 3e, 40, d1, 98, c0, e2, 7d, 61, 60, 40, 11, 34, a3, 75, 0d, d3, 3e, 40, ea, a6, 34, 65, 7e, 61, 60, 40, 24, 3e, cb, 81, 0b, d3, 3e, 40, f5, 06, 6c, c2, 7e, 61, 60, 40, ef, 1e, 2e, a6, 0a, d3, 3e, 40, 31, 83, 24, f8, 7e, 61, 60, 40, 71, 07, 29, f9, 0a, d3, 3e, 40, be, 9a, e0, f4, 7e, 61, 60, 40, d0, c2, 12, cd, 0b, d3, 3e, 40, 50, 62, fb, bc, 7e, 61, 60, 40, 1d, a2, 88, 47, 0d, d3, 3e, 40, 5f, fc, 79, c2, 7e, 61, 60, 40, 6a, a9, cb, 48, 0e, d3, 3e, 40, 44, 38, fe, f5, 7e, 61, 60, 40, ed, c1, 76, c0, 0d, d3, 3e, 40, 2e, 05, f5, 44, 7f, 61, 60, 40, 23, a3, 2e, 12, 0d, d3, 3e, 40, 7f, 95, ca, 7b, 7f, 61, 60, 40, 7c, 83, 43, 30, 0d, d3, 3e, 40, c2, 61, 8d, a6, 7f, 61, 60, 40, 1d, 98, a9, 37, 0d, d3, 3e, 40, 3c, a5, 83, f5, 7f, 61, 60, 40, 35, 80, 4f, 5d, 0d, d3, 3e, 40, 6b, d2, 42, 09, 80, 61, 60, 40, 7b, b7, 49, aa, 0e, d3, 3e, 40, 4d, 8a, 87, de, 7f, 61, 60, 40, c2, 2a, 2d, 0f, 11, d3, 3e, 40, a6, 75, 95, 9a, 7f, 61, 60, 40, 0e, 3d, fc, 11, 13, d3, 3e, 40, 16, 34, 58, 31, 7f, 61, 60, 40, 90, 3b, 0f, 68, 15, d3, 3e, 40, 2a, fa, 5f, 88, 7e, 61, 60, 40, 3b, 89, 1f, 9b, 18, d3, 3e, 40, bf, 09, 80, 4f, 7e, 61, 60, 40, 88, 86, 83, 8c, 19, d3, 3e, 40, e5, f0, 90, fc, 7d, 61, 60, 40, e1, 5c, 68, 53, 1a, d3, 3e, 40, 69, a9, b2, ab, 7d, 61, 60, 40, ab, 2c, b6, bb, 1a, d3, 3e, 40, e9, 8b, c9, 8a, 7d, 61, 60, 40, b1, 36, 3d, 5d, 1a, d3, 3e, 40, cc, 57, cc, 7f, 7d, 61, 60, 40, 7c, 5c, bb, b7, 19, d3, 3e, 40, 5b, d1, 16, 72, 7d, 61, 60, 40, 0b, 64, a1, 96, 19, d3, 3e, 40, 33, 40, 62, ee, 7c, 61, 60, 40, b6, cd, a3, ce, 1b, d3, 3e, 40, c9, 1f, 58, ca, 7c, 61, 60, 40, 38, 07, 44, 92, 1d, d3, 3e, 40, 78, 2e, c6, 9f, 7c, 61, 60, 40, 1a, 97, 6a, 99, 1e, d3, 3e, 40, 0a, b8, 91, f9, 7a, 61, 60, 40, 34, ed, 23, 36, 26, d3, 3e, 40, b9, 6e, 24, b2, 7a, 61, 60, 40, 33, aa, 62, e3, 27, d3, 3e, 40, fd, fb, 96, 7d, 7a, 61, 60, 40, fc, f1, d3, c2, 29, d3, 3e, 40, 78, bf, 6a, 62, 7a, 61, 60, 40, 6d, 4f, 51, 74, 2b, d3, 3e, 40, d8, 66, 1c, 50, 7a, 61, 60, 40, b9, ce, b7, ec, 2d, d3, 3e, 40, 81, 84, 59, 4f, 7a, 61, 60, 40, 6b, 4b, 22, 9d, 2f, d3, 3e, 40, 71, 8c, 34, 56, 7a, 61, 60, 40, 9f, 6c, 6a, a3, 31, d3, 3e, 40, e6, 44, 83, 60, 7a, 61, 60, 40, f8, 93, 45, 22, 33, d3, 3e, 40, f2, 21, 4d, 61, 7a, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 1c, c3, 6a, f2, ad, d2, 3e, 40, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40\n7016f78f-a818-4d0f-bcb3-ad6686d64cdb\n3\n2\n1\n\n\nb081e2be-6516-4385-a20a-61c2c04eaaa8\n3\n2\n05, 04, 00, 00, 00, 00, 00, 00, 99, 09, 03, 43, 88, 88, f6, 41, cd, 0c, 03, 43, 9a, 99, f6, 41, 05, 00, 00, 00, 03, 00, 00, 00, 02, 00, 00, 00, 01, 00, 00, 00, 2b, 00, 00, 00, 00, 00, 00, 00, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, a9, 3d, 8f, cb, 34, 61, 60, 40, 41, 9a, 6a, 93, 2e, d3, 3e, 40, 3a, d4, e2, 98, 34, 61, 60, 40, 90, 6b, fc, ef, 2c, d3, 3e, 40, 4a, 9c, dd, a6, 34, 61, 60, 40, 01, 13, 71, a0, 2b, d3, 3e, 40, cc, eb, f5, df, 34, 61, 60, 40, 73, 9d, 46, 5a, 2a, d3, 3e, 40, 98, 2b, e1, db, 34, 61, 60, 40, e2, 61, 8b, f1, 22, d3, 3e, 40, c1, b3, 3a, c2, 34, 61, 60, 40, e9, 37, 5d, 60, 20, d3, 3e, 40, 42, 38, e0, 9f, 34, 61, 60, 40, 61, ba, a6, c6, 1f, d3, 3e, 40, 9f, 0e, 1f, 69, 34, 61, 60, 40, f0, c8, c2, 51, 1e, d3, 3e, 40, b8, 29, a4, 50, 34, 61, 60, 40, 80, 31, 07, b3, 1c, d3, 3e, 40, da, a6, 43, 6a, 34, 61, 60, 40, 16, cc, fd, ad, 1b, d3, 3e, 40, 8f, 6e, a0, 9a, 34, 61, 60, 40, 9f, 14, 01, dc, 19, d3, 3e, 40, fe, 0a, f7, 9c, 34, 61, 60, 40, 32, 43, 8f, 8a, 12, d3, 3e, 40, 0e, c2, 7e, 7d, 34, 61, 60, 40, 93, bd, 3b, b8, 0f, d3, 3e, 40, c0, c0, 4b, 27, 34, 61, 60, 40, 58, 4b, 55, 68, 0d, d3, 3e, 40, c0, 0a, ff, 08, 34, 61, 60, 40, 47, cf, 92, 9f, 0b, d3, 3e, 40, 37, 22, 5c, 24, 34, 61, 60, 40, cc, 3c, c1, 17, 09, d3, 3e, 40, 84, 0e, 69, 3d, 34, 61, 60, 40, f6, 37, 70, 20, 07, d3, 3e, 40, f3, 4f, 99, 30, 34, 61, 60, 40, 64, b2, bd, b8, 02, d3, 3e, 40, bf, 34, 5e, 1d, 34, 61, 60, 40, 00, e9, 47, 35, ff, d2, 3e, 40, e1, 89, 81, f7, 33, 61, 60, 40, ae, 24, cd, ad, fd, d2, 3e, 40, cf, 51, 3a, 9d, 33, 61, 60, 40, 38, 01, 37, 9f, fb, d2, 3e, 40, 26, ea, 14, ca, 33, 61, 60, 40, 63, 19, 85, 9e, f9, d2, 3e, 40, 1d, 3c, 64, e1, 33, 61, 60, 40, 4c, 11, e8, 65, f7, d2, 3e, 40, 36, b2, 0f, d8, 33, 61, 60, 40, a1, cd, 07, 8a, f3, d2, 3e, 40, fa, 8d, fd, cc, 33, 61, 60, 40, d3, ed, 82, 47, ef, d2, 3e, 40, 79, e1, db, af, 33, 61, 60, 40, e0, 0a, c3, a8, ed, d2, 3e, 40, 12, 81, 06, 75, 33, 61, 60, 40, 8d, f7, 4d, db, eb, d2, 3e, 40, 23, ee, da, 73, 33, 61, 60, 40, dc, 5c, 46, fb, e9, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, cb, 4c, 1d, 6f, e8, d2, 3e, 40, ce, 7b, 99, b1, 33, 61, 60, 40, 7f, 90, cf, a5, e4, d2, 3e, 40, ad, a1, f0, b3, 33, 61, 60, 40, 17, 9a, 32, 0a, de, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 66, 88, 63, 5d, dc, d2, 3e, 40, 9a, 93, 08, 6e, 33, 61, 60, 40, 90, a0, b1, 5c, da, d2, 3e, 40, 1a, e5, 03, 7c, 33, 61, 60, 40, fc, 9e, ac, a6, d8, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, 92, 1c, 04, ab, d7, d2, 3e, 40, 79, 86, b5, a0, 33, 61, 60, 40, 60, 73, 09, e5, cf, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 0d, 92, ef, 66, ce, d2, 3e, 40, 23, 93, b4, 64, 33, 61, 60, 40, 61, 0c, fb, 29, cd, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9d, e9, 32, f0, 34, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, 06, 00, 00, 00, 00, 00, 00, 00, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, d6, 5c, f4, 69, 33, 61, 60, 40, 05, f1, 89, 8e, c7, d2, 3e, 40, de, f3, 9b, 40, 33, 61, 60, 40, 56, ba, c0, 87, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 89, b2, 46, 02, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, a5, 00, 00, 00, 00, 00, 00, 00, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40, cf, b3, 0b, 66, 39, 61, 60, 40, 66, da, b7, c0, 89, d2, 3e, 40, 15, e3, 73, 5c, 3a, 61, 60, 40, 3e, 88, b3, 0e, 86, d2, 3e, 40, 21, bd, ab, 95, 3e, 61, 60, 40, 68, ea, 29, fd, 7a, d2, 3e, 40, 3d, da, af, 70, 3f, 61, 60, 40, f7, 0d, 02, e1, 79, d2, 3e, 40, 25, 48, 0f, bd, 41, 61, 60, 40, a1, 7e, 6b, b5, 74, d2, 3e, 40, df, 70, 70, 00, 45, 61, 60, 40, 91, 61, 5c, 44, 70, d2, 3e, 40, b0, 5b, 13, f1, 47, 61, 60, 40, 3f, 6b, 86, 6d, 6e, d2, 3e, 40, 14, b1, ca, 7d, 49, 61, 60, 40, 39, a5, 6d, 10, 6e, d2, 3e, 40, c3, d2, 8b, 9d, 4a, 61, 60, 40, 2d, 21, 1f, f4, 6c, d2, 3e, 40, ee, fd, 87, a0, 4c, 61, 60, 40, c8, 96, ea, a4, 6c, d2, 3e, 40, 7f, 61, 31, 9e, 4c, 61, 60, 40, fd, 2d, fc, 3e, 6e, d2, 3e, 40, 6e, 79, e2, c2, 4c, 61, 60, 40, 72, 07, 9b, 4e, 73, d2, 3e, 40, 8a, 1c, 8f, 0a, 4d, 61, 60, 40, 04, 70, ae, bf, 77, d2, 3e, 40, fb, 28, 32, 59, 4d, 61, 60, 40, d4, e8, 24, 47, 79, d2, 3e, 40, 8c, 6a, a4, b4, 4d, 61, 60, 40, 26, fc, 99, 14, 7b, d2, 3e, 40, 85, 37, 6e, a8, 4d, 61, 60, 40, 5b, 39, 83, d8, 7c, d2, 3e, 40, 3f, d9, bd, aa, 4d, 61, 60, 40, b9, d0, 46, c2, 7f, d2, 3e, 40, fd, a7, 0e, fe, 4d, 61, 60, 40, e6, ae, 25, e4, 83, d2, 3e, 40, 8e, 8e, 5a, 4a, 4e, 61, 60, 40, 6d, e2, e4, 7e, 87, d2, 3e, 40, d4, 14, 87, 87, 4e, 61, 60, 40, b4, ab, 49, 1d, 88, d2, 3e, 40, 56, 8c, 1b, 00, 4f, 61, 60, 40, 43, 04, d5, 6c, 89, d2, 3e, 40, de, 9e, 1d, f9, 4e, 61, 60, 40, 76, 6e, 8e, fe, 8d, d2, 3e, 40, b7, df, 53, 99, 4f, 61, 60, 40, 84, d5, 0c, 34, 95, d2, 3e, 40, b7, 07, d0, d8, 4f, 61, 60, 40, 71, 7b, ec, a8, 96, d2, 3e, 40, 4a, 7a, 09, 2f, 50, 61, 60, 40, b9, 0a, 13, 5a, 97, d2, 3e, 40, 06, 18, 93, 87, 50, 61, 60, 40, 36, 53, 75, 1d, 97, d2, 3e, 40, 32, cf, d0, 93, 50, 61, 60, 40, ec, 91, 6b, e2, 9b, d2, 3e, 40, 44, df, 9b, ae, 50, 61, 60, 40, 80, e2, 6a, de, 9d, d2, 3e, 40, 23, 88, 95, ff, 50, 61, 60, 40, 42, 04, 63, 17, a1, d2, 3e, 40, e2, b1, 0c, 62, 51, 61, 60, 40, 3b, 19, d5, 4f, a3, d2, 3e, 40, 6c, 8d, 9e, e1, 51, 61, 60, 40, 05, cc, 32, 7a, a4, d2, 3e, 40, fd, 73, ea, 2d, 52, 61, 60, 40, 70, 1c, 80, 26, a5, d2, 3e, 40, 63, ac, 43, 29, 52, 61, 60, 40, a4, 51, 86, 4c, a7, d2, 3e, 40, 1f, 7f, 5a, 26, 52, 61, 60, 40, 85, a8, ca, 10, aa, d2, 3e, 40, d4, eb, 90, 47, 52, 61, 60, 40, d2, 2f, 65, 6e, ab, d2, 3e, 40, 18, 41, f6, 89, 52, 61, 60, 40, b3, 4c, 6b, 45, ae, d2, 3e, 40, a4, a8, 75, 13, 53, 61, 60, 40, a4, 6e, 6c, 18, b3, d2, 3e, 40, f2, 4e, 82, 5a, 53, 61, 60, 40, 51, c2, 05, 06, b4, d2, 3e, 40, d1, 52, a2, ba, 53, 61, 60, 40, d9, 71, 17, ef, b4, d2, 3e, 40, 15, 25, 65, ae, 53, 61, 60, 40, f6, b6, 94, 51, b7, d2, 3e, 40, 49, 9b, c6, d0, 53, 61, 60, 40, 8f, b0, 0d, b4, b9, d2, 3e, 40, a1, 8c, e4, 37, 54, 61, 60, 40, 3f, 46, 8a, 12, be, d2, 3e, 40, d5, 2a, c2, 99, 54, 61, 60, 40, c1, 3c, 69, 83, c1, d2, 3e, 40, f0, 41, 81, d7, 54, 61, 60, 40, f0, 47, a0, ad, c2, d2, 3e, 40, 02, 30, 15, 50, 55, 61, 60, 40, dd, ed, 7f, 22, c4, d2, 3e, 40, 25, 83, 55, 55, 55, 61, 60, 40, d1, a6, 18, 57, c4, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 0c, f9, b6, 4c, c5, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 17, 60, 66, 72, c6, d2, 3e, 40, 14, 9b, 06, 7a, 55, 61, 60, 40, 16, e4, 13, 24, c9, d2, 3e, 40, 39, 47, 8a, b9, 55, 61, 60, 40, 74, b5, 15, fb, cb, d2, 3e, 40, ec, 38, 46, fe, 55, 61, 60, 40, 78, 31, 37, 59, cf, d2, 3e, 40, c3, 7d, 42, 48, 56, 61, 60, 40, 12, bf, 16, 7f, d1, d2, 3e, 40, 67, 00, 47, b9, 56, 61, 60, 40, 95, 76, b4, bb, d1, d2, 3e, 40, 0a, 2a, 08, f0, 56, 61, 60, 40, 9a, bd, 22, ae, d2, d2, 3e, 40, 34, 55, 58, f2, 56, 61, 60, 40, 7b, da, 28, 85, d5, d2, 3e, 40, 60, 67, bc, 0d, 57, 61, 60, 40, b5, 86, 4d, c2, d7, d2, 3e, 40, 0d, 4e, be, 85, 57, 61, 60, 40, 01, 92, 95, d1, db, d2, 3e, 40, e3, 37, 94, c0, 57, 61, 60, 40, d0, 0a, 0c, 59, dd, d2, 3e, 40, 28, 8d, f9, 02, 58, 61, 60, 40, f9, d8, 62, 59, de, d2, 3e, 40, ab, 90, 7b, 85, 58, 61, 60, 40, be, b0, eb, cd, de, d2, 3e, 40, 44, 80, 9e, c9, 58, 61, 60, 40, e7, 12, a9, 91, df, d2, 3e, 40, d7, 6f, 35, d1, 58, 61, 60, 40, 2d, f4, 21, a5, e2, d2, 3e, 40, 72, 46, 6c, 2e, 59, 61, 60, 40, 26, 01, b1, 3f, e5, d2, 3e, 40, 6b, 08, 08, 92, 59, 61, 60, 40, a1, 1c, bb, fa, e7, d2, 3e, 40, 8e, de, ea, e5, 59, 61, 60, 40, 59, d7, 03, 0e, ea, d2, 3e, 40, f8, df, d3, 67, 5a, 61, 60, 40, ed, f5, a7, ba, eb, d2, 3e, 40, bd, 3c, 81, d6, 5a, 61, 60, 40, f2, 64, e3, 33, ec, d2, 3e, 40, ef, 26, f5, ee, 5a, 61, 60, 40, a4, b0, f0, cd, ed, d2, 3e, 40, 60, 0b, 1c, fe, 5a, 61, 60, 40, 14, 0e, 6e, 7f, ef, d2, 3e, 40, ea, e6, ad, 7d, 5b, 61, 60, 40, 54, b2, 06, 69, f2, d2, 3e, 40, 88, 80, 64, b6, 5b, 61, 60, 40, b3, 6e, 9d, 2e, f4, d2, 3e, 40, c9, ff, 05, fe, 57, 61, 60, 40, ec, 63, 7a, cc, 04, d3, 3e, 40, 8f, 4e, c0, 23, 58, 61, 60, 40, 74, 0a, 5a, 97, 06, d3, 3e, 40, 9e, 1a, 6f, f8, 5b, 61, 60, 40, c6, 88, bb, af, 07, d3, 3e, 40, 7e, 5b, a8, e3, 5e, 61, 60, 40, 26, 9a, df, e7, fa, d2, 3e, 40, ec, a3, 0c, 82, 62, 61, 60, 40, 01, e7, 92, 98, 0a, d3, 3e, 40, cd, 7c, 20, dc, 61, 61, 60, 40, 66, 5b, a5, 72, 15, d3, 3e, 40, 42, 02, c5, 16, 62, 61, 60, 40, 87, 88, 24, 45, 27, d3, 3e, 40, 06, 40, 51, e3, 62, 61, 60, 40, 81, df, aa, de, 26, d3, 3e, 40, 9a, a7, 17, d5, 62, 61, 60, 40, c5, 50, 14, 09, 22, d3, 3e, 40, 14, 98, ca, b2, 62, 61, 60, 40, a7, 23, ab, 1b, 22, d3, 3e, 40, 38, 1e, b5, 87, 62, 61, 60, 40, f5, 44, ee, 21, 15, d3, 3e, 40, 77, 15, ea, 9a, 6d, 61, 60, 40, 92, b5, fd, 60, 1d, d3, 3e, 40, 96, 1c, 3d, 9f, 6d, 61, 60, 40, 58, 1b, ec, 18, 1a, d3, 3e, 40, 2d, 0e, 0e, c6, 6d, 61, 60, 40, fc, 94, 51, cf, fc, d2, 3e, 40, ab, d7, 04, 83, 71, 61, 60, 40, 0b, d2, 6b, 37, ea, d2, 3e, 40, be, 5e, 85, 87, 75, 61, 60, 40, 13, 1a, 35, 1d, 09, d3, 3e, 40, 4e, a0, c2, f0, 75, 61, 60, 40, 85, fa, 3a, c9, 06, d3, 3e, 40, 83, cc, 3d, 40, 71, 61, 60, 40, 15, 21, 38, d8, e2, d2, 3e, 40, 77, 39, 27, 21, 71, 61, 60, 40, 7b, 57, 1e, 53, e0, d2, 3e, 40, fc, 1f, 90, e6, 70, 61, 60, 40, ca, 0a, 64, c7, dd, d2, 3e, 40, 96, ea, b6, cf, 70, 61, 60, 40, ee, 06, 18, 30, dd, d2, 3e, 40, e2, 78, 1d, f5, 70, 61, 60, 40, 95, c2, 4c, 73, dc, d2, 3e, 40, be, 50, a4, 21, 71, 61, 60, 40, d1, 60, 8d, 92, db, d2, 3e, 40, aa, f3, 70, 4e, 71, 61, 60, 40, a7, 3c, b5, 58, dc, d2, 3e, 40, 5e, 10, f4, c4, 71, 61, 60, 40, 70, 04, 7e, 94, de, d2, 3e, 40, 08, 4c, 1c, 83, 72, 61, 60, 40, f1, 85, 91, aa, e2, d2, 3e, 40, b3, 48, 8a, fd, 72, 61, 60, 40, f6, a8, d9, b2, e5, d2, 3e, 40, d4, 73, 93, 97, 73, 61, 60, 40, a0, e4, d6, f0, e9, d2, 3e, 40, 82, bb, 86, f5, 73, 61, 60, 40, 45, b0, eb, c3, ec, d2, 3e, 40, 07, 07, 0e, 77, 74, 61, 60, 40, 73, 99, a7, 2e, f1, d2, 3e, 40, d8, 77, 27, 9a, 75, 61, 60, 40, 5c, 2e, 48, 1e, fb, d2, 3e, 40, a2, 6b, 57, cf, 76, 61, 60, 40, 2d, 55, b3, 18, 06, d3, 3e, 40, 67, 12, b8, 1f, 77, 61, 60, 40, 67, 1f, 75, 85, 08, d3, 3e, 40, f2, dc, 0b, 64, 77, 61, 60, 40, ef, bb, 75, 40, 0a, d3, 3e, 40, 6e, f9, 22, 83, 77, 61, 60, 40, 65, 06, db, 9c, 0b, d3, 3e, 40, f1, a3, 96, bd, 77, 61, 60, 40, 8e, 1a, 4b, 0c, 0d, d3, 3e, 40, 43, 35, d4, 11, 78, 61, 60, 40, 46, b6, f8, b6, 0e, d3, 3e, 40, 37, 69, 48, 4c, 78, 61, 60, 40, 0a, 54, f2, f6, 0f, d3, 3e, 40, 83, 94, da, b6, 78, 61, 60, 40, 8c, 21, f9, 7d, 11, d3, 3e, 40, 0f, ff, d9, 24, 79, 61, 60, 40, 32, 2f, d1, 6a, 12, d3, 3e, 40, 52, 98, f2, 7f, 79, 61, 60, 40, c7, 69, 67, 1c, 13, d3, 3e, 40, 79, 42, c8, dc, 79, 61, 60, 40, 25, f4, f3, d9, 13, d3, 3e, 40, 3e, 60, bb, 07, 7a, 61, 60, 40, 7f, 4a, 81, 44, 14, d3, 3e, 40, ed, 66, 46, 3f, 7a, 61, 60, 40, fb, 94, 8e, 32, 15, d3, 3e, 40, f1, 0a, 0f, 5d, 7a, 61, 60, 40, 87, 5f, b4, 3e, 1a, d3, 3e, 40, 70, 83, 60, a9, 7c, 61, 60, 40, be, 5c, af, dc, 17, d3, 3e, 40, 26, 51, bd, a2, 7c, 61, 60, 40, 5a, 6e, 15, 36, 16, d3, 3e, 40, ea, b2, 98, d8, 7c, 61, 60, 40, 7e, b9, c3, e4, 15, d3, 3e, 40, c0, fc, f7, db, 7c, 61, 60, 40, ee, 8b, 0c, f1, 16, d3, 3e, 40, 75, 03, da, 5d, 7d, 61, 60, 40, 1f, 58, b1, 9f, 15, d3, 3e, 40, ee, 07, 16, 69, 7d, 61, 60, 40, 38, b9, 00, b0, 11, d3, 3e, 40, 4f, f1, 01, 7f, 7d, 61, 60, 40, 27, 02, b1, 79, 10, d3, 3e, 40, e4, 3c, 5c, a5, 7d, 61, 60, 40, 93, 27, 2a, ca, 0e, d3, 3e, 40, d1, 98, c0, e2, 7d, 61, 60, 40, 11, 34, a3, 75, 0d, d3, 3e, 40, ea, a6, 34, 65, 7e, 61, 60, 40, 24, 3e, cb, 81, 0b, d3, 3e, 40, f5, 06, 6c, c2, 7e, 61, 60, 40, ef, 1e, 2e, a6, 0a, d3, 3e, 40, 31, 83, 24, f8, 7e, 61, 60, 40, 71, 07, 29, f9, 0a, d3, 3e, 40, be, 9a, e0, f4, 7e, 61, 60, 40, d0, c2, 12, cd, 0b, d3, 3e, 40, 50, 62, fb, bc, 7e, 61, 60, 40, 1d, a2, 88, 47, 0d, d3, 3e, 40, 5f, fc, 79, c2, 7e, 61, 60, 40, 6a, a9, cb, 48, 0e, d3, 3e, 40, 44, 38, fe, f5, 7e, 61, 60, 40, ed, c1, 76, c0, 0d, d3, 3e, 40, 2e, 05, f5, 44, 7f, 61, 60, 40, 23, a3, 2e, 12, 0d, d3, 3e, 40, 7f, 95, ca, 7b, 7f, 61, 60, 40, 7c, 83, 43, 30, 0d, d3, 3e, 40, c2, 61, 8d, a6, 7f, 61, 60, 40, 1d, 98, a9, 37, 0d, d3, 3e, 40, 3c, a5, 83, f5, 7f, 61, 60, 40, 35, 80, 4f, 5d, 0d, d3, 3e, 40, 6b, d2, 42, 09, 80, 61, 60, 40, 7b, b7, 49, aa, 0e, d3, 3e, 40, 4d, 8a, 87, de, 7f, 61, 60, 40, c2, 2a, 2d, 0f, 11, d3, 3e, 40, a6, 75, 95, 9a, 7f, 61, 60, 40, 0e, 3d, fc, 11, 13, d3, 3e, 40, 16, 34, 58, 31, 7f, 61, 60, 40, 90, 3b, 0f, 68, 15, d3, 3e, 40, 2a, fa, 5f, 88, 7e, 61, 60, 40, 3b, 89, 1f, 9b, 18, d3, 3e, 40, bf, 09, 80, 4f, 7e, 61, 60, 40, 88, 86, 83, 8c, 19, d3, 3e, 40, e5, f0, 90, fc, 7d, 61, 60, 40, e1, 5c, 68, 53, 1a, d3, 3e, 40, 69, a9, b2, ab, 7d, 61, 60, 40, ab, 2c, b6, bb, 1a, d3, 3e, 40, e9, 8b, c9, 8a, 7d, 61, 60, 40, b1, 36, 3d, 5d, 1a, d3, 3e, 40, cc, 57, cc, 7f, 7d, 61, 60, 40, 7c, 5c, bb, b7, 19, d3, 3e, 40, 5b, d1, 16, 72, 7d, 61, 60, 40, 0b, 64, a1, 96, 19, d3, 3e, 40, 33, 40, 62, ee, 7c, 61, 60, 40, b6, cd, a3, ce, 1b, d3, 3e, 40, c9, 1f, 58, ca, 7c, 61, 60, 40, 38, 07, 44, 92, 1d, d3, 3e, 40, 78, 2e, c6, 9f, 7c, 61, 60, 40, 1a, 97, 6a, 99, 1e, d3, 3e, 40, 0a, b8, 91, f9, 7a, 61, 60, 40, 34, ed, 23, 36, 26, d3, 3e, 40, b9, 6e, 24, b2, 7a, 61, 60, 40, 33, aa, 62, e3, 27, d3, 3e, 40, fd, fb, 96, 7d, 7a, 61, 60, 40, fc, f1, d3, c2, 29, d3, 3e, 40, 78, bf, 6a, 62, 7a, 61, 60, 40, 6d, 4f, 51, 74, 2b, d3, 3e, 40, d8, 66, 1c, 50, 7a, 61, 60, 40, b9, ce, b7, ec, 2d, d3, 3e, 40, 81, 84, 59, 4f, 7a, 61, 60, 40, 6b, 4b, 22, 9d, 2f, d3, 3e, 40, 71, 8c, 34, 56, 7a, 61, 60, 40, 9f, 6c, 6a, a3, 31, d3, 3e, 40, e6, 44, 83, 60, 7a, 61, 60, 40, f8, 93, 45, 22, 33, d3, 3e, 40, f2, 21, 4d, 61, 7a, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 1c, c3, 6a, f2, ad, d2, 3e, 40, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40\n7016f78f-a818-4d0f-bcb3-ad6686d64cdb\n3\n2\n2\n\n\nb081e2be-6516-4385-a20a-61c2c04eaaa8\n3\n2\n05, 04, 00, 00, 00, 00, 00, 00, 99, 09, 03, 43, 88, 88, f6, 41, cd, 0c, 03, 43, 9a, 99, f6, 41, 05, 00, 00, 00, 03, 00, 00, 00, 02, 00, 00, 00, 01, 00, 00, 00, 2b, 00, 00, 00, 00, 00, 00, 00, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, a9, 3d, 8f, cb, 34, 61, 60, 40, 41, 9a, 6a, 93, 2e, d3, 3e, 40, 3a, d4, e2, 98, 34, 61, 60, 40, 90, 6b, fc, ef, 2c, d3, 3e, 40, 4a, 9c, dd, a6, 34, 61, 60, 40, 01, 13, 71, a0, 2b, d3, 3e, 40, cc, eb, f5, df, 34, 61, 60, 40, 73, 9d, 46, 5a, 2a, d3, 3e, 40, 98, 2b, e1, db, 34, 61, 60, 40, e2, 61, 8b, f1, 22, d3, 3e, 40, c1, b3, 3a, c2, 34, 61, 60, 40, e9, 37, 5d, 60, 20, d3, 3e, 40, 42, 38, e0, 9f, 34, 61, 60, 40, 61, ba, a6, c6, 1f, d3, 3e, 40, 9f, 0e, 1f, 69, 34, 61, 60, 40, f0, c8, c2, 51, 1e, d3, 3e, 40, b8, 29, a4, 50, 34, 61, 60, 40, 80, 31, 07, b3, 1c, d3, 3e, 40, da, a6, 43, 6a, 34, 61, 60, 40, 16, cc, fd, ad, 1b, d3, 3e, 40, 8f, 6e, a0, 9a, 34, 61, 60, 40, 9f, 14, 01, dc, 19, d3, 3e, 40, fe, 0a, f7, 9c, 34, 61, 60, 40, 32, 43, 8f, 8a, 12, d3, 3e, 40, 0e, c2, 7e, 7d, 34, 61, 60, 40, 93, bd, 3b, b8, 0f, d3, 3e, 40, c0, c0, 4b, 27, 34, 61, 60, 40, 58, 4b, 55, 68, 0d, d3, 3e, 40, c0, 0a, ff, 08, 34, 61, 60, 40, 47, cf, 92, 9f, 0b, d3, 3e, 40, 37, 22, 5c, 24, 34, 61, 60, 40, cc, 3c, c1, 17, 09, d3, 3e, 40, 84, 0e, 69, 3d, 34, 61, 60, 40, f6, 37, 70, 20, 07, d3, 3e, 40, f3, 4f, 99, 30, 34, 61, 60, 40, 64, b2, bd, b8, 02, d3, 3e, 40, bf, 34, 5e, 1d, 34, 61, 60, 40, 00, e9, 47, 35, ff, d2, 3e, 40, e1, 89, 81, f7, 33, 61, 60, 40, ae, 24, cd, ad, fd, d2, 3e, 40, cf, 51, 3a, 9d, 33, 61, 60, 40, 38, 01, 37, 9f, fb, d2, 3e, 40, 26, ea, 14, ca, 33, 61, 60, 40, 63, 19, 85, 9e, f9, d2, 3e, 40, 1d, 3c, 64, e1, 33, 61, 60, 40, 4c, 11, e8, 65, f7, d2, 3e, 40, 36, b2, 0f, d8, 33, 61, 60, 40, a1, cd, 07, 8a, f3, d2, 3e, 40, fa, 8d, fd, cc, 33, 61, 60, 40, d3, ed, 82, 47, ef, d2, 3e, 40, 79, e1, db, af, 33, 61, 60, 40, e0, 0a, c3, a8, ed, d2, 3e, 40, 12, 81, 06, 75, 33, 61, 60, 40, 8d, f7, 4d, db, eb, d2, 3e, 40, 23, ee, da, 73, 33, 61, 60, 40, dc, 5c, 46, fb, e9, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, cb, 4c, 1d, 6f, e8, d2, 3e, 40, ce, 7b, 99, b1, 33, 61, 60, 40, 7f, 90, cf, a5, e4, d2, 3e, 40, ad, a1, f0, b3, 33, 61, 60, 40, 17, 9a, 32, 0a, de, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 66, 88, 63, 5d, dc, d2, 3e, 40, 9a, 93, 08, 6e, 33, 61, 60, 40, 90, a0, b1, 5c, da, d2, 3e, 40, 1a, e5, 03, 7c, 33, 61, 60, 40, fc, 9e, ac, a6, d8, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, 92, 1c, 04, ab, d7, d2, 3e, 40, 79, 86, b5, a0, 33, 61, 60, 40, 60, 73, 09, e5, cf, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 0d, 92, ef, 66, ce, d2, 3e, 40, 23, 93, b4, 64, 33, 61, 60, 40, 61, 0c, fb, 29, cd, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9d, e9, 32, f0, 34, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, 06, 00, 00, 00, 00, 00, 00, 00, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, d6, 5c, f4, 69, 33, 61, 60, 40, 05, f1, 89, 8e, c7, d2, 3e, 40, de, f3, 9b, 40, 33, 61, 60, 40, 56, ba, c0, 87, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 89, b2, 46, 02, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, a5, 00, 00, 00, 00, 00, 00, 00, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40, cf, b3, 0b, 66, 39, 61, 60, 40, 66, da, b7, c0, 89, d2, 3e, 40, 15, e3, 73, 5c, 3a, 61, 60, 40, 3e, 88, b3, 0e, 86, d2, 3e, 40, 21, bd, ab, 95, 3e, 61, 60, 40, 68, ea, 29, fd, 7a, d2, 3e, 40, 3d, da, af, 70, 3f, 61, 60, 40, f7, 0d, 02, e1, 79, d2, 3e, 40, 25, 48, 0f, bd, 41, 61, 60, 40, a1, 7e, 6b, b5, 74, d2, 3e, 40, df, 70, 70, 00, 45, 61, 60, 40, 91, 61, 5c, 44, 70, d2, 3e, 40, b0, 5b, 13, f1, 47, 61, 60, 40, 3f, 6b, 86, 6d, 6e, d2, 3e, 40, 14, b1, ca, 7d, 49, 61, 60, 40, 39, a5, 6d, 10, 6e, d2, 3e, 40, c3, d2, 8b, 9d, 4a, 61, 60, 40, 2d, 21, 1f, f4, 6c, d2, 3e, 40, ee, fd, 87, a0, 4c, 61, 60, 40, c8, 96, ea, a4, 6c, d2, 3e, 40, 7f, 61, 31, 9e, 4c, 61, 60, 40, fd, 2d, fc, 3e, 6e, d2, 3e, 40, 6e, 79, e2, c2, 4c, 61, 60, 40, 72, 07, 9b, 4e, 73, d2, 3e, 40, 8a, 1c, 8f, 0a, 4d, 61, 60, 40, 04, 70, ae, bf, 77, d2, 3e, 40, fb, 28, 32, 59, 4d, 61, 60, 40, d4, e8, 24, 47, 79, d2, 3e, 40, 8c, 6a, a4, b4, 4d, 61, 60, 40, 26, fc, 99, 14, 7b, d2, 3e, 40, 85, 37, 6e, a8, 4d, 61, 60, 40, 5b, 39, 83, d8, 7c, d2, 3e, 40, 3f, d9, bd, aa, 4d, 61, 60, 40, b9, d0, 46, c2, 7f, d2, 3e, 40, fd, a7, 0e, fe, 4d, 61, 60, 40, e6, ae, 25, e4, 83, d2, 3e, 40, 8e, 8e, 5a, 4a, 4e, 61, 60, 40, 6d, e2, e4, 7e, 87, d2, 3e, 40, d4, 14, 87, 87, 4e, 61, 60, 40, b4, ab, 49, 1d, 88, d2, 3e, 40, 56, 8c, 1b, 00, 4f, 61, 60, 40, 43, 04, d5, 6c, 89, d2, 3e, 40, de, 9e, 1d, f9, 4e, 61, 60, 40, 76, 6e, 8e, fe, 8d, d2, 3e, 40, b7, df, 53, 99, 4f, 61, 60, 40, 84, d5, 0c, 34, 95, d2, 3e, 40, b7, 07, d0, d8, 4f, 61, 60, 40, 71, 7b, ec, a8, 96, d2, 3e, 40, 4a, 7a, 09, 2f, 50, 61, 60, 40, b9, 0a, 13, 5a, 97, d2, 3e, 40, 06, 18, 93, 87, 50, 61, 60, 40, 36, 53, 75, 1d, 97, d2, 3e, 40, 32, cf, d0, 93, 50, 61, 60, 40, ec, 91, 6b, e2, 9b, d2, 3e, 40, 44, df, 9b, ae, 50, 61, 60, 40, 80, e2, 6a, de, 9d, d2, 3e, 40, 23, 88, 95, ff, 50, 61, 60, 40, 42, 04, 63, 17, a1, d2, 3e, 40, e2, b1, 0c, 62, 51, 61, 60, 40, 3b, 19, d5, 4f, a3, d2, 3e, 40, 6c, 8d, 9e, e1, 51, 61, 60, 40, 05, cc, 32, 7a, a4, d2, 3e, 40, fd, 73, ea, 2d, 52, 61, 60, 40, 70, 1c, 80, 26, a5, d2, 3e, 40, 63, ac, 43, 29, 52, 61, 60, 40, a4, 51, 86, 4c, a7, d2, 3e, 40, 1f, 7f, 5a, 26, 52, 61, 60, 40, 85, a8, ca, 10, aa, d2, 3e, 40, d4, eb, 90, 47, 52, 61, 60, 40, d2, 2f, 65, 6e, ab, d2, 3e, 40, 18, 41, f6, 89, 52, 61, 60, 40, b3, 4c, 6b, 45, ae, d2, 3e, 40, a4, a8, 75, 13, 53, 61, 60, 40, a4, 6e, 6c, 18, b3, d2, 3e, 40, f2, 4e, 82, 5a, 53, 61, 60, 40, 51, c2, 05, 06, b4, d2, 3e, 40, d1, 52, a2, ba, 53, 61, 60, 40, d9, 71, 17, ef, b4, d2, 3e, 40, 15, 25, 65, ae, 53, 61, 60, 40, f6, b6, 94, 51, b7, d2, 3e, 40, 49, 9b, c6, d0, 53, 61, 60, 40, 8f, b0, 0d, b4, b9, d2, 3e, 40, a1, 8c, e4, 37, 54, 61, 60, 40, 3f, 46, 8a, 12, be, d2, 3e, 40, d5, 2a, c2, 99, 54, 61, 60, 40, c1, 3c, 69, 83, c1, d2, 3e, 40, f0, 41, 81, d7, 54, 61, 60, 40, f0, 47, a0, ad, c2, d2, 3e, 40, 02, 30, 15, 50, 55, 61, 60, 40, dd, ed, 7f, 22, c4, d2, 3e, 40, 25, 83, 55, 55, 55, 61, 60, 40, d1, a6, 18, 57, c4, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 0c, f9, b6, 4c, c5, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 17, 60, 66, 72, c6, d2, 3e, 40, 14, 9b, 06, 7a, 55, 61, 60, 40, 16, e4, 13, 24, c9, d2, 3e, 40, 39, 47, 8a, b9, 55, 61, 60, 40, 74, b5, 15, fb, cb, d2, 3e, 40, ec, 38, 46, fe, 55, 61, 60, 40, 78, 31, 37, 59, cf, d2, 3e, 40, c3, 7d, 42, 48, 56, 61, 60, 40, 12, bf, 16, 7f, d1, d2, 3e, 40, 67, 00, 47, b9, 56, 61, 60, 40, 95, 76, b4, bb, d1, d2, 3e, 40, 0a, 2a, 08, f0, 56, 61, 60, 40, 9a, bd, 22, ae, d2, d2, 3e, 40, 34, 55, 58, f2, 56, 61, 60, 40, 7b, da, 28, 85, d5, d2, 3e, 40, 60, 67, bc, 0d, 57, 61, 60, 40, b5, 86, 4d, c2, d7, d2, 3e, 40, 0d, 4e, be, 85, 57, 61, 60, 40, 01, 92, 95, d1, db, d2, 3e, 40, e3, 37, 94, c0, 57, 61, 60, 40, d0, 0a, 0c, 59, dd, d2, 3e, 40, 28, 8d, f9, 02, 58, 61, 60, 40, f9, d8, 62, 59, de, d2, 3e, 40, ab, 90, 7b, 85, 58, 61, 60, 40, be, b0, eb, cd, de, d2, 3e, 40, 44, 80, 9e, c9, 58, 61, 60, 40, e7, 12, a9, 91, df, d2, 3e, 40, d7, 6f, 35, d1, 58, 61, 60, 40, 2d, f4, 21, a5, e2, d2, 3e, 40, 72, 46, 6c, 2e, 59, 61, 60, 40, 26, 01, b1, 3f, e5, d2, 3e, 40, 6b, 08, 08, 92, 59, 61, 60, 40, a1, 1c, bb, fa, e7, d2, 3e, 40, 8e, de, ea, e5, 59, 61, 60, 40, 59, d7, 03, 0e, ea, d2, 3e, 40, f8, df, d3, 67, 5a, 61, 60, 40, ed, f5, a7, ba, eb, d2, 3e, 40, bd, 3c, 81, d6, 5a, 61, 60, 40, f2, 64, e3, 33, ec, d2, 3e, 40, ef, 26, f5, ee, 5a, 61, 60, 40, a4, b0, f0, cd, ed, d2, 3e, 40, 60, 0b, 1c, fe, 5a, 61, 60, 40, 14, 0e, 6e, 7f, ef, d2, 3e, 40, ea, e6, ad, 7d, 5b, 61, 60, 40, 54, b2, 06, 69, f2, d2, 3e, 40, 88, 80, 64, b6, 5b, 61, 60, 40, b3, 6e, 9d, 2e, f4, d2, 3e, 40, c9, ff, 05, fe, 57, 61, 60, 40, ec, 63, 7a, cc, 04, d3, 3e, 40, 8f, 4e, c0, 23, 58, 61, 60, 40, 74, 0a, 5a, 97, 06, d3, 3e, 40, 9e, 1a, 6f, f8, 5b, 61, 60, 40, c6, 88, bb, af, 07, d3, 3e, 40, 7e, 5b, a8, e3, 5e, 61, 60, 40, 26, 9a, df, e7, fa, d2, 3e, 40, ec, a3, 0c, 82, 62, 61, 60, 40, 01, e7, 92, 98, 0a, d3, 3e, 40, cd, 7c, 20, dc, 61, 61, 60, 40, 66, 5b, a5, 72, 15, d3, 3e, 40, 42, 02, c5, 16, 62, 61, 60, 40, 87, 88, 24, 45, 27, d3, 3e, 40, 06, 40, 51, e3, 62, 61, 60, 40, 81, df, aa, de, 26, d3, 3e, 40, 9a, a7, 17, d5, 62, 61, 60, 40, c5, 50, 14, 09, 22, d3, 3e, 40, 14, 98, ca, b2, 62, 61, 60, 40, a7, 23, ab, 1b, 22, d3, 3e, 40, 38, 1e, b5, 87, 62, 61, 60, 40, f5, 44, ee, 21, 15, d3, 3e, 40, 77, 15, ea, 9a, 6d, 61, 60, 40, 92, b5, fd, 60, 1d, d3, 3e, 40, 96, 1c, 3d, 9f, 6d, 61, 60, 40, 58, 1b, ec, 18, 1a, d3, 3e, 40, 2d, 0e, 0e, c6, 6d, 61, 60, 40, fc, 94, 51, cf, fc, d2, 3e, 40, ab, d7, 04, 83, 71, 61, 60, 40, 0b, d2, 6b, 37, ea, d2, 3e, 40, be, 5e, 85, 87, 75, 61, 60, 40, 13, 1a, 35, 1d, 09, d3, 3e, 40, 4e, a0, c2, f0, 75, 61, 60, 40, 85, fa, 3a, c9, 06, d3, 3e, 40, 83, cc, 3d, 40, 71, 61, 60, 40, 15, 21, 38, d8, e2, d2, 3e, 40, 77, 39, 27, 21, 71, 61, 60, 40, 7b, 57, 1e, 53, e0, d2, 3e, 40, fc, 1f, 90, e6, 70, 61, 60, 40, ca, 0a, 64, c7, dd, d2, 3e, 40, 96, ea, b6, cf, 70, 61, 60, 40, ee, 06, 18, 30, dd, d2, 3e, 40, e2, 78, 1d, f5, 70, 61, 60, 40, 95, c2, 4c, 73, dc, d2, 3e, 40, be, 50, a4, 21, 71, 61, 60, 40, d1, 60, 8d, 92, db, d2, 3e, 40, aa, f3, 70, 4e, 71, 61, 60, 40, a7, 3c, b5, 58, dc, d2, 3e, 40, 5e, 10, f4, c4, 71, 61, 60, 40, 70, 04, 7e, 94, de, d2, 3e, 40, 08, 4c, 1c, 83, 72, 61, 60, 40, f1, 85, 91, aa, e2, d2, 3e, 40, b3, 48, 8a, fd, 72, 61, 60, 40, f6, a8, d9, b2, e5, d2, 3e, 40, d4, 73, 93, 97, 73, 61, 60, 40, a0, e4, d6, f0, e9, d2, 3e, 40, 82, bb, 86, f5, 73, 61, 60, 40, 45, b0, eb, c3, ec, d2, 3e, 40, 07, 07, 0e, 77, 74, 61, 60, 40, 73, 99, a7, 2e, f1, d2, 3e, 40, d8, 77, 27, 9a, 75, 61, 60, 40, 5c, 2e, 48, 1e, fb, d2, 3e, 40, a2, 6b, 57, cf, 76, 61, 60, 40, 2d, 55, b3, 18, 06, d3, 3e, 40, 67, 12, b8, 1f, 77, 61, 60, 40, 67, 1f, 75, 85, 08, d3, 3e, 40, f2, dc, 0b, 64, 77, 61, 60, 40, ef, bb, 75, 40, 0a, d3, 3e, 40, 6e, f9, 22, 83, 77, 61, 60, 40, 65, 06, db, 9c, 0b, d3, 3e, 40, f1, a3, 96, bd, 77, 61, 60, 40, 8e, 1a, 4b, 0c, 0d, d3, 3e, 40, 43, 35, d4, 11, 78, 61, 60, 40, 46, b6, f8, b6, 0e, d3, 3e, 40, 37, 69, 48, 4c, 78, 61, 60, 40, 0a, 54, f2, f6, 0f, d3, 3e, 40, 83, 94, da, b6, 78, 61, 60, 40, 8c, 21, f9, 7d, 11, d3, 3e, 40, 0f, ff, d9, 24, 79, 61, 60, 40, 32, 2f, d1, 6a, 12, d3, 3e, 40, 52, 98, f2, 7f, 79, 61, 60, 40, c7, 69, 67, 1c, 13, d3, 3e, 40, 79, 42, c8, dc, 79, 61, 60, 40, 25, f4, f3, d9, 13, d3, 3e, 40, 3e, 60, bb, 07, 7a, 61, 60, 40, 7f, 4a, 81, 44, 14, d3, 3e, 40, ed, 66, 46, 3f, 7a, 61, 60, 40, fb, 94, 8e, 32, 15, d3, 3e, 40, f1, 0a, 0f, 5d, 7a, 61, 60, 40, 87, 5f, b4, 3e, 1a, d3, 3e, 40, 70, 83, 60, a9, 7c, 61, 60, 40, be, 5c, af, dc, 17, d3, 3e, 40, 26, 51, bd, a2, 7c, 61, 60, 40, 5a, 6e, 15, 36, 16, d3, 3e, 40, ea, b2, 98, d8, 7c, 61, 60, 40, 7e, b9, c3, e4, 15, d3, 3e, 40, c0, fc, f7, db, 7c, 61, 60, 40, ee, 8b, 0c, f1, 16, d3, 3e, 40, 75, 03, da, 5d, 7d, 61, 60, 40, 1f, 58, b1, 9f, 15, d3, 3e, 40, ee, 07, 16, 69, 7d, 61, 60, 40, 38, b9, 00, b0, 11, d3, 3e, 40, 4f, f1, 01, 7f, 7d, 61, 60, 40, 27, 02, b1, 79, 10, d3, 3e, 40, e4, 3c, 5c, a5, 7d, 61, 60, 40, 93, 27, 2a, ca, 0e, d3, 3e, 40, d1, 98, c0, e2, 7d, 61, 60, 40, 11, 34, a3, 75, 0d, d3, 3e, 40, ea, a6, 34, 65, 7e, 61, 60, 40, 24, 3e, cb, 81, 0b, d3, 3e, 40, f5, 06, 6c, c2, 7e, 61, 60, 40, ef, 1e, 2e, a6, 0a, d3, 3e, 40, 31, 83, 24, f8, 7e, 61, 60, 40, 71, 07, 29, f9, 0a, d3, 3e, 40, be, 9a, e0, f4, 7e, 61, 60, 40, d0, c2, 12, cd, 0b, d3, 3e, 40, 50, 62, fb, bc, 7e, 61, 60, 40, 1d, a2, 88, 47, 0d, d3, 3e, 40, 5f, fc, 79, c2, 7e, 61, 60, 40, 6a, a9, cb, 48, 0e, d3, 3e, 40, 44, 38, fe, f5, 7e, 61, 60, 40, ed, c1, 76, c0, 0d, d3, 3e, 40, 2e, 05, f5, 44, 7f, 61, 60, 40, 23, a3, 2e, 12, 0d, d3, 3e, 40, 7f, 95, ca, 7b, 7f, 61, 60, 40, 7c, 83, 43, 30, 0d, d3, 3e, 40, c2, 61, 8d, a6, 7f, 61, 60, 40, 1d, 98, a9, 37, 0d, d3, 3e, 40, 3c, a5, 83, f5, 7f, 61, 60, 40, 35, 80, 4f, 5d, 0d, d3, 3e, 40, 6b, d2, 42, 09, 80, 61, 60, 40, 7b, b7, 49, aa, 0e, d3, 3e, 40, 4d, 8a, 87, de, 7f, 61, 60, 40, c2, 2a, 2d, 0f, 11, d3, 3e, 40, a6, 75, 95, 9a, 7f, 61, 60, 40, 0e, 3d, fc, 11, 13, d3, 3e, 40, 16, 34, 58, 31, 7f, 61, 60, 40, 90, 3b, 0f, 68, 15, d3, 3e, 40, 2a, fa, 5f, 88, 7e, 61, 60, 40, 3b, 89, 1f, 9b, 18, d3, 3e, 40, bf, 09, 80, 4f, 7e, 61, 60, 40, 88, 86, 83, 8c, 19, d3, 3e, 40, e5, f0, 90, fc, 7d, 61, 60, 40, e1, 5c, 68, 53, 1a, d3, 3e, 40, 69, a9, b2, ab, 7d, 61, 60, 40, ab, 2c, b6, bb, 1a, d3, 3e, 40, e9, 8b, c9, 8a, 7d, 61, 60, 40, b1, 36, 3d, 5d, 1a, d3, 3e, 40, cc, 57, cc, 7f, 7d, 61, 60, 40, 7c, 5c, bb, b7, 19, d3, 3e, 40, 5b, d1, 16, 72, 7d, 61, 60, 40, 0b, 64, a1, 96, 19, d3, 3e, 40, 33, 40, 62, ee, 7c, 61, 60, 40, b6, cd, a3, ce, 1b, d3, 3e, 40, c9, 1f, 58, ca, 7c, 61, 60, 40, 38, 07, 44, 92, 1d, d3, 3e, 40, 78, 2e, c6, 9f, 7c, 61, 60, 40, 1a, 97, 6a, 99, 1e, d3, 3e, 40, 0a, b8, 91, f9, 7a, 61, 60, 40, 34, ed, 23, 36, 26, d3, 3e, 40, b9, 6e, 24, b2, 7a, 61, 60, 40, 33, aa, 62, e3, 27, d3, 3e, 40, fd, fb, 96, 7d, 7a, 61, 60, 40, fc, f1, d3, c2, 29, d3, 3e, 40, 78, bf, 6a, 62, 7a, 61, 60, 40, 6d, 4f, 51, 74, 2b, d3, 3e, 40, d8, 66, 1c, 50, 7a, 61, 60, 40, b9, ce, b7, ec, 2d, d3, 3e, 40, 81, 84, 59, 4f, 7a, 61, 60, 40, 6b, 4b, 22, 9d, 2f, d3, 3e, 40, 71, 8c, 34, 56, 7a, 61, 60, 40, 9f, 6c, 6a, a3, 31, d3, 3e, 40, e6, 44, 83, 60, 7a, 61, 60, 40, f8, 93, 45, 22, 33, d3, 3e, 40, f2, 21, 4d, 61, 7a, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 1c, c3, 6a, f2, ad, d2, 3e, 40, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40\n7016f78f-a818-4d0f-bcb3-ad6686d64cdb\n3\n2\n3\n\n\n\n\n\nある行を展開それぞれ別の行数で展開したいときの処理は次となる。\n\n\nCode\nselect \n  i, \n  unnest(generate_series(1, i))\nfrom \n  generate_series(1, 10) tbl(i)\n\n;\n\n\n\nDisplaying records 1 - 10\n\n\ni\nunnest(generate_series(1, i))\n\n\n\n\n1\n1\n\n\n2\n1\n\n\n2\n2\n\n\n3\n1\n\n\n3\n2\n\n\n3\n3\n\n\n4\n1\n\n\n4\n2\n\n\n4\n3\n\n\n4\n4\n\n\n\n\n\n計算結果をGeoJSONに書き出す。時間がかかるので未実行です。\n\n\nCode\nCOPY (\n  select *\n  from ag_pref_intersection\n)\nTO './data/intersection_ag_n03.geojson'\nWITH (FORMAT GDAL, DRIVER 'GeoJSON', SRS '4612')\n;\n\n\n\n\n3 Disconnect\n\n\nCode\ndbDisconnect(con, shutdown=TRUE)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "DuckDB",
      "My Tips",
      "交差演算"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_04_sql.html",
    "href": "contents/sql/duckdb/documentaion/guide_04_sql.html",
    "title": "SQL",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/sql/duckdb/documentation\")\nCode\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\nポスグレのチュートリアルを参考としたSQLのチュートリアルである。",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "SQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_04_sql.html#populating-a-table-with-rows",
    "href": "contents/sql/duckdb/documentaion/guide_04_sql.html#populating-a-table-with-rows",
    "title": "SQL",
    "section": "2.1 Populating a Table with Rows",
    "text": "2.1 Populating a Table with Rows\nINSERTは行単位でデータを追加するときに使われる。\nINSERT INTO weather\nVALUES ('San Francisco', 46, 50, 0.25, '1994-11-27');\n数値でないときにはシングルクォーテションで値をかこむ必要がある。\nINSERT INTO weather\nVALUES ('San Francisco', 46, 50, 0.25, '1994-11-27');\nインサートするときにカラムの順序は異なっていても大丈夫である。\nINSERT INTO weather (date, city, temp_hi, temp_lo)\nVALUES ('1994-11-29', 'Hayward', 54, 37);\n\nMany developers consider explicitly listing the columns better style than relying on the order implicitly.\n\nバルクロードするときにはCOPYを使うことでより高速になる。\nCOPY weather\nFROM 'weather.csv'",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "SQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_04_sql.html#querying-a-table",
    "href": "contents/sql/duckdb/documentaion/guide_04_sql.html#querying-a-table",
    "title": "SQL",
    "section": "2.2 Querying a Table",
    "text": "2.2 Querying a Table\n\n\nCode\nCREATE OR REPLACE TABLE weather as SELECT * FROM read_csv('weather.csv')\n\n\n\n\nCode\nSELECT *\nFROM weather;\n\n\n\n3 records\n\n\ncity\ntemp_lo\ntemp_hi\nprcp\ndate\n\n\n\n\nSan Francisco\n46\n50\n0.25\n1994-11-27\n\n\nSan Francisco\n43\n57\n0.00\n1994-11-29\n\n\nHayward\n37\n54\nNA\n1994-11-29\n\n\n\n\n\n\n\nCode\nSELECT city, (temp_hi + temp_lo) / 2 AS temp_avg, date\nFROM weather;\n\n\n\n3 records\n\n\ncity\ntemp_avg\ndate\n\n\n\n\nSan Francisco\n48.0\n1994-11-27\n\n\nSan Francisco\n50.0\n1994-11-29\n\n\nHayward\n45.5\n1994-11-29\n\n\n\n\n\n\n\nCode\nSELECT *\nFROM weather\nWHERE city = 'San Francisco' AND prcp &gt; 0.0;\n\n\n\n1 records\n\n\ncity\ntemp_lo\ntemp_hi\nprcp\ndate\n\n\n\n\nSan Francisco\n46\n50\n0.25\n1994-11-27\n\n\n\n\n\n\n\nCode\nSELECT DISTINCT city\nFROM weather;\n\n\n\n2 records\n\n\ncity\n\n\n\n\nSan Francisco\n\n\nHayward\n\n\n\n\n\n\n\nCode\nSELECT DISTINCT city\nFROM weather\nORDER BY city;\n\n\n\n2 records\n\n\ncity\n\n\n\n\nHayward\n\n\nSan Francisco",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "SQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_04_sql.html#joins-between-tables",
    "href": "contents/sql/duckdb/documentaion/guide_04_sql.html#joins-between-tables",
    "title": "SQL",
    "section": "2.3 Joins between Tables",
    "text": "2.3 Joins between Tables\n\n\nCode\nCREATE OR REPLACE TABLE cities as SELECT * FROM read_csv('city.csv')\n\n\n２つのテーブルを組合せる方法として、用意したすべての組合せを考えた上でフィルターをかける方法がある。\n\n\nCode\nSELECT *\nFROM weather, cities\n\n\n\n3 records\n\n\n\n\n\n\n\n\n\n\n\n\ncity\ntemp_lo\ntemp_hi\nprcp\ndate\nname\nlat\nlon\n\n\n\n\nSan Francisco\n46\n50\n0.25\n1994-11-27\nSan Francisco\n-194\n53\n\n\nSan Francisco\n43\n57\n0.00\n1994-11-29\nSan Francisco\n-194\n53\n\n\nHayward\n37\n54\nNA\n1994-11-29\nSan Francisco\n-194\n53\n\n\n\n\n\n\n\nCode\nSELECT *\nFROM weather, cities\nWHERE city = name;\n\n\n\n2 records\n\n\n\n\n\n\n\n\n\n\n\n\ncity\ntemp_lo\ntemp_hi\nprcp\ndate\nname\nlat\nlon\n\n\n\n\nSan Francisco\n46\n50\n0.25\n1994-11-27\nSan Francisco\n-194\n53\n\n\nSan Francisco\n43\n57\n0.00\n1994-11-29\nSan Francisco\n-194\n53\n\n\n\n\n\nもしカラム名が被るときには次のようにテーブル名をつける。 見てわかるようにテーブル名にエイリアスを使うことが可能である。\n\n\nCode\nSELECT t.city, t.temp_lo, t.temp_hi, t.prcp, t.date, cities.lon, cities.lat\nFROM weather t, cities\nWHERE cities.name = t.city;\n\n\n\n2 records\n\n\ncity\ntemp_lo\ntemp_hi\nprcp\ndate\nlon\nlat\n\n\n\n\nSan Francisco\n46\n50\n0.25\n1994-11-27\n53\n-194\n\n\nSan Francisco\n43\n57\n0.00\n1994-11-29\n53\n-194\n\n\n\n\n\n左外部結合を考える。\n\n\nCode\nSELECT *\nFROM weather\nLEFT OUTER JOIN cities ON weather.city = cities.name;\n\n\n\n3 records\n\n\n\n\n\n\n\n\n\n\n\n\ncity\ntemp_lo\ntemp_hi\nprcp\ndate\nname\nlat\nlon\n\n\n\n\nSan Francisco\n46\n50\n0.25\n1994-11-27\nSan Francisco\n-194\n53\n\n\nSan Francisco\n43\n57\n0.00\n1994-11-29\nSan Francisco\n-194\n53\n\n\nHayward\n37\n54\nNA\n1994-11-29\nNA\nNA\nNA",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "SQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_04_sql.html#aggregate-functions",
    "href": "contents/sql/duckdb/documentaion/guide_04_sql.html#aggregate-functions",
    "title": "SQL",
    "section": "2.4 Aggregate Functions",
    "text": "2.4 Aggregate Functions\n\ncount, sum, avg, max, min\n\n\n\nCode\nSELECT max(temp_lo)\nFROM weather;\n\n\n\n1 records\n\n\nmax(temp_lo)\n\n\n\n\n46\n\n\n\n\n\n集約関数は次のようにWHERE句で使えない。 これは集約計算の対象がWHEREで絞った結果であるため。\n-- これはエラーになる\nSELECT city\nFROM weather\nWHERE temp_lo = max(temp_lo);     -- WRONG\nWHERE句で使うにはスカラー値をクエリする必要がある。サブクエリにすることで独立したクエリとして計算が行える。。\n\n\nCode\nSELECT city\nFROM weather\nWHERE temp_lo = (SELECT max(temp_lo) FROM weather);\n\n\n\n1 records\n\n\ncity\n\n\n\n\nSan Francisco\n\n\n\n\n\n\n\nCode\nSELECT city, max(temp_lo)\nFROM weather\nGROUP BY city;\n\n\n\n2 records\n\n\ncity\nmax(temp_lo)\n\n\n\n\nHayward\n37\n\n\nSan Francisco\n46\n\n\n\n\n\n計算結果に対してフィルターすることが可能である。\n\n\nCode\nSELECT city, max(temp_lo) as A\nFROM weather\nGROUP BY city\nHAVING A &lt; 40;\n\n\n\n1 records\n\n\ncity\nA\n\n\n\n\nHayward\n37\n\n\n\n\n\n\n\nCode\nSELECT city, max(temp_lo)\nFROM weather\nWHERE city LIKE 'S%'            -- (1)\nGROUP BY city\nHAVING max(temp_lo) &lt; 40;\n\n\n\n0 records\n\n\ncity\nmax(temp_lo)\n\n\n\n\n\n\n\nHAVINGは常に集約値を使うことに注意する。",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "SQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_04_sql.html#updates",
    "href": "contents/sql/duckdb/documentaion/guide_04_sql.html#updates",
    "title": "SQL",
    "section": "2.5 Updates",
    "text": "2.5 Updates\nUPDATEコマンドを使うことで、既存の行を更新することができる。\n\n\nCode\nUPDATE weather\nSET temp_hi = temp_hi - 2,  temp_lo = temp_lo - 2\nWHERE date &gt; '1994-11-28';\n\n\n\n\nCode\nSELECT *\nFROM weather;\n\n\n\n3 records\n\n\ncity\ntemp_lo\ntemp_hi\nprcp\ndate\n\n\n\n\nSan Francisco\n46\n50\n0.25\n1994-11-27\n\n\nSan Francisco\n41\n55\n0.00\n1994-11-29\n\n\nHayward\n35\n52\nNA\n1994-11-29",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "SQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_04_sql.html#deletions",
    "href": "contents/sql/duckdb/documentaion/guide_04_sql.html#deletions",
    "title": "SQL",
    "section": "2.6 Deletions",
    "text": "2.6 Deletions\n\n\nCode\nDELETE FROM weather\nWHERE city = 'Hayward';\n\n\n\n\nCode\nSELECT *\nFROM weather;\n\n\n\n2 records\n\n\ncity\ntemp_lo\ntemp_hi\nprcp\ndate\n\n\n\n\nSan Francisco\n46\n50\n0.25\n1994-11-27\n\n\nSan Francisco\n41\n55\n0.00\n1994-11-29",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "SQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_04_sql.html#array-type",
    "href": "contents/sql/duckdb/documentaion/guide_04_sql.html#array-type",
    "title": "SQL",
    "section": "3.1 Array Type",
    "text": "3.1 Array Type\nduckdbのパッケージバージョンが1.0.0になったけど、 arrayはまだ使うことが出来なかった・・・\n#| connection: con\nSELECT [3, 2, 1]::INTEGER[3];",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "SQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_04_sql.html#general-purpose-window-functions",
    "href": "contents/sql/duckdb/documentaion/guide_04_sql.html#general-purpose-window-functions",
    "title": "SQL",
    "section": "4.1 General-Purpose Window Functions",
    "text": "4.1 General-Purpose Window Functions\n\ncume_dist\ndense_rank\nfirst_value\nfirst\nlag\nlast_value\nlast\nlead\nnth_value\nntile：区分化\npercent_rank\nrank_dense\nrank\nrow_number\n\nタイル化でデータを分割してみる。\n\n\nCode\nWITH tiled as (\n  SELECT ntile(10) OVER() T, *\n  FROM penguins\n)\nSELECT T, COUNT(*)\nFROM tiled\nGROUP BY T\nORDER BY T\n\n\n\nDisplaying records 1 - 10\n\n\nT\ncount_star()\n\n\n\n\n1\n35\n\n\n2\n35\n\n\n3\n35\n\n\n4\n35\n\n\n5\n34\n\n\n6\n34\n\n\n7\n34\n\n\n8\n34\n\n\n9\n34\n\n\n10\n34",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "SQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_04_sql.html#ignoring-nulls",
    "href": "contents/sql/duckdb/documentaion/guide_04_sql.html#ignoring-nulls",
    "title": "SQL",
    "section": "4.2 Ignoring NULLs",
    "text": "4.2 Ignoring NULLs\n下記の関数はNULLを無視することができる。\n\nlag\nlead\nfirst_value\nlast_value\nnth_value\n\nlag(column, 3 IGNORE NULLS)となる。",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "SQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_04_sql.html#evaluation",
    "href": "contents/sql/duckdb/documentaion/guide_04_sql.html#evaluation",
    "title": "SQL",
    "section": "4.3 Evaluation",
    "text": "4.3 Evaluation\n\n4.3.1 Partition and Ordering\n\n\nCode\nCREATE OR REPLACE TABLE \"Generation History\" AS\n  FROM 'data/csv/power-plant-generation-history.csv';\n\n\n\n\nCode\nSELECT * FROM \"Generation History\" LIMIT 5;\n\n\n\n5 records\n\n\nMWh\nDate\nPlant\n\n\n\n\n564337\n2019-01-02\nBoston\n\n\n507405\n2019-01-03\nBoston\n\n\n528523\n2019-01-04\nBoston\n\n\n469538\n2019-01-05\nBoston\n\n\n474163\n2019-01-06\nBoston\n\n\n\n\n\nまずはrow_numberの挙動をみる。\n\n\nCode\nSELECT\n    \"Plant\",\n    \"Date\",\n    row_number() OVER (PARTITION BY \"Plant\" ORDER  BY \"Date\") AS \"Row\"\nFROM \"Generation History\"\nORDER BY 1, 2;\n\n\n\nDisplaying records 1 - 10\n\n\nPlant\nDate\nRow\n\n\n\n\nBoston\n2019-01-02\n1\n\n\nBoston\n2019-01-03\n2\n\n\nBoston\n2019-01-04\n3\n\n\nBoston\n2019-01-05\n4\n\n\nBoston\n2019-01-06\n5\n\n\nBoston\n2019-01-07\n6\n\n\nBoston\n2019-01-08\n7\n\n\nBoston\n2019-01-09\n8\n\n\nBoston\n2019-01-10\n9\n\n\nBoston\n2019-01-11\n10\n\n\n\n\n\n\n\n4.3.2 Framing\nPRECEDINGやFOLLOWINGを使うことで、相対的な位置関係を使うことが可能である。 この距離はROWSの整数としても指定することができる。 フレームのデフォルト値はUNBOUNDED PRECEDINGからCURRENT ROWである。 EXCLUDE句は使用するうと現在の行の周りの行をフレームから除外することができる。\n\nRANGEフレーミングを行う。\n\n\nCode\nSELECT \"Plant\", \"Date\",\n    avg(\"MWh\") OVER (\n        PARTITION BY \"Plant\"\n        ORDER BY \"Date\" ASC\n        RANGE BETWEEN INTERVAL 3 DAYS PRECEDING\n                  AND INTERVAL 3 DAYS FOLLOWING)\n        AS \"MWh 7-day Moving Average\"\nFROM \"Generation History\"\nORDER BY 1, 2;\n\n\n\nDisplaying records 1 - 10\n\n\nPlant\nDate\nMWh 7-day Moving Average\n\n\n\n\nBoston\n2019-01-02\n517450.8\n\n\nBoston\n2019-01-03\n508793.2\n\n\nBoston\n2019-01-04\n508529.8\n\n\nBoston\n2019-01-05\n523459.9\n\n\nBoston\n2019-01-06\n526067.1\n\n\nBoston\n2019-01-07\n524938.7\n\n\nBoston\n2019-01-08\n518294.6\n\n\nBoston\n2019-01-09\n520665.4\n\n\nBoston\n2019-01-10\n528859.0\n\n\nBoston\n2019-01-11\n532466.7\n\n\n\n\n\n範囲がないときにNULLになるようにする。\n\n\nCode\nSELECT \"Plant\", \"Date\",\n    CASE\n        WHEN COUNT(\"MWh\") OVER (\n            PARTITION BY \"Plant\"\n            ORDER BY \"Date\" ASC\n            RANGE BETWEEN INTERVAL 3 DAYS PRECEDING\n                      AND INTERVAL 3 DAYS FOLLOWING\n        ) &gt;= 7 THEN\n            avg(\"MWh\") OVER (\n                PARTITION BY \"Plant\"\n                ORDER BY \"Date\" ASC\n                RANGE BETWEEN INTERVAL 3 DAYS PRECEDING\n                          AND INTERVAL 3 DAYS FOLLOWING\n            )\n        ELSE\n            NULL\n    END AS \"MWh 7-day Moving Average\"\nFROM \"Generation History\"\nORDER BY 1, 2;\n\n\n\nDisplaying records 1 - 10\n\n\nPlant\nDate\nMWh 7-day Moving Average\n\n\n\n\nBoston\n2019-01-02\nNA\n\n\nBoston\n2019-01-03\nNA\n\n\nBoston\n2019-01-04\nNA\n\n\nBoston\n2019-01-05\n523459.9\n\n\nBoston\n2019-01-06\n526067.1\n\n\nBoston\n2019-01-07\n524938.7\n\n\nBoston\n2019-01-08\n518294.6\n\n\nBoston\n2019-01-09\n520665.4\n\n\nBoston\n2019-01-10\n528859.0\n\n\nBoston\n2019-01-11\nNA\n\n\n\n\n\n\n\n4.3.3 WINDOW Clauses\nWINDOWを選言することでマクロのような使い方ができる。\n\n\nCode\nSELECT \"Plant\", \"Date\",\n    min(\"MWh\") OVER seven AS \"MWh 7-day Moving Minimum\",\n    avg(\"MWh\") OVER seven AS \"MWh 7-day Moving Average\",\n    max(\"MWh\") OVER seven AS \"MWh 7-day Moving Maximum\",\n    min(\"MWh\") OVER three AS \"MWh 3-day Moving Minimum\",\n    avg(\"MWh\") OVER three AS \"MWh 3-day Moving Average\",\n    max(\"MWh\") OVER three AS \"MWh 3-day Moving Maximum\"\nFROM \"Generation History\"\nWINDOW\n    seven AS (\n        PARTITION BY \"Plant\"\n        ORDER BY \"Date\" ASC\n        RANGE BETWEEN INTERVAL 3 DAYS PRECEDING\n                  AND INTERVAL 3 DAYS FOLLOWING),\n    three AS (\n        PARTITION BY \"Plant\"\n        ORDER BY \"Date\" ASC\n        RANGE BETWEEN INTERVAL 1 DAYS PRECEDING\n        AND INTERVAL 1 DAYS FOLLOWING)\nORDER BY 1, 2;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\nPlant\nDate\nMWh 7-day Moving Minimum\nMWh 7-day Moving Average\nMWh 7-day Moving Maximum\nMWh 3-day Moving Minimum\nMWh 3-day Moving Average\nMWh 3-day Moving Maximum\n\n\n\n\nBoston\n2019-01-02\n469538\n517450.8\n564337\n507405\n535871.0\n564337\n\n\nBoston\n2019-01-03\n469538\n508793.2\n564337\n507405\n533421.7\n564337\n\n\nBoston\n2019-01-04\n469538\n508529.8\n564337\n469538\n501822.0\n528523\n\n\nBoston\n2019-01-05\n469538\n523459.9\n613040\n469538\n490741.3\n528523\n\n\nBoston\n2019-01-06\n469538\n526067.1\n613040\n469538\n483638.0\n507213\n\n\nBoston\n2019-01-07\n469538\n524938.7\n613040\n474163\n531472.0\n613040\n\n\nBoston\n2019-01-08\n469538\n518294.6\n613040\n507213\n567613.7\n613040\n\n\nBoston\n2019-01-09\n474163\n520665.4\n613040\n499506\n565044.7\n613040\n\n\nBoston\n2019-01-10\n482014\n528859.0\n613040\n482014\n521369.3\n582588\n\n\nBoston\n2019-01-11\n482014\n532466.7\n613040\n482014\n489218.0\n499506\n\n\n\n\n\n\n\n4.3.4 Filtering the Results of Window Functions Using QUALIFY\n\n\nCode\nSELECT \"Plant\", \"Date\",\n    min(\"MWh\") OVER seven AS \"MWh 7-day Moving Minimum\",\n    quantile_cont(\"MWh\", [0.25, 0.5, 0.75]) OVER seven\n        AS \"MWh 7-day Moving IQR\",\n    max(\"MWh\") OVER seven AS \"MWh 7-day Moving Maximum\",\nFROM \"Generation History\"\nWINDOW seven AS (\n    PARTITION BY \"Plant\"\n    ORDER BY \"Date\" ASC\n    RANGE BETWEEN INTERVAL 3 DAYS PRECEDING\n              AND INTERVAL 3 DAYS FOLLOWING)\nORDER BY 1, 2;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\nPlant\nDate\nMWh 7-day Moving Minimum\nMWh 7-day Moving IQR\nMWh 7-day Moving Maximum\n\n\n\n\nBoston\n2019-01-02\n469538\n497938.2, 517964.0, 537476.5\n564337\n\n\nBoston\n2019-01-03\n469538\n474163, 507405, 528523\n564337\n\n\nBoston\n2019-01-04\n469538\n482425.5, 507309.0, 523243.5\n564337\n\n\nBoston\n2019-01-05\n469538\n490688, 507405, 546430\n613040\n\n\nBoston\n2019-01-06\n469538\n490688.0, 507405.0, 555555.5\n613040\n\n\nBoston\n2019-01-07\n469538\n486834.5, 507213.0, 555555.5\n613040\n\n\nBoston\n2019-01-08\n469538\n478088.5, 499506.0, 544900.5\n613040\n\n\nBoston\n2019-01-09\n474163\n484074.0, 499506.0, 544900.5\n613040\n\n\nBoston\n2019-01-10\n482014\n492820, 507213, 557053\n613040\n\n\nBoston\n2019-01-11\n482014\n489477.0, 515512.0, 569820.5\n613040",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "SQL"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_01_data_import_export.html",
    "href": "contents/sql/duckdb/documentaion/guide_01_data_import_export.html",
    "title": "Data Import & Export",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/sql/duckdb/documentation\")\nCode\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Data Import & Export"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_01_data_import_export.html#prerequisites",
    "href": "contents/sql/duckdb/documentaion/guide_01_data_import_export.html#prerequisites",
    "title": "Data Import & Export",
    "section": "7.1 Prerequisites",
    "text": "7.1 Prerequisites\n#| connection: con\nINSTALL httpfs;\nLOAD httpfs;",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Data Import & Export"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_01_data_import_export.html#credentials-and-configuration",
    "href": "contents/sql/duckdb/documentaion/guide_01_data_import_export.html#credentials-and-configuration",
    "title": "Data Import & Export",
    "section": "7.2 Credentials and Configuration",
    "text": "7.2 Credentials and Configuration\n#| connection: con\nCREATE SECRET (\n    TYPE S3,\n    KEY_ID 'AKIAIOSFODNN7EXAMPLE',\n    SECRET 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n    REGION 'us-east-1'\n);",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Data Import & Export"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_01_data_import_export.html#querying",
    "href": "contents/sql/duckdb/documentaion/guide_01_data_import_export.html#querying",
    "title": "Data Import & Export",
    "section": "7.3 Querying",
    "text": "7.3 Querying\n#| connection: con\nSELECT * FROM read_parquet('s3://⟨bucket⟩/⟨file⟩');",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Data Import & Export"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/LearningFromMaster2nd/ch01_MagicSQL.html",
    "href": "contents/sql/duckdb/LearningFromMaster2nd/ch01_MagicSQL.html",
    "title": "交差演算",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\n\n\n\n\nCode\ncur_dir &lt;- here::here(\"contents/sql/duckdb/mytips\")\n\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nlibrary(here)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 Setup\n\n\nCode\ncon &lt;- dbConnect(duckdb(here(cur_dir, \"data.db\")))\n\n\n\n\nCode\nINSTALL icu;\nINSTALL spatial;\nINSTALL httpfs;\nINSTALL json;\n\n\n\n\nCode\nLOAD icu;\nLOAD spatial;\nLOAD httpfs;\nLOAD json;\n\n\n\n\nCode\nSHOW ALL TABLES;\n\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\ndatabase\nschema\nname\ncolumn_names\ncolumn_types\ntemporary\n\n\n\n\ndata\nmain\nag\nMESH_ID , SHICODE , PTN_2015 , HITOKU2020, GASSAN2020, PTN_2020 , PT0_2020 , PT1_2020 , PT2_2020 , PT3_2020 , PT4_2020 , PT5_2020 , PT6_2020 , PT7_2020 , PT8_2020 , PT9_2020 , PT10_2020 , PT11_2020 , PT12_2020 , PT13_2020 , PT14_2020 , PT15_2020 , PT16_2020 , PT17_2020 , PT18_2020 , PT19_2020 , PTA_2020 , PTB_2020 , PTC_2020 , PTD_2020 , PTE_2020 , RTA_2020 , RTB_2020 , RTC_2020 , RTD_2020 , RTE_2020 , HITOKU2025, GASSAN2025, PTN_2025 , PT0_2025 , PT1_2025 , PT2_2025 , PT3_2025 , PT4_2025 , PT5_2025 , PT6_2025 , PT7_2025 , PT8_2025 , PT9_2025 , PT10_2025 , PT11_2025 , PT12_2025 , PT13_2025 , PT14_2025 , PT15_2025 , PT16_2025 , PT17_2025 , PT18_2025 , PT19_2025 , PTA_2025 , PTB_2025 , PTC_2025 , PTD_2025 , PTE_2025 , RTA_2025 , RTB_2025 , RTC_2025 , RTD_2025 , RTE_2025 , HITOKU2030, GASSAN2030, PTN_2030 , PT0_2030 , PT1_2030 , PT2_2030 , PT3_2030 , PT4_2030 , PT5_2030 , PT6_2030 , PT7_2030 , PT8_2030 , PT9_2030 , PT10_2030 , PT11_2030 , PT12_2030 , PT13_2030 , PT14_2030 , PT15_2030 , PT16_2030 , PT17_2030 , PT18_2030 , PT19_2030 , PTA_2030 , PTB_2030 , PTC_2030 , PTD_2030 , PTE_2030 , RTA_2030 , RTB_2030 , RTC_2030 , RTD_2030 , RTE_2030 , HITOKU2035, GASSAN2035, PTN_2035 , PT0_2035 , PT1_2035 , PT2_2035 , PT3_2035 , PT4_2035 , PT5_2035 , PT6_2035 , PT7_2035 , PT8_2035 , PT9_2035 , PT10_2035 , PT11_2035 , PT12_2035 , PT13_2035 , PT14_2035 , PT15_2035 , PT16_2035 , PT17_2035 , PT18_2035 , PT19_2035 , PTA_2035 , PTB_2035 , PTC_2035 , PTD_2035 , PTE_2035 , RTA_2035 , RTB_2035 , RTC_2035 , RTD_2035 , RTE_2035 , HITOKU2040, GASSAN2040, PTN_2040 , PT0_2040 , PT1_2040 , PT2_2040 , PT3_2040 , PT4_2040 , PT5_2040 , PT6_2040 , PT7_2040 , PT8_2040 , PT9_2040 , PT10_2040 , PT11_2040 , PT12_2040 , PT13_2040 , PT14_2040 , PT15_2040 , PT16_2040 , PT17_2040 , PT18_2040 , PT19_2040 , PTA_2040 , PTB_2040 , PTC_2040 , PTD_2040 , PTE_2040 , RTA_2040 , RTB_2040 , RTC_2040 , RTD_2040 , RTE_2040 , HITOKU2045, GASSAN2045, PTN_2045 , PT0_2045 , PT1_2045 , PT2_2045 , PT3_2045 , PT4_2045 , PT5_2045 , PT6_2045 , PT7_2045 , PT8_2045 , PT9_2045 , PT10_2045 , PT11_2045 , PT12_2045 , PT13_2045 , PT14_2045 , PT15_2045 , PT16_2045 , PT17_2045 , PT18_2045 , PT19_2045 , PTA_2045 , PTB_2045 , PTC_2045 , PTD_2045 , PTE_2045 , RTA_2045 , RTB_2045 , RTC_2045 , RTD_2045 , RTE_2045 , HITOKU2050, GASSAN2050, PTN_2050 , PT0_2050 , PT1_2050 , PT2_2050 , PT3_2050 , PT4_2050 , PT5_2050 , PT6_2050 , PT7_2050 , PT8_2050 , PT9_2050 , PT10_2050 , PT11_2050 , PT12_2050 , PT13_2050 , PT14_2050 , PT15_2050 , PT16_2050 , PT17_2050 , PT18_2050 , PT19_2050 , PTA_2050 , PTB_2050 , PTC_2050 , PTD_2050 , PTE_2050 , RTA_2050 , RTB_2050 , RTC_2050 , RTD_2050 , RTE_2050 , geom\nBIGINT , INTEGER , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , VARCHAR , BIGINT , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , DOUBLE , GEOMETRY\nFALSE\n\n\ndata\nmain\nag_pref_intersection\nN03_001 , N03_002 , N03_003 , N03_004 , N03_005 , N03_007 , MESH_ID , SHICODE , PTN_2015, geom\nVARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , BIGINT , INTEGER , DOUBLE , GEOMETRY\nFALSE\n\n\ndata\nmain\npref\nN03_001, N03_002, N03_003, N03_004, N03_005, N03_007, geom\nVARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , GEOMETRY\nFALSE\n\n\ndata\nmain\nta\nid , ng , N03_001 , N03_002 , N03_003 , N03_004 , N03_005 , N03_007 , MESH_ID , SHICODE , PTN_2015, geom\nUUID , INTEGER , VARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , BIGINT , INTEGER , DOUBLE , GEOMETRY\nFALSE\n\n\ndata\nmain\ntb\nid, ng\nUUID , INTEGER\nFALSE\n\n\n\n\n\n\n\nCode\nSELECT DISTINCT * FROM duckdb_functions() WHERE function_name LIKE 'st_%' ORDER BY function_name;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatabase_name\ndatabase_oid\nschema_name\nfunction_name\nfunction_type\ndescription\ncomment\nreturn_type\nparameters\nparameter_types\nvarargs\nmacro_definition\nhas_side_effects\ninternal\nfunction_oid\nexample\nstability\n\n\n\n\nsystem\n0\nmain\nst_read_meta\ntable\nNA\nNA\nNA\ncol0\nVARCHAR[]\nNA\nNA\nNA\nTRUE\n1549\nNA\nNA\n\n\nsystem\n0\nmain\nst_read_meta\ntable\nNA\nNA\nNA\ncol0\nVARCHAR\nNA\nNA\nNA\nTRUE\n1549\nNA\nNA\n\n\nsystem\n0\nmain\nstarts_with\nscalar\nReturns true if string begins with search_string\nNA\nBOOLEAN\nstring , search_string\nVARCHAR, VARCHAR\nNA\nNA\nFALSE\nTRUE\n934\nstarts_with(‘abc’,‘a’)\nCONSISTENT\n\n\nsystem\n0\nmain\nstats\nscalar\nReturns a string with statistics about the expression. Expression can be a column, constant, or SQL expression\nNA\nVARCHAR\nexpression\nANY\nNA\nNA\nTRUE\nTRUE\n936\nstats(5)\nVOLATILE\n\n\nsystem\n0\nmain\nstddev\naggregate\nReturns the sample standard deviation\nNA\nDOUBLE\nx\nDOUBLE\nNA\nNA\nFALSE\nTRUE\n938\nsqrt(var_samp(x))\nCONSISTENT\n\n\nsystem\n0\nmain\nstddev_pop\naggregate\nReturns the population standard deviation.\nNA\nDOUBLE\nx\nDOUBLE\nNA\nNA\nFALSE\nTRUE\n940\nsqrt(var_pop(x))\nCONSISTENT\n\n\nsystem\n0\nmain\nstddev_samp\naggregate\nReturns the sample standard deviation\nNA\nDOUBLE\nx\nDOUBLE\nNA\nNA\nFALSE\nTRUE\n942\nsqrt(var_samp(x))\nCONSISTENT\n\n\nsystem\n0\nmain\nstorage_info\npragma\nNA\nNA\nNA\ncol0\nVARCHAR\nNA\nNA\nNA\nTRUE\n302\nNA\nNA\n\n\nsystem\n0\nmain\nstr_split\nscalar\nSplits the string along the separator\nNA\nVARCHAR[]\nstring , separator\nVARCHAR, VARCHAR\nNA\nNA\nFALSE\nTRUE\n944\nstring_split(‘hello-world’, ‘-’)\nCONSISTENT\n\n\nsystem\n0\nmain\nstr_split_regex\nscalar\nSplits the string along the regex\nNA\nVARCHAR[]\nstring , separator\nVARCHAR, VARCHAR\nNA\nNA\nFALSE\nTRUE\n946\nstring_split_regex(‘hello␣world; 42’, ‘;?␣’)\nCONSISTENT\n\n\n\n\n\n\n\n2 交差演算\n国土数値情報の行政区域データと農業地域データの交差集合を作成する。\n\n\nCode\nCREATE OR REPLACE TABLE pref AS SELECT * FROM './data/N03-20240101_GML/N03-20240101.shp';\nCREATE OR REPLACE TABLE ag AS (\n  SELECT * FROM './data/ag/1km_mesh_2018_01.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_02.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_03.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_04.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_05.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_06.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_07.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_08.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_09.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_10.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_11.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_12.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_13.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_14.shp'  \n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_15.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_16.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_17.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_18.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_19.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_20.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_21.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_22.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_23.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_24.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_25.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_26.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_27.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_28.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_29.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_30.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_31.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_32.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_33.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_34.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_35.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_36.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_37.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_38.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_39.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_40.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_41.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_42.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_43.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_44.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_45.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_46.shp'\n    UNION ALL\n  SELECT * FROM './data/ag/1km_mesh_2018_47.shp'\n);\n\n\n\n\nCode\ndescribe pref;\n\n\n\n7 records\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\n\nN03_001\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nN03_002\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nN03_003\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nN03_004\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nN03_005\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nN03_007\nVARCHAR\nYES\nNA\nNA\nNA\n\n\ngeom\nGEOMETRY\nYES\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\ndescribe ag;\n\n\n\nDisplaying records 1 - 10\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\n\nMESH_ID\nBIGINT\nYES\nNA\nNA\nNA\n\n\nSHICODE\nINTEGER\nYES\nNA\nNA\nNA\n\n\nPTN_2015\nDOUBLE\nYES\nNA\nNA\nNA\n\n\nHITOKU2020\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nGASSAN2020\nBIGINT\nYES\nNA\nNA\nNA\n\n\nPTN_2020\nDOUBLE\nYES\nNA\nNA\nNA\n\n\nPT0_2020\nDOUBLE\nYES\nNA\nNA\nNA\n\n\nPT1_2020\nDOUBLE\nYES\nNA\nNA\nNA\n\n\nPT2_2020\nDOUBLE\nYES\nNA\nNA\nNA\n\n\nPT3_2020\nDOUBLE\nYES\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\nselect count(*), ST_GeometryType(geom) as type  from pref group by ST_GeometryType(geom) ;\n\n\n\n1 records\n\n\ncount_star()\ntype\n\n\n\n\n124134\nPOLYGON\n\n\n\n\n\n\n\nCode\nselect \n  count(*) as cnt\n  , sum(cast(ST_IsRing(geom) as integer)) as has_ring \n  , ST_GeometryType(geom) as type\nfrom \n  ag \ngroup by \n  ST_GeometryType(geom) ;\n\n\n\n1 records\n\n\ncnt\nhas_ring\ntype\n\n\n\n\n178347\n0\nPOLYGON\n\n\n\n\n\n\n\nCode\nselect count(*), ST_GeometryType(geom) as type  from ag group by ST_GeometryType(geom) ;\n\n\n\n1 records\n\n\ncount_star()\ntype\n\n\n\n\n178347\nPOLYGON\n\n\n\n\n\n約10万どおしの演算であるが、数分で処理できる。すごい！\n\n\nCode\ncreate or replace table ag_pref_intersection as\nselect\n  COLUMNS(pref.* EXCLUDE(geom)), \n  ag.MESH_ID, \n  ag.SHICODE, \n  ag.PTN_2015,\n  st_intersection(pref.geom, ag.geom) as geom\nfrom pref, ag\nwhere st_intersects(pref.geom, ag.geom)\n;\n\n\n\n\nCode\nprint(glue::glue(\"{difftime(Sys.time(), s, units = 'secs')} 秒\"))\n#&gt; 143.159198045731 秒\n\n\nデータ方を確認すると少し形状タイプがおかしいかも。\n\n\nCode\nselect count(*) as cnt\nfrom ag_pref_intersection ;\n\n\n\n1 records\n\n\ncnt\n\n\n\n\n265301\n\n\n\n\n\n次の結果をみるとどうやら混じっている。コレクションについて、 QGISで具体的に確認したところ、行政境界とメッシュ境界がタッチしているところや一致しているところで発生する事象である。マルチポリゴンもそうであるが基本的にはsimplifyしてポリゴンだけ残せば大丈夫だと思う。\n\n\nCode\nselect \n  COUNT(*) as cnt\n  , COUNT_IF(ST_IsRing(geom)) as has_ring \n  , ST_GeometryType(geom) as type\nfrom \n  ag_pref_intersection \ngroup by \n  ST_GeometryType(geom) ;\n\n\n\n3 records\n\n\ncnt\nhas_ring\ntype\n\n\n\n\n256571\n0\nPOLYGON\n\n\n8715\n0\nMULTIPOLYGON\n\n\n15\n0\nGEOMETRYCOLLECTION\n\n\n\n\n\nどうやらWITH句でランダムを使うと変わってしまうらしい。またマルチポリゴンをポリゴンにキャストする方法は実装されてないようである。\n\n\nCode\nwith \nta as (\n  select \n    gen_random_uuid() as id\n    , ST_NGeometries(geom) as ng\n    , ROW_NUMBER() OVER() as row\n    , geom\n  from \n    ag_pref_intersection\n  where\n    ST_GeometryType(geom) = 'MULTIPOLYGON'\n  limit 2\n) \n, tb as (\n  select \n    id, ng, row\n  from \n    ta\n  limit 2\n)\n, ta_tb as (\n  select \n    *, \n    unnest(generate_series(1, ta.ng)) as idx\n  from\n   ta\n   inner join tb\n   on ta.row = tb.row\n)\n\nselect \n    *\nfrom\n  ta_tb\n;\n\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\nid\nng\nrow\ngeom\nid_1\nng_1\nrow_1\nidx\n\n\n\n\n500b3229-8039-4271-82c9-426fe9ee415d\n2\n1\n05, 04, 5b, 00, 00, 00, 00, 00, cc, 0c, 03, 43, 99, 99, f6, 41, 00, 10, 03, 43, ab, aa, f6, 41, 05, 00, 00, 00, 02, 00, 00, 00, 02, 00, 00, 00, 01, 00, 00, 00, ab, 00, 00, 00, 00, 00, 00, 00, 18, c5, 4a, 83, fd, 61, 60, 40, d4, e6, 53, 55, 55, d5, 3e, 40, 97, 8a, 58, 87, fd, 61, 60, 40, cf, d1, 40, b2, 54, d5, 3e, 40, 8f, b4, 2b, 5f, fd, 61, 60, 40, 53, f0, 74, e4, 51, d5, 3e, 40, 51, 92, fc, 28, fd, 61, 60, 40, 78, 5f, 49, 7d, 4f, d5, 3e, 40, b1, 31, cb, 78, fd, 61, 60, 40, 3e, 96, 85, 49, 4d, d5, 3e, 40, 76, c1, 22, b7, fd, 61, 60, 40, 86, f8, db, 2c, 4b, d5, 3e, 40, ff, 76, 1b, cc, fd, 61, 60, 40, 23, 7e, 60, ef, 47, d5, 3e, 40, 21, 99, 94, d6, fd, 61, 60, 40, 9c, 67, 40, 4b, 44, d5, 3e, 40, e5, 19, 5c, bc, fd, 61, 60, 40, e4, 8f, 58, 41, 42, d5, 3e, 40, 3a, e7, c3, 8d, fd, 61, 60, 40, 63, b6, 18, c7, 3e, d5, 3e, 40, 5d, bd, a6, e1, fd, 61, 60, 40, 28, ed, 54, 93, 3c, d5, 3e, 40, 43, 16, 34, f0, fd, 61, 60, 40, ba, 0b, 2a, 30, 38, d5, 3e, 40, 31, 06, 69, d5, fd, 61, 60, 40, 03, ea, d2, 61, 33, d5, 3e, 40, b1, 59, 47, b8, fd, 61, 60, 40, 93, 52, 17, c3, 31, d5, 3e, 40, fe, 1d, d8, 91, fd, 61, 60, 40, 9a, b1, 21, 65, 2f, d5, 3e, 40, 98, e3, 9b, c1, fd, 61, 60, 40, e3, d9, 39, 5b, 2d, d5, 3e, 40, ff, 76, 1b, cc, fd, 61, 60, 40, cc, d1, 9c, 22, 2b, d5, 3e, 40, ff, c0, ce, ad, fd, 61, 60, 40, f8, dc, 04, 3d, 26, d5, 3e, 40, 0e, 30, 86, 81, fd, 61, 60, 40, 9a, 5a, fd, ab, 23, d5, 3e, 40, fc, 69, 6e, 48, fd, 61, 60, 40, 89, 67, 73, 16, 22, d5, 3e, 40, d0, 8a, b4, fc, fc, 61, 60, 40, c0, 97, 76, f5, 20, d5, 3e, 40, da, 1f, 79, fe, fc, 61, 60, 40, 91, 20, a6, 8e, 1f, d5, 3e, 40, d2, 3e, 1e, 46, fd, 61, 60, 40, 92, 9c, f8, dc, 1c, d5, 3e, 40, f5, 91, 5e, 4b, fd, 61, 60, 40, cf, 5d, 61, ad, 19, d5, 3e, 40, f4, ee, 67, 2f, fd, 61, 60, 40, 91, 6f, 59, ff, 13, d5, 3e, 40, 37, 1e, 34, 07, fd, 61, 60, 40, 38, 2a, 32, 98, 11, d5, 3e, 40, b6, bb, c5, cb, fc, 61, 60, 40, a5, 28, 2d, e2, 0f, d5, 3e, 40, f1, e1, ba, ab, fc, 61, 60, 40, b1, 45, 6d, 43, 0e, d5, 3e, 40, b6, 71, 12, ea, fc, 61, 60, 40, fa, 8a, 24, 30, 0c, d5, 3e, 40, fc, f7, 3e, 27, fd, 61, 60, 40, ad, 03, 8a, d2, 0a, d5, 3e, 40, ca, da, 20, 3f, fd, 61, 60, 40, 69, e3, cf, 8a, 04, d5, 3e, 40, 75, e5, 3c, 2e, fd, 61, 60, 40, 71, a1, 8d, 84, ff, d4, 3e, 40, 84, 54, f4, 01, fd, 61, 60, 40, 55, 0d, 16, dc, fc, d4, 3e, 40, 9c, ca, 9f, f8, fc, 61, 60, 40, 62, db, 5b, f7, fa, d4, 3e, 40, ec, fc, 99, 49, fd, 61, 60, 40, 4b, 05, 1a, 0e, f9, d4, 3e, 40, 63, fd, ed, 52, fd, 61, 60, 40, 2f, e8, 69, 32, f6, d4, 3e, 40, 5b, 3e, ca, 3c, fd, 61, 60, 40, 5c, c6, 01, 7f, ee, d4, 3e, 40, 49, 2e, ff, 21, fd, 61, 60, 40, ec, b7, 7e, 13, ed, d4, 3e, 40, 37, 68, e7, e8, fc, 61, 60, 40, d5, 75, a3, ed, ea, d4, 3e, 40, 7a, f2, d9, cf, fc, 61, 60, 40, 41, ae, dc, 24, e9, d4, 3e, 40, 8c, b8, f1, 08, fd, 61, 60, 40, ad, ac, d7, 6e, e7, d4, 3e, 40, d2, e3, f7, 36, fd, 61, 60, 40, f6, 85, f5, 1e, e5, d4, 3e, 40, 1e, 1a, b8, 31, fd, 61, 60, 40, 28, a6, 70, dc, e0, d4, 3e, 40, 9e, 6d, 96, 14, fd, 61, 60, 40, dc, 9a, 28, cd, dc, d4, 3e, 40, 02, aa, b5, b9, fc, 61, 60, 40, de, f9, db, 24, da, d4, 3e, 40, 24, a4, b2, 84, fc, 61, 60, 40, eb, 16, 1c, 86, d8, d4, 3e, 40, 3f, cc, e4, ef, fc, 61, 60, 40, 4b, 42, ce, 6d, d5, d4, 3e, 40, 2f, 5f, 10, f1, fc, 61, 60, 40, 81, 2d, 65, cf, d4, d4, 3e, 40, da, 1f, 79, fe, fc, 61, 60, 40, fc, d7, 03, f7, cd, d4, 3e, 40, 84, f9, cd, f2, fc, 61, 60, 40, 98, 7a, 27, b0, ca, d4, 3e, 40, 9c, 47, fd, a9, fc, 61, 60, 40, d5, 3b, 90, 80, c7, d4, 3e, 40, 68, 87, e8, a5, fc, 61, 60, 40, e2, ec, 36, a5, c5, d4, 3e, 40, 2f, ba, 36, 00, fd, 61, 60, 40, 13, c3, ba, 63, c4, d4, 3e, 40, 7b, 18, 73, 3a, fd, 61, 60, 40, 7f, 8f, 5a, 5e, c2, d4, 3e, 40, 0f, 06, 27, 6d, fd, 61, 60, 40, 98, c9, da, 20, bf, d4, 3e, 40, a0, 91, 4c, aa, fd, 61, 60, 40, 7d, 45, 1c, 8a, b9, d4, 3e, 40, f5, b9, da, 8a, fd, 61, 60, 40, cd, 92, 00, 35, b5, d4, 3e, 40, 6e, 85, a1, ef, fd, 61, 60, 40, da, af, 40, 96, b3, d4, 3e, 40, a2, 6d, 32, 33, fe, 61, 60, 40, 51, 94, 95, 70, b2, d4, 3e, 40, e8, 3d, 12, 52, fe, 61, 60, 40, b2, 7a, db, da, af, d4, 3e, 40, e8, 3d, 12, 52, fe, 61, 60, 40, 2b, 2a, 7d, 49, ac, d4, 3e, 40, d6, 2d, 47, 37, fe, 61, 60, 40, f6, d7, d7, 2c, aa, d4, 3e, 40, 55, 81, 25, 1a, fe, 61, 60, 40, c2, 0e, 6b, 43, a8, d4, 3e, 40, 46, 6d, 94, 55, fe, 61, 60, 40, 98, 72, 6f, 92, a7, d4, 3e, 40, f9, 03, 2a, 8b, fe, 61, 60, 40, d4, 9a, e6, 1d, a7, d4, 3e, 40, 8a, 1d, 20, a7, fe, 61, 60, 40, c9, 16, 98, 01, a6, d4, 3e, 40, 0b, ca, 41, c4, fe, 61, 60, 40, 2b, 2a, 36, 74, a0, d4, 3e, 40, ea, 4a, bf, d5, fe, 61, 60, 40, a4, f6, 76, d9, 9c, d4, 3e, 40, 6a, 9e, 9d, b8, fe, 61, 60, 40, ec, 1e, 8f, cf, 9a, d4, 3e, 40, ad, cd, 69, 90, fe, 61, 60, 40, 76, ac, fe, 7a, 98, d4, 3e, 40, 15, 04, e0, b6, fe, 61, 60, 40, 8f, fe, 92, b2, 97, d4, 3e, 40, 69, c6, 19, f8, fe, 61, 60, 40, 65, eb, cf, 34, 97, d4, 3e, 40, cb, 96, af, 42, ff, 61, 60, 40, f2, aa, 22, 6a, 8f, d4, 3e, 40, 7d, 2f, 28, 4d, ff, 61, 60, 40, c9, dc, cb, 69, 8e, d4, 3e, 40, 30, 43, 1b, 34, ff, 61, 60, 40, ee, 2e, 01, 0c, 8c, d4, 3e, 40, b8, 9f, d0, 0e, ff, 61, 60, 40, 5a, fb, a0, 06, 8a, d4, 3e, 40, 41, e3, 99, 02, ff, 61, 60, 40, c6, f9, 9b, 50, 88, d4, 3e, 40, cb, 96, af, 42, ff, 61, 60, 40, 79, 55, 62, fc, 86, d4, 3e, 40, da, d2, bc, 46, ff, 61, 60, 40, e7, 9d, 54, 45, 82, d4, 3e, 40, c9, 65, e8, 47, ff, 61, 60, 40, 6d, d6, cf, 51, 7d, d4, 3e, 40, c1, 8f, bb, 1f, ff, 61, 60, 40, 7f, e1, f5, 96, 7b, d4, 3e, 40, 27, 14, ab, d1, fe, 61, 60, 40, a4, 33, 2b, 39, 79, d4, 3e, 40, 73, 72, e7, 0b, ff, 61, 60, 40, fe, 88, 0b, b2, 78, d4, 3e, 40, 40, 55, c9, 23, ff, 61, 60, 40, c9, a2, ff, d1, 76, d4, 3e, 40, ea, 72, 3b, 15, ff, 61, 60, 40, 06, 2a, 2a, b5, 73, d4, 3e, 40, 95, 22, 31, f5, fe, 61, 60, 40, a3, e9, ec, 64, 70, d4, 3e, 40, ae, 26, ad, ca, fe, 61, 60, 40, e0, aa, 55, 35, 6d, d4, 3e, 40, c7, e6, 0b, a3, fe, 61, 60, 40, f3, 04, 76, c0, 6b, d4, 3e, 40, 91, 5b, 84, 43, fe, 61, 60, 40, 82, f6, f2, 54, 6a, d4, 3e, 40, f1, 2d, fd, 62, fe, 61, 60, 40, 17, 74, 4a, 59, 69, d4, 3e, 40, b5, d6, 40, 88, fe, 61, 60, 40, 60, 7f, c3, 58, 67, d4, 3e, 40, 70, f3, 0a, 67, fe, 61, 60, 40, f1, 9d, 98, f5, 62, d4, 3e, 40, 6f, 50, 14, 4b, fe, 61, 60, 40, c9, 68, 33, 3a, 5f, d4, 3e, 40, 9b, 3a, fc, 26, fe, 61, 60, 40, fa, 0c, 5c, a9, 5d, d4, 3e, 40, bc, 91, 02, d6, fd, 61, 60, 40, ee, d7, 07, d3, 5c, d4, 3e, 40, 6e, 02, ff, a0, fd, 61, 60, 40, c5, 09, b1, d2, 5b, d4, 3e, 40, 29, 47, 45, bf, fd, 61, 60, 40, 20, db, e3, 99, 58, d4, 3e, 40, 29, 30, 3c, ad, fd, 61, 60, 40, 6e, 5d, 7b, b0, 56, d4, 3e, 40, a7, cf, b0, 46, fd, 61, 60, 40, e7, 29, bc, 15, 53, d4, 3e, 40, 3f, cc, e4, ef, fc, 61, 60, 40, 8f, 9f, 28, 31, 51, d4, 3e, 40, a5, 83, 7e, 71, fc, 61, 60, 40, 71, 8f, 5e, 3a, 51, d4, 3e, 40, 14, ed, 2a, a4, fc, 61, 60, 40, c7, 85, bc, 4b, 4d, d4, 3e, 40, a5, 06, 21, c0, fc, 61, 60, 40, 09, b6, 60, 95, 4a, d4, 3e, 40, fa, d3, 88, 91, fc, 61, 60, 40, 23, a1, e6, 11, 47, d4, 3e, 40, f0, 16, 48, 50, fc, 61, 60, 40, d8, 5b, 60, 15, 43, d4, 3e, 40, d5, a4, 62, 03, fc, 61, 60, 40, 08, 6c, 22, c1, 41, d4, 3e, 40, 98, 25, 2a, e9, fb, 61, 60, 40, 5c, 7a, 94, 47, 40, d4, 3e, 40, c6, 5d, 27, 6f, fc, 61, 60, 40, b1, a2, 4d, a8, 3c, d4, 3e, 40, a3, ad, dd, 85, fc, 61, 60, 40, 59, 03, fe, 6a, 3a, d4, 3e, 40, d7, 6d, f2, 89, fc, 61, 60, 40, f6, a5, 21, 24, 37, d4, 3e, 40, c6, 5d, 27, 6f, fc, 61, 60, 40, bb, f9, fc, e6, 34, d4, 3e, 40, 9a, 7e, 6d, 23, fc, 61, 60, 40, c8, c7, 42, 02, 33, d4, 3e, 40, 11, 6e, 4e, ff, fb, 61, 60, 40, db, 21, 63, 8d, 31, d4, 3e, 40, 83, d3, 34, 88, fc, 61, 60, 40, 95, d4, 50, 3d, 2e, d4, 3e, 40, 60, 23, eb, 9e, fc, 61, 60, 40, 22, b1, 42, 69, 26, d4, 3e, 40, 72, c1, 86, 98, fc, 61, 60, 40, 5f, 55, 0c, 43, 23, d4, 3e, 40, e8, 0d, 71, 58, fc, 61, 60, 40, 84, c4, e0, db, 20, d4, 3e, 40, 33, a1, 3a, 37, fc, 61, 60, 40, 4a, 18, bc, 9e, 1e, d4, 3e, 40, 2c, 63, d6, 9a, fc, 61, 60, 40, 20, 67, 04, 95, 1d, d4, 3e, 40, 9c, fd, 49, c8, fc, 61, 60, 40, 6f, 38, 96, f1, 1b, d4, 3e, 40, 9c, fd, 49, c8, fc, 61, 60, 40, d8, 04, 67, dc, 13, d4, 3e, 40, c7, 5b, 44, 9a, fc, 61, 60, 40, 9e, 75, e1, 95, 11, d4, 3e, 40, 24, a4, b2, 84, fc, 61, 60, 40, c8, 53, f1, a7, 0f, d4, 3e, 40, 62, d7, 54, e8, fc, 61, 60, 40, c3, 78, 1c, f2, 0e, d4, 3e, 40, a7, cf, b0, 46, fd, 61, 60, 40, 5f, 38, df, a1, 0b, d4, 3e, 40, 98, 16, 46, 91, fd, 61, 60, 40, 2c, 45, ed, dc, 06, d4, 3e, 40, 7e, 6f, d3, 9f, fd, 61, 60, 40, 51, 97, 22, 7f, 04, d4, 3e, 40, b1, 8c, f1, 87, fd, 61, 60, 40, 24, d6, e2, 53, 00, d4, 3e, 40, a2, 6d, 32, 33, fe, 61, 60, 40, bf, fc, b3, be, ff, d3, 3e, 40, e1, 8d, 7e, 94, fe, 61, 60, 40, 07, 25, cc, b4, fd, d3, 3e, 40, 8c, 76, 63, e1, fe, 61, 60, 40, fd, 1c, d0, e6, f9, d3, 3e, 40, 73, 00, b8, ea, fe, 61, 60, 40, 7c, 92, 8a, b2, f6, d3, 3e, 40, 61, 62, 1c, f1, fe, 61, 60, 40, 1e, 2d, 22, 18, f4, d3, 3e, 40, 52, 4e, 8b, 2c, ff, 61, 60, 40, d1, d7, e2, 09, f3, d3, 3e, 40, 88, 8f, 5f, aa, ff, 61, 60, 40, b9, e7, 59, 46, f3, d3, 3e, 40, 6f, 41, 30, f3, ff, 61, 60, 40, 78, 16, 69, 54, f3, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, d9, bf, 4a, 23, f3, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 7d, 2c, db, 87, 8d, d3, 3e, 40, b6, 9d, 86, 0d, fe, 61, 60, 40, 5a, f8, 9c, e9, 95, d3, 3e, 40, 98, e3, 9b, c1, fd, 61, 60, 40, 6e, 8a, a1, 7e, 92, d3, 3e, 40, d5, 84, 40, 70, fd, 61, 60, 40, 7c, 7c, 0d, 84, 8e, d3, 3e, 40, 3d, 51, 35, 21, fd, 61, 60, 40, 02, 16, e7, 12, 88, d3, 3e, 40, 7c, c5, 2f, c9, fc, 61, 60, 40, e1, cb, be, d7, 82, d3, 3e, 40, 17, e0, 09, 5d, fc, 61, 60, 40, 36, 42, c5, 8c, 7e, d3, 3e, 40, 0a, 4c, 8b, 20, fc, 61, 60, 40, 08, 2c, 02, 61, 7c, d3, 3e, 40, d3, 34, e1, c4, fb, 61, 60, 40, 0a, 88, 0c, 55, 78, d3, 3e, 40, 1c, fa, b7, 63, fb, 61, 60, 40, 59, 2e, ca, 55, 74, d3, 3e, 40, df, 96, eb, 14, fb, 61, 60, 40, 9c, 49, b2, 46, 71, d3, 3e, 40, 6c, 17, 47, a3, fa, 61, 60, 40, 80, b4, 8d, ac, 6d, d3, 3e, 40, 03, be, b7, f6, f9, 61, 60, 40, ca, 7f, 76, f7, 68, d3, 3e, 40, 49, 9b, dd, 6c, f9, 61, 60, 40, 2b, f9, 75, 33, 65, d3, 3e, 40, 26, a9, 8e, 4d, f9, 61, 60, 40, 03, c7, 68, 94, 63, d3, 3e, 40, ff, 48, a1, c4, f8, 61, 60, 40, 95, cd, 7a, 03, 5c, d3, 3e, 40, 30, 38, 43, a4, f8, 61, 60, 40, 43, b9, 58, 44, 59, d3, 3e, 40, 59, 04, 85, 95, f8, 61, 60, 40, 80, 2d, 72, f9, 56, d3, 3e, 40, f1, 56, 7c, 94, f8, 61, 60, 40, 70, fe, fe, 4b, 54, d3, 3e, 40, 30, c3, 93, 9e, f8, 61, 60, 40, 2a, 6d, 7e, b7, 51, d3, 3e, 40, d9, c4, 64, d2, f8, 61, 60, 40, 08, 7e, da, fc, 4d, d3, 3e, 40, 4b, ee, 10, fc, f8, 61, 60, 40, 27, 30, 77, 0f, 4b, d3, 3e, 40, 42, 9d, e0, 05, fb, 61, 60, 40, 3c, 48, ad, 02, 45, d3, 3e, 40, 72, 8e, 65, ba, fa, 61, 60, 40, 4a, a5, 05, 53, 40, d3, 3e, 40, 27, e9, e3, 9f, fe, 61, 60, 40, 0f, f2, 63, 94, 33, d3, 3e, 40, 6e, ff, 2a, bd, ff, 61, 60, 40, 5c, b3, 83, b4, 40, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 1c, f6, 0d, 1a, 4a, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 01, ef, 6b, 8f, 3d, d3, 3e, 40, e4, 43, 32, df, ff, 61, 60, 40, f8, fd, 7a, 09, 3e, d3, 3e, 40, b2, 28, e7, e9, fe, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9a, 99, 99, 99, 99, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9a, 99, 99, 99, 99, 61, 60, 40, 55, 55, 55, 55, 55, d5, 3e, 40, ed, bb, 4a, 83, fd, 61, 60, 40, 55, 55, 55, 55, 55, d5, 3e, 40, 18, c5, 4a, 83, fd, 61, 60, 40, d4, e6, 53, 55, 55, d5, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, 06, 00, 00, 00, 00, 00, 00, 00, b2, 3d, 52, fb, ff, 61, 60, 40, 71, b6, 8b, f9, 51, d3, 3e, 40, 19, bd, 13, e6, ff, 61, 60, 40, 7d, 38, 2f, eb, 51, d3, 3e, 40, 9a, 6f, 00, da, ff, 61, 60, 40, fd, c6, 79, 2d, 58, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, e4, ec, 07, 2a, 5b, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 6b, cd, 17, 37, 50, d3, 3e, 40, b2, 3d, 52, fb, ff, 61, 60, 40, 71, b6, 8b, f9, 51, d3, 3e, 40\n28611a56-da80-42f1-b60f-ffdcd54f9155\n2\n1\n1\n\n\n500b3229-8039-4271-82c9-426fe9ee415d\n2\n1\n05, 04, 5b, 00, 00, 00, 00, 00, cc, 0c, 03, 43, 99, 99, f6, 41, 00, 10, 03, 43, ab, aa, f6, 41, 05, 00, 00, 00, 02, 00, 00, 00, 02, 00, 00, 00, 01, 00, 00, 00, ab, 00, 00, 00, 00, 00, 00, 00, 18, c5, 4a, 83, fd, 61, 60, 40, d4, e6, 53, 55, 55, d5, 3e, 40, 97, 8a, 58, 87, fd, 61, 60, 40, cf, d1, 40, b2, 54, d5, 3e, 40, 8f, b4, 2b, 5f, fd, 61, 60, 40, 53, f0, 74, e4, 51, d5, 3e, 40, 51, 92, fc, 28, fd, 61, 60, 40, 78, 5f, 49, 7d, 4f, d5, 3e, 40, b1, 31, cb, 78, fd, 61, 60, 40, 3e, 96, 85, 49, 4d, d5, 3e, 40, 76, c1, 22, b7, fd, 61, 60, 40, 86, f8, db, 2c, 4b, d5, 3e, 40, ff, 76, 1b, cc, fd, 61, 60, 40, 23, 7e, 60, ef, 47, d5, 3e, 40, 21, 99, 94, d6, fd, 61, 60, 40, 9c, 67, 40, 4b, 44, d5, 3e, 40, e5, 19, 5c, bc, fd, 61, 60, 40, e4, 8f, 58, 41, 42, d5, 3e, 40, 3a, e7, c3, 8d, fd, 61, 60, 40, 63, b6, 18, c7, 3e, d5, 3e, 40, 5d, bd, a6, e1, fd, 61, 60, 40, 28, ed, 54, 93, 3c, d5, 3e, 40, 43, 16, 34, f0, fd, 61, 60, 40, ba, 0b, 2a, 30, 38, d5, 3e, 40, 31, 06, 69, d5, fd, 61, 60, 40, 03, ea, d2, 61, 33, d5, 3e, 40, b1, 59, 47, b8, fd, 61, 60, 40, 93, 52, 17, c3, 31, d5, 3e, 40, fe, 1d, d8, 91, fd, 61, 60, 40, 9a, b1, 21, 65, 2f, d5, 3e, 40, 98, e3, 9b, c1, fd, 61, 60, 40, e3, d9, 39, 5b, 2d, d5, 3e, 40, ff, 76, 1b, cc, fd, 61, 60, 40, cc, d1, 9c, 22, 2b, d5, 3e, 40, ff, c0, ce, ad, fd, 61, 60, 40, f8, dc, 04, 3d, 26, d5, 3e, 40, 0e, 30, 86, 81, fd, 61, 60, 40, 9a, 5a, fd, ab, 23, d5, 3e, 40, fc, 69, 6e, 48, fd, 61, 60, 40, 89, 67, 73, 16, 22, d5, 3e, 40, d0, 8a, b4, fc, fc, 61, 60, 40, c0, 97, 76, f5, 20, d5, 3e, 40, da, 1f, 79, fe, fc, 61, 60, 40, 91, 20, a6, 8e, 1f, d5, 3e, 40, d2, 3e, 1e, 46, fd, 61, 60, 40, 92, 9c, f8, dc, 1c, d5, 3e, 40, f5, 91, 5e, 4b, fd, 61, 60, 40, cf, 5d, 61, ad, 19, d5, 3e, 40, f4, ee, 67, 2f, fd, 61, 60, 40, 91, 6f, 59, ff, 13, d5, 3e, 40, 37, 1e, 34, 07, fd, 61, 60, 40, 38, 2a, 32, 98, 11, d5, 3e, 40, b6, bb, c5, cb, fc, 61, 60, 40, a5, 28, 2d, e2, 0f, d5, 3e, 40, f1, e1, ba, ab, fc, 61, 60, 40, b1, 45, 6d, 43, 0e, d5, 3e, 40, b6, 71, 12, ea, fc, 61, 60, 40, fa, 8a, 24, 30, 0c, d5, 3e, 40, fc, f7, 3e, 27, fd, 61, 60, 40, ad, 03, 8a, d2, 0a, d5, 3e, 40, ca, da, 20, 3f, fd, 61, 60, 40, 69, e3, cf, 8a, 04, d5, 3e, 40, 75, e5, 3c, 2e, fd, 61, 60, 40, 71, a1, 8d, 84, ff, d4, 3e, 40, 84, 54, f4, 01, fd, 61, 60, 40, 55, 0d, 16, dc, fc, d4, 3e, 40, 9c, ca, 9f, f8, fc, 61, 60, 40, 62, db, 5b, f7, fa, d4, 3e, 40, ec, fc, 99, 49, fd, 61, 60, 40, 4b, 05, 1a, 0e, f9, d4, 3e, 40, 63, fd, ed, 52, fd, 61, 60, 40, 2f, e8, 69, 32, f6, d4, 3e, 40, 5b, 3e, ca, 3c, fd, 61, 60, 40, 5c, c6, 01, 7f, ee, d4, 3e, 40, 49, 2e, ff, 21, fd, 61, 60, 40, ec, b7, 7e, 13, ed, d4, 3e, 40, 37, 68, e7, e8, fc, 61, 60, 40, d5, 75, a3, ed, ea, d4, 3e, 40, 7a, f2, d9, cf, fc, 61, 60, 40, 41, ae, dc, 24, e9, d4, 3e, 40, 8c, b8, f1, 08, fd, 61, 60, 40, ad, ac, d7, 6e, e7, d4, 3e, 40, d2, e3, f7, 36, fd, 61, 60, 40, f6, 85, f5, 1e, e5, d4, 3e, 40, 1e, 1a, b8, 31, fd, 61, 60, 40, 28, a6, 70, dc, e0, d4, 3e, 40, 9e, 6d, 96, 14, fd, 61, 60, 40, dc, 9a, 28, cd, dc, d4, 3e, 40, 02, aa, b5, b9, fc, 61, 60, 40, de, f9, db, 24, da, d4, 3e, 40, 24, a4, b2, 84, fc, 61, 60, 40, eb, 16, 1c, 86, d8, d4, 3e, 40, 3f, cc, e4, ef, fc, 61, 60, 40, 4b, 42, ce, 6d, d5, d4, 3e, 40, 2f, 5f, 10, f1, fc, 61, 60, 40, 81, 2d, 65, cf, d4, d4, 3e, 40, da, 1f, 79, fe, fc, 61, 60, 40, fc, d7, 03, f7, cd, d4, 3e, 40, 84, f9, cd, f2, fc, 61, 60, 40, 98, 7a, 27, b0, ca, d4, 3e, 40, 9c, 47, fd, a9, fc, 61, 60, 40, d5, 3b, 90, 80, c7, d4, 3e, 40, 68, 87, e8, a5, fc, 61, 60, 40, e2, ec, 36, a5, c5, d4, 3e, 40, 2f, ba, 36, 00, fd, 61, 60, 40, 13, c3, ba, 63, c4, d4, 3e, 40, 7b, 18, 73, 3a, fd, 61, 60, 40, 7f, 8f, 5a, 5e, c2, d4, 3e, 40, 0f, 06, 27, 6d, fd, 61, 60, 40, 98, c9, da, 20, bf, d4, 3e, 40, a0, 91, 4c, aa, fd, 61, 60, 40, 7d, 45, 1c, 8a, b9, d4, 3e, 40, f5, b9, da, 8a, fd, 61, 60, 40, cd, 92, 00, 35, b5, d4, 3e, 40, 6e, 85, a1, ef, fd, 61, 60, 40, da, af, 40, 96, b3, d4, 3e, 40, a2, 6d, 32, 33, fe, 61, 60, 40, 51, 94, 95, 70, b2, d4, 3e, 40, e8, 3d, 12, 52, fe, 61, 60, 40, b2, 7a, db, da, af, d4, 3e, 40, e8, 3d, 12, 52, fe, 61, 60, 40, 2b, 2a, 7d, 49, ac, d4, 3e, 40, d6, 2d, 47, 37, fe, 61, 60, 40, f6, d7, d7, 2c, aa, d4, 3e, 40, 55, 81, 25, 1a, fe, 61, 60, 40, c2, 0e, 6b, 43, a8, d4, 3e, 40, 46, 6d, 94, 55, fe, 61, 60, 40, 98, 72, 6f, 92, a7, d4, 3e, 40, f9, 03, 2a, 8b, fe, 61, 60, 40, d4, 9a, e6, 1d, a7, d4, 3e, 40, 8a, 1d, 20, a7, fe, 61, 60, 40, c9, 16, 98, 01, a6, d4, 3e, 40, 0b, ca, 41, c4, fe, 61, 60, 40, 2b, 2a, 36, 74, a0, d4, 3e, 40, ea, 4a, bf, d5, fe, 61, 60, 40, a4, f6, 76, d9, 9c, d4, 3e, 40, 6a, 9e, 9d, b8, fe, 61, 60, 40, ec, 1e, 8f, cf, 9a, d4, 3e, 40, ad, cd, 69, 90, fe, 61, 60, 40, 76, ac, fe, 7a, 98, d4, 3e, 40, 15, 04, e0, b6, fe, 61, 60, 40, 8f, fe, 92, b2, 97, d4, 3e, 40, 69, c6, 19, f8, fe, 61, 60, 40, 65, eb, cf, 34, 97, d4, 3e, 40, cb, 96, af, 42, ff, 61, 60, 40, f2, aa, 22, 6a, 8f, d4, 3e, 40, 7d, 2f, 28, 4d, ff, 61, 60, 40, c9, dc, cb, 69, 8e, d4, 3e, 40, 30, 43, 1b, 34, ff, 61, 60, 40, ee, 2e, 01, 0c, 8c, d4, 3e, 40, b8, 9f, d0, 0e, ff, 61, 60, 40, 5a, fb, a0, 06, 8a, d4, 3e, 40, 41, e3, 99, 02, ff, 61, 60, 40, c6, f9, 9b, 50, 88, d4, 3e, 40, cb, 96, af, 42, ff, 61, 60, 40, 79, 55, 62, fc, 86, d4, 3e, 40, da, d2, bc, 46, ff, 61, 60, 40, e7, 9d, 54, 45, 82, d4, 3e, 40, c9, 65, e8, 47, ff, 61, 60, 40, 6d, d6, cf, 51, 7d, d4, 3e, 40, c1, 8f, bb, 1f, ff, 61, 60, 40, 7f, e1, f5, 96, 7b, d4, 3e, 40, 27, 14, ab, d1, fe, 61, 60, 40, a4, 33, 2b, 39, 79, d4, 3e, 40, 73, 72, e7, 0b, ff, 61, 60, 40, fe, 88, 0b, b2, 78, d4, 3e, 40, 40, 55, c9, 23, ff, 61, 60, 40, c9, a2, ff, d1, 76, d4, 3e, 40, ea, 72, 3b, 15, ff, 61, 60, 40, 06, 2a, 2a, b5, 73, d4, 3e, 40, 95, 22, 31, f5, fe, 61, 60, 40, a3, e9, ec, 64, 70, d4, 3e, 40, ae, 26, ad, ca, fe, 61, 60, 40, e0, aa, 55, 35, 6d, d4, 3e, 40, c7, e6, 0b, a3, fe, 61, 60, 40, f3, 04, 76, c0, 6b, d4, 3e, 40, 91, 5b, 84, 43, fe, 61, 60, 40, 82, f6, f2, 54, 6a, d4, 3e, 40, f1, 2d, fd, 62, fe, 61, 60, 40, 17, 74, 4a, 59, 69, d4, 3e, 40, b5, d6, 40, 88, fe, 61, 60, 40, 60, 7f, c3, 58, 67, d4, 3e, 40, 70, f3, 0a, 67, fe, 61, 60, 40, f1, 9d, 98, f5, 62, d4, 3e, 40, 6f, 50, 14, 4b, fe, 61, 60, 40, c9, 68, 33, 3a, 5f, d4, 3e, 40, 9b, 3a, fc, 26, fe, 61, 60, 40, fa, 0c, 5c, a9, 5d, d4, 3e, 40, bc, 91, 02, d6, fd, 61, 60, 40, ee, d7, 07, d3, 5c, d4, 3e, 40, 6e, 02, ff, a0, fd, 61, 60, 40, c5, 09, b1, d2, 5b, d4, 3e, 40, 29, 47, 45, bf, fd, 61, 60, 40, 20, db, e3, 99, 58, d4, 3e, 40, 29, 30, 3c, ad, fd, 61, 60, 40, 6e, 5d, 7b, b0, 56, d4, 3e, 40, a7, cf, b0, 46, fd, 61, 60, 40, e7, 29, bc, 15, 53, d4, 3e, 40, 3f, cc, e4, ef, fc, 61, 60, 40, 8f, 9f, 28, 31, 51, d4, 3e, 40, a5, 83, 7e, 71, fc, 61, 60, 40, 71, 8f, 5e, 3a, 51, d4, 3e, 40, 14, ed, 2a, a4, fc, 61, 60, 40, c7, 85, bc, 4b, 4d, d4, 3e, 40, a5, 06, 21, c0, fc, 61, 60, 40, 09, b6, 60, 95, 4a, d4, 3e, 40, fa, d3, 88, 91, fc, 61, 60, 40, 23, a1, e6, 11, 47, d4, 3e, 40, f0, 16, 48, 50, fc, 61, 60, 40, d8, 5b, 60, 15, 43, d4, 3e, 40, d5, a4, 62, 03, fc, 61, 60, 40, 08, 6c, 22, c1, 41, d4, 3e, 40, 98, 25, 2a, e9, fb, 61, 60, 40, 5c, 7a, 94, 47, 40, d4, 3e, 40, c6, 5d, 27, 6f, fc, 61, 60, 40, b1, a2, 4d, a8, 3c, d4, 3e, 40, a3, ad, dd, 85, fc, 61, 60, 40, 59, 03, fe, 6a, 3a, d4, 3e, 40, d7, 6d, f2, 89, fc, 61, 60, 40, f6, a5, 21, 24, 37, d4, 3e, 40, c6, 5d, 27, 6f, fc, 61, 60, 40, bb, f9, fc, e6, 34, d4, 3e, 40, 9a, 7e, 6d, 23, fc, 61, 60, 40, c8, c7, 42, 02, 33, d4, 3e, 40, 11, 6e, 4e, ff, fb, 61, 60, 40, db, 21, 63, 8d, 31, d4, 3e, 40, 83, d3, 34, 88, fc, 61, 60, 40, 95, d4, 50, 3d, 2e, d4, 3e, 40, 60, 23, eb, 9e, fc, 61, 60, 40, 22, b1, 42, 69, 26, d4, 3e, 40, 72, c1, 86, 98, fc, 61, 60, 40, 5f, 55, 0c, 43, 23, d4, 3e, 40, e8, 0d, 71, 58, fc, 61, 60, 40, 84, c4, e0, db, 20, d4, 3e, 40, 33, a1, 3a, 37, fc, 61, 60, 40, 4a, 18, bc, 9e, 1e, d4, 3e, 40, 2c, 63, d6, 9a, fc, 61, 60, 40, 20, 67, 04, 95, 1d, d4, 3e, 40, 9c, fd, 49, c8, fc, 61, 60, 40, 6f, 38, 96, f1, 1b, d4, 3e, 40, 9c, fd, 49, c8, fc, 61, 60, 40, d8, 04, 67, dc, 13, d4, 3e, 40, c7, 5b, 44, 9a, fc, 61, 60, 40, 9e, 75, e1, 95, 11, d4, 3e, 40, 24, a4, b2, 84, fc, 61, 60, 40, c8, 53, f1, a7, 0f, d4, 3e, 40, 62, d7, 54, e8, fc, 61, 60, 40, c3, 78, 1c, f2, 0e, d4, 3e, 40, a7, cf, b0, 46, fd, 61, 60, 40, 5f, 38, df, a1, 0b, d4, 3e, 40, 98, 16, 46, 91, fd, 61, 60, 40, 2c, 45, ed, dc, 06, d4, 3e, 40, 7e, 6f, d3, 9f, fd, 61, 60, 40, 51, 97, 22, 7f, 04, d4, 3e, 40, b1, 8c, f1, 87, fd, 61, 60, 40, 24, d6, e2, 53, 00, d4, 3e, 40, a2, 6d, 32, 33, fe, 61, 60, 40, bf, fc, b3, be, ff, d3, 3e, 40, e1, 8d, 7e, 94, fe, 61, 60, 40, 07, 25, cc, b4, fd, d3, 3e, 40, 8c, 76, 63, e1, fe, 61, 60, 40, fd, 1c, d0, e6, f9, d3, 3e, 40, 73, 00, b8, ea, fe, 61, 60, 40, 7c, 92, 8a, b2, f6, d3, 3e, 40, 61, 62, 1c, f1, fe, 61, 60, 40, 1e, 2d, 22, 18, f4, d3, 3e, 40, 52, 4e, 8b, 2c, ff, 61, 60, 40, d1, d7, e2, 09, f3, d3, 3e, 40, 88, 8f, 5f, aa, ff, 61, 60, 40, b9, e7, 59, 46, f3, d3, 3e, 40, 6f, 41, 30, f3, ff, 61, 60, 40, 78, 16, 69, 54, f3, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, d9, bf, 4a, 23, f3, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 7d, 2c, db, 87, 8d, d3, 3e, 40, b6, 9d, 86, 0d, fe, 61, 60, 40, 5a, f8, 9c, e9, 95, d3, 3e, 40, 98, e3, 9b, c1, fd, 61, 60, 40, 6e, 8a, a1, 7e, 92, d3, 3e, 40, d5, 84, 40, 70, fd, 61, 60, 40, 7c, 7c, 0d, 84, 8e, d3, 3e, 40, 3d, 51, 35, 21, fd, 61, 60, 40, 02, 16, e7, 12, 88, d3, 3e, 40, 7c, c5, 2f, c9, fc, 61, 60, 40, e1, cb, be, d7, 82, d3, 3e, 40, 17, e0, 09, 5d, fc, 61, 60, 40, 36, 42, c5, 8c, 7e, d3, 3e, 40, 0a, 4c, 8b, 20, fc, 61, 60, 40, 08, 2c, 02, 61, 7c, d3, 3e, 40, d3, 34, e1, c4, fb, 61, 60, 40, 0a, 88, 0c, 55, 78, d3, 3e, 40, 1c, fa, b7, 63, fb, 61, 60, 40, 59, 2e, ca, 55, 74, d3, 3e, 40, df, 96, eb, 14, fb, 61, 60, 40, 9c, 49, b2, 46, 71, d3, 3e, 40, 6c, 17, 47, a3, fa, 61, 60, 40, 80, b4, 8d, ac, 6d, d3, 3e, 40, 03, be, b7, f6, f9, 61, 60, 40, ca, 7f, 76, f7, 68, d3, 3e, 40, 49, 9b, dd, 6c, f9, 61, 60, 40, 2b, f9, 75, 33, 65, d3, 3e, 40, 26, a9, 8e, 4d, f9, 61, 60, 40, 03, c7, 68, 94, 63, d3, 3e, 40, ff, 48, a1, c4, f8, 61, 60, 40, 95, cd, 7a, 03, 5c, d3, 3e, 40, 30, 38, 43, a4, f8, 61, 60, 40, 43, b9, 58, 44, 59, d3, 3e, 40, 59, 04, 85, 95, f8, 61, 60, 40, 80, 2d, 72, f9, 56, d3, 3e, 40, f1, 56, 7c, 94, f8, 61, 60, 40, 70, fe, fe, 4b, 54, d3, 3e, 40, 30, c3, 93, 9e, f8, 61, 60, 40, 2a, 6d, 7e, b7, 51, d3, 3e, 40, d9, c4, 64, d2, f8, 61, 60, 40, 08, 7e, da, fc, 4d, d3, 3e, 40, 4b, ee, 10, fc, f8, 61, 60, 40, 27, 30, 77, 0f, 4b, d3, 3e, 40, 42, 9d, e0, 05, fb, 61, 60, 40, 3c, 48, ad, 02, 45, d3, 3e, 40, 72, 8e, 65, ba, fa, 61, 60, 40, 4a, a5, 05, 53, 40, d3, 3e, 40, 27, e9, e3, 9f, fe, 61, 60, 40, 0f, f2, 63, 94, 33, d3, 3e, 40, 6e, ff, 2a, bd, ff, 61, 60, 40, 5c, b3, 83, b4, 40, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 1c, f6, 0d, 1a, 4a, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 01, ef, 6b, 8f, 3d, d3, 3e, 40, e4, 43, 32, df, ff, 61, 60, 40, f8, fd, 7a, 09, 3e, d3, 3e, 40, b2, 28, e7, e9, fe, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9a, 99, 99, 99, 99, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9a, 99, 99, 99, 99, 61, 60, 40, 55, 55, 55, 55, 55, d5, 3e, 40, ed, bb, 4a, 83, fd, 61, 60, 40, 55, 55, 55, 55, 55, d5, 3e, 40, 18, c5, 4a, 83, fd, 61, 60, 40, d4, e6, 53, 55, 55, d5, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, 06, 00, 00, 00, 00, 00, 00, 00, b2, 3d, 52, fb, ff, 61, 60, 40, 71, b6, 8b, f9, 51, d3, 3e, 40, 19, bd, 13, e6, ff, 61, 60, 40, 7d, 38, 2f, eb, 51, d3, 3e, 40, 9a, 6f, 00, da, ff, 61, 60, 40, fd, c6, 79, 2d, 58, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, e4, ec, 07, 2a, 5b, d3, 3e, 40, 00, 00, 00, 00, 00, 62, 60, 40, 6b, cd, 17, 37, 50, d3, 3e, 40, b2, 3d, 52, fb, ff, 61, 60, 40, 71, b6, 8b, f9, 51, d3, 3e, 40\n28611a56-da80-42f1-b60f-ffdcd54f9155\n2\n1\n2\n\n\n3f40251d-dea3-4e85-bf7f-ddfd7fa2b6b8\n3\n2\n05, 04, bd, 00, 00, 00, 00, 00, 99, 09, 03, 43, 88, 88, f6, 41, cd, 0c, 03, 43, 9a, 99, f6, 41, 05, 00, 00, 00, 03, 00, 00, 00, 02, 00, 00, 00, 01, 00, 00, 00, 2b, 00, 00, 00, 00, 00, 00, 00, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, a9, 3d, 8f, cb, 34, 61, 60, 40, 41, 9a, 6a, 93, 2e, d3, 3e, 40, 3a, d4, e2, 98, 34, 61, 60, 40, 90, 6b, fc, ef, 2c, d3, 3e, 40, 4a, 9c, dd, a6, 34, 61, 60, 40, 01, 13, 71, a0, 2b, d3, 3e, 40, cc, eb, f5, df, 34, 61, 60, 40, 73, 9d, 46, 5a, 2a, d3, 3e, 40, 98, 2b, e1, db, 34, 61, 60, 40, e2, 61, 8b, f1, 22, d3, 3e, 40, c1, b3, 3a, c2, 34, 61, 60, 40, e9, 37, 5d, 60, 20, d3, 3e, 40, 42, 38, e0, 9f, 34, 61, 60, 40, 61, ba, a6, c6, 1f, d3, 3e, 40, 9f, 0e, 1f, 69, 34, 61, 60, 40, f0, c8, c2, 51, 1e, d3, 3e, 40, b8, 29, a4, 50, 34, 61, 60, 40, 80, 31, 07, b3, 1c, d3, 3e, 40, da, a6, 43, 6a, 34, 61, 60, 40, 16, cc, fd, ad, 1b, d3, 3e, 40, 8f, 6e, a0, 9a, 34, 61, 60, 40, 9f, 14, 01, dc, 19, d3, 3e, 40, fe, 0a, f7, 9c, 34, 61, 60, 40, 32, 43, 8f, 8a, 12, d3, 3e, 40, 0e, c2, 7e, 7d, 34, 61, 60, 40, 93, bd, 3b, b8, 0f, d3, 3e, 40, c0, c0, 4b, 27, 34, 61, 60, 40, 58, 4b, 55, 68, 0d, d3, 3e, 40, c0, 0a, ff, 08, 34, 61, 60, 40, 47, cf, 92, 9f, 0b, d3, 3e, 40, 37, 22, 5c, 24, 34, 61, 60, 40, cc, 3c, c1, 17, 09, d3, 3e, 40, 84, 0e, 69, 3d, 34, 61, 60, 40, f6, 37, 70, 20, 07, d3, 3e, 40, f3, 4f, 99, 30, 34, 61, 60, 40, 64, b2, bd, b8, 02, d3, 3e, 40, bf, 34, 5e, 1d, 34, 61, 60, 40, 00, e9, 47, 35, ff, d2, 3e, 40, e1, 89, 81, f7, 33, 61, 60, 40, ae, 24, cd, ad, fd, d2, 3e, 40, cf, 51, 3a, 9d, 33, 61, 60, 40, 38, 01, 37, 9f, fb, d2, 3e, 40, 26, ea, 14, ca, 33, 61, 60, 40, 63, 19, 85, 9e, f9, d2, 3e, 40, 1d, 3c, 64, e1, 33, 61, 60, 40, 4c, 11, e8, 65, f7, d2, 3e, 40, 36, b2, 0f, d8, 33, 61, 60, 40, a1, cd, 07, 8a, f3, d2, 3e, 40, fa, 8d, fd, cc, 33, 61, 60, 40, d3, ed, 82, 47, ef, d2, 3e, 40, 79, e1, db, af, 33, 61, 60, 40, e0, 0a, c3, a8, ed, d2, 3e, 40, 12, 81, 06, 75, 33, 61, 60, 40, 8d, f7, 4d, db, eb, d2, 3e, 40, 23, ee, da, 73, 33, 61, 60, 40, dc, 5c, 46, fb, e9, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, cb, 4c, 1d, 6f, e8, d2, 3e, 40, ce, 7b, 99, b1, 33, 61, 60, 40, 7f, 90, cf, a5, e4, d2, 3e, 40, ad, a1, f0, b3, 33, 61, 60, 40, 17, 9a, 32, 0a, de, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 66, 88, 63, 5d, dc, d2, 3e, 40, 9a, 93, 08, 6e, 33, 61, 60, 40, 90, a0, b1, 5c, da, d2, 3e, 40, 1a, e5, 03, 7c, 33, 61, 60, 40, fc, 9e, ac, a6, d8, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, 92, 1c, 04, ab, d7, d2, 3e, 40, 79, 86, b5, a0, 33, 61, 60, 40, 60, 73, 09, e5, cf, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 0d, 92, ef, 66, ce, d2, 3e, 40, 23, 93, b4, 64, 33, 61, 60, 40, 61, 0c, fb, 29, cd, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9d, e9, 32, f0, 34, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, 06, 00, 00, 00, 00, 00, 00, 00, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, d6, 5c, f4, 69, 33, 61, 60, 40, 05, f1, 89, 8e, c7, d2, 3e, 40, de, f3, 9b, 40, 33, 61, 60, 40, 56, ba, c0, 87, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 89, b2, 46, 02, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, a5, 00, 00, 00, 00, 00, 00, 00, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40, cf, b3, 0b, 66, 39, 61, 60, 40, 66, da, b7, c0, 89, d2, 3e, 40, 15, e3, 73, 5c, 3a, 61, 60, 40, 3e, 88, b3, 0e, 86, d2, 3e, 40, 21, bd, ab, 95, 3e, 61, 60, 40, 68, ea, 29, fd, 7a, d2, 3e, 40, 3d, da, af, 70, 3f, 61, 60, 40, f7, 0d, 02, e1, 79, d2, 3e, 40, 25, 48, 0f, bd, 41, 61, 60, 40, a1, 7e, 6b, b5, 74, d2, 3e, 40, df, 70, 70, 00, 45, 61, 60, 40, 91, 61, 5c, 44, 70, d2, 3e, 40, b0, 5b, 13, f1, 47, 61, 60, 40, 3f, 6b, 86, 6d, 6e, d2, 3e, 40, 14, b1, ca, 7d, 49, 61, 60, 40, 39, a5, 6d, 10, 6e, d2, 3e, 40, c3, d2, 8b, 9d, 4a, 61, 60, 40, 2d, 21, 1f, f4, 6c, d2, 3e, 40, ee, fd, 87, a0, 4c, 61, 60, 40, c8, 96, ea, a4, 6c, d2, 3e, 40, 7f, 61, 31, 9e, 4c, 61, 60, 40, fd, 2d, fc, 3e, 6e, d2, 3e, 40, 6e, 79, e2, c2, 4c, 61, 60, 40, 72, 07, 9b, 4e, 73, d2, 3e, 40, 8a, 1c, 8f, 0a, 4d, 61, 60, 40, 04, 70, ae, bf, 77, d2, 3e, 40, fb, 28, 32, 59, 4d, 61, 60, 40, d4, e8, 24, 47, 79, d2, 3e, 40, 8c, 6a, a4, b4, 4d, 61, 60, 40, 26, fc, 99, 14, 7b, d2, 3e, 40, 85, 37, 6e, a8, 4d, 61, 60, 40, 5b, 39, 83, d8, 7c, d2, 3e, 40, 3f, d9, bd, aa, 4d, 61, 60, 40, b9, d0, 46, c2, 7f, d2, 3e, 40, fd, a7, 0e, fe, 4d, 61, 60, 40, e6, ae, 25, e4, 83, d2, 3e, 40, 8e, 8e, 5a, 4a, 4e, 61, 60, 40, 6d, e2, e4, 7e, 87, d2, 3e, 40, d4, 14, 87, 87, 4e, 61, 60, 40, b4, ab, 49, 1d, 88, d2, 3e, 40, 56, 8c, 1b, 00, 4f, 61, 60, 40, 43, 04, d5, 6c, 89, d2, 3e, 40, de, 9e, 1d, f9, 4e, 61, 60, 40, 76, 6e, 8e, fe, 8d, d2, 3e, 40, b7, df, 53, 99, 4f, 61, 60, 40, 84, d5, 0c, 34, 95, d2, 3e, 40, b7, 07, d0, d8, 4f, 61, 60, 40, 71, 7b, ec, a8, 96, d2, 3e, 40, 4a, 7a, 09, 2f, 50, 61, 60, 40, b9, 0a, 13, 5a, 97, d2, 3e, 40, 06, 18, 93, 87, 50, 61, 60, 40, 36, 53, 75, 1d, 97, d2, 3e, 40, 32, cf, d0, 93, 50, 61, 60, 40, ec, 91, 6b, e2, 9b, d2, 3e, 40, 44, df, 9b, ae, 50, 61, 60, 40, 80, e2, 6a, de, 9d, d2, 3e, 40, 23, 88, 95, ff, 50, 61, 60, 40, 42, 04, 63, 17, a1, d2, 3e, 40, e2, b1, 0c, 62, 51, 61, 60, 40, 3b, 19, d5, 4f, a3, d2, 3e, 40, 6c, 8d, 9e, e1, 51, 61, 60, 40, 05, cc, 32, 7a, a4, d2, 3e, 40, fd, 73, ea, 2d, 52, 61, 60, 40, 70, 1c, 80, 26, a5, d2, 3e, 40, 63, ac, 43, 29, 52, 61, 60, 40, a4, 51, 86, 4c, a7, d2, 3e, 40, 1f, 7f, 5a, 26, 52, 61, 60, 40, 85, a8, ca, 10, aa, d2, 3e, 40, d4, eb, 90, 47, 52, 61, 60, 40, d2, 2f, 65, 6e, ab, d2, 3e, 40, 18, 41, f6, 89, 52, 61, 60, 40, b3, 4c, 6b, 45, ae, d2, 3e, 40, a4, a8, 75, 13, 53, 61, 60, 40, a4, 6e, 6c, 18, b3, d2, 3e, 40, f2, 4e, 82, 5a, 53, 61, 60, 40, 51, c2, 05, 06, b4, d2, 3e, 40, d1, 52, a2, ba, 53, 61, 60, 40, d9, 71, 17, ef, b4, d2, 3e, 40, 15, 25, 65, ae, 53, 61, 60, 40, f6, b6, 94, 51, b7, d2, 3e, 40, 49, 9b, c6, d0, 53, 61, 60, 40, 8f, b0, 0d, b4, b9, d2, 3e, 40, a1, 8c, e4, 37, 54, 61, 60, 40, 3f, 46, 8a, 12, be, d2, 3e, 40, d5, 2a, c2, 99, 54, 61, 60, 40, c1, 3c, 69, 83, c1, d2, 3e, 40, f0, 41, 81, d7, 54, 61, 60, 40, f0, 47, a0, ad, c2, d2, 3e, 40, 02, 30, 15, 50, 55, 61, 60, 40, dd, ed, 7f, 22, c4, d2, 3e, 40, 25, 83, 55, 55, 55, 61, 60, 40, d1, a6, 18, 57, c4, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 0c, f9, b6, 4c, c5, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 17, 60, 66, 72, c6, d2, 3e, 40, 14, 9b, 06, 7a, 55, 61, 60, 40, 16, e4, 13, 24, c9, d2, 3e, 40, 39, 47, 8a, b9, 55, 61, 60, 40, 74, b5, 15, fb, cb, d2, 3e, 40, ec, 38, 46, fe, 55, 61, 60, 40, 78, 31, 37, 59, cf, d2, 3e, 40, c3, 7d, 42, 48, 56, 61, 60, 40, 12, bf, 16, 7f, d1, d2, 3e, 40, 67, 00, 47, b9, 56, 61, 60, 40, 95, 76, b4, bb, d1, d2, 3e, 40, 0a, 2a, 08, f0, 56, 61, 60, 40, 9a, bd, 22, ae, d2, d2, 3e, 40, 34, 55, 58, f2, 56, 61, 60, 40, 7b, da, 28, 85, d5, d2, 3e, 40, 60, 67, bc, 0d, 57, 61, 60, 40, b5, 86, 4d, c2, d7, d2, 3e, 40, 0d, 4e, be, 85, 57, 61, 60, 40, 01, 92, 95, d1, db, d2, 3e, 40, e3, 37, 94, c0, 57, 61, 60, 40, d0, 0a, 0c, 59, dd, d2, 3e, 40, 28, 8d, f9, 02, 58, 61, 60, 40, f9, d8, 62, 59, de, d2, 3e, 40, ab, 90, 7b, 85, 58, 61, 60, 40, be, b0, eb, cd, de, d2, 3e, 40, 44, 80, 9e, c9, 58, 61, 60, 40, e7, 12, a9, 91, df, d2, 3e, 40, d7, 6f, 35, d1, 58, 61, 60, 40, 2d, f4, 21, a5, e2, d2, 3e, 40, 72, 46, 6c, 2e, 59, 61, 60, 40, 26, 01, b1, 3f, e5, d2, 3e, 40, 6b, 08, 08, 92, 59, 61, 60, 40, a1, 1c, bb, fa, e7, d2, 3e, 40, 8e, de, ea, e5, 59, 61, 60, 40, 59, d7, 03, 0e, ea, d2, 3e, 40, f8, df, d3, 67, 5a, 61, 60, 40, ed, f5, a7, ba, eb, d2, 3e, 40, bd, 3c, 81, d6, 5a, 61, 60, 40, f2, 64, e3, 33, ec, d2, 3e, 40, ef, 26, f5, ee, 5a, 61, 60, 40, a4, b0, f0, cd, ed, d2, 3e, 40, 60, 0b, 1c, fe, 5a, 61, 60, 40, 14, 0e, 6e, 7f, ef, d2, 3e, 40, ea, e6, ad, 7d, 5b, 61, 60, 40, 54, b2, 06, 69, f2, d2, 3e, 40, 88, 80, 64, b6, 5b, 61, 60, 40, b3, 6e, 9d, 2e, f4, d2, 3e, 40, c9, ff, 05, fe, 57, 61, 60, 40, ec, 63, 7a, cc, 04, d3, 3e, 40, 8f, 4e, c0, 23, 58, 61, 60, 40, 74, 0a, 5a, 97, 06, d3, 3e, 40, 9e, 1a, 6f, f8, 5b, 61, 60, 40, c6, 88, bb, af, 07, d3, 3e, 40, 7e, 5b, a8, e3, 5e, 61, 60, 40, 26, 9a, df, e7, fa, d2, 3e, 40, ec, a3, 0c, 82, 62, 61, 60, 40, 01, e7, 92, 98, 0a, d3, 3e, 40, cd, 7c, 20, dc, 61, 61, 60, 40, 66, 5b, a5, 72, 15, d3, 3e, 40, 42, 02, c5, 16, 62, 61, 60, 40, 87, 88, 24, 45, 27, d3, 3e, 40, 06, 40, 51, e3, 62, 61, 60, 40, 81, df, aa, de, 26, d3, 3e, 40, 9a, a7, 17, d5, 62, 61, 60, 40, c5, 50, 14, 09, 22, d3, 3e, 40, 14, 98, ca, b2, 62, 61, 60, 40, a7, 23, ab, 1b, 22, d3, 3e, 40, 38, 1e, b5, 87, 62, 61, 60, 40, f5, 44, ee, 21, 15, d3, 3e, 40, 77, 15, ea, 9a, 6d, 61, 60, 40, 92, b5, fd, 60, 1d, d3, 3e, 40, 96, 1c, 3d, 9f, 6d, 61, 60, 40, 58, 1b, ec, 18, 1a, d3, 3e, 40, 2d, 0e, 0e, c6, 6d, 61, 60, 40, fc, 94, 51, cf, fc, d2, 3e, 40, ab, d7, 04, 83, 71, 61, 60, 40, 0b, d2, 6b, 37, ea, d2, 3e, 40, be, 5e, 85, 87, 75, 61, 60, 40, 13, 1a, 35, 1d, 09, d3, 3e, 40, 4e, a0, c2, f0, 75, 61, 60, 40, 85, fa, 3a, c9, 06, d3, 3e, 40, 83, cc, 3d, 40, 71, 61, 60, 40, 15, 21, 38, d8, e2, d2, 3e, 40, 77, 39, 27, 21, 71, 61, 60, 40, 7b, 57, 1e, 53, e0, d2, 3e, 40, fc, 1f, 90, e6, 70, 61, 60, 40, ca, 0a, 64, c7, dd, d2, 3e, 40, 96, ea, b6, cf, 70, 61, 60, 40, ee, 06, 18, 30, dd, d2, 3e, 40, e2, 78, 1d, f5, 70, 61, 60, 40, 95, c2, 4c, 73, dc, d2, 3e, 40, be, 50, a4, 21, 71, 61, 60, 40, d1, 60, 8d, 92, db, d2, 3e, 40, aa, f3, 70, 4e, 71, 61, 60, 40, a7, 3c, b5, 58, dc, d2, 3e, 40, 5e, 10, f4, c4, 71, 61, 60, 40, 70, 04, 7e, 94, de, d2, 3e, 40, 08, 4c, 1c, 83, 72, 61, 60, 40, f1, 85, 91, aa, e2, d2, 3e, 40, b3, 48, 8a, fd, 72, 61, 60, 40, f6, a8, d9, b2, e5, d2, 3e, 40, d4, 73, 93, 97, 73, 61, 60, 40, a0, e4, d6, f0, e9, d2, 3e, 40, 82, bb, 86, f5, 73, 61, 60, 40, 45, b0, eb, c3, ec, d2, 3e, 40, 07, 07, 0e, 77, 74, 61, 60, 40, 73, 99, a7, 2e, f1, d2, 3e, 40, d8, 77, 27, 9a, 75, 61, 60, 40, 5c, 2e, 48, 1e, fb, d2, 3e, 40, a2, 6b, 57, cf, 76, 61, 60, 40, 2d, 55, b3, 18, 06, d3, 3e, 40, 67, 12, b8, 1f, 77, 61, 60, 40, 67, 1f, 75, 85, 08, d3, 3e, 40, f2, dc, 0b, 64, 77, 61, 60, 40, ef, bb, 75, 40, 0a, d3, 3e, 40, 6e, f9, 22, 83, 77, 61, 60, 40, 65, 06, db, 9c, 0b, d3, 3e, 40, f1, a3, 96, bd, 77, 61, 60, 40, 8e, 1a, 4b, 0c, 0d, d3, 3e, 40, 43, 35, d4, 11, 78, 61, 60, 40, 46, b6, f8, b6, 0e, d3, 3e, 40, 37, 69, 48, 4c, 78, 61, 60, 40, 0a, 54, f2, f6, 0f, d3, 3e, 40, 83, 94, da, b6, 78, 61, 60, 40, 8c, 21, f9, 7d, 11, d3, 3e, 40, 0f, ff, d9, 24, 79, 61, 60, 40, 32, 2f, d1, 6a, 12, d3, 3e, 40, 52, 98, f2, 7f, 79, 61, 60, 40, c7, 69, 67, 1c, 13, d3, 3e, 40, 79, 42, c8, dc, 79, 61, 60, 40, 25, f4, f3, d9, 13, d3, 3e, 40, 3e, 60, bb, 07, 7a, 61, 60, 40, 7f, 4a, 81, 44, 14, d3, 3e, 40, ed, 66, 46, 3f, 7a, 61, 60, 40, fb, 94, 8e, 32, 15, d3, 3e, 40, f1, 0a, 0f, 5d, 7a, 61, 60, 40, 87, 5f, b4, 3e, 1a, d3, 3e, 40, 70, 83, 60, a9, 7c, 61, 60, 40, be, 5c, af, dc, 17, d3, 3e, 40, 26, 51, bd, a2, 7c, 61, 60, 40, 5a, 6e, 15, 36, 16, d3, 3e, 40, ea, b2, 98, d8, 7c, 61, 60, 40, 7e, b9, c3, e4, 15, d3, 3e, 40, c0, fc, f7, db, 7c, 61, 60, 40, ee, 8b, 0c, f1, 16, d3, 3e, 40, 75, 03, da, 5d, 7d, 61, 60, 40, 1f, 58, b1, 9f, 15, d3, 3e, 40, ee, 07, 16, 69, 7d, 61, 60, 40, 38, b9, 00, b0, 11, d3, 3e, 40, 4f, f1, 01, 7f, 7d, 61, 60, 40, 27, 02, b1, 79, 10, d3, 3e, 40, e4, 3c, 5c, a5, 7d, 61, 60, 40, 93, 27, 2a, ca, 0e, d3, 3e, 40, d1, 98, c0, e2, 7d, 61, 60, 40, 11, 34, a3, 75, 0d, d3, 3e, 40, ea, a6, 34, 65, 7e, 61, 60, 40, 24, 3e, cb, 81, 0b, d3, 3e, 40, f5, 06, 6c, c2, 7e, 61, 60, 40, ef, 1e, 2e, a6, 0a, d3, 3e, 40, 31, 83, 24, f8, 7e, 61, 60, 40, 71, 07, 29, f9, 0a, d3, 3e, 40, be, 9a, e0, f4, 7e, 61, 60, 40, d0, c2, 12, cd, 0b, d3, 3e, 40, 50, 62, fb, bc, 7e, 61, 60, 40, 1d, a2, 88, 47, 0d, d3, 3e, 40, 5f, fc, 79, c2, 7e, 61, 60, 40, 6a, a9, cb, 48, 0e, d3, 3e, 40, 44, 38, fe, f5, 7e, 61, 60, 40, ed, c1, 76, c0, 0d, d3, 3e, 40, 2e, 05, f5, 44, 7f, 61, 60, 40, 23, a3, 2e, 12, 0d, d3, 3e, 40, 7f, 95, ca, 7b, 7f, 61, 60, 40, 7c, 83, 43, 30, 0d, d3, 3e, 40, c2, 61, 8d, a6, 7f, 61, 60, 40, 1d, 98, a9, 37, 0d, d3, 3e, 40, 3c, a5, 83, f5, 7f, 61, 60, 40, 35, 80, 4f, 5d, 0d, d3, 3e, 40, 6b, d2, 42, 09, 80, 61, 60, 40, 7b, b7, 49, aa, 0e, d3, 3e, 40, 4d, 8a, 87, de, 7f, 61, 60, 40, c2, 2a, 2d, 0f, 11, d3, 3e, 40, a6, 75, 95, 9a, 7f, 61, 60, 40, 0e, 3d, fc, 11, 13, d3, 3e, 40, 16, 34, 58, 31, 7f, 61, 60, 40, 90, 3b, 0f, 68, 15, d3, 3e, 40, 2a, fa, 5f, 88, 7e, 61, 60, 40, 3b, 89, 1f, 9b, 18, d3, 3e, 40, bf, 09, 80, 4f, 7e, 61, 60, 40, 88, 86, 83, 8c, 19, d3, 3e, 40, e5, f0, 90, fc, 7d, 61, 60, 40, e1, 5c, 68, 53, 1a, d3, 3e, 40, 69, a9, b2, ab, 7d, 61, 60, 40, ab, 2c, b6, bb, 1a, d3, 3e, 40, e9, 8b, c9, 8a, 7d, 61, 60, 40, b1, 36, 3d, 5d, 1a, d3, 3e, 40, cc, 57, cc, 7f, 7d, 61, 60, 40, 7c, 5c, bb, b7, 19, d3, 3e, 40, 5b, d1, 16, 72, 7d, 61, 60, 40, 0b, 64, a1, 96, 19, d3, 3e, 40, 33, 40, 62, ee, 7c, 61, 60, 40, b6, cd, a3, ce, 1b, d3, 3e, 40, c9, 1f, 58, ca, 7c, 61, 60, 40, 38, 07, 44, 92, 1d, d3, 3e, 40, 78, 2e, c6, 9f, 7c, 61, 60, 40, 1a, 97, 6a, 99, 1e, d3, 3e, 40, 0a, b8, 91, f9, 7a, 61, 60, 40, 34, ed, 23, 36, 26, d3, 3e, 40, b9, 6e, 24, b2, 7a, 61, 60, 40, 33, aa, 62, e3, 27, d3, 3e, 40, fd, fb, 96, 7d, 7a, 61, 60, 40, fc, f1, d3, c2, 29, d3, 3e, 40, 78, bf, 6a, 62, 7a, 61, 60, 40, 6d, 4f, 51, 74, 2b, d3, 3e, 40, d8, 66, 1c, 50, 7a, 61, 60, 40, b9, ce, b7, ec, 2d, d3, 3e, 40, 81, 84, 59, 4f, 7a, 61, 60, 40, 6b, 4b, 22, 9d, 2f, d3, 3e, 40, 71, 8c, 34, 56, 7a, 61, 60, 40, 9f, 6c, 6a, a3, 31, d3, 3e, 40, e6, 44, 83, 60, 7a, 61, 60, 40, f8, 93, 45, 22, 33, d3, 3e, 40, f2, 21, 4d, 61, 7a, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 1c, c3, 6a, f2, ad, d2, 3e, 40, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40\n3121f7f1-9e78-42bd-bfd7-2ac6261c808f\n3\n2\n1\n\n\n3f40251d-dea3-4e85-bf7f-ddfd7fa2b6b8\n3\n2\n05, 04, bd, 00, 00, 00, 00, 00, 99, 09, 03, 43, 88, 88, f6, 41, cd, 0c, 03, 43, 9a, 99, f6, 41, 05, 00, 00, 00, 03, 00, 00, 00, 02, 00, 00, 00, 01, 00, 00, 00, 2b, 00, 00, 00, 00, 00, 00, 00, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, a9, 3d, 8f, cb, 34, 61, 60, 40, 41, 9a, 6a, 93, 2e, d3, 3e, 40, 3a, d4, e2, 98, 34, 61, 60, 40, 90, 6b, fc, ef, 2c, d3, 3e, 40, 4a, 9c, dd, a6, 34, 61, 60, 40, 01, 13, 71, a0, 2b, d3, 3e, 40, cc, eb, f5, df, 34, 61, 60, 40, 73, 9d, 46, 5a, 2a, d3, 3e, 40, 98, 2b, e1, db, 34, 61, 60, 40, e2, 61, 8b, f1, 22, d3, 3e, 40, c1, b3, 3a, c2, 34, 61, 60, 40, e9, 37, 5d, 60, 20, d3, 3e, 40, 42, 38, e0, 9f, 34, 61, 60, 40, 61, ba, a6, c6, 1f, d3, 3e, 40, 9f, 0e, 1f, 69, 34, 61, 60, 40, f0, c8, c2, 51, 1e, d3, 3e, 40, b8, 29, a4, 50, 34, 61, 60, 40, 80, 31, 07, b3, 1c, d3, 3e, 40, da, a6, 43, 6a, 34, 61, 60, 40, 16, cc, fd, ad, 1b, d3, 3e, 40, 8f, 6e, a0, 9a, 34, 61, 60, 40, 9f, 14, 01, dc, 19, d3, 3e, 40, fe, 0a, f7, 9c, 34, 61, 60, 40, 32, 43, 8f, 8a, 12, d3, 3e, 40, 0e, c2, 7e, 7d, 34, 61, 60, 40, 93, bd, 3b, b8, 0f, d3, 3e, 40, c0, c0, 4b, 27, 34, 61, 60, 40, 58, 4b, 55, 68, 0d, d3, 3e, 40, c0, 0a, ff, 08, 34, 61, 60, 40, 47, cf, 92, 9f, 0b, d3, 3e, 40, 37, 22, 5c, 24, 34, 61, 60, 40, cc, 3c, c1, 17, 09, d3, 3e, 40, 84, 0e, 69, 3d, 34, 61, 60, 40, f6, 37, 70, 20, 07, d3, 3e, 40, f3, 4f, 99, 30, 34, 61, 60, 40, 64, b2, bd, b8, 02, d3, 3e, 40, bf, 34, 5e, 1d, 34, 61, 60, 40, 00, e9, 47, 35, ff, d2, 3e, 40, e1, 89, 81, f7, 33, 61, 60, 40, ae, 24, cd, ad, fd, d2, 3e, 40, cf, 51, 3a, 9d, 33, 61, 60, 40, 38, 01, 37, 9f, fb, d2, 3e, 40, 26, ea, 14, ca, 33, 61, 60, 40, 63, 19, 85, 9e, f9, d2, 3e, 40, 1d, 3c, 64, e1, 33, 61, 60, 40, 4c, 11, e8, 65, f7, d2, 3e, 40, 36, b2, 0f, d8, 33, 61, 60, 40, a1, cd, 07, 8a, f3, d2, 3e, 40, fa, 8d, fd, cc, 33, 61, 60, 40, d3, ed, 82, 47, ef, d2, 3e, 40, 79, e1, db, af, 33, 61, 60, 40, e0, 0a, c3, a8, ed, d2, 3e, 40, 12, 81, 06, 75, 33, 61, 60, 40, 8d, f7, 4d, db, eb, d2, 3e, 40, 23, ee, da, 73, 33, 61, 60, 40, dc, 5c, 46, fb, e9, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, cb, 4c, 1d, 6f, e8, d2, 3e, 40, ce, 7b, 99, b1, 33, 61, 60, 40, 7f, 90, cf, a5, e4, d2, 3e, 40, ad, a1, f0, b3, 33, 61, 60, 40, 17, 9a, 32, 0a, de, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 66, 88, 63, 5d, dc, d2, 3e, 40, 9a, 93, 08, 6e, 33, 61, 60, 40, 90, a0, b1, 5c, da, d2, 3e, 40, 1a, e5, 03, 7c, 33, 61, 60, 40, fc, 9e, ac, a6, d8, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, 92, 1c, 04, ab, d7, d2, 3e, 40, 79, 86, b5, a0, 33, 61, 60, 40, 60, 73, 09, e5, cf, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 0d, 92, ef, 66, ce, d2, 3e, 40, 23, 93, b4, 64, 33, 61, 60, 40, 61, 0c, fb, 29, cd, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9d, e9, 32, f0, 34, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, 06, 00, 00, 00, 00, 00, 00, 00, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, d6, 5c, f4, 69, 33, 61, 60, 40, 05, f1, 89, 8e, c7, d2, 3e, 40, de, f3, 9b, 40, 33, 61, 60, 40, 56, ba, c0, 87, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 89, b2, 46, 02, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, a5, 00, 00, 00, 00, 00, 00, 00, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40, cf, b3, 0b, 66, 39, 61, 60, 40, 66, da, b7, c0, 89, d2, 3e, 40, 15, e3, 73, 5c, 3a, 61, 60, 40, 3e, 88, b3, 0e, 86, d2, 3e, 40, 21, bd, ab, 95, 3e, 61, 60, 40, 68, ea, 29, fd, 7a, d2, 3e, 40, 3d, da, af, 70, 3f, 61, 60, 40, f7, 0d, 02, e1, 79, d2, 3e, 40, 25, 48, 0f, bd, 41, 61, 60, 40, a1, 7e, 6b, b5, 74, d2, 3e, 40, df, 70, 70, 00, 45, 61, 60, 40, 91, 61, 5c, 44, 70, d2, 3e, 40, b0, 5b, 13, f1, 47, 61, 60, 40, 3f, 6b, 86, 6d, 6e, d2, 3e, 40, 14, b1, ca, 7d, 49, 61, 60, 40, 39, a5, 6d, 10, 6e, d2, 3e, 40, c3, d2, 8b, 9d, 4a, 61, 60, 40, 2d, 21, 1f, f4, 6c, d2, 3e, 40, ee, fd, 87, a0, 4c, 61, 60, 40, c8, 96, ea, a4, 6c, d2, 3e, 40, 7f, 61, 31, 9e, 4c, 61, 60, 40, fd, 2d, fc, 3e, 6e, d2, 3e, 40, 6e, 79, e2, c2, 4c, 61, 60, 40, 72, 07, 9b, 4e, 73, d2, 3e, 40, 8a, 1c, 8f, 0a, 4d, 61, 60, 40, 04, 70, ae, bf, 77, d2, 3e, 40, fb, 28, 32, 59, 4d, 61, 60, 40, d4, e8, 24, 47, 79, d2, 3e, 40, 8c, 6a, a4, b4, 4d, 61, 60, 40, 26, fc, 99, 14, 7b, d2, 3e, 40, 85, 37, 6e, a8, 4d, 61, 60, 40, 5b, 39, 83, d8, 7c, d2, 3e, 40, 3f, d9, bd, aa, 4d, 61, 60, 40, b9, d0, 46, c2, 7f, d2, 3e, 40, fd, a7, 0e, fe, 4d, 61, 60, 40, e6, ae, 25, e4, 83, d2, 3e, 40, 8e, 8e, 5a, 4a, 4e, 61, 60, 40, 6d, e2, e4, 7e, 87, d2, 3e, 40, d4, 14, 87, 87, 4e, 61, 60, 40, b4, ab, 49, 1d, 88, d2, 3e, 40, 56, 8c, 1b, 00, 4f, 61, 60, 40, 43, 04, d5, 6c, 89, d2, 3e, 40, de, 9e, 1d, f9, 4e, 61, 60, 40, 76, 6e, 8e, fe, 8d, d2, 3e, 40, b7, df, 53, 99, 4f, 61, 60, 40, 84, d5, 0c, 34, 95, d2, 3e, 40, b7, 07, d0, d8, 4f, 61, 60, 40, 71, 7b, ec, a8, 96, d2, 3e, 40, 4a, 7a, 09, 2f, 50, 61, 60, 40, b9, 0a, 13, 5a, 97, d2, 3e, 40, 06, 18, 93, 87, 50, 61, 60, 40, 36, 53, 75, 1d, 97, d2, 3e, 40, 32, cf, d0, 93, 50, 61, 60, 40, ec, 91, 6b, e2, 9b, d2, 3e, 40, 44, df, 9b, ae, 50, 61, 60, 40, 80, e2, 6a, de, 9d, d2, 3e, 40, 23, 88, 95, ff, 50, 61, 60, 40, 42, 04, 63, 17, a1, d2, 3e, 40, e2, b1, 0c, 62, 51, 61, 60, 40, 3b, 19, d5, 4f, a3, d2, 3e, 40, 6c, 8d, 9e, e1, 51, 61, 60, 40, 05, cc, 32, 7a, a4, d2, 3e, 40, fd, 73, ea, 2d, 52, 61, 60, 40, 70, 1c, 80, 26, a5, d2, 3e, 40, 63, ac, 43, 29, 52, 61, 60, 40, a4, 51, 86, 4c, a7, d2, 3e, 40, 1f, 7f, 5a, 26, 52, 61, 60, 40, 85, a8, ca, 10, aa, d2, 3e, 40, d4, eb, 90, 47, 52, 61, 60, 40, d2, 2f, 65, 6e, ab, d2, 3e, 40, 18, 41, f6, 89, 52, 61, 60, 40, b3, 4c, 6b, 45, ae, d2, 3e, 40, a4, a8, 75, 13, 53, 61, 60, 40, a4, 6e, 6c, 18, b3, d2, 3e, 40, f2, 4e, 82, 5a, 53, 61, 60, 40, 51, c2, 05, 06, b4, d2, 3e, 40, d1, 52, a2, ba, 53, 61, 60, 40, d9, 71, 17, ef, b4, d2, 3e, 40, 15, 25, 65, ae, 53, 61, 60, 40, f6, b6, 94, 51, b7, d2, 3e, 40, 49, 9b, c6, d0, 53, 61, 60, 40, 8f, b0, 0d, b4, b9, d2, 3e, 40, a1, 8c, e4, 37, 54, 61, 60, 40, 3f, 46, 8a, 12, be, d2, 3e, 40, d5, 2a, c2, 99, 54, 61, 60, 40, c1, 3c, 69, 83, c1, d2, 3e, 40, f0, 41, 81, d7, 54, 61, 60, 40, f0, 47, a0, ad, c2, d2, 3e, 40, 02, 30, 15, 50, 55, 61, 60, 40, dd, ed, 7f, 22, c4, d2, 3e, 40, 25, 83, 55, 55, 55, 61, 60, 40, d1, a6, 18, 57, c4, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 0c, f9, b6, 4c, c5, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 17, 60, 66, 72, c6, d2, 3e, 40, 14, 9b, 06, 7a, 55, 61, 60, 40, 16, e4, 13, 24, c9, d2, 3e, 40, 39, 47, 8a, b9, 55, 61, 60, 40, 74, b5, 15, fb, cb, d2, 3e, 40, ec, 38, 46, fe, 55, 61, 60, 40, 78, 31, 37, 59, cf, d2, 3e, 40, c3, 7d, 42, 48, 56, 61, 60, 40, 12, bf, 16, 7f, d1, d2, 3e, 40, 67, 00, 47, b9, 56, 61, 60, 40, 95, 76, b4, bb, d1, d2, 3e, 40, 0a, 2a, 08, f0, 56, 61, 60, 40, 9a, bd, 22, ae, d2, d2, 3e, 40, 34, 55, 58, f2, 56, 61, 60, 40, 7b, da, 28, 85, d5, d2, 3e, 40, 60, 67, bc, 0d, 57, 61, 60, 40, b5, 86, 4d, c2, d7, d2, 3e, 40, 0d, 4e, be, 85, 57, 61, 60, 40, 01, 92, 95, d1, db, d2, 3e, 40, e3, 37, 94, c0, 57, 61, 60, 40, d0, 0a, 0c, 59, dd, d2, 3e, 40, 28, 8d, f9, 02, 58, 61, 60, 40, f9, d8, 62, 59, de, d2, 3e, 40, ab, 90, 7b, 85, 58, 61, 60, 40, be, b0, eb, cd, de, d2, 3e, 40, 44, 80, 9e, c9, 58, 61, 60, 40, e7, 12, a9, 91, df, d2, 3e, 40, d7, 6f, 35, d1, 58, 61, 60, 40, 2d, f4, 21, a5, e2, d2, 3e, 40, 72, 46, 6c, 2e, 59, 61, 60, 40, 26, 01, b1, 3f, e5, d2, 3e, 40, 6b, 08, 08, 92, 59, 61, 60, 40, a1, 1c, bb, fa, e7, d2, 3e, 40, 8e, de, ea, e5, 59, 61, 60, 40, 59, d7, 03, 0e, ea, d2, 3e, 40, f8, df, d3, 67, 5a, 61, 60, 40, ed, f5, a7, ba, eb, d2, 3e, 40, bd, 3c, 81, d6, 5a, 61, 60, 40, f2, 64, e3, 33, ec, d2, 3e, 40, ef, 26, f5, ee, 5a, 61, 60, 40, a4, b0, f0, cd, ed, d2, 3e, 40, 60, 0b, 1c, fe, 5a, 61, 60, 40, 14, 0e, 6e, 7f, ef, d2, 3e, 40, ea, e6, ad, 7d, 5b, 61, 60, 40, 54, b2, 06, 69, f2, d2, 3e, 40, 88, 80, 64, b6, 5b, 61, 60, 40, b3, 6e, 9d, 2e, f4, d2, 3e, 40, c9, ff, 05, fe, 57, 61, 60, 40, ec, 63, 7a, cc, 04, d3, 3e, 40, 8f, 4e, c0, 23, 58, 61, 60, 40, 74, 0a, 5a, 97, 06, d3, 3e, 40, 9e, 1a, 6f, f8, 5b, 61, 60, 40, c6, 88, bb, af, 07, d3, 3e, 40, 7e, 5b, a8, e3, 5e, 61, 60, 40, 26, 9a, df, e7, fa, d2, 3e, 40, ec, a3, 0c, 82, 62, 61, 60, 40, 01, e7, 92, 98, 0a, d3, 3e, 40, cd, 7c, 20, dc, 61, 61, 60, 40, 66, 5b, a5, 72, 15, d3, 3e, 40, 42, 02, c5, 16, 62, 61, 60, 40, 87, 88, 24, 45, 27, d3, 3e, 40, 06, 40, 51, e3, 62, 61, 60, 40, 81, df, aa, de, 26, d3, 3e, 40, 9a, a7, 17, d5, 62, 61, 60, 40, c5, 50, 14, 09, 22, d3, 3e, 40, 14, 98, ca, b2, 62, 61, 60, 40, a7, 23, ab, 1b, 22, d3, 3e, 40, 38, 1e, b5, 87, 62, 61, 60, 40, f5, 44, ee, 21, 15, d3, 3e, 40, 77, 15, ea, 9a, 6d, 61, 60, 40, 92, b5, fd, 60, 1d, d3, 3e, 40, 96, 1c, 3d, 9f, 6d, 61, 60, 40, 58, 1b, ec, 18, 1a, d3, 3e, 40, 2d, 0e, 0e, c6, 6d, 61, 60, 40, fc, 94, 51, cf, fc, d2, 3e, 40, ab, d7, 04, 83, 71, 61, 60, 40, 0b, d2, 6b, 37, ea, d2, 3e, 40, be, 5e, 85, 87, 75, 61, 60, 40, 13, 1a, 35, 1d, 09, d3, 3e, 40, 4e, a0, c2, f0, 75, 61, 60, 40, 85, fa, 3a, c9, 06, d3, 3e, 40, 83, cc, 3d, 40, 71, 61, 60, 40, 15, 21, 38, d8, e2, d2, 3e, 40, 77, 39, 27, 21, 71, 61, 60, 40, 7b, 57, 1e, 53, e0, d2, 3e, 40, fc, 1f, 90, e6, 70, 61, 60, 40, ca, 0a, 64, c7, dd, d2, 3e, 40, 96, ea, b6, cf, 70, 61, 60, 40, ee, 06, 18, 30, dd, d2, 3e, 40, e2, 78, 1d, f5, 70, 61, 60, 40, 95, c2, 4c, 73, dc, d2, 3e, 40, be, 50, a4, 21, 71, 61, 60, 40, d1, 60, 8d, 92, db, d2, 3e, 40, aa, f3, 70, 4e, 71, 61, 60, 40, a7, 3c, b5, 58, dc, d2, 3e, 40, 5e, 10, f4, c4, 71, 61, 60, 40, 70, 04, 7e, 94, de, d2, 3e, 40, 08, 4c, 1c, 83, 72, 61, 60, 40, f1, 85, 91, aa, e2, d2, 3e, 40, b3, 48, 8a, fd, 72, 61, 60, 40, f6, a8, d9, b2, e5, d2, 3e, 40, d4, 73, 93, 97, 73, 61, 60, 40, a0, e4, d6, f0, e9, d2, 3e, 40, 82, bb, 86, f5, 73, 61, 60, 40, 45, b0, eb, c3, ec, d2, 3e, 40, 07, 07, 0e, 77, 74, 61, 60, 40, 73, 99, a7, 2e, f1, d2, 3e, 40, d8, 77, 27, 9a, 75, 61, 60, 40, 5c, 2e, 48, 1e, fb, d2, 3e, 40, a2, 6b, 57, cf, 76, 61, 60, 40, 2d, 55, b3, 18, 06, d3, 3e, 40, 67, 12, b8, 1f, 77, 61, 60, 40, 67, 1f, 75, 85, 08, d3, 3e, 40, f2, dc, 0b, 64, 77, 61, 60, 40, ef, bb, 75, 40, 0a, d3, 3e, 40, 6e, f9, 22, 83, 77, 61, 60, 40, 65, 06, db, 9c, 0b, d3, 3e, 40, f1, a3, 96, bd, 77, 61, 60, 40, 8e, 1a, 4b, 0c, 0d, d3, 3e, 40, 43, 35, d4, 11, 78, 61, 60, 40, 46, b6, f8, b6, 0e, d3, 3e, 40, 37, 69, 48, 4c, 78, 61, 60, 40, 0a, 54, f2, f6, 0f, d3, 3e, 40, 83, 94, da, b6, 78, 61, 60, 40, 8c, 21, f9, 7d, 11, d3, 3e, 40, 0f, ff, d9, 24, 79, 61, 60, 40, 32, 2f, d1, 6a, 12, d3, 3e, 40, 52, 98, f2, 7f, 79, 61, 60, 40, c7, 69, 67, 1c, 13, d3, 3e, 40, 79, 42, c8, dc, 79, 61, 60, 40, 25, f4, f3, d9, 13, d3, 3e, 40, 3e, 60, bb, 07, 7a, 61, 60, 40, 7f, 4a, 81, 44, 14, d3, 3e, 40, ed, 66, 46, 3f, 7a, 61, 60, 40, fb, 94, 8e, 32, 15, d3, 3e, 40, f1, 0a, 0f, 5d, 7a, 61, 60, 40, 87, 5f, b4, 3e, 1a, d3, 3e, 40, 70, 83, 60, a9, 7c, 61, 60, 40, be, 5c, af, dc, 17, d3, 3e, 40, 26, 51, bd, a2, 7c, 61, 60, 40, 5a, 6e, 15, 36, 16, d3, 3e, 40, ea, b2, 98, d8, 7c, 61, 60, 40, 7e, b9, c3, e4, 15, d3, 3e, 40, c0, fc, f7, db, 7c, 61, 60, 40, ee, 8b, 0c, f1, 16, d3, 3e, 40, 75, 03, da, 5d, 7d, 61, 60, 40, 1f, 58, b1, 9f, 15, d3, 3e, 40, ee, 07, 16, 69, 7d, 61, 60, 40, 38, b9, 00, b0, 11, d3, 3e, 40, 4f, f1, 01, 7f, 7d, 61, 60, 40, 27, 02, b1, 79, 10, d3, 3e, 40, e4, 3c, 5c, a5, 7d, 61, 60, 40, 93, 27, 2a, ca, 0e, d3, 3e, 40, d1, 98, c0, e2, 7d, 61, 60, 40, 11, 34, a3, 75, 0d, d3, 3e, 40, ea, a6, 34, 65, 7e, 61, 60, 40, 24, 3e, cb, 81, 0b, d3, 3e, 40, f5, 06, 6c, c2, 7e, 61, 60, 40, ef, 1e, 2e, a6, 0a, d3, 3e, 40, 31, 83, 24, f8, 7e, 61, 60, 40, 71, 07, 29, f9, 0a, d3, 3e, 40, be, 9a, e0, f4, 7e, 61, 60, 40, d0, c2, 12, cd, 0b, d3, 3e, 40, 50, 62, fb, bc, 7e, 61, 60, 40, 1d, a2, 88, 47, 0d, d3, 3e, 40, 5f, fc, 79, c2, 7e, 61, 60, 40, 6a, a9, cb, 48, 0e, d3, 3e, 40, 44, 38, fe, f5, 7e, 61, 60, 40, ed, c1, 76, c0, 0d, d3, 3e, 40, 2e, 05, f5, 44, 7f, 61, 60, 40, 23, a3, 2e, 12, 0d, d3, 3e, 40, 7f, 95, ca, 7b, 7f, 61, 60, 40, 7c, 83, 43, 30, 0d, d3, 3e, 40, c2, 61, 8d, a6, 7f, 61, 60, 40, 1d, 98, a9, 37, 0d, d3, 3e, 40, 3c, a5, 83, f5, 7f, 61, 60, 40, 35, 80, 4f, 5d, 0d, d3, 3e, 40, 6b, d2, 42, 09, 80, 61, 60, 40, 7b, b7, 49, aa, 0e, d3, 3e, 40, 4d, 8a, 87, de, 7f, 61, 60, 40, c2, 2a, 2d, 0f, 11, d3, 3e, 40, a6, 75, 95, 9a, 7f, 61, 60, 40, 0e, 3d, fc, 11, 13, d3, 3e, 40, 16, 34, 58, 31, 7f, 61, 60, 40, 90, 3b, 0f, 68, 15, d3, 3e, 40, 2a, fa, 5f, 88, 7e, 61, 60, 40, 3b, 89, 1f, 9b, 18, d3, 3e, 40, bf, 09, 80, 4f, 7e, 61, 60, 40, 88, 86, 83, 8c, 19, d3, 3e, 40, e5, f0, 90, fc, 7d, 61, 60, 40, e1, 5c, 68, 53, 1a, d3, 3e, 40, 69, a9, b2, ab, 7d, 61, 60, 40, ab, 2c, b6, bb, 1a, d3, 3e, 40, e9, 8b, c9, 8a, 7d, 61, 60, 40, b1, 36, 3d, 5d, 1a, d3, 3e, 40, cc, 57, cc, 7f, 7d, 61, 60, 40, 7c, 5c, bb, b7, 19, d3, 3e, 40, 5b, d1, 16, 72, 7d, 61, 60, 40, 0b, 64, a1, 96, 19, d3, 3e, 40, 33, 40, 62, ee, 7c, 61, 60, 40, b6, cd, a3, ce, 1b, d3, 3e, 40, c9, 1f, 58, ca, 7c, 61, 60, 40, 38, 07, 44, 92, 1d, d3, 3e, 40, 78, 2e, c6, 9f, 7c, 61, 60, 40, 1a, 97, 6a, 99, 1e, d3, 3e, 40, 0a, b8, 91, f9, 7a, 61, 60, 40, 34, ed, 23, 36, 26, d3, 3e, 40, b9, 6e, 24, b2, 7a, 61, 60, 40, 33, aa, 62, e3, 27, d3, 3e, 40, fd, fb, 96, 7d, 7a, 61, 60, 40, fc, f1, d3, c2, 29, d3, 3e, 40, 78, bf, 6a, 62, 7a, 61, 60, 40, 6d, 4f, 51, 74, 2b, d3, 3e, 40, d8, 66, 1c, 50, 7a, 61, 60, 40, b9, ce, b7, ec, 2d, d3, 3e, 40, 81, 84, 59, 4f, 7a, 61, 60, 40, 6b, 4b, 22, 9d, 2f, d3, 3e, 40, 71, 8c, 34, 56, 7a, 61, 60, 40, 9f, 6c, 6a, a3, 31, d3, 3e, 40, e6, 44, 83, 60, 7a, 61, 60, 40, f8, 93, 45, 22, 33, d3, 3e, 40, f2, 21, 4d, 61, 7a, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 1c, c3, 6a, f2, ad, d2, 3e, 40, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40\n3121f7f1-9e78-42bd-bfd7-2ac6261c808f\n3\n2\n2\n\n\n3f40251d-dea3-4e85-bf7f-ddfd7fa2b6b8\n3\n2\n05, 04, bd, 00, 00, 00, 00, 00, 99, 09, 03, 43, 88, 88, f6, 41, cd, 0c, 03, 43, 9a, 99, f6, 41, 05, 00, 00, 00, 03, 00, 00, 00, 02, 00, 00, 00, 01, 00, 00, 00, 2b, 00, 00, 00, 00, 00, 00, 00, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, a9, 3d, 8f, cb, 34, 61, 60, 40, 41, 9a, 6a, 93, 2e, d3, 3e, 40, 3a, d4, e2, 98, 34, 61, 60, 40, 90, 6b, fc, ef, 2c, d3, 3e, 40, 4a, 9c, dd, a6, 34, 61, 60, 40, 01, 13, 71, a0, 2b, d3, 3e, 40, cc, eb, f5, df, 34, 61, 60, 40, 73, 9d, 46, 5a, 2a, d3, 3e, 40, 98, 2b, e1, db, 34, 61, 60, 40, e2, 61, 8b, f1, 22, d3, 3e, 40, c1, b3, 3a, c2, 34, 61, 60, 40, e9, 37, 5d, 60, 20, d3, 3e, 40, 42, 38, e0, 9f, 34, 61, 60, 40, 61, ba, a6, c6, 1f, d3, 3e, 40, 9f, 0e, 1f, 69, 34, 61, 60, 40, f0, c8, c2, 51, 1e, d3, 3e, 40, b8, 29, a4, 50, 34, 61, 60, 40, 80, 31, 07, b3, 1c, d3, 3e, 40, da, a6, 43, 6a, 34, 61, 60, 40, 16, cc, fd, ad, 1b, d3, 3e, 40, 8f, 6e, a0, 9a, 34, 61, 60, 40, 9f, 14, 01, dc, 19, d3, 3e, 40, fe, 0a, f7, 9c, 34, 61, 60, 40, 32, 43, 8f, 8a, 12, d3, 3e, 40, 0e, c2, 7e, 7d, 34, 61, 60, 40, 93, bd, 3b, b8, 0f, d3, 3e, 40, c0, c0, 4b, 27, 34, 61, 60, 40, 58, 4b, 55, 68, 0d, d3, 3e, 40, c0, 0a, ff, 08, 34, 61, 60, 40, 47, cf, 92, 9f, 0b, d3, 3e, 40, 37, 22, 5c, 24, 34, 61, 60, 40, cc, 3c, c1, 17, 09, d3, 3e, 40, 84, 0e, 69, 3d, 34, 61, 60, 40, f6, 37, 70, 20, 07, d3, 3e, 40, f3, 4f, 99, 30, 34, 61, 60, 40, 64, b2, bd, b8, 02, d3, 3e, 40, bf, 34, 5e, 1d, 34, 61, 60, 40, 00, e9, 47, 35, ff, d2, 3e, 40, e1, 89, 81, f7, 33, 61, 60, 40, ae, 24, cd, ad, fd, d2, 3e, 40, cf, 51, 3a, 9d, 33, 61, 60, 40, 38, 01, 37, 9f, fb, d2, 3e, 40, 26, ea, 14, ca, 33, 61, 60, 40, 63, 19, 85, 9e, f9, d2, 3e, 40, 1d, 3c, 64, e1, 33, 61, 60, 40, 4c, 11, e8, 65, f7, d2, 3e, 40, 36, b2, 0f, d8, 33, 61, 60, 40, a1, cd, 07, 8a, f3, d2, 3e, 40, fa, 8d, fd, cc, 33, 61, 60, 40, d3, ed, 82, 47, ef, d2, 3e, 40, 79, e1, db, af, 33, 61, 60, 40, e0, 0a, c3, a8, ed, d2, 3e, 40, 12, 81, 06, 75, 33, 61, 60, 40, 8d, f7, 4d, db, eb, d2, 3e, 40, 23, ee, da, 73, 33, 61, 60, 40, dc, 5c, 46, fb, e9, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, cb, 4c, 1d, 6f, e8, d2, 3e, 40, ce, 7b, 99, b1, 33, 61, 60, 40, 7f, 90, cf, a5, e4, d2, 3e, 40, ad, a1, f0, b3, 33, 61, 60, 40, 17, 9a, 32, 0a, de, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 66, 88, 63, 5d, dc, d2, 3e, 40, 9a, 93, 08, 6e, 33, 61, 60, 40, 90, a0, b1, 5c, da, d2, 3e, 40, 1a, e5, 03, 7c, 33, 61, 60, 40, fc, 9e, ac, a6, d8, d2, 3e, 40, 83, 1b, 7a, a2, 33, 61, 60, 40, 92, 1c, 04, ab, d7, d2, 3e, 40, 79, 86, b5, a0, 33, 61, 60, 40, 60, 73, 09, e5, cf, d2, 3e, 40, 8b, c9, 2a, 8b, 33, 61, 60, 40, 0d, 92, ef, 66, ce, d2, 3e, 40, 23, 93, b4, 64, 33, 61, 60, 40, 61, 0c, fb, 29, cd, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 9d, e9, 32, f0, 34, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 18, 4c, 15, ef, 34, 61, 60, 40, e1, 1f, be, 65, 31, d3, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, 06, 00, 00, 00, 00, 00, 00, 00, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, d6, 5c, f4, 69, 33, 61, 60, 40, 05, f1, 89, 8e, c7, d2, 3e, 40, de, f3, 9b, 40, 33, 61, 60, 40, 56, ba, c0, 87, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 89, b2, 46, 02, c0, d2, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 6e, da, 40, 45, cb, d2, 3e, 40, ac, d6, 7d, 58, 33, 61, 60, 40, 3a, d7, 95, 6e, c9, d2, 3e, 40, 02, 00, 00, 00, 01, 00, 00, 00, a5, 00, 00, 00, 00, 00, 00, 00, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40, cf, b3, 0b, 66, 39, 61, 60, 40, 66, da, b7, c0, 89, d2, 3e, 40, 15, e3, 73, 5c, 3a, 61, 60, 40, 3e, 88, b3, 0e, 86, d2, 3e, 40, 21, bd, ab, 95, 3e, 61, 60, 40, 68, ea, 29, fd, 7a, d2, 3e, 40, 3d, da, af, 70, 3f, 61, 60, 40, f7, 0d, 02, e1, 79, d2, 3e, 40, 25, 48, 0f, bd, 41, 61, 60, 40, a1, 7e, 6b, b5, 74, d2, 3e, 40, df, 70, 70, 00, 45, 61, 60, 40, 91, 61, 5c, 44, 70, d2, 3e, 40, b0, 5b, 13, f1, 47, 61, 60, 40, 3f, 6b, 86, 6d, 6e, d2, 3e, 40, 14, b1, ca, 7d, 49, 61, 60, 40, 39, a5, 6d, 10, 6e, d2, 3e, 40, c3, d2, 8b, 9d, 4a, 61, 60, 40, 2d, 21, 1f, f4, 6c, d2, 3e, 40, ee, fd, 87, a0, 4c, 61, 60, 40, c8, 96, ea, a4, 6c, d2, 3e, 40, 7f, 61, 31, 9e, 4c, 61, 60, 40, fd, 2d, fc, 3e, 6e, d2, 3e, 40, 6e, 79, e2, c2, 4c, 61, 60, 40, 72, 07, 9b, 4e, 73, d2, 3e, 40, 8a, 1c, 8f, 0a, 4d, 61, 60, 40, 04, 70, ae, bf, 77, d2, 3e, 40, fb, 28, 32, 59, 4d, 61, 60, 40, d4, e8, 24, 47, 79, d2, 3e, 40, 8c, 6a, a4, b4, 4d, 61, 60, 40, 26, fc, 99, 14, 7b, d2, 3e, 40, 85, 37, 6e, a8, 4d, 61, 60, 40, 5b, 39, 83, d8, 7c, d2, 3e, 40, 3f, d9, bd, aa, 4d, 61, 60, 40, b9, d0, 46, c2, 7f, d2, 3e, 40, fd, a7, 0e, fe, 4d, 61, 60, 40, e6, ae, 25, e4, 83, d2, 3e, 40, 8e, 8e, 5a, 4a, 4e, 61, 60, 40, 6d, e2, e4, 7e, 87, d2, 3e, 40, d4, 14, 87, 87, 4e, 61, 60, 40, b4, ab, 49, 1d, 88, d2, 3e, 40, 56, 8c, 1b, 00, 4f, 61, 60, 40, 43, 04, d5, 6c, 89, d2, 3e, 40, de, 9e, 1d, f9, 4e, 61, 60, 40, 76, 6e, 8e, fe, 8d, d2, 3e, 40, b7, df, 53, 99, 4f, 61, 60, 40, 84, d5, 0c, 34, 95, d2, 3e, 40, b7, 07, d0, d8, 4f, 61, 60, 40, 71, 7b, ec, a8, 96, d2, 3e, 40, 4a, 7a, 09, 2f, 50, 61, 60, 40, b9, 0a, 13, 5a, 97, d2, 3e, 40, 06, 18, 93, 87, 50, 61, 60, 40, 36, 53, 75, 1d, 97, d2, 3e, 40, 32, cf, d0, 93, 50, 61, 60, 40, ec, 91, 6b, e2, 9b, d2, 3e, 40, 44, df, 9b, ae, 50, 61, 60, 40, 80, e2, 6a, de, 9d, d2, 3e, 40, 23, 88, 95, ff, 50, 61, 60, 40, 42, 04, 63, 17, a1, d2, 3e, 40, e2, b1, 0c, 62, 51, 61, 60, 40, 3b, 19, d5, 4f, a3, d2, 3e, 40, 6c, 8d, 9e, e1, 51, 61, 60, 40, 05, cc, 32, 7a, a4, d2, 3e, 40, fd, 73, ea, 2d, 52, 61, 60, 40, 70, 1c, 80, 26, a5, d2, 3e, 40, 63, ac, 43, 29, 52, 61, 60, 40, a4, 51, 86, 4c, a7, d2, 3e, 40, 1f, 7f, 5a, 26, 52, 61, 60, 40, 85, a8, ca, 10, aa, d2, 3e, 40, d4, eb, 90, 47, 52, 61, 60, 40, d2, 2f, 65, 6e, ab, d2, 3e, 40, 18, 41, f6, 89, 52, 61, 60, 40, b3, 4c, 6b, 45, ae, d2, 3e, 40, a4, a8, 75, 13, 53, 61, 60, 40, a4, 6e, 6c, 18, b3, d2, 3e, 40, f2, 4e, 82, 5a, 53, 61, 60, 40, 51, c2, 05, 06, b4, d2, 3e, 40, d1, 52, a2, ba, 53, 61, 60, 40, d9, 71, 17, ef, b4, d2, 3e, 40, 15, 25, 65, ae, 53, 61, 60, 40, f6, b6, 94, 51, b7, d2, 3e, 40, 49, 9b, c6, d0, 53, 61, 60, 40, 8f, b0, 0d, b4, b9, d2, 3e, 40, a1, 8c, e4, 37, 54, 61, 60, 40, 3f, 46, 8a, 12, be, d2, 3e, 40, d5, 2a, c2, 99, 54, 61, 60, 40, c1, 3c, 69, 83, c1, d2, 3e, 40, f0, 41, 81, d7, 54, 61, 60, 40, f0, 47, a0, ad, c2, d2, 3e, 40, 02, 30, 15, 50, 55, 61, 60, 40, dd, ed, 7f, 22, c4, d2, 3e, 40, 25, 83, 55, 55, 55, 61, 60, 40, d1, a6, 18, 57, c4, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 0c, f9, b6, 4c, c5, d2, 3e, 40, 58, 6d, c9, 6d, 55, 61, 60, 40, 17, 60, 66, 72, c6, d2, 3e, 40, 14, 9b, 06, 7a, 55, 61, 60, 40, 16, e4, 13, 24, c9, d2, 3e, 40, 39, 47, 8a, b9, 55, 61, 60, 40, 74, b5, 15, fb, cb, d2, 3e, 40, ec, 38, 46, fe, 55, 61, 60, 40, 78, 31, 37, 59, cf, d2, 3e, 40, c3, 7d, 42, 48, 56, 61, 60, 40, 12, bf, 16, 7f, d1, d2, 3e, 40, 67, 00, 47, b9, 56, 61, 60, 40, 95, 76, b4, bb, d1, d2, 3e, 40, 0a, 2a, 08, f0, 56, 61, 60, 40, 9a, bd, 22, ae, d2, d2, 3e, 40, 34, 55, 58, f2, 56, 61, 60, 40, 7b, da, 28, 85, d5, d2, 3e, 40, 60, 67, bc, 0d, 57, 61, 60, 40, b5, 86, 4d, c2, d7, d2, 3e, 40, 0d, 4e, be, 85, 57, 61, 60, 40, 01, 92, 95, d1, db, d2, 3e, 40, e3, 37, 94, c0, 57, 61, 60, 40, d0, 0a, 0c, 59, dd, d2, 3e, 40, 28, 8d, f9, 02, 58, 61, 60, 40, f9, d8, 62, 59, de, d2, 3e, 40, ab, 90, 7b, 85, 58, 61, 60, 40, be, b0, eb, cd, de, d2, 3e, 40, 44, 80, 9e, c9, 58, 61, 60, 40, e7, 12, a9, 91, df, d2, 3e, 40, d7, 6f, 35, d1, 58, 61, 60, 40, 2d, f4, 21, a5, e2, d2, 3e, 40, 72, 46, 6c, 2e, 59, 61, 60, 40, 26, 01, b1, 3f, e5, d2, 3e, 40, 6b, 08, 08, 92, 59, 61, 60, 40, a1, 1c, bb, fa, e7, d2, 3e, 40, 8e, de, ea, e5, 59, 61, 60, 40, 59, d7, 03, 0e, ea, d2, 3e, 40, f8, df, d3, 67, 5a, 61, 60, 40, ed, f5, a7, ba, eb, d2, 3e, 40, bd, 3c, 81, d6, 5a, 61, 60, 40, f2, 64, e3, 33, ec, d2, 3e, 40, ef, 26, f5, ee, 5a, 61, 60, 40, a4, b0, f0, cd, ed, d2, 3e, 40, 60, 0b, 1c, fe, 5a, 61, 60, 40, 14, 0e, 6e, 7f, ef, d2, 3e, 40, ea, e6, ad, 7d, 5b, 61, 60, 40, 54, b2, 06, 69, f2, d2, 3e, 40, 88, 80, 64, b6, 5b, 61, 60, 40, b3, 6e, 9d, 2e, f4, d2, 3e, 40, c9, ff, 05, fe, 57, 61, 60, 40, ec, 63, 7a, cc, 04, d3, 3e, 40, 8f, 4e, c0, 23, 58, 61, 60, 40, 74, 0a, 5a, 97, 06, d3, 3e, 40, 9e, 1a, 6f, f8, 5b, 61, 60, 40, c6, 88, bb, af, 07, d3, 3e, 40, 7e, 5b, a8, e3, 5e, 61, 60, 40, 26, 9a, df, e7, fa, d2, 3e, 40, ec, a3, 0c, 82, 62, 61, 60, 40, 01, e7, 92, 98, 0a, d3, 3e, 40, cd, 7c, 20, dc, 61, 61, 60, 40, 66, 5b, a5, 72, 15, d3, 3e, 40, 42, 02, c5, 16, 62, 61, 60, 40, 87, 88, 24, 45, 27, d3, 3e, 40, 06, 40, 51, e3, 62, 61, 60, 40, 81, df, aa, de, 26, d3, 3e, 40, 9a, a7, 17, d5, 62, 61, 60, 40, c5, 50, 14, 09, 22, d3, 3e, 40, 14, 98, ca, b2, 62, 61, 60, 40, a7, 23, ab, 1b, 22, d3, 3e, 40, 38, 1e, b5, 87, 62, 61, 60, 40, f5, 44, ee, 21, 15, d3, 3e, 40, 77, 15, ea, 9a, 6d, 61, 60, 40, 92, b5, fd, 60, 1d, d3, 3e, 40, 96, 1c, 3d, 9f, 6d, 61, 60, 40, 58, 1b, ec, 18, 1a, d3, 3e, 40, 2d, 0e, 0e, c6, 6d, 61, 60, 40, fc, 94, 51, cf, fc, d2, 3e, 40, ab, d7, 04, 83, 71, 61, 60, 40, 0b, d2, 6b, 37, ea, d2, 3e, 40, be, 5e, 85, 87, 75, 61, 60, 40, 13, 1a, 35, 1d, 09, d3, 3e, 40, 4e, a0, c2, f0, 75, 61, 60, 40, 85, fa, 3a, c9, 06, d3, 3e, 40, 83, cc, 3d, 40, 71, 61, 60, 40, 15, 21, 38, d8, e2, d2, 3e, 40, 77, 39, 27, 21, 71, 61, 60, 40, 7b, 57, 1e, 53, e0, d2, 3e, 40, fc, 1f, 90, e6, 70, 61, 60, 40, ca, 0a, 64, c7, dd, d2, 3e, 40, 96, ea, b6, cf, 70, 61, 60, 40, ee, 06, 18, 30, dd, d2, 3e, 40, e2, 78, 1d, f5, 70, 61, 60, 40, 95, c2, 4c, 73, dc, d2, 3e, 40, be, 50, a4, 21, 71, 61, 60, 40, d1, 60, 8d, 92, db, d2, 3e, 40, aa, f3, 70, 4e, 71, 61, 60, 40, a7, 3c, b5, 58, dc, d2, 3e, 40, 5e, 10, f4, c4, 71, 61, 60, 40, 70, 04, 7e, 94, de, d2, 3e, 40, 08, 4c, 1c, 83, 72, 61, 60, 40, f1, 85, 91, aa, e2, d2, 3e, 40, b3, 48, 8a, fd, 72, 61, 60, 40, f6, a8, d9, b2, e5, d2, 3e, 40, d4, 73, 93, 97, 73, 61, 60, 40, a0, e4, d6, f0, e9, d2, 3e, 40, 82, bb, 86, f5, 73, 61, 60, 40, 45, b0, eb, c3, ec, d2, 3e, 40, 07, 07, 0e, 77, 74, 61, 60, 40, 73, 99, a7, 2e, f1, d2, 3e, 40, d8, 77, 27, 9a, 75, 61, 60, 40, 5c, 2e, 48, 1e, fb, d2, 3e, 40, a2, 6b, 57, cf, 76, 61, 60, 40, 2d, 55, b3, 18, 06, d3, 3e, 40, 67, 12, b8, 1f, 77, 61, 60, 40, 67, 1f, 75, 85, 08, d3, 3e, 40, f2, dc, 0b, 64, 77, 61, 60, 40, ef, bb, 75, 40, 0a, d3, 3e, 40, 6e, f9, 22, 83, 77, 61, 60, 40, 65, 06, db, 9c, 0b, d3, 3e, 40, f1, a3, 96, bd, 77, 61, 60, 40, 8e, 1a, 4b, 0c, 0d, d3, 3e, 40, 43, 35, d4, 11, 78, 61, 60, 40, 46, b6, f8, b6, 0e, d3, 3e, 40, 37, 69, 48, 4c, 78, 61, 60, 40, 0a, 54, f2, f6, 0f, d3, 3e, 40, 83, 94, da, b6, 78, 61, 60, 40, 8c, 21, f9, 7d, 11, d3, 3e, 40, 0f, ff, d9, 24, 79, 61, 60, 40, 32, 2f, d1, 6a, 12, d3, 3e, 40, 52, 98, f2, 7f, 79, 61, 60, 40, c7, 69, 67, 1c, 13, d3, 3e, 40, 79, 42, c8, dc, 79, 61, 60, 40, 25, f4, f3, d9, 13, d3, 3e, 40, 3e, 60, bb, 07, 7a, 61, 60, 40, 7f, 4a, 81, 44, 14, d3, 3e, 40, ed, 66, 46, 3f, 7a, 61, 60, 40, fb, 94, 8e, 32, 15, d3, 3e, 40, f1, 0a, 0f, 5d, 7a, 61, 60, 40, 87, 5f, b4, 3e, 1a, d3, 3e, 40, 70, 83, 60, a9, 7c, 61, 60, 40, be, 5c, af, dc, 17, d3, 3e, 40, 26, 51, bd, a2, 7c, 61, 60, 40, 5a, 6e, 15, 36, 16, d3, 3e, 40, ea, b2, 98, d8, 7c, 61, 60, 40, 7e, b9, c3, e4, 15, d3, 3e, 40, c0, fc, f7, db, 7c, 61, 60, 40, ee, 8b, 0c, f1, 16, d3, 3e, 40, 75, 03, da, 5d, 7d, 61, 60, 40, 1f, 58, b1, 9f, 15, d3, 3e, 40, ee, 07, 16, 69, 7d, 61, 60, 40, 38, b9, 00, b0, 11, d3, 3e, 40, 4f, f1, 01, 7f, 7d, 61, 60, 40, 27, 02, b1, 79, 10, d3, 3e, 40, e4, 3c, 5c, a5, 7d, 61, 60, 40, 93, 27, 2a, ca, 0e, d3, 3e, 40, d1, 98, c0, e2, 7d, 61, 60, 40, 11, 34, a3, 75, 0d, d3, 3e, 40, ea, a6, 34, 65, 7e, 61, 60, 40, 24, 3e, cb, 81, 0b, d3, 3e, 40, f5, 06, 6c, c2, 7e, 61, 60, 40, ef, 1e, 2e, a6, 0a, d3, 3e, 40, 31, 83, 24, f8, 7e, 61, 60, 40, 71, 07, 29, f9, 0a, d3, 3e, 40, be, 9a, e0, f4, 7e, 61, 60, 40, d0, c2, 12, cd, 0b, d3, 3e, 40, 50, 62, fb, bc, 7e, 61, 60, 40, 1d, a2, 88, 47, 0d, d3, 3e, 40, 5f, fc, 79, c2, 7e, 61, 60, 40, 6a, a9, cb, 48, 0e, d3, 3e, 40, 44, 38, fe, f5, 7e, 61, 60, 40, ed, c1, 76, c0, 0d, d3, 3e, 40, 2e, 05, f5, 44, 7f, 61, 60, 40, 23, a3, 2e, 12, 0d, d3, 3e, 40, 7f, 95, ca, 7b, 7f, 61, 60, 40, 7c, 83, 43, 30, 0d, d3, 3e, 40, c2, 61, 8d, a6, 7f, 61, 60, 40, 1d, 98, a9, 37, 0d, d3, 3e, 40, 3c, a5, 83, f5, 7f, 61, 60, 40, 35, 80, 4f, 5d, 0d, d3, 3e, 40, 6b, d2, 42, 09, 80, 61, 60, 40, 7b, b7, 49, aa, 0e, d3, 3e, 40, 4d, 8a, 87, de, 7f, 61, 60, 40, c2, 2a, 2d, 0f, 11, d3, 3e, 40, a6, 75, 95, 9a, 7f, 61, 60, 40, 0e, 3d, fc, 11, 13, d3, 3e, 40, 16, 34, 58, 31, 7f, 61, 60, 40, 90, 3b, 0f, 68, 15, d3, 3e, 40, 2a, fa, 5f, 88, 7e, 61, 60, 40, 3b, 89, 1f, 9b, 18, d3, 3e, 40, bf, 09, 80, 4f, 7e, 61, 60, 40, 88, 86, 83, 8c, 19, d3, 3e, 40, e5, f0, 90, fc, 7d, 61, 60, 40, e1, 5c, 68, 53, 1a, d3, 3e, 40, 69, a9, b2, ab, 7d, 61, 60, 40, ab, 2c, b6, bb, 1a, d3, 3e, 40, e9, 8b, c9, 8a, 7d, 61, 60, 40, b1, 36, 3d, 5d, 1a, d3, 3e, 40, cc, 57, cc, 7f, 7d, 61, 60, 40, 7c, 5c, bb, b7, 19, d3, 3e, 40, 5b, d1, 16, 72, 7d, 61, 60, 40, 0b, 64, a1, 96, 19, d3, 3e, 40, 33, 40, 62, ee, 7c, 61, 60, 40, b6, cd, a3, ce, 1b, d3, 3e, 40, c9, 1f, 58, ca, 7c, 61, 60, 40, 38, 07, 44, 92, 1d, d3, 3e, 40, 78, 2e, c6, 9f, 7c, 61, 60, 40, 1a, 97, 6a, 99, 1e, d3, 3e, 40, 0a, b8, 91, f9, 7a, 61, 60, 40, 34, ed, 23, 36, 26, d3, 3e, 40, b9, 6e, 24, b2, 7a, 61, 60, 40, 33, aa, 62, e3, 27, d3, 3e, 40, fd, fb, 96, 7d, 7a, 61, 60, 40, fc, f1, d3, c2, 29, d3, 3e, 40, 78, bf, 6a, 62, 7a, 61, 60, 40, 6d, 4f, 51, 74, 2b, d3, 3e, 40, d8, 66, 1c, 50, 7a, 61, 60, 40, b9, ce, b7, ec, 2d, d3, 3e, 40, 81, 84, 59, 4f, 7a, 61, 60, 40, 6b, 4b, 22, 9d, 2f, d3, 3e, 40, 71, 8c, 34, 56, 7a, 61, 60, 40, 9f, 6c, 6a, a3, 31, d3, 3e, 40, e6, 44, 83, 60, 7a, 61, 60, 40, f8, 93, 45, 22, 33, d3, 3e, 40, f2, 21, 4d, 61, 7a, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 33, 33, 33, 33, 33, d3, 3e, 40, 99, 99, 99, 99, 99, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 11, 11, 11, 11, 11, d1, 3e, 40, 33, 33, 33, 33, 33, 61, 60, 40, 1c, c3, 6a, f2, ad, d2, 3e, 40, 1e, c5, 48, 15, 36, 61, 60, 40, bd, 69, 95, c1, 9a, d2, 3e, 40\n3121f7f1-9e78-42bd-bfd7-2ac6261c808f\n3\n2\n3\n\n\n\n\n\nある行を展開それぞれ別の行数で展開したいときの処理は次となる。\n\n\nCode\nselect \n  i, \n  unnest(generate_series(1, i))\nfrom \n  generate_series(1, 10) tbl(i)\n\n;\n\n\n\nDisplaying records 1 - 10\n\n\ni\nunnest(generate_series(1, i))\n\n\n\n\n1\n1\n\n\n2\n1\n\n\n2\n2\n\n\n3\n1\n\n\n3\n2\n\n\n3\n3\n\n\n4\n1\n\n\n4\n2\n\n\n4\n3\n\n\n4\n4\n\n\n\n\n\n計算結果をGeoJSONに書き出す。時間がかかるので未実行です。\n\n\nCode\nCOPY (\n  select *\n  from ag_pref_intersection\n)\nTO './data/intersection_ag_n03.geojson'\nWITH (FORMAT GDAL, DRIVER 'GeoJSON', SRS '4612')\n;\n\n\n\n\n3 Disconnect\n\n\nCode\ndbDisconnect(con, shutdown=TRUE)\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/test/python/notebook.html",
    "href": "contents/test/python/notebook.html",
    "title": "タイトルだよ",
    "section": "",
    "text": "Quartoでノートブックが使えるのかどうか問題がある。\n\n\nCode\nimport numpy as np\n\nnp.random.uniform(size = 10)\n\n\narray([0.22329003, 0.07488251, 0.4208198 , 0.07878708, 0.67163988,\n       0.24023422, 0.46245836, 0.53287132, 0.97138486, 0.13631092])",
    "crumbs": [
      "Test",
      "Python",
      "タイトルだよ"
    ]
  },
  {
    "objectID": "contents/test/python/notebook.html#見出し２",
    "href": "contents/test/python/notebook.html#見出し２",
    "title": "タイトルだよ",
    "section": "",
    "text": "Quartoでノートブックが使えるのかどうか問題がある。\n\n\nCode\nimport numpy as np\n\nnp.random.uniform(size = 10)\n\n\narray([0.22329003, 0.07488251, 0.4208198 , 0.07878708, 0.67163988,\n       0.24023422, 0.46245836, 0.53287132, 0.97138486, 0.13631092])",
    "crumbs": [
      "Test",
      "Python",
      "タイトルだよ"
    ]
  },
  {
    "objectID": "contents/test/bash/run_gdal.html",
    "href": "contents/test/bash/run_gdal.html",
    "title": "GDAL",
    "section": "",
    "text": "1 はじめに\n\ngdalを走らせられるかどうか\n日本語のパスでも問題なく動くには動くことがわかる\n\n\n\n2 works\n\n\nCode\ngdalinfo --version\n#&gt; GDAL 3.4.3, released 2022/04/22\n\n\n\n\nCode\ngdalinfo ./日本語フォルダ/stacking.tif\n#&gt; Driver: GTiff/GeoTIFF\n#&gt; Files: ./日本語フォルダ/stacking.tif\n#&gt; Size is 316, 192\n#&gt; Coordinate System is:\n#&gt; GEOGCRS[\"JGD2000\",\n#&gt;     DATUM[\"Japanese Geodetic Datum 2000\",\n#&gt;         ELLIPSOID[\"GRS 1980\",6378137,298.257222101004,\n#&gt;             LENGTHUNIT[\"metre\",1]],\n#&gt;         ID[\"EPSG\",6612]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         ANGLEUNIT[\"degree\",0.0174532925199433,\n#&gt;             ID[\"EPSG\",9122]]],\n#&gt;     CS[ellipsoidal,2],\n#&gt;         AXIS[\"latitude\",north,\n#&gt;             ORDER[1],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433,\n#&gt;                 ID[\"EPSG\",9122]]],\n#&gt;         AXIS[\"longitude\",east,\n#&gt;             ORDER[2],\n#&gt;             ANGLEUNIT[\"degree\",0.0174532925199433,\n#&gt;                 ID[\"EPSG\",9122]]]]\n#&gt; Data axis to CRS axis mapping: 2,1\n#&gt; Origin = (138.937500000000000,35.899999999999999)\n#&gt; Pixel Size = (0.003125000000000,-0.002083333000000)\n#&gt; Metadata:\n#&gt;   AREA_OR_POINT=Area\n#&gt; Image Structure Metadata:\n#&gt;   COMPRESSION=LZW\n#&gt;   INTERLEAVE=PIXEL\n#&gt; Corner Coordinates:\n#&gt; Upper Left  ( 138.9375000,  35.9000000) (138d56'15.00\"E, 35d54' 0.00\"N)\n#&gt; Lower Left  ( 138.9375000,  35.5000001) (138d56'15.00\"E, 35d30' 0.00\"N)\n#&gt; Upper Right ( 139.9250000,  35.9000000) (139d55'30.00\"E, 35d54' 0.00\"N)\n#&gt; Lower Right ( 139.9250000,  35.5000001) (139d55'30.00\"E, 35d30' 0.00\"N)\n#&gt; Center      ( 139.4312500,  35.7000000) (139d25'52.50\"E, 35d42' 0.00\"N)\n#&gt; Band 1 Block=316x1 Type=Float64, ColorInterp=Gray\n#&gt;   NoData Value=0\n#&gt; Band 2 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 3 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 4 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 5 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 6 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 7 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 8 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 9 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 10 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 11 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n#&gt; Band 12 Block=316x1 Type=Float64, ColorInterp=Undefined\n#&gt;   NoData Value=0\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Test",
      "Bash",
      "GDAL"
    ]
  },
  {
    "objectID": "contents/test/R/renv.html",
    "href": "contents/test/R/renv.html",
    "title": "renv",
    "section": "",
    "text": "1 renvはどのように指定すればいいか\n\nプロジェクトファイルがあるフォルダで指定するときはそのフォルダになる\nプロジェクトファイルがないフォルダで指定するときはルートフォルダになる\n正確には１番近い親フォルダになっているみたい\nなのでこの場合のhereの挙動には注意する\n\n\n\nCode\nhere::here()\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\ncur_dir &lt;- here::here(\"contents\", \"test\", \"R\")\nprint(cur_dir)\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R/contents/test/R\"\n# renv::init(cur_dir)\n\nprint(getwd())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\nprint(here::here())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\n\n\n\n\nCode\nrenv::activate()\n\n\n\n\nCode\nprint(getwd())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\nprint(here::here())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\nprint(cur_dir)\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R/contents/test/R\"\n\n\n\n\nCode\nlibrary(here)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Test",
      "R",
      "renv"
    ]
  },
  {
    "objectID": "contents/utils/01_写真GPS/index.html",
    "href": "contents/utils/01_写真GPS/index.html",
    "title": "写真データのGISデータ化",
    "section": "",
    "text": "1 はじめに\niPhoneをはじめとしたスマートフォンで写真を撮ると位置情報が付与されている。 また、ミラーレス一眼レフカメラでもスマートフォンと連携して位置情報が付与される。 現場調査において位置情報というのは非常に重要な情報である。\nここでは、写真データに付与された位置情報をGISデータに変換する方法を紹介する。\n\n\n2 やり方\n大きく４つのステップに分かれている。\n\nJPEGへの変換\nJPEGから位置情報をCSVで出力する\nCSVをGeoJSONに変換する\n画像をサムネイル化する\n\n# convert HEIC to jpg\nmogrify -format jpg *.HEIC\n\n# extract gps coordates from jpgs to output.csv\nexiftool -gpslatitude -gpslongitude -n -csv *.jpg &gt; output.csv\n\n# convert output.csv to geojson\nogr2ogr -f \"GeoJSON\" output.geojson output.csv -oo X_POSSIBLE_NAMES=gpslongitude  -oo Y_POSSIBLE_NAMES=gpslatitude -oo KEEP_GEOM_COLUMNS=NO\n\n# create thumbnails\nmkdir -p thumbnails\nfor file in *.jpg; do \n    filename=$(basename \"$file\")\n    convert \"$file\" -thumbnail 200x \"thumbnails/$filename\"\ndone\nここまで為ておくことで、写真データを地図上に表示することができるようになる。 WebGISはもちろんであるがQGISでも可能である。たとえば、次をTipsに設定することで閲覧が可能となる。 これはプロジェクトフォルダーからの相対パスでファイルを指定している。ファイル名自体はSourceFileという名前の属性で保存されている。\n&lt;a href=\"file:///[% @project_folder %]/../../../09_写真/20231120-21_SiteVisit_Dulkadioglu/suzuki/[% SourceFile %]\"&gt;\n&lt;img src=\"file:///[% @project_folder %]/../../../09_写真/20231120-21_SiteVisit_Dulkadioglu/suzuki/[% SourceFile %]\" alt=\"hoge\" width=\"300\"&gt;\n&lt;/a&gt;\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Utils",
      "01_写真GPS"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html",
    "href": "contents/notes/ggplot2/01_randamnote.html",
    "title": "Tips",
    "section": "",
    "text": "Code\ncur_dir &lt;- here::here(\"contents/notes/ggplot2\")\n\n\nshowtext::showtext_auto()\nsysfonts::font_add_google(\"Noto Sans JP\")\n\npacman::p_load(\n    tidyverse, \n    here, \n    showtext,\n    ggplot2, \n    ggridges,\n    cowplot, \n    ggsci, \n    GGally,\n    ggprism,\n    palmerpenguins, \n    ggalluvial, \n    waffle,\n    ggokabeito, \n    patchwork\n)",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#reference",
    "href": "contents/notes/ggplot2/01_randamnote.html#reference",
    "title": "Tips",
    "section": "1.1 Reference",
    "text": "1.1 Reference\n\nggplot2\nrmarkdown\nggplot2 extensions",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#リファレンンスラインの配置",
    "href": "contents/notes/ggplot2/01_randamnote.html#リファレンンスラインの配置",
    "title": "Tips",
    "section": "2.1 リファレンンスラインの配置",
    "text": "2.1 リファレンンスラインの配置\n\n\nCode\niris |&gt; \n    ggplot(aes(Species, Sepal.Width, color = Species)) + \n    geom_jitter() + \n    stat_summary(fun = mean, geom = \"point\", linewidth = 3, size = 10, fill = \"white\") +\n    theme_bw() + \n    scale_color_aaas()\n#&gt; Warning in stat_summary(fun = mean, geom = \"point\", linewidth = 3, size = 10, :\n#&gt; Ignoring unknown parameters: `linewidth`\n\n\n\n\n\n\n\n\n\n\n\nCode\niris |&gt; \n    ggplot(aes(Species, Sepal.Width, color = Species, group = Species)) + \n    geom_jitter() + \n    geom_segment(\n      aes(x = as.numeric(Species)-.3, \n          xend = as.numeric(Species) + .3, \n          y = mean(Sepal.Width),\n          yend = mean(Sepal.Width)), color = \"black\") +\n    theme_bw() + \n    scale_color_aaas()\n\n\n\n\n\n\n\n\n\n\n\nCode\niris |&gt; \n    ggplot(aes(Species, Sepal.Width, color = Species, group = Species)) + \n    geom_jitter() + \n    stat_summary(\n      fun = mean, \n      geom = \"segment\",\n      linewidth = 3, \n      mapping = aes(x = as.numeric(Species)-.3, \n                    xend = as.numeric(Species) + .3, \n                    yend = ..y.. # 統計変換後の値である\n                  )\n    ) + \n    theme_bw() + \n    scale_color_aaas()\n#&gt; Warning: The dot-dot notation (`..y..`) was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `after_stat(y)` instead.\n\n\n\n\n\n\n\n\n\n..y..のような値として他にも次がある。\n\n..count..：データの項目数を表します。たとえば、geom_bar()やgeom_histogram()で使用されると、各ビンに含まれるアイ- テムの総数を示します。\n..density..：密度を表します。geom_histogram()において、yを密度にマッピングすることで、面積の合計が1になるように- 調整されたヒストグラムを表示できます。\n..binwidth..：ヒストグラムのビンの幅を指します。これは、geom_histogram()でビンの幅を調整する際に参照されることが- あります。\n..x..と..y..：統計変換によって計算されたx値やy値を指します。これらは、特にstat_summary()で平均値や中央値などを計- 算する際に使われることがあります。\n..ymin..と..ymax..：エラーバーの下限と上限を表します。geom_errorbar()やgeom_pointrange()で使用され、平均値や中央- 値の信頼区間や範囲を示します。\n..width..と..height..：geom_tile()やgeom_raster()などでタイルやピクセルの幅と高さを調整する際に使用されることが- あります。\n..group..：データをグループ化するために使用される内部変数です。特定のgeomやstat関数で複数のグループを処理する際に使われます。",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#カーネル密度と経験累積分布関数",
    "href": "contents/notes/ggplot2/01_randamnote.html#カーネル密度と経験累積分布関数",
    "title": "Tips",
    "section": "2.2 カーネル密度と経験累積分布関数",
    "text": "2.2 カーネル密度と経験累積分布関数\n経験累積分布関数とカーネル密度プロットは常に描画すること. できればそれを組み合わせて描画すること。 経験累積分布関数はさらに目的変数の累積分布関数と比較をすること.\n\n\nCode\ndata &lt;- iris\ng_dens &lt;- \n    data %&gt;%\n    ggplot(aes(Sepal.Length, color = Species)) + \n    geom_line(stat = \"density\", size = 1.5) + \n    theme_bw() + \n    labs(x = \"Sepal.Length\", y = \"density\") + \n    scale_color_aaas()\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\nCode\ng_ecdf &lt;- \n    data %&gt;%\n    ggplot(aes(Sepal.Length, color = Species)) + \n    stat_ecdf(size = 1.5) + \n    theme_bw() + \n    labs(x = \"Sepal.Length\", y = \"density\") + \n    scale_color_aaas() +\n    guides(color = \"none\")\n\n\n\n\nCode\n\nplot_grid(\n    g_dens + labs(x = NULL), \n    g_ecdf, \n    rel_widths = c(1, 1), \n    ncol = 1,  \n    align = \"v\",  # 軸を合わせるのに必要 \n    axis = \"tblr\" # 余白を合わせるのに必要(特に凡例があったりなかったりするとき)\n)",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#関数曲線",
    "href": "contents/notes/ggplot2/01_randamnote.html#関数曲線",
    "title": "Tips",
    "section": "2.3 関数曲線",
    "text": "2.3 関数曲線\n確率密度関数などを記述するときに使う.\n\n\nCode\ngenerate_dnorm &lt;- function (param) {\n    lift( partial )(\n        update_list(\n            param, .f = dnorm, .first = FALSE )\n        )\n}\n\nparam1 &lt;-\n    list( mean = 1.23 ) %&gt;%\n    update_list(\n        sd =  ~ .1\n    )\n#&gt; Warning: `update_list()` was deprecated in purrr 1.0.0.\nparam2 &lt;-\n    param1 %&gt;%\n    update_list(\n        mean = 1.48631\n    )\n\ndnorm1 &lt;- generate_dnorm( param1 )\n#&gt; Warning: `lift()` was deprecated in purrr 1.0.0.\n#&gt; Warning: The `.first` argument of `partial()` is deprecated as of purrr 0.3.0.\n#&gt; ℹ The deprecated feature was likely used in the purrr package.\n#&gt;   Please report the issue at &lt;https://github.com/tidyverse/purrr/issues&gt;.\ndnorm2 &lt;- generate_dnorm( param2 )\nfill_poly &lt;- data_frame(\n        x = seq(qnorm(.9, mean = param1$mean, sd = param1$sd), 2, length.out = 20)\n    ) %&gt;%\n    mutate(y = map_dbl(x, dnorm1))\n#&gt; Warning: `data_frame()` was deprecated in tibble 1.1.0.\n#&gt; ℹ Please use `tibble()` instead.\n\ngrh &lt;-\n    data_frame( x = seq(0, 2, .01) ) %&gt;%\n    ggplot() +\n    geom_abline(slope = 0, intercept = 0) +\n    geom_ribbon(data = fill_poly, aes(x = x, ymax = y, ymin = 0), fill = \"pink\", alpha = .5) +\n    stat_function(fun = dnorm1, colour = \"red\",  lwd = 1.2 ) +\n    stat_function(fun = dnorm2, colour = \"blue\", lwd = 1.2 ) +\n    ylim(c(0, 4)) +\n    scale_x_continuous(limits = c(0.9, 1.8), breaks = seq(0.9, 1.8, .1)) +\n    labs(x = \"\", y = \"\") +\n    theme_bw() +\n    theme(\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.text.y  = element_blank(),\n        axis.text.x  = element_text(size = 12)\n    )\ngrh",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#確率楕円",
    "href": "contents/notes/ggplot2/01_randamnote.html#確率楕円",
    "title": "Tips",
    "section": "2.4 確率楕円",
    "text": "2.4 確率楕円\nデフォルトでは、2次元t分布の95%確率となる領域を記述してくれる。 つまり、異常値の発見に役立つ. geomパラメータを変更することで塗りつぶしを行うこともできるし、 t分布の変わりに正規分布を使うことも可能である. 単純に円を記述することも可能である.\n\n\nCode\ngp &lt;-\n    iris %&gt;% \n    ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) + \n    theme_bw() + \n    labs(x = \"Sepal.Length\", y = \"Sepal.Width\") + \n    coord_equal() +\n    theme(\n        axis.ticks.length = unit(-2, \"mm\"), \n        axis.title.x = element_text(margin = margin(t = 1, unit = \"line\")),\n        axis.text.x  = element_text(margin = margin(t = 1, unit = \"line\")),\n        axis.title.y = element_text(margin = margin(r = 1, unit = \"line\")), \n        axis.text.y  = element_text(margin = margin(r = 1, unit = \"line\")),\n        plot.margin = margin(1, 0, 0, 0, \"line\")\n    ) + \n    scale_color_viridis_d()\n\ngp + \n    geom_point(size = 3, alpha = .5) + \n    stat_ellipse() + \n    stat_ellipse(type = \"norm\", lty = 2) + \n    stat_ellipse(type = \"euclid\", lty = 3)\n\n\n\n\n\n\n\n\n\n\n\nCode\ngp + \n    stat_ellipse(\n        aes(fill = Species), \n        type = \"norm\", \n        geom = \"polygon\", alpha = .2, color = \"transparent\") + \n    geom_point(size = 3, alpha = .5, position = \"jitter\") +\n    scale_fill_viridis_d()",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#annotate関数",
    "href": "contents/notes/ggplot2/01_randamnote.html#annotate関数",
    "title": "Tips",
    "section": "2.5 annotate関数",
    "text": "2.5 annotate関数\n\n2.5.1 annotate\nannotate関数ではジオメトリを追加するが、 典型的なgeom functionとは方法がことなり、 ジオメトリの属性はデータフレームの変数ではなく ベクトルで与えらる。 テキストラベルなど小さなアノテーションを追加するとき、 あるいは、データフレームにしたくないデータを持っているときに有用である。\n\n\nCode\np &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + theme_bw()\nlift(plot_grid)(list(\n    p + annotate(\"text\", x = 4, y = 25, label = \"Some text\"), \n    p + annotate(\"text\", x = 2:5, y = 25, label = \"Some text\"), \n    p + annotate(\"rect\", xmin = 3, xmax = 4.2, ymin = 12, ymax = 21, alpha = .2),\n    p + annotate(\"segment\", x = 2.5, xend = 4, y = 15, yend = 25, colour = \"blue\"),\n    p + annotate(\"pointrange\", x = 3.5, y = 20, ymin = 12, ymax = 28, colour = \"red\", size = 1.5),\n    p + annotate(\"text\", x = 2:3, y = 20:21, label = c(\"my label\", \"label 2\")),\n    p + annotate(\"text\", x = 4, y = 25, label = \"italic(R) ^ 2 == 0.75\",   parse = TRUE),\n    p + annotate(\"text\", x = 4, y = 25, label = \"paste(italic(R) ^ 2, \\\" = .75\\\")\", parse = TRUE), \n    ncol = 2, \n    rel_heights = c(4, 4)\n))\n\n\n\n\n\n\n\n\n\nなんか頑張ったら、こんなのもかけた。。。 これがあれば特定の範囲のデータについて、分布を記述することが可能となる。 めっちゃこれが便利だというシーンは思い浮かばないけどキレイなグラフを 記述するという意味では良い経験になったと思う.\n\n\nCode\nd  &lt;- density(mtcars$mpg)\nd2 &lt;- density(mtcars$wt)\np + \n    annotate(\n        \"polygon\",\n        x = (5 - 4 * d$y), # シフトやスケールが必要\n        y = d$x,\n        color = \"red\", \n        fill = \"transparent\") +  \n    annotate(\n        \"polygon\", \n        x = d2$x, \n        y = d2$y * 6 + 5, \n        color = \"blue\", \n        fill = \"transparent\")",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#annotation",
    "href": "contents/notes/ggplot2/01_randamnote.html#annotation",
    "title": "Tips",
    "section": "2.6 annotation",
    "text": "2.6 annotation\nすべてのパネルので同じ統計アノテーションとして使う特別なgeomである。 これらのアノテーションはscaleには影響を与えない.\n\n2.6.1 annotation_custom\n下記の例を見るとなんとなく使い方はわかる. あとは記述したい絵を考えたうえで、色々と工夫を重ねるのが一番良いと思う.\n適当なGrobを作成できれば後は勢いで作成できる?\n使い方の用途としては，全体を小さく書いておき メインは全体のうち拡大したい箇所，という方法にして置くのがわかりやすい気がする．\n\n\nCode\nbase &lt;- \n    iris %&gt;% \n    ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) + \n    geom_point() + \n    lims(y = c(2, 5))\n\ng &lt;-\n  ggplotGrob(\n      ggplot(iris, aes(x = Sepal.Length, color = Species)) +\n      geom_line(stat = \"density\") + \n      guides(color = FALSE) + \n      theme(\n          panel.background = element_rect(fill = \"transparent\"), \n          plot.background  = element_rect(fill = \"transparent\"), \n          panel.grid   = element_blank(), \n          axis.title.y = element_blank(), \n          axis.line.y  = element_blank(), \n          axis.text.y  = element_blank(),\n          axis.ticks   = element_blank()\n      ))\n#&gt; Warning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\n#&gt; of ggplot2 3.3.4.\n \nbase + \n    annotation_custom(grob = g, xmin = 6, xmax = 8, ymin = 4, ymax = 5)\n\n\n\n\n\n\n\n\n\n\n\n2.6.2 annotation_logticks\nこれではないけどマイナーグリッドにチックを入れる方法として annotateにgeom = “segment”を使う方法が考えだされていたけど， 正直しんどい気がする．．．．\n\n\nCode\na &lt;- ggplot(msleep, aes(bodywt, brainwt)) +\n geom_point(na.rm = TRUE) +\n scale_x_log10(\n   breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n   labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n ) +\n scale_y_log10(\n   breaks = scales::trans_breaks(\"log10\", function(x) 10^x),\n   labels = scales::trans_format(\"log10\", scales::math_format(10^.x))\n ) +\n theme_bw()\n\nlift(plot_grid)(list(\n    a + annotation_logticks(),               # Default: log ticks on bottom and left\n    a + annotation_logticks(sides = \"lr\"),   # Log ticks for y, on left and right\n    a + annotation_logticks(sides = \"trbl\")  # All four sides\n))\n\n\n\n\n\n\n\n\n\n\n\n2.6.3 annotation_map\n\n\nCode\nif (require(\"maps\")) {\nusamap &lt;- map_data(\"state\")\n\nseal.sub &lt;- subset(seals, long &gt; -130 & lat &lt; 45 & lat &gt; 40)\nggplot(seal.sub, aes(x = long, y = lat)) +\n  annotation_map(usamap, fill = NA, colour = \"grey50\") +\n  geom_segment(aes(xend = long + delta_long, yend = lat + delta_lat))\n}\n#&gt;  要求されたパッケージ maps をロード中です\n#&gt; Warning: パッケージ 'maps' はバージョン 4.3.3 の R の下で造られました\n#&gt; \n#&gt;  次のパッケージを付け加えます: 'maps'\n#&gt;  以下のオブジェクトは 'package:purrr' からマスクされています:\n#&gt; \n#&gt;     map\n\n\n\n\n\n\n\n\n\n\n\nCode\nif (require(\"maps\")) {\nseal2 &lt;- transform(seal.sub,\n  latr = cut(lat, 2),\n  longr = cut(long, 2))\n\nggplot(seal2,  aes(x = long, y = lat)) +\n  annotation_map(usamap, fill = NA, colour = \"grey50\") +\n  geom_segment(aes(xend = long + delta_long, yend = lat + delta_lat)) +\n  facet_grid(latr ~ longr, scales = \"free\", space = \"free\")\n}\n\n\n\n\n\n\n\n\n\n\n\n2.6.4 annotation_raster\n画像を読み込めば画像も行ける？ 16進数の色に変換しておけば良いのか?\n\n\nCode\n# Generate data\nrainbow &lt;- matrix(hcl(seq(0, 360, length.out = 50 * 50), 80, 70), nrow = 50)\nggplot(mtcars, aes(mpg, wt)) +\n  geom_point() +\n  annotation_raster(rainbow, 15, 20, 3, 4)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# To fill up whole plot\nggplot(mtcars, aes(mpg, wt)) +\n  annotation_raster(rainbow, -Inf, Inf, -Inf, Inf) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nCode\nrainbow2 &lt;- matrix(hcl(seq(0, 360, length.out = 10), 80, 70), nrow = 1)\nggplot(mtcars, aes(mpg, wt)) +\n  annotation_raster(rainbow2, -Inf, Inf, -Inf, Inf) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(png)\nlibrary(grid)\nimg &lt;- readPNG(system.file(\"img\", \"Rlogo.png\", package = \"png\"))\ng   &lt;- rasterGrob(img, interpolate = TRUE)\nqplot(1:10, 1:10, geom = \"blank\") + \n    annotation_custom(g, xmin = 1, xmax = 5, ymin = 1, ymax = 2) + \n    geom_point()\n#&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0.",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#凡例のハンドリング",
    "href": "contents/notes/ggplot2/01_randamnote.html#凡例のハンドリング",
    "title": "Tips",
    "section": "2.7 凡例のハンドリング",
    "text": "2.7 凡例のハンドリング\n凡例の操作は難しい… とりあえず、次を操作できるようになりたい.\n\n凡例の出し入れ\n凡例の位置(panelの中か外か)\n凡例のタイトル\n凡例のキーの操作\n凡例の文字列\n\nまずは複数の凡例の順序と凡例自体の位置を制御する例. themeで見た目を整える事もできるけど, guides(guide_legend)でも 大概見た目を加工することができる. 以下の例では 標準的な出力に加えて、凡例を色々と操作してみた状態のプロット。\n\n\nCode\nbase &lt;- \n  ggplot(mpg, aes(displ, cty)) +\n  geom_point(aes(size = hwy, colour = cyl, shape = drv)) + \n  theme_bw()\n\nlift(plot_grid)(list(\n  base, \n  base + guides(\n   colour = guide_colourbar(order = 1, draw.llim = FALSE),\n   shape  = guide_legend(order = 2),\n   size   = guide_legend(\n     order = 3, \n     title = \"MyWay\", \n     keywidth = unit(2, \"cm\"),\n     reverse = 1, \n     title.position = \"top\", # or bottom, left, right\n     title.theme = element_text(color = \"red\", face = \"italic\"), \n     title.hjust = 1, \n     label.theme = element_text(size = 14),\n     direction = \"vertical\"\n    )\n ), \n ncol = 2\n))\n\n\n\n\n\n\n\n\n\nカラーバーなどでの操作を細かく行うための使い方。 guidesとかlegendとかではないけど、一応凡例が操作されているといいうことで. これってスゴい便利なのでは？ 例えば普通の連続値の場合にはscale_fill_continuousを使う.\n\n\nCode\ndf &lt;- expand.grid(X1 = 1:10, X2 = 1:10)\ndf$value &lt;- df$X1 * df$X2\n\np &lt;- ggplot(df, aes(X1, X2)) + geom_tile(aes(fill = value)) + theme_bw() + coord_equal()\n\n# Coloursteps guide is the default for binned colour scales\nlift(plot_grid)(list(\n  p , \n  p + \n    geom_hline(yintercept = 1:10, color = \"lightgrey\", lty = 2) + \n    geom_vline(xintercept = 1:10, color = \"lightgrey\", lty = 2),\n  p + scale_fill_binned(), \n  p + scale_fill_binned(breaks = c(10, 25, 50)), \n  p + scale_fill_binned(breaks = c(10, 25, 50), guide = guide_coloursteps(even.steps = FALSE)), \n  p + scale_fill_binned(show.limits = TRUE),\n  ncol = 2\n))\n\n\n\n\n\n\n\n\n\n凡例をpanelの中に記述する場合の書き方は次の通り.\n\n\nCode\ndf &lt;- tibble(x = 1:3, y = 1:3, z = c(\"a\", \"b\", \"c\"))\nbase &lt;- \n  df %&gt;%\n  ggplot(aes(x, y, color = z), size = 3) + \n  geom_point()\n\nlift(plot_grid)(list(\n  base, \n  base + theme(\n    legend.position = c(0, 1),       # アンカーの相対座標\n    legend.justification = c(0, 1)), # アンカーの相対座標と紐付ける凡例の相対座標\n  base + theme(\n    legend.position = c(.5, .5), \n    legend.justification = c(.5, .5)),\n  base + theme(\n    legend.position = c(.5, .5), \n    legend.justification = c(0, 1)), \n  base + theme(\n    legend.position = c(1, 0), \n    legend.justification = c(1, 0)\n  ), \n  ncol = 2\n))\n#&gt; Warning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n#&gt; 3.5.0.\n#&gt; ℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\nマニュアルで凡例を作成する. ここを参考にした. これはしっくりするやり方だと思う.\n\n\nCode\n\ncolors &lt;- c(\"Sepal Width\" = \"blue\", \"Petal Length\" = \"red\", \"Petal Width\" = \"orange\")\n\nggplot(iris, aes(x = Sepal.Length)) +\n    geom_line(aes(y = Sepal.Width, color = \"Sepal Width\"), size = 1.5) +\n    geom_line(aes(y = Petal.Length, color = \"Petal Length\"), size = 1.5) +\n    geom_line(aes(y = Petal.Width, color = \"Petal Width\"), size = 1.5) +\n    labs(x = \"Year\",\n         y = \"(%)\",\n         color = \"Legend\") +\n    scale_color_manual(values = colors)\n\n\n\n\n\n\n\n\n\n凡例を消したい場合の対処方法. ここを参考にしている.\n\n\nCode\n# レイヤー単位で指定\np &lt;- \n  ggplot(ToothGrowth, aes(x = dose, y = len))+ \n  geom_boxplot(aes(fill = dose), show.legend = FALSE) +\n  scale_fill_viridis_d()\np\n#&gt; Warning: Continuous x aesthetic\n#&gt; ℹ did you forget `aes(group = ...)`?\n#&gt; Warning: The following aesthetics were dropped during statistical transformation: fill.\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping structure in\n#&gt;   the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n#&gt;   variable into a factor?\n\n\n\n\n\n\n\n\n\n\n\nCode\n# グラフを作成してから指定\np + theme(legend.position = \"none\")\n#&gt; Warning: Continuous x aesthetic\n#&gt; ℹ did you forget `aes(group = ...)`?\n#&gt; Warning: The following aesthetics were dropped during statistical transformation: fill.\n#&gt; ℹ This can happen when ggplot fails to infer the correct grouping structure in\n#&gt;   the data.\n#&gt; ℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n#&gt;   variable into a factor?\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 特定の軸に対して処理をしたい場合の指定\nmtcars$cyl&lt;-as.factor(mtcars$cyl)\nmtcars$gear &lt;- as.factor(mtcars$gear)\n\n# Scatter plot\np2 &lt;- ggplot(data = mtcars, aes(x = mpg, y = wt))+\n    geom_point(aes(color = cyl, size = qsec, shape = gear)) +\n  scale_color_viridis_d()\np2\n\n\n\n\n\n\n\n\n\n\n\nCode\np2 + guides(color = \"none\", size = \"none\")\n\n\n\n\n\n\n\n\n\n\n2.7.1 文字や位置を微調整したい\nstackoverflowが参考になる.\n水平方向に調整する.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(mtcars, aes(factor(cyl), fill = factor(cyl))) + \n  geom_bar() +\n  coord_flip() +\n  scale_fill_brewer(\"Cyl\", palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = 'top', \n        legend.spacing.x = unit(1.0, 'cm'))\n\n\n\n\n\n\n\n\n\n鉛直方向に調整する.\n\n\nCode\nlibrary(ggplot2)\n\nggplot(mtcars, aes(y = factor(cyl), fill = factor(cyl))) + \n  geom_bar() +\n  theme(legend.spacing.y = unit(1.0, 'cm'))  +\n  ## important additional element\n  guides(fill = guide_legend(byrow = TRUE))\n\n\n\n\n\n\n\n\n\nカラーバーなどは個別に微調整するみたい. バーの長さも自分で調整ができる見たい.\n\n\nCode\nggplot(mtcars, aes(mpg, wt)) +\n  geom_point(aes(fill = hp), pch = I(21), size = 5)+\n  scale_fill_viridis_c(guide = FALSE) +\n  theme_classic(base_size = 14) +\n  theme(legend.position = 'top', \n        legend.spacing.x = unit(0.5, 'cm'),\n        legend.text = element_text(margin = margin(t = 10))) +\n  guides(fill = guide_colorbar(title = \"HP\",\n                               label.position = \"bottom\",\n                               title.position = \"left\", title.vjust = 1,\n                               # draw border around the legend\n                               frame.colour = \"black\",\n                               barwidth = 25,\n                               barheight = 2)) \n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(mtcars) +\n  aes(x = cyl, fill = factor(cyl)) +\n  geom_bar() +\n  scale_fill_brewer(\"Cyl\", palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.key.size = unit(1, \"cm\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\ndraw_key_polygon3 &lt;- function(data, params, size) {\n  lwd &lt;- min(data$size, min(size) / 4)\n  \n  grid::rectGrob(\n    width = grid::unit(0.6, \"npc\"),\n    height = grid::unit(0.6, \"npc\"),\n    gp = grid::gpar(\n      col = data$colour,\n      fill = alpha(data$fill, data$alpha),\n      lty = data$linetype,\n      lwd = lwd * .pt,\n      linejoin = \"mitre\"\n    ))\n}\n\n### this step is not needed anymore per tjebo's comment below\n### see also: https://ggplot2.tidyverse.org/reference/draw_key.html\n# register new key drawing function, \n# the effect is global & persistent throughout the R session\n# GeomBar$draw_key = draw_key_polygon3\n\nggplot(mtcars) +\n  aes(x = cyl, fill = factor(cyl)) +\n  geom_bar(key_glyph = \"polygon3\") +\n  scale_fill_brewer(\"Cyl\", palette = \"Dark2\") +\n  theme_minimal(base_size = 14) +\n  theme(legend.key = element_rect(color = NA, fill = NA),\n        legend.key.size = unit(1.5, \"cm\")) +\n  theme(legend.title.align = 0.5)\n#&gt; Warning: The `legend.title.align` argument of `theme()` is deprecated as of ggplot2\n#&gt; 3.5.0.\n#&gt; ℹ Please use theme(legend.title = element_text(hjust)) instead.",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#テキスト",
    "href": "contents/notes/ggplot2/01_randamnote.html#テキスト",
    "title": "Tips",
    "section": "2.8 テキスト",
    "text": "2.8 テキスト\n\n2.8.1 hjust/vjusts\n凡例の位置設定と同じ考え方が，どうやらテキストも同様の考え方で対応でてきる気がする. これをみると、ラベルの左下が(0, 0)で、右上が(1, 1)となっている. ラベルの右端を点xに合わせたい場合には、hjust=1に調整するなど下のものを 参考としてもらいたい.\n\n\nCode\n\nd &lt;- tibble(\n  x = 1:3 %&gt;% rep(3), \n  y = 1:3 %&gt;% rep(each = 3), \n  hjust = c(0, .5, 1) %&gt;% rep(3), \n  vjust = c(0, .5, 1) %&gt;% rep(each = 3)\n) %&gt;%\n  mutate(\n    t = sprintf(\"h:%s\\nv:%s\", hjust, vjust)\n  )\n\ng &lt;- \n  d %&gt;% \n  ggplot(aes(x, y, label = t)) + \n  coord_equal() + \n  theme_bw()\n\nplot(\n  g +\n    geom_label(aes(hjust = hjust, vjust = vjust)) + \n    geom_point(color = \"gold\", size = 2)\n  )\n\n\n\n\n\n\n\n\n\n\n\n2.8.2 position\nposition系の関数を使うと気の利いた調整が行える.\n\n\nCode\ndf &lt;- data.frame(\n  x = factor(c(1, 1, 2, 2)),\n  y = c(1, 3, 2, 1),\n  grp = c(\"a\", \"b\", \"a\", \"b\")\n)\n\n\n\n\nCode\nggplot(data = df, aes(x, y, group = grp)) +\n  geom_col(aes(fill = grp), position = \"dodge\") +\n  # x軸の中心からそれぞれどれだけ離すか\n  geom_text(aes(label = y), position = position_dodge(0.9))\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Use you can't nudge and dodge text, so instead adjust the y position\nggplot(data = df, aes(x, y, group = grp)) +\n  geom_col(aes(fill = grp), position = \"dodge\") +\n  geom_text(\n    aes(label = y, y = y + 0.05),\n    position = position_dodge(0.9),\n    vjust = 0\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# To place text in the middle of each bar in a stacked barplot, you\n# need to set the vjust parameter of position_stack()\nggplot(data = df, aes(x, y, group = grp)) +\n geom_col(aes(fill = grp)) +\n # スタックされているところは，つまり，境界のところ\n # この場合のvjustはスタックされているところで調整してくれている(文字の大きさでない)\n geom_text(aes(label = y), position = position_stack(vjust = 0.5))",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#facet_grid",
    "href": "contents/notes/ggplot2/01_randamnote.html#facet_grid",
    "title": "Tips",
    "section": "2.9 facet_grid",
    "text": "2.9 facet_grid\n基本的な使い方はいいが，ここではストリップラベルをずらす例を紹介．\n\n\nCode\ndata &lt;- \n  tibble(\n    rows = sprintf(\"row_%d\", 1:2), \n    cols = sprintf(\"col_%d\", 1:2)\n  ) %&gt;%\n  tidyr::expand(rows, cols) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    data = list(tibble(x = rnorm(1000), y = rnorm(1000)))\n  ) %&gt;%\n  unnest(data)\n\n\n\n\nCode\ndata %&gt;%\n  ggplot() + \n  geom_point(aes(x, y), size = 1) + \n  coord_equal(xlim = c(-3, 3), ylim = c(-3, 3)) + \n  theme_cowplot() + \n  facet_grid(\n    rows = vars(rows), \n    cols = vars(cols), \n    switch = \"y\" # 行のラベルを左に置く\n  ) + \n  scale_y_continuous(position = \"right\") + \n  theme(\n    # RStudioの予測にも出てこないけど，\n    # leftにおいたyのstripにアングルを与えるには次を使うこと\n    strip.text.y.left = element_text(angle = 0, face = \"bold\")\n  )",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#summary_stat",
    "href": "contents/notes/ggplot2/01_randamnote.html#summary_stat",
    "title": "Tips",
    "section": "2.10 summary_stat",
    "text": "2.10 summary_stat\ngeom関数とstat関数は一緒の関係であるが, stat_summaryにより関数を適用してプロットを行うことが可能となる. 正直使いにくいイメージがあるので，個人的には 事前に集計したレイヤーを重ねるという方法を常に選択しがちである．\n\n\nCode\np &lt;- \n  ggplot(\n    data = filter(penguins, complete.cases(penguins)), \n    mapping = aes(\n      x = species,\n      y = bill_length_mm\n    )\n  ) + \n  theme_minimal() + \n  theme(\n    panel.background = element_rect(color = \"black\")\n  )\n\np + \n  geom_point() + \n  stat_summary(fun = \"mean\", color = \"red\", size = 4)\n#&gt; Warning: Removed 3 rows containing missing values or values outside the scale range\n#&gt; (`geom_segment()`).",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#マルチレベルのwrap",
    "href": "contents/notes/ggplot2/01_randamnote.html#マルチレベルのwrap",
    "title": "Tips",
    "section": "2.11 マルチレベルのwrap",
    "text": "2.11 マルチレベルのwrap\n自然言語処理でドキュメントごとに単語頻度上位N件の 度数をバープロットで描画するときなど， ドキュメントでfacetしてその中でwordを並び変えてプロットしたいときがある．\nこのとき，何も考えずに行うと期待したプロットが作成できない． これに対処するには，並び換えを全体で行えるようにkeyを調整した上で， プロットする際にキーラベル名を調整するという処理が必要である.\n\n\nCode\ndata &lt;- \n  tibble(\n    key   = c(\"a\", \"a\", \"b\", \"b\"), \n    value = c(1, 2, 2, 1), \n    term  = c(\"w1\", \"w2\", \"w1\", \"w2\")\n  )\n\n# 並び変えてプロットしても\n# グラフは同じ順番になってしまう\n# これをfacetごとに降順に並び変えたい\ndata %&gt;% \n  group_by(key) %&gt;% \n  arrange(value) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(reorder(term, value), value)) + \n  geom_col() + \n  facet_wrap(vars(key), scales = \"free\")\n\n\n\n\n\n\n\n\n\n一応このようにやれば対応が可能である.\n\n\nCode\ndata %&gt;%\n  mutate(new_key = factor(str_c(term, key, sep = \"___\"))) %&gt;%\n  arrange(value) %&gt;%\n  ggplot(aes(reorder(new_key, value), value)) + \n  geom_col() + \n  facet_wrap(vars(key), scales = \"free\") + \n  scale_x_discrete(label = function(x) sub(\"___.+$\", \"\", x), drop = TRUE)",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#trans",
    "href": "contents/notes/ggplot2/01_randamnote.html#trans",
    "title": "Tips",
    "section": "2.12 trans",
    "text": "2.12 trans\ntrans系やscale系の関数を使うことで，データをggplot内で変換することが可能となるはず． たとえばscale_x_logなどがそれに該当する.\n\n\nCode\ntibble(\n  x = 1:10, \n  y = 1:10\n) %&gt;% \n  ggplot(aes(x, y)) + \n  geom_line() + \n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n同じことを違う遣り方で実行してみる.\n\n\nCode\ntibble(\n  x = 1:10, \n  y = 1:10\n) %&gt;% \n  ggplot(aes(x, y)) + \n  geom_line() + \n  scale_y_continuous(trans = \"log10\")\n\n\n\n\n\n\n\n\n\nさらに別のやり方を試す.\n\n\nCode\ntibble(\n  x = 1:10, \n  y = 1:10\n) %&gt;% \n  ggplot(aes(x, y)) + \n  geom_line() + \n  scale_y_continuous(\n    trans = scales::trans_new(\"log10\", log10, function(x) 10 ** x), \n    minor_breaks = seq(1, 10, .5), \n    breaks = c(1, 6, 10)\n  ) + \n  theme(\n    axis.ticks.length = unit(-2, \"mm\")\n  )",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#グラフの順番に凡例の順番を合わす",
    "href": "contents/notes/ggplot2/01_randamnote.html#グラフの順番に凡例の順番を合わす",
    "title": "Tips",
    "section": "2.13 グラフの順番に凡例の順番を合わす",
    "text": "2.13 グラフの順番に凡例の順番を合わす\n参考文献\n\n\nCode\nlibrary(tidyverse)  # 1.3.0\n\nusa_crop_yields &lt;- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-01/key_crop_yields.csv\") %&gt;%\n  rename_with(~ gsub(\" \\\\(tonnes per hectare\\\\)\", \"\", .)) %&gt;%\n  pivot_longer(Wheat:Bananas, names_to = \"crop\", values_to = \"yield\") %&gt;%\n  rename_with(tolower) %&gt;%\n  filter(entity == \"United States\", !is.na(yield)) %&gt;%\n  select(year, crop, yield)\n#&gt; Rows: 13075 Columns: 14\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr  (2): Entity, Code\n#&gt; dbl (12): Year, Wheat (tonnes per hectare), Rice (tonnes per hectare), Maize...\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nusa_crop_yields\n#&gt; # A tibble: 522 × 3\n#&gt;     year crop     yield\n#&gt;    &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n#&gt;  1  1961 Wheat     1.61\n#&gt;  2  1961 Rice      3.82\n#&gt;  3  1961 Maize     3.92\n#&gt;  4  1961 Soybeans  1.69\n#&gt;  5  1961 Potatoes 22.2 \n#&gt;  6  1961 Beans     1.54\n#&gt;  7  1961 Peas      1.19\n#&gt;  8  1961 Barley    1.65\n#&gt;  9  1961 Bananas  10.5 \n#&gt; 10  1962 Wheat     1.68\n#&gt; # ℹ 512 more rows\n\n\n\n\nCode\nusa_crop_yields %&gt;%\n  mutate(crop = fct_reorder2(crop, year, yield)) %&gt;%\n  ggplot(aes(year, yield, color = crop)) +\n  geom_line() +\n  labs(x = NULL, y = NULL, color = NULL)",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#histgram",
    "href": "contents/notes/ggplot2/01_randamnote.html#histgram",
    "title": "Tips",
    "section": "2.14 histgram",
    "text": "2.14 histgram\nヒストグラムをきれいに書きたい.\n\n\nCode\ndata &lt;- tibble(a = runif(100), b = sample(c(\"a\", \"b\"), 100, replace = TRUE))\ndata %&gt;%\n  ggplot(aes(x = a)) + \n  geom_histogram(aes(fill = b), center = 0, breaks = seq(0, 1., .1), color = \"lightgrey\", closed = \"left\") +\n  geom_text(\n    stat = \"bin\",\n    # count, dnesity, ncount, ndensity\n    aes(label = ..count.., y = ..count.., fill = b),\n    breaks = seq(0, 1., .1),\n    closed = \"left\",\n    vjust = -.5,\n    position = position_stack(vjust = .5)\n  ) + \n  geom_text(\n    stat = \"bin\", \n    aes(x = a, label = after_stat(count), y = after_stat(count)), \n    breaks = seq(0, 1., .1), \n    closed = \"left\", \n    vjust = -.5, \n    data = data\n  ) +\n  scale_x_continuous(breaks = seq(0, 1., .1))\n#&gt; Warning in geom_text(stat = \"bin\", aes(label = ..count.., y = ..count.., :\n#&gt; Ignoring unknown aesthetics: fill\n\n\n\n\n\n\n\n\n\n左に閉じたときにどのような挙動になるのかを確認したい. [0, .1), [.1, .2), …., [.9, 1.]となっていることがわかる.\n\n\nCode\ndata &lt;- tibble(a = sample(c(0, .1, .5,  1.), 100, TRUE), b = sample(c(\"a\", \"b\"), 100, replace = TRUE))\ndata %&gt;%\n  ggplot(aes(x = a)) + \n  geom_histogram(\n    aes(fill = b), \n    center = 0,\n    breaks = seq(0, 1., .1), \n    color = \"lightgrey\", \n    closed = \"left\"\n  ) +\n  geom_text(\n    stat = \"bin\",\n    aes(label = ..count.., y = ..count.., group = b),\n    breaks = seq(0, 1., .1),\n    closed = \"left\",\n    vjust = -.5,\n    position = position_stack(vjust = .5)\n  ) + \n  scale_x_continuous(breaks = seq(0, 1., .1))",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#geom_smooth",
    "href": "contents/notes/ggplot2/01_randamnote.html#geom_smooth",
    "title": "Tips",
    "section": "2.15 geom_smooth",
    "text": "2.15 geom_smooth\n\n\nCode\ndf &lt;- tibble(\n    x = seq(1, 100, 1), \n    e = runif(n = 100)\n) |&gt; \n    mutate(y = x * .05 + e)\n\ndf |&gt; \n    ggplot(aes(x, y)) + \n    geom_point() + \n    geom_line(data = ~ . |&gt; filter(x &lt;= 50), col = \"red\") + \n    geom_smooth(data = ~ . |&gt; filter(x &gt; 50), col = \"blue\")\n#&gt; `geom_smooth()` using method = 'loess' and formula = 'y ~ x'",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#ggmosaic",
    "href": "contents/notes/ggplot2/01_randamnote.html#ggmosaic",
    "title": "Tips",
    "section": "3.1 ggmosaic",
    "text": "3.1 ggmosaic\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggmosaic)\n#&gt; Warning: パッケージ 'ggmosaic' はバージョン 4.3.2 の R の下で造られました\n#&gt; \n#&gt;  次のパッケージを付け加えます: 'ggmosaic'\n#&gt;  以下のオブジェクトは 'package:GGally' からマスクされています:\n#&gt; \n#&gt;     happy\nlibrary(dplyr)\n\ncc &lt;- count(diamonds, cut, clarity)\n\nggplot(cc) + \n    geom_mosaic(\n        aes(weight = n, x = product(cut), fill = clarity)\n    )\n#&gt; Warning: The `scale_name` argument of `continuous_scale()` is deprecated as of ggplot2\n#&gt; 3.5.0.\n#&gt; Warning: The `trans` argument of `continuous_scale()` is deprecated as of ggplot2 3.5.0.\n#&gt; ℹ Please use the `transform` argument instead.\n#&gt; Warning: `unite_()` was deprecated in tidyr 1.2.0.\n#&gt; ℹ Please use `unite()` instead.\n#&gt; ℹ The deprecated feature was likely used in the ggmosaic package.\n#&gt;   Please report the issue at &lt;https://github.com/haleyjeppson/ggmosaic&gt;.",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#ggokabeito",
    "href": "contents/notes/ggplot2/01_randamnote.html#ggokabeito",
    "title": "Tips",
    "section": "3.2 ggokabeito",
    "text": "3.2 ggokabeito\nsafecolorらしい．\n\n\nCode\niris |&gt; \n    ggplot(aes(Sepal.Length, Sepal.Width, color = Species)) + \n    geom_point(size = 4) + \n    theme_bw() + \n    scale_color_okabe_ito()",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#ggprism",
    "href": "contents/notes/ggplot2/01_randamnote.html#ggprism",
    "title": "Tips",
    "section": "3.3 ggprism",
    "text": "3.3 ggprism\nマイナーチック を打てるようです.\n他にも色々軸の加工が可能なためいろいろとチェックしてみるのがいい．\n\n3.3.1 Minor ticks\n\n\nCode\np &lt;- ggplot(\n  data = ToothGrowth, \n  mapping = aes(\n    x = factor(supp), \n    y = len\n  )\n) + \n  geom_boxplot(aes(fill = factor(supp))) + \n  theme_prism() + \n  theme(legend.position = \"none\")\np1 &lt;- p + scale_y_continuous(guide = guide_prism_minor())\np2 &lt;- p + guides(y = guide_prism_minor())\np2\n#&gt; Warning: The S3 guide system was deprecated in ggplot2 3.5.0.\n#&gt; ℹ It has been replaced by a ggproto system that can be extended.\n\n\n\n\n\n\n\n\n\n\n\nCode\np &lt;- \n  ggplot(\n    data = ToothGrowth, \n    mapping = aes(\n      x = factor(dose), \n      y = len\n    )\n  ) + \n  stat_summary(\n    mapping = aes(fill = factor(dose)), na.rm = TRUE, \n    geom = \"col\", fun = mean, colour = \"black\", size = .9\n  ) + \n  theme_classic() + \n  theme(legend.position = \"none\")\n\n\n# 書きたかったグラフはこれや！！！！！\n# これが土木学会のグラフや\np + \n  scale_y_continuous(\n    guide = \"prism_minor\", \n    limits = c(0, 30), \n    expand = c(0, 0), \n    minor_breaks = seq(0, 30)\n  ) + \n  theme(\n    axis.ticks.length.y = unit(10, \"pt\"), \n    prism.ticks.length.y = unit(5, \"pt\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Brackets axis\n\n\nCode\np1 &lt;- ggplot(ToothGrowth, aes(x = factor(dose), y = len)) + \n  geom_jitter(aes(shape = factor(dose)), width = 0.2, size = 2) + \n  scale_shape_prism() + \n  theme_prism() + \n  theme(legend.position = \"none\") + \n  scale_y_continuous(limits = c(0, 40), guide = \"prism_offset\")\n\np2 &lt;- p1 + scale_x_discrete(guide = \"prism_bracket\")\n\np2\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Border with minor ticks\n\n\nCode\nbase &lt;- \n  ggplot(mpg, aes(x = displ, y = cty)) +\n  geom_point(aes(colour = class))\nbase\n\n\n\n\n\n\n\n\n\nannotation_ticksを使うをスゴく簡単にチックを打てる.\n\n\nCode\np &lt;- base + theme_prism(border = TRUE) +\n  theme(legend.position = c(0.8, 0.75)) +\n  coord_cartesian(clip = \"off\") + \n  guides(\n    x = \"prism_minor\", y = \"prism_minor\"\n  )\np_annot &lt;- \n  p + \n  annotation_ticks(\n    sides = \"tr\", \n    type = \"both\", \n    outside = TRUE,\n    tick.length = unit(14/2, \"pt\"),\n    minor.length = unit(14/4, \"pt\"))\np_annot\n\n\n離散値の場合にも出来るようでしたがあまりにも トリッキーにつき省略いたします.",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#gghalves",
    "href": "contents/notes/ggplot2/01_randamnote.html#gghalves",
    "title": "Tips",
    "section": "3.4 gghalves",
    "text": "3.4 gghalves\n\n(gghalves)[https://github.com/erocoar/gghalves]\n\nggplo2によるハーフ＆ハーフのプロットの合成を容易にする。\n\n\nCode\nlibrary(gghalves)\n#&gt; Warning: パッケージ 'gghalves' はバージョン 4.3.2 の R の下で造られました\n\n\n\n\nCode\niris %&gt;%\n  ggplot(aes(Species, Sepal.Width, color = Species)) + \n  geom_half_point() + \n  geom_half_violin()",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#ggally",
    "href": "contents/notes/ggplot2/01_randamnote.html#ggally",
    "title": "Tips",
    "section": "3.5 ggally",
    "text": "3.5 ggally\n\nggally\n\n\n\nCode\nlibrary(GGally)\n\n\n\n3.5.1 Types of Plots\n\ncontinuous: when both x and y variables are continuous\ncomboHorizontal: when x is continuous and y is discrete\ncomboVorizontal: when x is discrete and y is continuous\ndiscrete: when both x and y variables are discrete\nna: when all x data and all y data is NA\n\n\n\n3.5.2 ggpairs\nいろいろカスタマイズができるのはわかるが、 やりたいものがないとどのようなグラフを作成すべきであるのかがわからない。 こんため\n\n3.5.2.1 basic example\n\n\nCode\npm &lt;- \n  iris %&gt;%\n  ggpairs(\n    mapping = aes(color = Species), \n    columns = c(\"Sepal.Length\", \"Petal.Length\", \"Species\")\n  ) + \n  theme_bw()\npm\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCode\npm &lt;- \n  iris %&gt;%\n  ggpairs(\n    mapping = aes(color = Species), \n    columns = c(\"Sepal.Length\", \"Petal.Length\", \"Species\"), \n    lower   = list(\n      continuous = \"smooth\", \n      combo = \"facetdensity\", \n      mappping = aes(color = Species)\n    )\n  ) + \n  theme_bw()\npm\n\n\n\n\n\n\n\n\n\n\n\nCode\npm &lt;- \n  iris %&gt;%\n  ggpairs(\n    mapping = aes(color = Species), \n    columns = c(\"Sepal.Length\", \"Petal.Length\", \"Species\"), \n    upper = \"blank\", \n    diag  = NULL, \n    lower   = list(\n      continuous = \"smooth\", \n      combo = \"facetdensity\", \n      mappping = aes()\n    )\n  ) + \n  theme_bw()\npm\n\n\n\n\n\n\n\n\n\n\n\n3.5.2.2 custom functions\n\n\nCode\ndata(tips, package = \"reshape\")\nmy_bin &lt;- function(data, mapping, ..., low = \"#132B43\", high = \"#56B1F7\") {\n  ggplot(data = data, mapping = mapping) +\n    geom_bin2d(...) +\n    scale_fill_gradient(low = low, high = high) + \n    theme(\n      axis.text = element_text(color = \"red\")\n    )\n}\npm &lt;- ggpairs(\n  tips, \n  columns = c(\"total_bill\", \"time\", \"tip\"), \n  lower = list(\n    continuous = my_bin\n  )\n)\npm\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCode\npm &lt;- ggpairs(\n  tips, columns = c(\"total_bill\", \"time\", \"tip\"),\n  lower = list(\n    combo = wrap(\"facethist\", binwidth = 1),\n    continuous = wrap(my_bin, binwidth = c(5, 0.5), high = \"red\")\n  )\n)\npm\n\n\n\n\n\n\n\n\n\n\n\n3.5.2.3 plot matrix subsetting\n\n\nCode\npm &lt;- ggpairs(tips, columns = c(\"total_bill\", \"time\", \"tip\"))\np  &lt;- pm[3, 1]\np  &lt;- p + aes(color = time)\np\n\n\n\n\n\n\n\n\n\n\n\nCode\npm[3,1] &lt;- p\npm\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCode\ncustom_car &lt;- ggpairs(mtcars[, c(\"mpg\", \"wt\", \"cyl\")], \n                      upper = \"blank\", title = \"Custom Example\")\n# ggplot example taken from example(geom_text)\nplot &lt;- ggplot2::ggplot(mtcars, ggplot2::aes(x=wt, y=mpg, label=rownames(mtcars)))\nplot &lt;- plot +\n    ggplot2::geom_text(ggplot2::aes(colour=factor(cyl)), size = 3) +\n    ggplot2::scale_colour_discrete(l=40)\ncustom_car[1, 2] &lt;- plot\npersonal_plot &lt;- ggally_text(\n  \"ggpairs allows you\\nto put in your\\nown plot.\\nLike that one.\\n &lt;---\"\n)\ncustom_car[1, 3] &lt;- personal_plot\n(custom_car)\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n#&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Remove binwidth warning from ggplot2\npm &lt;- ggpairs(tips, 2:3, lower = list(combo = wrap(\"facethist\", binwidth = 0.5)))\npm\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Remove panel grid lines from correlation plots\npm &lt;- ggpairs(\n  flea, columns = 2:4,\n  upper = list(continuous = wrap(ggally_cor, display_grid = FALSE))\n)\npm + theme(\n  strip.background = element_rect(fill = \"black\"), \n  strip.text = element_text(color = \"white\"))\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.3 ggnetworkmap\n\n\nCode\nsuppressMessages(library(ggplot2))\nsuppressMessages(library(maps))\nsuppressMessages(library(network))\nsuppressMessages(library(sna))\n\nairports &lt;- read.csv(\"http://datasets.flowingdata.com/tuts/maparcs/airports.csv\", header = TRUE)\nrownames(airports) &lt;- airports$iata\n\n# select some random flights\nset.seed(1234)\nflights &lt;- data.frame(\n  origin = sample(airports[200:400, ]$iata, 200, replace = TRUE),\n  destination = sample(airports[200:400, ]$iata, 200, replace = TRUE)\n)\n\n# convert to network\nflights &lt;- network(flights, directed = TRUE)\n\n# add geographic coordinates\nflights %v% \"lat\" &lt;- airports[ network.vertex.names(flights), \"lat\" ]\nflights %v% \"lon\" &lt;- airports[ network.vertex.names(flights), \"long\" ]\n\n# drop isolated airports\ndelete.vertices(flights, which(degree(flights) &lt; 2))\n\n# compute degree centrality\nflights %v% \"degree\" &lt;- degree(flights, gmode = \"digraph\")\n\n# add random groups\nflights %v% \"mygroup\" &lt;- sample(letters[1:4], network.size(flights), replace = TRUE)\n\n# create a map of the USA\nusa &lt;- ggplot(map_data(\"usa\"), aes(x = long, y = lat)) +\n  geom_polygon(aes(group = group), color = \"grey65\",\n               fill = \"#f9f9f9\", size = 0.2)\n\n# trim flights\ndelete.vertices(flights, which(flights %v% \"lon\" &lt; min(usa$data$long)))\ndelete.vertices(flights, which(flights %v% \"lon\" &gt; max(usa$data$long)))\ndelete.vertices(flights, which(flights %v% \"lat\" &lt; min(usa$data$lat)))\ndelete.vertices(flights, which(flights %v% \"lat\" &gt; max(usa$data$lat)))\n\n# overlay network data to map\nggnetworkmap(usa, flights, size = 4, great.circles = TRUE,\n             node.group = mygroup, segment.color = \"steelblue\",\n             ring.group = degree, weight = degree)",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#gganimate",
    "href": "contents/notes/ggplot2/01_randamnote.html#gganimate",
    "title": "Tips",
    "section": "3.6 gganimate",
    "text": "3.6 gganimate\ngithub\n\n\nCode\nlibrary(gganimate)\n\nggplot(mtcars, aes(factor(cyl), mpg)) + \n  geom_boxplot() + \n  transition_states(\n    gear, \n    transition_length = 2, \n    state_length = 1\n  ) + \n  enter_fade() + \n  exit_shrink() + \n  ease_aes('sine-in-out')",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#waffle",
    "href": "contents/notes/ggplot2/01_randamnote.html#waffle",
    "title": "Tips",
    "section": "3.7 waffle",
    "text": "3.7 waffle\n\n\nCode\nwaffle(c(30, 25, 20, 5), rows = 8)",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#ggupset",
    "href": "contents/notes/ggplot2/01_randamnote.html#ggupset",
    "title": "Tips",
    "section": "3.8 ggupset",
    "text": "3.8 ggupset\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggupset)\nggplot(tidy_movies[1:100, ], aes(x=Genres)) +\n  geom_bar() +\n  scale_x_upset(reverse = TRUE, sets=c(\"Drama\", \"Action\"))\n#&gt; Warning: Removed 33 rows containing non-finite outside the scale range\n#&gt; (`stat_count()`).\n\n\n\n\n\n\n\n\n\nCode\n\n ggplot(tidy_movies[1:100, ], aes(x=Genres)) +\n   geom_bar() +\n   scale_x_upset(n_intersections = 5, ytrans=\"sqrt\")\n#&gt; Warning: Removed 28 rows containing non-finite outside the scale range\n#&gt; (`stat_count()`).\n\n\n\n\n\n\n\n\n\nCode\n\n ggplot(tidy_movies[1:100, ], aes(x=Genres, y=year)) +\n   geom_boxplot() +\n   scale_x_upset(intersections = list(c(\"Drama\", \"Comedy\"), c(\"Short\"), c(\"Short\", \"Animation\")),\n                 sets = c(\"Drama\", \"Comedy\", \"Short\", \"Animation\", \"Horror\"))\n#&gt; Warning: Removed 90 rows containing missing values or values outside the scale range\n#&gt; (`stat_boxplot()`).",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#heat-map-with-geom_tile",
    "href": "contents/notes/ggplot2/01_randamnote.html#heat-map-with-geom_tile",
    "title": "Tips",
    "section": "3.9 Heat map with geom_tile",
    "text": "3.9 Heat map with geom_tile\n\n3.9.1 Sample Data\n\n\nCode\nlibrary(reshape)\n#&gt; \n#&gt;  次のパッケージを付け加えます: 'reshape'\n#&gt;  以下のオブジェクトは 'package:cowplot' からマスクされています:\n#&gt; \n#&gt;     stamp\n#&gt;  以下のオブジェクトは 'package:lubridate' からマスクされています:\n#&gt; \n#&gt;     stamp\n#&gt;  以下のオブジェクトは 'package:dplyr' からマスクされています:\n#&gt; \n#&gt;     rename\n#&gt;  以下のオブジェクトは 'package:tidyr' からマスクされています:\n#&gt; \n#&gt;     expand, smiths\n\n# Data \nset.seed(8)\nm &lt;- matrix(round(rnorm(200), 2), 10, 10)\n#&gt; Warning in matrix(round(rnorm(200), 2), 10, 10): data length differs from size\n#&gt; of matrix: [200 != 10 x 10]\ncolnames(m) &lt;- paste(\"Col\", 1:10)\nrownames(m) &lt;- paste(\"Row\", 1:10)\n\n# Transform the matrix in long format\ndf &lt;- melt(m)\n#&gt; Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\n#&gt; the caller; using TRUE\n#&gt; Warning in type.convert.default(X[[i]], ...): 'as.is' should be specified by\n#&gt; the caller; using TRUE\ncolnames(df) &lt;- c(\"x\", \"y\", \"value\")\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile() + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(\n    color = \"white\", \n    lwd = 1.5, \n    linetype = 1.\n  ) + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(color = \"black\") + \n  geom_text(aes(label = value), color = \"white\", size = 4) + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\n3.9.2 Color Palette\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(color = \"black\") + \n  scale_fill_gradient(low = \"white\", high = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(color = \"black\") + \n  scale_fill_gradient2(\n    low = \"#075AFF\", \n    mid = \"#FFFFCC\", \n    high = \"#FF0000\"\n  ) + \n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(color = \"black\") + \n  scale_fill_gradientn(colors = hcl.colors(20, \"RdYlGn\")) +\n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\n3.9.3 Legend customization\n\n\nCode\nggplot(df, aes(x = x, y = y, fill = value)) + \n  geom_tile(color = \"black\") + \n  coord_fixed() + \n  guides(\n    fill = guide_colourbar(barwidth = .5, barheight = 20)\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y, fill = value)) +\n  geom_tile(color = \"black\") +\n  coord_fixed() +\n  guides(fill = guide_colourbar(title = \"Title\")) \n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y, fill = value)) +\n  geom_tile(color = \"black\") +\n  coord_fixed() +\n  guides(fill = guide_colourbar(label = FALSE,\n                                ticks = FALSE)) \n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nggplot(df, aes(x = x, y = y, fill = value)) +\n  geom_tile(color = \"black\") +\n  coord_fixed() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#alluvial-plot",
    "href": "contents/notes/ggplot2/01_randamnote.html#alluvial-plot",
    "title": "Tips",
    "section": "3.10 Alluvial plot",
    "text": "3.10 Alluvial plot\n\n3.10.1 sample dataset\n\n\nCode\ndata(\"vaccinations\")\nvaccinations\n#&gt;        survey freq subject  response start_date   end_date\n#&gt; 1   ms153_NSA   48       1    Always 2010-09-22 2010-10-25\n#&gt; 2   ms153_NSA    9       2    Always 2010-09-22 2010-10-25\n#&gt; 3   ms153_NSA   66       3    Always 2010-09-22 2010-10-25\n#&gt; 4   ms153_NSA    1       4    Always 2010-09-22 2010-10-25\n#&gt; 5   ms153_NSA   11       5    Always 2010-09-22 2010-10-25\n#&gt; 6   ms153_NSA    1       6    Always 2010-09-22 2010-10-25\n#&gt; 7   ms153_NSA    5       7    Always 2010-09-22 2010-10-25\n#&gt; 8   ms153_NSA    4       8    Always 2010-09-22 2010-10-25\n#&gt; 9   ms153_NSA   24       9   Missing 2010-09-22 2010-10-25\n#&gt; 10  ms153_NSA    4      10   Missing 2010-09-22 2010-10-25\n#&gt; 11  ms153_NSA   29      11   Missing 2010-09-22 2010-10-25\n#&gt; 12  ms153_NSA    1      12   Missing 2010-09-22 2010-10-25\n#&gt; 13  ms153_NSA    1      13   Missing 2010-09-22 2010-10-25\n#&gt; 14  ms153_NSA    9      14   Missing 2010-09-22 2010-10-25\n#&gt; 15  ms153_NSA   10      15   Missing 2010-09-22 2010-10-25\n#&gt; 16  ms153_NSA   14      16   Missing 2010-09-22 2010-10-25\n#&gt; 17  ms153_NSA    3      17     Never 2010-09-22 2010-10-25\n#&gt; 18  ms153_NSA    7      18     Never 2010-09-22 2010-10-25\n#&gt; 19  ms153_NSA   42      19     Never 2010-09-22 2010-10-25\n#&gt; 20  ms153_NSA   20      20     Never 2010-09-22 2010-10-25\n#&gt; 21  ms153_NSA    2      21     Never 2010-09-22 2010-10-25\n#&gt; 22  ms153_NSA   36      22     Never 2010-09-22 2010-10-25\n#&gt; 23  ms153_NSA    7      23     Never 2010-09-22 2010-10-25\n#&gt; 24  ms153_NSA    2      24     Never 2010-09-22 2010-10-25\n#&gt; 25  ms153_NSA    4      25     Never 2010-09-22 2010-10-25\n#&gt; 26  ms153_NSA    9      26     Never 2010-09-22 2010-10-25\n#&gt; 27  ms153_NSA   48      27 Sometimes 2010-09-22 2010-10-25\n#&gt; 28  ms153_NSA   11      28 Sometimes 2010-09-22 2010-10-25\n#&gt; 29  ms153_NSA   99      29 Sometimes 2010-09-22 2010-10-25\n#&gt; 30  ms153_NSA    3      30 Sometimes 2010-09-22 2010-10-25\n#&gt; 31  ms153_NSA   41      31 Sometimes 2010-09-22 2010-10-25\n#&gt; 32  ms153_NSA  168      32 Sometimes 2010-09-22 2010-10-25\n#&gt; 33  ms153_NSA    2      33 Sometimes 2010-09-22 2010-10-25\n#&gt; 34  ms153_NSA   23      34 Sometimes 2010-09-22 2010-10-25\n#&gt; 35  ms153_NSA   22      35 Sometimes 2010-09-22 2010-10-25\n#&gt; 36  ms153_NSA   38      36 Sometimes 2010-09-22 2010-10-25\n#&gt; 37  ms153_NSA    1      37 Sometimes 2010-09-22 2010-10-25\n#&gt; 38  ms153_NSA   15      38 Sometimes 2010-09-22 2010-10-25\n#&gt; 39  ms153_NSA  113      39 Sometimes 2010-09-22 2010-10-25\n#&gt; 40  ms432_NSA   48       1    Always 2015-06-04 2015-10-05\n#&gt; 41  ms432_NSA    9       2    Always 2015-06-04 2015-10-05\n#&gt; 42  ms432_NSA   66       3   Missing 2015-06-04 2015-10-05\n#&gt; 43  ms432_NSA    1       4   Missing 2015-06-04 2015-10-05\n#&gt; 44  ms432_NSA   11       5   Missing 2015-06-04 2015-10-05\n#&gt; 45  ms432_NSA    1       6     Never 2015-06-04 2015-10-05\n#&gt; 46  ms432_NSA    5       7 Sometimes 2015-06-04 2015-10-05\n#&gt; 47  ms432_NSA    4       8 Sometimes 2015-06-04 2015-10-05\n#&gt; 48  ms432_NSA   24       9    Always 2015-06-04 2015-10-05\n#&gt; 49  ms432_NSA    4      10    Always 2015-06-04 2015-10-05\n#&gt; 50  ms432_NSA   29      11   Missing 2015-06-04 2015-10-05\n#&gt; 51  ms432_NSA    1      12   Missing 2015-06-04 2015-10-05\n#&gt; 52  ms432_NSA    1      13   Missing 2015-06-04 2015-10-05\n#&gt; 53  ms432_NSA    9      14   Missing 2015-06-04 2015-10-05\n#&gt; 54  ms432_NSA   10      15 Sometimes 2015-06-04 2015-10-05\n#&gt; 55  ms432_NSA   14      16 Sometimes 2015-06-04 2015-10-05\n#&gt; 56  ms432_NSA    3      17    Always 2015-06-04 2015-10-05\n#&gt; 57  ms432_NSA    7      18   Missing 2015-06-04 2015-10-05\n#&gt; 58  ms432_NSA   42      19   Missing 2015-06-04 2015-10-05\n#&gt; 59  ms432_NSA   20      20   Missing 2015-06-04 2015-10-05\n#&gt; 60  ms432_NSA    2      21     Never 2015-06-04 2015-10-05\n#&gt; 61  ms432_NSA   36      22     Never 2015-06-04 2015-10-05\n#&gt; 62  ms432_NSA    7      23     Never 2015-06-04 2015-10-05\n#&gt; 63  ms432_NSA    2      24 Sometimes 2015-06-04 2015-10-05\n#&gt; 64  ms432_NSA    4      25 Sometimes 2015-06-04 2015-10-05\n#&gt; 65  ms432_NSA    9      26 Sometimes 2015-06-04 2015-10-05\n#&gt; 66  ms432_NSA   48      27    Always 2015-06-04 2015-10-05\n#&gt; 67  ms432_NSA   11      28    Always 2015-06-04 2015-10-05\n#&gt; 68  ms432_NSA   99      29   Missing 2015-06-04 2015-10-05\n#&gt; 69  ms432_NSA    3      30   Missing 2015-06-04 2015-10-05\n#&gt; 70  ms432_NSA   41      31   Missing 2015-06-04 2015-10-05\n#&gt; 71  ms432_NSA  168      32   Missing 2015-06-04 2015-10-05\n#&gt; 72  ms432_NSA    2      33     Never 2015-06-04 2015-10-05\n#&gt; 73  ms432_NSA   23      34     Never 2015-06-04 2015-10-05\n#&gt; 74  ms432_NSA   22      35     Never 2015-06-04 2015-10-05\n#&gt; 75  ms432_NSA   38      36 Sometimes 2015-06-04 2015-10-05\n#&gt; 76  ms432_NSA    1      37 Sometimes 2015-06-04 2015-10-05\n#&gt; 77  ms432_NSA   15      38 Sometimes 2015-06-04 2015-10-05\n#&gt; 78  ms432_NSA  113      39 Sometimes 2015-06-04 2015-10-05\n#&gt; 79  ms460_NSA   48       1    Always 2016-09-27 2016-10-25\n#&gt; 80  ms460_NSA    9       2 Sometimes 2016-09-27 2016-10-25\n#&gt; 81  ms460_NSA   66       3    Always 2016-09-27 2016-10-25\n#&gt; 82  ms460_NSA    1       4     Never 2016-09-27 2016-10-25\n#&gt; 83  ms460_NSA   11       5 Sometimes 2016-09-27 2016-10-25\n#&gt; 84  ms460_NSA    1       6    Always 2016-09-27 2016-10-25\n#&gt; 85  ms460_NSA    5       7    Always 2016-09-27 2016-10-25\n#&gt; 86  ms460_NSA    4       8 Sometimes 2016-09-27 2016-10-25\n#&gt; 87  ms460_NSA   24       9    Always 2016-09-27 2016-10-25\n#&gt; 88  ms460_NSA    4      10 Sometimes 2016-09-27 2016-10-25\n#&gt; 89  ms460_NSA   29      11    Always 2016-09-27 2016-10-25\n#&gt; 90  ms460_NSA    1      12   Missing 2016-09-27 2016-10-25\n#&gt; 91  ms460_NSA    1      13     Never 2016-09-27 2016-10-25\n#&gt; 92  ms460_NSA    9      14 Sometimes 2016-09-27 2016-10-25\n#&gt; 93  ms460_NSA   10      15    Always 2016-09-27 2016-10-25\n#&gt; 94  ms460_NSA   14      16 Sometimes 2016-09-27 2016-10-25\n#&gt; 95  ms460_NSA    3      17    Always 2016-09-27 2016-10-25\n#&gt; 96  ms460_NSA    7      18    Always 2016-09-27 2016-10-25\n#&gt; 97  ms460_NSA   42      19     Never 2016-09-27 2016-10-25\n#&gt; 98  ms460_NSA   20      20 Sometimes 2016-09-27 2016-10-25\n#&gt; 99  ms460_NSA    2      21   Missing 2016-09-27 2016-10-25\n#&gt; 100 ms460_NSA   36      22     Never 2016-09-27 2016-10-25\n#&gt; 101 ms460_NSA    7      23 Sometimes 2016-09-27 2016-10-25\n#&gt; 102 ms460_NSA    2      24    Always 2016-09-27 2016-10-25\n#&gt; 103 ms460_NSA    4      25     Never 2016-09-27 2016-10-25\n#&gt; 104 ms460_NSA    9      26 Sometimes 2016-09-27 2016-10-25\n#&gt; 105 ms460_NSA   48      27    Always 2016-09-27 2016-10-25\n#&gt; 106 ms460_NSA   11      28 Sometimes 2016-09-27 2016-10-25\n#&gt; 107 ms460_NSA   99      29    Always 2016-09-27 2016-10-25\n#&gt; 108 ms460_NSA    3      30   Missing 2016-09-27 2016-10-25\n#&gt; 109 ms460_NSA   41      31     Never 2016-09-27 2016-10-25\n#&gt; 110 ms460_NSA  168      32 Sometimes 2016-09-27 2016-10-25\n#&gt; 111 ms460_NSA    2      33    Always 2016-09-27 2016-10-25\n#&gt; 112 ms460_NSA   23      34     Never 2016-09-27 2016-10-25\n#&gt; 113 ms460_NSA   22      35 Sometimes 2016-09-27 2016-10-25\n#&gt; 114 ms460_NSA   38      36    Always 2016-09-27 2016-10-25\n#&gt; 115 ms460_NSA    1      37   Missing 2016-09-27 2016-10-25\n#&gt; 116 ms460_NSA   15      38     Never 2016-09-27 2016-10-25\n#&gt; 117 ms460_NSA  113      39 Sometimes 2016-09-27 2016-10-25\n\n\n\n\n3.10.2 plot\n次の２つを使うことができるようになるのでコレを使うこと. 曲線の形は色々と選択することが可能である.\n\ngeom_alluvium\ngeom_stratum\n\n\n\nCode\n\nggplot(\n  data = vaccinations, \n  aes(\n    axis1 = survey, \n    axis2 = response, \n    y = freq\n  )) + \n  geom_alluvium(aes(fill = response), curve_type = \"sine\") + \n  geom_stratum() + \n  geom_text(\n    stat = \"stratum\", \n    aes(label = after_stat(stratum))\n  ) + \n  scale_x_discrete(limits = c(\"Survey\", \"Response\"), expand = c(.15, .05)) +\n  theme_void()\n\n\n\n\n\n\n\n\n\nもちろん色のカスタマイズも可能である.\n\n\nCode\ncolors &lt;- hcl.colors(4, \"Red-Blue\")\n\n\nggplot(\n  data = vaccinations, \n  aes(\n    axis1 = survey, \n    axis2 = response, \n    y = freq\n  ) \n) + \n  geom_alluvium(aes(fill = response)) + \n  geom_stratum() + \n  geom_text(\n    stat = \"stratum\", \n    aes(label = after_stat(stratum))\n  ) + \n  scale_x_discrete(limits = c(\"Survey\", \"Response\"), expand = c(.15, .05)) + \n  scale_fill_manual(values = colors) + \n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nCode\nhcl.colors(4, \"Red-Blue\")\n#&gt; [1] \"#A93154\" \"#BC569E\" \"#B788CD\" \"#AEB6E5\"",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#ggbumps",
    "href": "contents/notes/ggplot2/01_randamnote.html#ggbumps",
    "title": "Tips",
    "section": "3.11 ggbumps",
    "text": "3.11 ggbumps\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggbump)\nlibrary(tidyverse)\n\n\ngithub\n\n\nCode\n\ndf &lt;- tibble(country = c(\"India\", \"India\", \"India\", \"Sweden\", \"Sweden\", \"Sweden\", \"Germany\", \"Germany\", \"Germany\", \"Finland\", \"Finland\", \"Finland\"),\n             year = c(2011, 2012, 2013, 2011, 2012, 2013, 2011, 2012, 2013, 2011, 2012, 2013),\n             value = c(492, 246, 246, 369, 123, 492, 246, 369, 123, 123, 492, 369))\n\ndf &lt;- df %&gt;% \n  group_by(year) %&gt;% \n  mutate(rank = rank(value, ties.method = \"random\")) %&gt;% \n  ungroup()\n\nknitr::kable(head(df))\n\n\n\n\n\ncountry\nyear\nvalue\nrank\n\n\n\n\nIndia\n2011\n492\n4\n\n\nIndia\n2012\n246\n2\n\n\nIndia\n2013\n246\n2\n\n\nSweden\n2011\n369\n3\n\n\nSweden\n2012\n123\n1\n\n\nSweden\n2013\n492\n4\n\n\n\n\n\n\n\nCode\nggplot(df, aes(year, rank, color = country)) +\n    geom_bump()\n\n\n\n\n\n\n\n\n\n\n\nCode\npacman::p_load(tidyverse, cowplot, wesanderson)\nggplot(df, aes(year, rank, color = country)) +\n  geom_point(size = 7) +\n  geom_text(data = df %&gt;% filter(year == min(year)),\n            aes(x = year - .1, label = country), size = 5, hjust = 1) +\n  geom_text(data = df %&gt;% filter(year == max(year)),\n            aes(x = year + .1, label = country), size = 5, hjust = 0) +\n  geom_bump(size = 2, smooth = 8) +\n  scale_x_continuous(limits = c(2010.6, 2013.4),\n                     breaks = seq(2011, 2013, 1)) +\n  theme_minimal_grid(font_size = 14, line_size = 0) +\n  theme(legend.position = \"none\",\n        panel.grid.major = element_blank()) +\n  labs(y = \"RANK\",\n       x = NULL) +\n  scale_y_reverse() +\n  scale_color_manual(values = wes_palette(n = 4, name = \"GrandBudapest1\"))\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Original df\ndf &lt;- tibble(season = c(\"Spring\", \"Pre-season\", \"Summer\", \"Season finale\", \"Autumn\", \"Winter\", \n                        \"Spring\", \"Pre-season\", \"Summer\", \"Season finale\", \"Autumn\", \"Winter\", \n                        \"Spring\", \"Pre-season\", \"Summer\", \"Season finale\", \"Autumn\", \"Winter\",\n                        \"Spring\", \"Pre-season\", \"Summer\", \"Season finale\", \"Autumn\", \"Winter\"),\n             rank = c(1, 3, 4, 2, 1, 4,\n                      2, 4, 1, 3, 2, 3,\n                      4, 1, 2, 4, 4, 1,\n                      3, 2, 3, 1, 3, 2),\n             player = c(rep(\"David\", 6),\n                        rep(\"Anna\", 6),\n                        rep(\"Franz\", 6),\n                        rep(\"Ika\", 6)))\n\n# Create factors and order factor\n# チュートリアルでは文字列などでも描画出来ていたけどここではうまくいかなった\n# x軸を数値に変更すると上手く描画できる\ndf &lt;- df %&gt;% \n  mutate(season = factor(season, levels = unique(season), ordered = TRUE)) \n\n# Add manual axis labels to plot\nggplot(df, aes(season, rank, color = player)) +\n  geom_bump(size = 2, smooth = 20, show.legend = F) +\n  geom_point(size = 5, aes(shape = player)) +\n  theme_minimal_grid(font_size = 10, line_size = 0) +\n  theme(panel.grid.major = element_blank(),\n        axis.ticks = element_blank()) +\n  scale_color_manual(values = wes_palette(n = 4, name = \"IsleofDogs1\"))\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group\n\n#&gt; Warning in compute_group(...): 'StatBump' needs at least two observations per\n#&gt; group",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#ggridges",
    "href": "contents/notes/ggplot2/01_randamnote.html#ggridges",
    "title": "Tips",
    "section": "3.12 ggridges",
    "text": "3.12 ggridges\n公式ページをみるのが１番．どのようなグラフが描けるのかは，サンプルを見て欲しい．グラフの幅が拡がること間違いなし．\n\n\nCode\ndata &lt;- data.frame(x = 1:5, y = rep(1, 5), height = c(0, 1, 3, 4, 2))\nggplot(data, aes(x, y, height = height)) + geom_ridgeline()\n\n\n\n\n\n\n\n\n\n\n\nCode\n\ndata &lt;- data.frame(x = 1:5, y = rep(1, 5), height = c(0, 1, -1, 3, 2))\nplot_base &lt;- ggplot(data, aes(x, y, height = height))\n\nplot_base + geom_ridgeline() | plot_base + geom_ridgeline(min_height = -2)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(iris, aes(x=Sepal.Length, y=Species, fill = factor(stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\", calc_ecdf = TRUE,\n    quantiles = 4, quantile_lines = TRUE\n  ) +\n  scale_fill_viridis_d(name = \"Quartiles\")\n#&gt; Warning: `stat(quantile)` was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `after_stat(quantile)` instead.\n#&gt; Picking joint bandwidth of 0.181\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(iris, aes(x = Sepal.Length, y = Species, fill = factor(stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\",\n    calc_ecdf = TRUE,\n    quantiles = c(0.025, 0.975)\n  ) +\n  scale_fill_manual(\n    name = \"Probability\", values = c(\"#FF0000A0\", \"#A0A0A0A0\", \"#0000FFA0\"),\n    labels = c(\"(0, 0.025]\", \"(0.025, 0.975]\", \"(0.975, 1]\")\n  )\n#&gt; Picking joint bandwidth of 0.181\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(iris, aes(x = Sepal.Length, y = Species)) +\n  geom_density_ridges(\n    jittered_points = TRUE, quantile_lines = TRUE, scale = 0.9, alpha = 0.7,\n    vline_size = 1, vline_color = \"red\",\n    point_size = 0.4, point_alpha = 1,\n    position = position_raincloud(adjust_vlines = TRUE)\n  )\n#&gt; Picking joint bandwidth of 0.181\n#&gt; Warning: Use of the `vline_size` or `size` aesthetic are deprecated, please use\n#&gt; `linewidth` instead of `size` and `vline_width` instead of `vline_size`.\n#&gt; Warning: Use of the `vline_size` aesthetics is deprecated, please use\n#&gt; `vline_width` instead of `vline_size`.\n\n#&gt; Warning: Use of the `vline_size` aesthetics is deprecated, please use\n#&gt; `vline_width` instead of `vline_size`.\n\n#&gt; Warning: Use of the `vline_size` aesthetics is deprecated, please use\n#&gt; `vline_width` instead of `vline_size`.\n\n\n\n\n\n\n\n\n\nスケールは縦スケールである．重なりを避けたい場合には小さくすればいいし，重なりがあっても強調したい場合には大きい値を設定すればよい.\n\n\nCode\nggplot(lincoln_weather, aes(x = `Mean Temperature [F]`, y = Month, fill = stat(x))) +\n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) +\n  scale_fill_viridis_c(name = \"Temp. [F]\", option = \"C\") +\n  labs(title = 'Temperatures in Lincoln NE in 2016')\n#&gt; Picking joint bandwidth of 3.37",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/01_randamnote.html#ggblend",
    "href": "contents/notes/ggplot2/01_randamnote.html#ggblend",
    "title": "Tips",
    "section": "3.13 ggblend",
    "text": "3.13 ggblend\nlink\n透過色でグラフを記述すると，レンダリングの順番でグラフの見え方が変わる. pngにしているとブレンドが機能しないとのことである. RStudioのレンダリングをcario pngにするとRStudio上でレンダリングすることが可能であった.\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggblend)\ntheme_set(ggdist::theme_ggdist() + theme(\n  plot.title = element_text(size = rel(1), lineheight = 1.1, face = \"bold\"),\n  plot.subtitle = element_text(face = \"italic\"),\n  panel.border = element_rect(color = \"gray75\", fill = NA)\n))\n\nset.seed(1234)\ndf_a = data.frame(x = rnorm(500, 0), y = rnorm(500, 1), set = \"a\")\ndf_b = data.frame(x = rnorm(500, 1), y = rnorm(500, 2), set = \"b\")\n\ndf_ab = rbind(df_a, df_b) |&gt;\n  transform(order = \"draw a then b\")\n\ndf_ba = rbind(df_b, df_a) |&gt;\n  transform(order = \"draw b then a\")\n\ndf = rbind(df_ab, df_ba)\n\n\n\n\nCode\ndf |&gt;\n  ggplot(aes(x, y, color = set)) +\n  geom_point(size = 3, alpha = 0.5) +\n  scale_color_brewer(palette = \"Set1\") +\n  facet_grid(~ order) +\n  labs(title = \"geom_point() without blending\", subtitle = \"Draw order matters.\")\n\n\n\n\n\n\n\n\n\n\n\nCode\noptions(ggblend.check_blend = FALSE)\ndf |&gt;\n  ggplot(aes(x, y, color = set)) +\n  geom_point(size = 3, alpha = 0.5) |&gt; blend(\"multiply\") +\n  scale_color_brewer(palette = \"Set1\") +\n  facet_grid(~ order) +\n  labs(\n    title = \"geom_point(alpha = 0.5) |&gt; blend('multiply')\",\n    subtitle = \"Draw order does not matter, but color is too dark.\"\n  )\n#&gt; Warning in drawDetails.GridGroup(x, recording = FALSE): Group definition failed\n\n#&gt; Warning in drawDetails.GridGroup(x, recording = FALSE): Group definition failed",
    "crumbs": [
      "Notes",
      "ggplot2",
      "Tips"
    ]
  },
  {
    "objectID": "contents/notes/development/03_R.html",
    "href": "contents/notes/development/03_R.html",
    "title": "R",
    "section": "",
    "text": "下記を参考にRのバージョン管理を試行する\n\nRのバージョン管理システム\nイマドキRのインストール事情",
    "crumbs": [
      "Notes",
      "development",
      "R"
    ]
  },
  {
    "objectID": "contents/notes/development/03_R.html#インストール",
    "href": "contents/notes/development/03_R.html#インストール",
    "title": "R",
    "section": "2.1 インストール",
    "text": "2.1 インストール\nwindows環境ではインストーラーがある。",
    "crumbs": [
      "Notes",
      "development",
      "R"
    ]
  },
  {
    "objectID": "contents/notes/development/03_R.html#使い方",
    "href": "contents/notes/development/03_R.html#使い方",
    "title": "R",
    "section": "2.2 使い方",
    "text": "2.2 使い方\n\n\nRの最新バージョンを追加する\nrig add release\n特定のバージョンを追加する\nrig add 4.2\n現在使えるインストール可能なバージョンをリストアップする\nrig available\nデフォルトのバージョンを表示する\nrig default\nデフォルトのバージョンを設定する\nrig set 4.2\nバージョンを切り換える\nrig switch 4.4.2\nRStudioを起動する\nrig rstudio\nRスクリプトを起動する\nrig run \nrig system add-pak                 -- install or update pak for an R version\nrig system clean-registry          -- clean stale R related entries in the registry\nrig system make-links              -- create R-* quick links\nrig system rtools                  -- manage Rtools installations\nrig system setup-user-lib          -- set up automatic user package libraries [alias: create-lib]\nrig system update-rtools40         -- update Rtools40 MSYS2 packages",
    "crumbs": [
      "Notes",
      "development",
      "R"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html",
    "href": "contents/notes/development/01_Python.html",
    "title": "Python",
    "section": "",
    "text": "pyenvはpythonのバージョンを切り換えるためのツールである。\nwindows環境をサポートしていないので、pyenv-winを利用すること。WSLもサポートしていないとのことである。\n\n\npowershellを管理者権限で実行する必要がある。\n最新版のバージョン７で実行するとインストールが成功した。\n実行が成功すると次のコマンドでpyenvのバージョンを判定する。\npyenv --version\nうまくインストールが成功しないときには、実行ポリシーの影響が考えられる。\nその際にはこのページを参照する。\n\n\n\n\n次のコマンドでインストール可能なpythonのバージョンを確認する。\npyenv install -l\n次のコマンドでpythonのバージョンをインストールする。\npyenv install 3.9.7\nいまつかるpythonのバージョンを確認する。\npyenv versions\n次のコマンドでグローバルのpythonのバージョンを設定する。\npyenv global 3.9.7\n次のコマンドでpythonのバージョンを確認する。\npython --version\n次のコマンドでローカルのpythonのバージョンを設定する。\npyenv local 3.9.7\n次のコマンドでローカルのpythonのバージョンを確認する。\npython --version\nローカルを設定すると、そのディレクトリ以下のpythonのバージョンが設定される。\n\n\n\npyenv-winを利用すると、pythonのバージョンを切り替えることができる。\nもしpythonがシステム環境変数に含まれている場合には次のパスをそれらのパスよりも優先させる必要がある。\n%USERPROFILE%\\.pyenv\\pyenv-win\\bin\n%USERPROFILE%\\.pyenv\\pyenv-win\\shims\n\n\n\nパスを削除する。パスのフォルダを丸ごと削除する、でよいらしい。",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#インストール",
    "href": "contents/notes/development/01_Python.html#インストール",
    "title": "Python",
    "section": "",
    "text": "powershellを管理者権限で実行する必要がある。\n最新版のバージョン７で実行するとインストールが成功した。\n実行が成功すると次のコマンドでpyenvのバージョンを判定する。\npyenv --version\nうまくインストールが成功しないときには、実行ポリシーの影響が考えられる。\nその際にはこのページを参照する。",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#pythonのインストール",
    "href": "contents/notes/development/01_Python.html#pythonのインストール",
    "title": "Python",
    "section": "",
    "text": "次のコマンドでインストール可能なpythonのバージョンを確認する。\npyenv install -l\n次のコマンドでpythonのバージョンをインストールする。\npyenv install 3.9.7\nいまつかるpythonのバージョンを確認する。\npyenv versions\n次のコマンドでグローバルのpythonのバージョンを設定する。\npyenv global 3.9.7\n次のコマンドでpythonのバージョンを確認する。\npython --version\n次のコマンドでローカルのpythonのバージョンを設定する。\npyenv local 3.9.7\n次のコマンドでローカルのpythonのバージョンを確認する。\npython --version\nローカルを設定すると、そのディレクトリ以下のpythonのバージョンが設定される。",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#注意点パスの設定",
    "href": "contents/notes/development/01_Python.html#注意点パスの設定",
    "title": "Python",
    "section": "",
    "text": "pyenv-winを利用すると、pythonのバージョンを切り替えることができる。\nもしpythonがシステム環境変数に含まれている場合には次のパスをそれらのパスよりも優先させる必要がある。\n%USERPROFILE%\\.pyenv\\pyenv-win\\bin\n%USERPROFILE%\\.pyenv\\pyenv-win\\shims",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#アンインストール",
    "href": "contents/notes/development/01_Python.html#アンインストール",
    "title": "Python",
    "section": "",
    "text": "パスを削除する。パスのフォルダを丸ごと削除する、でよいらしい。",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#インストール-1",
    "href": "contents/notes/development/01_Python.html#インストール-1",
    "title": "Python",
    "section": "2.1 インストール",
    "text": "2.1 インストール\n次のコマンドで公式インストーラーからインストールが可能である。\n(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\nあとは次のフォルダのパスを通すことでよい。\n\nアンインストールは次のコマンドでアル。\ncurl -sSL https://install.python-poetry.org | python3 - --uninstall",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#プジェクト",
    "href": "contents/notes/development/01_Python.html#プジェクト",
    "title": "Python",
    "section": "2.2 プジェクト",
    "text": "2.2 プジェクト\n新規プロジェクトの場合と、既存のプロジェクトの場合がある。\npoetry new my_project\nパッケージ開発でないときには次のコマンドで設定は適当な回答で大丈夫である。\ncd my_project\npoetry init\npoetryは２つのプロジェクトモードがある。１つはパッケージモードで、もうひとつはnon package modeである。",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#起動",
    "href": "contents/notes/development/01_Python.html#起動",
    "title": "Python",
    "section": "2.3 起動",
    "text": "2.3 起動\nプロジェクトを立ち上げたら基本的には準備よいので、次のコマンドでプロジェクトを立ち上げる。 そうすると設定したプロジェクトが立ち上がる。このとき指定したpythonのインタプリタがインストールされている必要があるので、 事前にpyenvでインストールしておく必要がある。\npoetry shell",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#その他",
    "href": "contents/notes/development/01_Python.html#その他",
    "title": "Python",
    "section": "2.4 その他",
    "text": "2.4 その他\n\n2.4.1 依存関係のインストール\npoetry install\nパッケージのインストールは次のようにおこなう。\npoetry add numpy\nデフォルトだと仮想環境はアプリケーションのディレクトリに作成されるため、プロジェクト直下に作成するには次のようにする。\npoetry config virtualenvs.in-project true\n次のように設定すると任意のディレクトリに仮想環境を作成することができる。\npoetry config virtualenvs.path &lt;path&gt;\n設定したconfigは次のコマンドで確認できる。\npoetry config --list\n仮想環境を削除するには次のコマンドを実行する。\npoetry env list 仮想環境を調べる\npoetry env remove &lt;仮想環境パスまたはPythonバージョン&gt;\npoetryは通常の依存関係と開発用の依存関係を区別している。開発用の依存関係をインストールするには次のコマンドを実行する。\npoetry add --dev pytest\n開発用の依存関係をインストールするには次のコマンドを実行する。\npoetry install --with dev\n開発用の依存関係を除いてインストールするには次のようにする\npoetry install --no-dev\n開発作業では次のコマンドでリンターなどを実行する。\npoetry run pytest\npoetry run flake8\nプロジェクトの情報を確認するには次のコマンドを実行する。\npoetry show",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#はじめに",
    "href": "contents/notes/development/01_Python.html#はじめに",
    "title": "Python",
    "section": "3.1 はじめに",
    "text": "3.1 はじめに\n\n公式サイト",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#installing",
    "href": "contents/notes/development/01_Python.html#installing",
    "title": "Python",
    "section": "3.2 Installing",
    "text": "3.2 Installing\npowershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n\n消したいときにはファイルを直接消してしまって大丈夫です。\nrm $HOME\\.local\\bin\\uv.exe\nrm $HOME\\.local\\bin\\uvx.exe",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#features",
    "href": "contents/notes/development/01_Python.html#features",
    "title": "Python",
    "section": "3.3 Features",
    "text": "3.3 Features",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/development/01_Python.html#プロジェクト",
    "href": "contents/notes/development/01_Python.html#プロジェクト",
    "title": "Python",
    "section": "3.4 プロジェクト",
    "text": "3.4 プロジェクト\n\n3.4.1 プロジェクト作成\n\npyprojectがないときに--no-workspaceを使う\npyprojectがあるときには--no-workspaceを使わない\n\nuv init dev --no-workspace",
    "crumbs": [
      "Notes",
      "development",
      "Python"
    ]
  },
  {
    "objectID": "contents/notes/bash/01_multicore.html",
    "href": "contents/notes/bash/01_multicore.html",
    "title": "マルチコア",
    "section": "",
    "text": "マルチコアでプログラムを動かす方法についてです。\nプログラム内で分けなくても書ができる形で動かす方法を紹介します。\nxargsでも出来るようです。\nところで, これを使う必要がある処理が思いつかない・・・？"
  },
  {
    "objectID": "contents/notes/bash/01_multicore.html#基本",
    "href": "contents/notes/bash/01_multicore.html#基本",
    "title": "マルチコア",
    "section": "2.1 基本",
    "text": "2.1 基本\nparallel --jobs 4 some_command {} ::: file1 file2 file3 file4 file5\nparallel --jobs 4 gzip {} ::: *.txt\nls *.csv | parallel -j 6 \"echo 'File: {}'\""
  },
  {
    "objectID": "contents/books/12_いきなりPython/ch06_笑顔キャプチャー.html",
    "href": "contents/books/12_いきなりPython/ch06_笑顔キャプチャー.html",
    "title": "笑顔キャプチャー",
    "section": "",
    "text": "Code\nfrom ultralytics import YOLO\n\n\nCreating new Ultralytics Settings v0.0.6 file  \nView Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\114012\\AppData\\Roaming\\Ultralytics\\settings.json'\nUpdate Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n\n\n\n\nCode\nmodel = YOLO('yolov8s.pt')\n\n\n\n\nCode\nimport cv2\n\ncap = cv2.VideoCapture(0)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if ret:\n        results = model(frame, verbose=False)\n        img_annotated = results[0].plot() # Annotate the image with bounding boxes\n        cv2.imshow('frame', img_annotated)\n\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    \ncap.release()\ncv2.destroyAllWindows()\n\n\n\n1 人数を表示しよう\n\n\nCode\nimport cv2\nimport numpy as np\n\ndef  text_overwrite_to_image(img, text, position ):\n    \n    img_width = img.shape[1]\n    \n    extension_height = 40\n    black_rectangle = np.zeros((extension_height, img_width, 3), dtype=np.uint8)\n\n    extended_img = np.vstack((black_rectangle, img))\n    \n    cv2.putText(\n        extended_img, \n        text=text, \n        org=position, \n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=1,\n        color=(255, 255, 255),\n        thickness=2\n    )    \n    \n    return extended_img\n\n\ncap = cv2.VideoCapture(0)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if ret:\n        results = model(frame, verbose=False, classes=0) # for person class\n        img_annotated = results[0].plot() # Annotate the image with bounding boxes\n        \n        class_list = results[0].boxes.cls\n        count_class = len(class_list)\n        \n        img_annotated = text_overwrite_to_image(img_annotated, f'Number of people: {count_class}', (10, 25))\n        \n        cv2.imshow('frame', img_annotated)\n\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    \ncap.release()\ncv2.destroyAllWindows()\n\n\n\n\n2 笑顔を検出する\n\n\nCode\nimport cv2\nimport numpy as np\n\ndef  text_overwrite_to_image(img, text, position ):\n    \n    img_width = img.shape[1]\n    \n    extension_height = 40\n    black_rectangle = np.zeros((extension_height, img_width, 3), dtype=np.uint8)\n\n    extended_img = np.vstack((black_rectangle, img))\n    \n    cv2.putText(\n        extended_img, \n        text=text, \n        org=position, \n        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n        fontScale=1,\n        color=(255, 255, 255),\n        thickness=2\n    )    \n    \n    return extended_img\n\n\ndef detect_faces(frame, person_box, face_cascade, smile_cascade):\n    x1, y1, x2, y2 = [int(v) for v in person_box]\n    roi_gray = cv2.cvtColor(frame[y1:y2, x1:x2], cv2.COLOR_BGR2GRAY)\n    faces = face_cascade.detectMultiScale(roi_gray, 1.1, 4)\n    \n    for (fx, fy, fw, fh) in faces:\n        roi_gray_face = roi_gray[fy:fy+fh, fx:fx+fw]\n        face_top_left = (x1+fx, y1+fy)\n        face_bottom_right = (x1+fx+fw, y1+fy+fh)\n        cv2.rectangle(frame, face_top_left, face_bottom_right, (0, 255, 0), 2)\n        \n        smiles = smile_cascade.detectMultiScale(roi_gray_face, 1.1, 10)\n        print(smiles)\n        for (sx, sy, sw, sh) in smiles:\n            smile_top_left = (x1 + fx + sx, y1 + fy + sy)\n            smile_bottom_right = (x1 + fx + sx + sw, y1 + fy + sy + sh)\n            cv2.rectangle(frame, smile_top_left, smile_bottom_right, (255, 0, 0), 2)\n        \n    return frame\n\n\nface_cascade  = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\nsmile_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')\ncap = cv2.VideoCapture(0)\n\ntry:\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if ret:\n            results = model(frame, verbose=False, classes=0) # for person class\n            img_annotated = results[0].plot() # Annotate the image with bounding boxes\n\n            class_list = results[0].boxes.cls\n            count_class = len(class_list)\n\n            person_boxes = results[0].boxes.xyxy\n\n            for box in person_boxes:\n                img_annotated = detect_faces(img_annotated, box, face_cascade, smile_cascade)\n\n            img_annotated = text_overwrite_to_image(img_annotated, f'Number of people: {count_class}', (10, 25))\n\n            cv2.imshow('frame', img_annotated)\n\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\nexcept Exception as e:\n    print(e)\nfinally:\n    cap.release()\n    cv2.destroyAllWindows()\n\n\n[[98 47 44 22]]\n()\n[[29 55 44 22]\n [ 3 44 46 23]]\n[[110  44  43  22]]\n[[27 56 45 22]]\n[[25 54 44 22]]\n()\n[[25 53 45 22]]\n[[23 51 47 23]]\n[[ 31  96 101  50]]\n[[31 93 84 42]]\n[[31 95 87 43]]\n[[87 42 45 23]\n [30 91 85 43]]\n[[91 43 44 22]\n [33 95 87 43]]\n[[35 95 83 42]]\n[[34 97 86 43]]\n[[85 41 44 22]\n [31 93 83 41]]\n[[18 42 42 21]\n [88 44 43 21]\n [33 97 82 41]]\n[[ 36 102  83  41]]\n[[15 38 42 21]\n [85 41 43 22]\n [33 96 71 35]]\n[[ 39 104  69  35]]\n[[36 96 77 39]]\n[[19 43 39 19]\n [32 91 82 41]\n [39 80 60 30]]\n[[90 45 44 22]\n [33 92 80 40]]\n[[32 89 79 40]]\n[[15 36 45 23]\n [31 90 82 41]]\n[[16 40 45 22]\n [32 92 82 41]]\n[[31 92 82 41]]\n[[87 44 43 21]\n [41 84 53 26]\n [33 93 78 39]]\n[[35 98 79 40]]\n()\n[[ 38 101  68  34]]\n[[36 99 74 37]]\n[[ 39 101  69  35]]\n[[ 38 104  76  38]]\n[[20 39 43 21]\n [32 96 75 37]]\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "いきなりPython",
      "笑顔キャプチャー"
    ]
  },
  {
    "objectID": "contents/books/12_いきなりPython/ch04_ため口矯正アプリ.html",
    "href": "contents/books/12_いきなりPython/ch04_ため口矯正アプリ.html",
    "title": "ため口矯正あぷり",
    "section": "",
    "text": "1 はじめに\nPCのマイクに話しかけた内容を自動で文字起こしするアプリを作成する。\n\n\nCode\nimport speech_recognition as sr\nimport re\n\n\n\n\nCode\ndef tamego_to_teineigo(text):\n    \"\"\"ため口を丁寧語に変換する関数\"\"\"\n    \n    patterns = {\n        r'だ': 'です',\n        r'です': 'です',\n        r'だよ': 'ですよ',\n        r'こんにちは': 'ごきげんよう', \n        r'ありがとう': 'ありがとうございます',\n        r'さようなら': 'さようなら',\n        r'おはよう': 'おはようございます',\n        r'おやすみ': 'おやすみなさい',\n        r'だね$': 'ですね',\n        r'だ$': 'です',\n    }\n    \n    sentences = text.split(' ')\n    transformed_sentences = []\n    for sentence in sentences:\n        for pattern, replacement in patterns.items():\n            sentence = re.sub(pattern, replacement, sentence)\n        sentence = re.sub(r'([ぁ-ん])\\1+$', r'\\1', sentence) # 重複したひらがなを一文字にする\n        transformed_sentences.append(sentence)\n    \n    return ' '.join(transformed_sentences)\n\ntamego_to_teineigo(\"こんにちは 今日はいい天気だねえええええええええ\")\n\n\n'ごきげんよう 今日はいい天気ですねえ'\n\n\n\n\nCode\nr = sr.Recognizer()\nduration = 1\nwith sr.Microphone() as source:\n    r.adjust_for_ambient_noise(source, duration=duration)\n    print(\"Say something!\")\n    audio = r.listen(source)\n\n\nSay something!\n\n\n\n\nCode\ntry:\n    audio_text = r.recognize_google(audio, language='ja')\n    print(f\"音声認識結果：{audio_text}\")\n    audio_text = tamego_to_teineigo(audio_text)\n    print(f\"丁寧語変換結果：{audio_text}\")\nexcept sr.UnknownValueError:\n    print(\"Google Speech Recognition could not understand audio\")\nexcept sr.RequestError as e:\n    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n\n\n音声認識結果：こんにちは 今日はいい天気だね\n丁寧語変換結果：こんにちは 今日はいい天気だね\n\n\n\n\nCode\n'こんにちは'.replace('こんにちは', 'ごきげんよう')\n\n\n'ごきげんよう'\n\n\n\n\nCode\ntamego_to_teineigo('こんにちは さようなら')\n\n\nこんにちは だ です\nこんにちは です です\nこんにちは だよ ですよ\nこんにちは こんにちは ごきげんよう\nこんにちは ありがとう ありがとうございます\nこんにちは さようなら さようなら\nこんにちは おはよう おはようございます\nこんにちは おやすみ おやすみなさい\nさようなら だ です\nさようなら です です\nさようなら だよ ですよ\nさようなら こんにちは ごきげんよう\nさようなら ありがとう ありがとうございます\nさようなら さようなら さようなら\nさようなら おはよう おはようございます\nさようなら おやすみ おやすみなさい\n\n\n'こんにちは さようなら'\n\n\n\n\nCode\n'こんにちは さようなら'.split(' ')\n\n\n['こんにちは', 'さようなら']\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "いきなりPython",
      "ため口矯正あぷり"
    ]
  },
  {
    "objectID": "contents/books/12_いきなりPython/ch02_推理力ゲーム.html",
    "href": "contents/books/12_いきなりPython/ch02_推理力ゲーム.html",
    "title": "推理力ゲーム",
    "section": "",
    "text": "Code\ndef check_hit_and_blow(secret, guess):\n    hit = 0\n    blow = 0\n    hit_and_blow = 0\n    for i in range(len(secret)):\n        if secret[i] == guess[i]:\n            hit += 1\n        if secret[i] in guess:\n            hit_and_blow += 1\n\n    blow = hit_and_blow - hit\n    return hit, blow\n\n\n\n\nCode\nprint(\"数当てゲームスタート\")\nprint('あなたは1桁から9桁の桁数を指定してください')\n\nwhile 1:\n    n = int(input('桁数を入力してください: '))\n    if 1 &lt;= n &lt;= 9:\n        break\n    print('1桁から9桁の桁数を指定してください')\n\nimport random\nnumbers = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nsecret_numbers = random.sample(numbers, n)\n\n# ユーザーの試行回数を初期化\ntrial_count = 0\n\nwhile 1:\n    while 1:\n        guess_number = input(f\"{n}桁の数字を入力してください: \")\n        guess_list = [int(i) for i in guess_number]\n        if len(guess_list) == n:\n            break\n    trial_count += 1\n    print(f\"{trial_count}回目の試行です\")\n    \n    hit, blow = check_hit_and_blow(secret_numbers, guess_list)\n    if hit == n:\n        print(f\"おめでとうございます！{trial_count}回目で正解しました\")\n        break\n    else:\n        print(f\"{hit}hit, {blow}blowです。もう一度挑戦してください\")\n        print()\n\n\n数当てゲームスタート\nあなたは1桁から9桁の桁数を指定してください\n1回目の試行です\n0hit, 0blowです。もう一度挑戦してください\n\n2回目の試行です\n1hit, 0blowです。もう一度挑戦してください\n\n3回目の試行です\n0hit, 0blowです。もう一度挑戦してください\n\n4回目の試行です\n0hit, 1blowです。もう一度挑戦してください\n\n5回目の試行です\n0hit, 0blowです。もう一度挑戦してください\n\n6回目の試行です\n1hit, 0blowです。もう一度挑戦してください\n\n7回目の試行です\n1hit, 0blowです。もう一度挑戦してください\n\n8回目の試行です\nおめでとうございます！8回目で正解しました\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "いきなりPython",
      "推理力ゲーム"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch52_ローカルレベルモデルの基本.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch52_ローカルレベルモデルの基本.html",
    "title": "ローカルレベルモデルの基本",
    "section": "",
    "text": "1 分析の準備\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n\n\n2 ローカルレベルモデル\n\\[\n\\begin{align}\n\\mu_t &= \\mu_{t-1} + \\eta_t \\\\\n\\alpha_t &= \\mu_t \\\\\ny_t &= \\mu_t + \\epsilon_t\n\\end{align}\n\\]\n\n\n3 ローカルレベルモデルのシミュレーション\n\n\nCode\nnp.random.seed(0)\n\n# ランダムウォーク系列\nsim_size = 100\nmu = stats.norm.rvs(size=sim_size, loc=0, scale=1).round(1).cumsum() + 30\n\n# ランダムウォーク系列に観測誤差を加える\ny = mu + stats.norm.rvs(size=sim_size, loc=0, scale=5).round(1)\n\n# データフレームにまとめる\nlocal_level_df = pd.DataFrame({'mu': mu, 'y': y})\nlocal_level_df.plot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "ローカルレベルモデルの基本"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch44_モデル選択.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch44_モデル選択.html",
    "title": "モデル選択",
    "section": "",
    "text": "単位根検定:DF検定とADF検定\n単位根検定:KPSS検定\n季節単位根検定:DHF検定\n季節単位根検定:OCSB検定\n情報量規準の利用\n残差診断と残差の可視化\n単位根検定の実装\n自動予測アプローチの実装\nモデルの診断\nsktimeを使ったモデル選択",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "モデル選択"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch44_モデル選択.html#単純な単位根検定",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch44_モデル選択.html#単純な単位根検定",
    "title": "モデル選択",
    "section": "5.1 単純な単位根検定",
    "text": "5.1 単純な単位根検定\n\n\nCode\npm.arima.ndiffs(np.log(train['sales']), test='kpss')\n\n\n1\n\n\n\n\nCode\npm.arima.nsdiffs(np.log(train['sales']), m=12, test='ch')\n\n\n0\n\n\n外生変数を利用する場合は、SARIMAモデルを残差に対して適用するため、残差に対して単位根検定をおこなう。\n\n\nCode\nols_resid = smf.ols('np.log(sales) ~ discount', data=train).fit().resid\n\n\n\n\nCode\n# 残差に対して単位根検定を実施して、差分を取る必要性を判断する\npm.arima.ndiffs(ols_resid, test='kpss')\n\n\n1\n\n\n\n\nCode\npm.arima.nsdiffs(ols_resid, m=12, test='ocsb')\n\n\n1",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "モデル選択"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch42_ARIMAモデル.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch42_ARIMAモデル.html",
    "title": "ARIMAモデル",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as tsa\n\nfrom sklearn.linear_model import LinearRegression\nfrom sktime.forecasting.compose import make_reduction\n\nfrom matplotlib import rcParams\nrcParams['font.family'] = 'sans-serif'\nrcParams['font.sans-serif'] = 'Meiryo'",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "ARIMAモデル"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch42_ARIMAモデル.html#シミュレーション",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch42_ARIMAモデル.html#シミュレーション",
    "title": "ARIMAモデル",
    "section": "2.1 シミュレーション",
    "text": "2.1 シミュレーション\n1次のAR過程で回帰係数が正の場合と負の場合を考える。\n\\[\n\\begin{align}\ny_t &= 0.8y_{t-1} + \\epsilon_t\\\\\ny_t &= -0.8y_{t-1} + \\epsilon_t\n\\end{align}\n\\]\n\n\nCode\nar_data = pd.read_csv(\n    './book-python-tsa-intro/book-data/4-2-1-ar-data.csv', index_col='date', parse_dates=True, dtype='float'\n)\n\nar_data.index.freq = 'D'\n\nar_data.head()\n\n\n\n\n\n\n\n\n\nposi\nnega\n\n\ndate\n\n\n\n\n\n\n2023-01-01\n1.788628\n-0.231497\n\n\n2023-01-02\n1.867413\n0.680778\n\n\n2023-01-03\n1.590428\n-1.115185\n\n\n2023-01-04\n-0.591151\n2.312491\n\n\n2023-01-05\n-0.750309\n-2.169583\n\n\n\n\n\n\n\n\n\nCode\nar_data.plot(subplots=True)\n\n\narray([&lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;], dtype=object)",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "ARIMAモデル"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch42_ARIMAモデル.html#ar過程に従うデータの自己相関",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch42_ARIMAモデル.html#ar過程に従うデータの自己相関",
    "title": "ARIMAモデル",
    "section": "2.2 AR過程に従うデータの自己相関",
    "text": "2.2 AR過程に従うデータの自己相関\n\\(\\|\\phi\\|&lt;1\\)なのでどちらの過程でも収束する様子がわかる。また、負の場合には自己相関が振動していることがわかる。\n\n\nCode\nfig, ax = plt.subplots(2, 1, figsize=(8, 4), tight_layout=True)\n\n_ = sm.graphics.tsa.plot_acf(ar_data['posi'], lags=40, title='AR posi ACF', ax=ax[0])\n_ = sm.graphics.tsa.plot_acf(ar_data['nega'], lags=40, title='AR nega ACF', ax=ax[1])",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "ARIMAモデル"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch42_ARIMAモデル.html#ar過程に従うデータの偏自己相関",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch42_ARIMAモデル.html#ar過程に従うデータの偏自己相関",
    "title": "ARIMAモデル",
    "section": "2.3 AR過程に従うデータの偏自己相関",
    "text": "2.3 AR過程に従うデータの偏自己相関\n偏自己相関とは\\(k-1\\)次のまでの自己相関の影響を排除したうえで、純粋な\\(k\\)次の自己相関を求めるものである。\n\n\nCode\nfig, ax = plt.subplots(2, 1, figsize=(8, 4), tight_layout=True)\n\n_ = sm.graphics.tsa.plot_pacf(ar_data['posi'], lags=40, title='AR posi ACF', ax=ax[0])\n_ = sm.graphics.tsa.plot_pacf(ar_data['nega'], lags=40, title='AR nega ACF', ax=ax[1])",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "ARIMAモデル"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch42_ARIMAモデル.html#ma過程に従うデータの例",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch42_ARIMAモデル.html#ma過程に従うデータの例",
    "title": "ARIMAモデル",
    "section": "3.1 MA過程に従うデータの例",
    "text": "3.1 MA過程に従うデータの例\n\\[\n\\begin{align}\ny_t &= 0.8\\epsilon_{t-1} + \\epsilon_t\\\\\ny_t &= -0.8\\epsilon_{t-1} + \\epsilon_t\n\\end{align}\n\\]\n\n\nCode\nma_data = pd.read_csv(\"./book-python-tsa-intro/book-data/4-2-2-ma-data.csv\", index_col='date', parse_dates=True, dtype='float')\n\nma_data.index.freq='D'\n\nma_data.plot(subplots=True)\n\n\narray([&lt;Axes: xlabel='date'&gt;, &lt;Axes: xlabel='date'&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(2, 1, figsize=(8, 4), tight_layout=True)\n\n_ = sm.graphics.tsa.plot_acf(ma_data['posi'], lags=40, title='MA posi ACF', ax=ax[0])\n_ = sm.graphics.tsa.plot_acf(ma_data['nega'], lags=40, title='MA nega ACF', ax=ax[1])",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "ARIMAモデル"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch34_指数平滑化法とその周辺.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch34_指数平滑化法とその周辺.html",
    "title": "指数平滑化方とその周辺",
    "section": "",
    "text": "Code\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nsns.set()\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as tsa\n\nfrom sktime.utils.plotting import plot_series\n\nfrom sktime.performance_metrics.forecasting import (\n    mean_absolute_percentage_error,\n    mean_absolute_scaled_error, \n    mean_squared_error,\n    mean_absolute_error,\n    MeanAbsoluteError,\n)\n\nfrom sktime.transformations.series.detrend import (\n    Deseasonalizer, \n    Detrender,\n)\n\n\nfrom sktime.forecasting.trend import PolynomialTrendForecaster\nfrom sktime.forecasting.naive import NaiveForecaster\n\nfrom sktime.transformations.series.difference import Differencer\nfrom sktime.transformations.series.boxcox import LogTransformer\n\nfrom sktime.forecasting.compose import (\n    TransformedTargetForecaster,\n    EnsembleForecaster,\n    MultiplexForecaster,\n    StackingForecaster, \n)\nfrom sktime.transformations.compose import OptionalPassthrough\n\nfrom sktime.forecasting.model_selection import (\n    temporal_train_test_split,\n    ForecastingGridSearchCV,\n    SlidingWindowSplitter,\n    ExpandingWindowSplitter\n)\nfrom sktime.forecasting.model_evaluation import evaluate\n\nfrom statsmodels.tsa.deterministic import TimeTrend\n\nimport dask.dataframe as dd\n\nfrom matplotlib import rcParams\nrcParams['font.family'] = 'sans-serif'\nrcParams['font.sans-serif'] = 'Meiryo'\n\npd.set_option('display.unicode.east_asian_width', True)\n\n\n\n\nCode\nair_passengers = sm.datasets.get_rdataset('AirPassengers').data\n\ndate_index = pd.period_range(\n    start='1949-01', periods=len(air_passengers), freq='M'\n)\nair_passengers.index = date_index\n\nair_passengers = air_passengers.drop(air_passengers.columns[0], axis=1)\nair_passengers\n\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\n1949-01\n112\n\n\n1949-02\n118\n\n\n1949-03\n132\n\n\n1949-04\n129\n\n\n1949-05\n121\n\n\n...\n...\n\n\n1960-08\n606\n\n\n1960-09\n508\n\n\n1960-10\n461\n\n\n1960-11\n390\n\n\n1960-12\n432\n\n\n\n\n144 rows × 1 columns\n\n\n\n\n\nCode\ntrain, test = temporal_train_test_split(air_passengers, test_size=36)\n\nfh = np.arange(1, len(test) + 1)",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "指数平滑化方とその周辺"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch34_指数平滑化法とその周辺.html#飛行機乗客データへの適用",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch34_指数平滑化法とその周辺.html#飛行機乗客データへの適用",
    "title": "指数平滑化方とその周辺",
    "section": "6.1 飛行機乗客データへの適用",
    "text": "6.1 飛行機乗客データへの適用\n\n\nCode\nhw_air = tsa.ExponentialSmoothing(\n    train, \n    trend='add', \n    seasonal='add', \n    seasonal_periods=12\n).fit()\n\npred = hw_air.forecast(len(test))\n\nmean_absolute_error(test, pred)\n\n\n21.54495633795129\n\n\n\n\nCode\nfig, ax = plot_series(train, hw_air.level, test ,pred, labels=['Train', 'Holt-Winters', 'Test', 'Holt-Winters (forecast)'], markers=['', '', 'o', 'x'])\nfig.set_size_inches(12, 6)",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "指数平滑化方とその周辺"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html",
    "title": "季節調整とトレンド除去",
    "section": "",
    "text": "増加トレンドと季節性のある時系列データを扱う方法について説明します。\n季節調整とトレンド除去の考え方\n移動平均法による加法型の季節調整\n移動平均法による乗法型の季節調整\n回帰分析によるトレンド除去\n差分による季節調整とトレンド除去",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#トレンドと循環成分",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#トレンドと循環成分",
    "title": "季節調整とトレンド除去",
    "section": "3.1 トレンドと循環成分",
    "text": "3.1 トレンドと循環成分\nT+Cを抽出して、差分を取る\n\n\nCode\n# 12時点移動平均\nma_12 = air_passengers[\"value\"].rolling(window=12, center=True).mean()\n\n# 更に、移動平均を取ることで、季節変動を取り除く\n# つまり\ntrend = ma_12.rolling(window=2, center=True).mean().shift(-1)\ntrend[\"1949\"]\n\n\n1949-01-01           NaN\n1949-02-01           NaN\n1949-03-01           NaN\n1949-04-01           NaN\n1949-05-01           NaN\n1949-06-01           NaN\n1949-07-01    126.791667\n1949-08-01    127.250000\n1949-09-01    127.958333\n1949-10-01    128.583333\n1949-11-01    129.000000\n1949-12-01    129.750000\nFreq: MS, Name: value, dtype: float64\n\n\n\n\nCode\n(air_passengers[\"value\"].loc[\"1949-01-01\":\"1949-12-01\"]).sum() / 12, ma_12.iloc[7]\n\n\n(126.66666666666667, 126.91666666666667)\n\n\n\n\nCode\n((air_passengers[\"value\"].loc[\"1949-02-01\":\"1950-01-01\"]).sum() + (air_passengers[\"value\"].loc[\"1949-01-01\":\"1949-12-01\"]).sum()) / 24, trend[\"1949-07-01\"]\n\n\n(126.79166666666667, 126.79166666666667)\n\n\n\n\nCode\ndetrend = air_passengers[\"value\"] - trend\ndetrend.plot()",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#季節成分",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#季節成分",
    "title": "季節調整とトレンド除去",
    "section": "3.2 季節成分",
    "text": "3.2 季節成分\n不規則西部は長期的な平均値をとることによって無視できると考える。すなわり、\\(S_t+I_t\\)の平均値を季節成分\\(S_t\\)として抽出する。\n\n\nCode\nseasonal_year = detrend.groupby(detrend.index.month).mean()\n\n# 季節成分の平均値が0になるように調整\nseasonal_year = seasonal_year - np.mean(seasonal_year)\n\nseasonal_year\n\n\n1    -24.748737\n2    -36.188131\n3     -2.241162\n4     -8.036616\n5     -4.506313\n6     35.402778\n7     63.830808\n8     62.823232\n9     16.520202\n10   -20.642677\n11   -53.593434\n12   -28.619949\nName: value, dtype: float64\n\n\n\n\nCode\n# 季節成分の長さをtrendに合わせる\nseasonal = pd.concat([seasonal_year] * (len(trend) // 12))\nseasonal.index = air_passengers.index\nseasonal.plot()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 季節調整済み\ndeseasonalized = air_passengers[\"value\"] - seasonal\ndeseasonalized.plot()\n\n\n\n\n\n\n\n\n\n上記の結果をみると、季節成分は残っていることがわかります。",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#不規則成分",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#不規則成分",
    "title": "季節調整とトレンド除去",
    "section": "3.3 不規則成分",
    "text": "3.3 不規則成分\nT+C+Sを抽出して、差分を取る\n\n\nCode\nresid = air_passengers[\"value\"] - trend - seasonal\nresid.plot()\n\n\n\n\n\n\n\n\n\n確かに不規則な変動が生じていることがわかります。",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#sktimeの利用",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#sktimeの利用",
    "title": "季節調整とトレンド除去",
    "section": "4.1 sktimeの利用",
    "text": "4.1 sktimeの利用\n\n\nCode\ntransformer = Deseasonalizer(sp=12, model=\"additive\")\ndesea_sk = transformer.fit_transform(air_passengers.to_period())\ndesea_sk.plot()",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#tc成分の除去",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#tc成分の除去",
    "title": "季節調整とトレンド除去",
    "section": "5.1 TC成分の除去",
    "text": "5.1 TC成分の除去\n\n\nCode\ndetrend_mul = air_passengers[\"value\"] / trend\n\ndetrend_mul.plot()",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#季節成分-1",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#季節成分-1",
    "title": "季節調整とトレンド除去",
    "section": "5.2 季節成分",
    "text": "5.2 季節成分\n\n\nCode\nseasonal_year_mul = detrend_mul.groupby(detrend_mul.index.month).mean()\n\nseasonal_year_mul = seasonal_year_mul / np.mean(seasonal_year_mul)\n\nseasonal_year_mul\n\n\n1     0.910230\n2     0.883625\n3     1.007366\n4     0.975906\n5     0.981378\n6     1.112776\n7     1.226556\n8     1.219911\n9     1.060492\n10    0.921757\n11    0.801178\n12    0.898824\nName: value, dtype: float64\n\n\n\n\nCode\nseasonal_mul = pd.Series(np.tile(seasonal_year_mul, len(trend) // 12))\nseasonal_mul.index = air_passengers.index\nseasonal_mul.plot()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndeseasonalized_mul = air_passengers[\"value\"] / seasonal_mul\ndeseasonalized_mul.plot()",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#不規則成分-1",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#不規則成分-1",
    "title": "季節調整とトレンド除去",
    "section": "5.3 不規則成分",
    "text": "5.3 不規則成分\n\n\nCode\nresid_mul = air_passengers[\"value\"] / (trend * seasonal_mul)\nresid_mul.plot()",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#sktime",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#sktime",
    "title": "季節調整とトレンド除去",
    "section": "6.1 sktime",
    "text": "6.1 sktime\n\n\nCode\ntransformer_mul = Deseasonalizer(sp=12, model=\"multiplicative\")\n(transformer_mul.fit_transform(air_passengers.to_period())).plot()",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#説明変数",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#説明変数",
    "title": "季節調整とトレンド除去",
    "section": "7.1 説明変数",
    "text": "7.1 説明変数\n\n\nCode\ntrend_generator = TimeTrend(constant=True, order=1) # 1次\nexog = trend_generator.in_sample(air_passengers.index)\nexog\n\n\n\n\n\n\n\n\n\nconst\ntrend\n\n\n\n\n1949-01-01\n1.0\n1.0\n\n\n1949-02-01\n1.0\n2.0\n\n\n1949-03-01\n1.0\n3.0\n\n\n1949-04-01\n1.0\n4.0\n\n\n1949-05-01\n1.0\n5.0\n\n\n...\n...\n...\n\n\n1960-08-01\n1.0\n140.0\n\n\n1960-09-01\n1.0\n141.0\n\n\n1960-10-01\n1.0\n142.0\n\n\n1960-11-01\n1.0\n143.0\n\n\n1960-12-01\n1.0\n144.0\n\n\n\n\n144 rows × 2 columns",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#加法型のトレンド除去",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#加法型のトレンド除去",
    "title": "季節調整とトレンド除去",
    "section": "7.2 加法型のトレンド除去",
    "text": "7.2 加法型のトレンド除去\n\n\nCode\nlm_model = sm.OLS(air_passengers, exog).fit()\ndetrend_ols = air_passengers[\"value\"] - lm_model.fittedvalues\ndetrend_ols.plot()",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#乗法型のトレンド除去",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#乗法型のトレンド除去",
    "title": "季節調整とトレンド除去",
    "section": "7.3 乗法型のトレンド除去",
    "text": "7.3 乗法型のトレンド除去\n\n\nCode\ndetrend_ols_mul = air_passengers[\"value\"] / lm_model.fittedvalues\ndetrend_ols_mul.plot()",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#sktimeを用いた実装",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch32_季節調整とトレンド除去.html#sktimeを用いた実装",
    "title": "季節調整とトレンド除去",
    "section": "7.4 sktimeを用いた実装",
    "text": "7.4 sktimeを用いた実装\n\n\nCode\ntransformer_trend = Detrender(\n    forecaster=PolynomialTrendForecaster(degree=1), model=\"additive\"\n)\ndetrend_ols_sk = transformer_trend.fit_transform(air_passengers.to_period())\ndetrend_ols_sk.plot()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 乗法型のトレンド除去\ntransformer_trend_mul = Detrender(\n    forecaster=PolynomialTrendForecaster(degree=1), model=\"multiplicative\"\n)\ndetrend_ols_sk_mul = transformer_trend_mul.fit_transform(air_passengers.to_period())\ndetrend_ols_sk_mul.plot()",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "季節調整とトレンド除去"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch06_時系列データのシミュレーションと見せかけの回帰.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch06_時系列データのシミュレーションと見せかけの回帰.html",
    "title": "時系列データのシミュレーションと見せかけの回帰",
    "section": "",
    "text": "1 はじめに\n\n正規ホワイトノイズ\nランダムウォーク\nDurbin-Watson統計量\n\n\n\n2 分析準備\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport japanize_matplotlib\nimport seaborn as sns\nsns.set()\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as tsa\n\nfrom matplotlib import rcParams\nrcParams['font.family'] = 'sans-serif'\nrcParams['font.sans-serif'] = 'Meiryo'\n\npd.set_option('display.unicode.east_asian_width', True)\n\n\n\n\n3 正規ホワイトノイズ系列のシミュレーション\n\n\nCode\nnp.random.seed(1)\n\nstats.norm.rvs(loc=0, scale=1, size=4)\n\n\narray([ 1.62434536, -0.61175641, -0.52817175, -1.07296862])\n\n\n\n\nCode\nn_sim = 50\nseq_len = 100\nnp.random.seed(1)\nfor i in range(1, n_sim):\n    hn = stats.norm.rvs(loc=0, scale=1, size=seq_len)\n    sns.lineplot(x = range(0, seq_len), y = hn)\n\n\n\n\n\n\n\n\n\n\n\n4 ランダムサンプリング系列のシミュレーション\n\n\nCode\nn_sim = 50\nseq_len = 100\nnp.random.seed(1)\nfor i in range(1, n_sim):\n    hn = stats.norm.rvs(loc=0, scale=1, size=seq_len).cumsum()\n    sns.lineplot(x = range(0, seq_len), y = hn)\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "時系列データのシミュレーションと見せかけの回帰"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch04_pandasによる日付処理の基本.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch04_pandasによる日付処理の基本.html",
    "title": "pandasによる日付処理の基本",
    "section": "",
    "text": "1 時系列データの読込\n\n日付情報をインデックスにする\n日付情報の単位は読み込んだ後に指定する\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\nts_month = pd.read_csv(\n    './book-python-tsa-intro/book-data/2-4-1-time-series-month.csv', \n    index_col=\"time\", \n    parse_dates=True, \n    dtype={'sales':'float', }\n)\n\nts_month.index.freq = 'MS' # 月初\nts_month\n\n\n\n\n\n\n\n\n\nsales\n\n\ntime\n\n\n\n\n\n2000-01-01\n10.0\n\n\n2000-02-01\n5.0\n\n\n2000-03-01\n8.0\n\n\n2000-04-01\n14.0\n\n\n2000-05-01\n9.0\n\n\n\n\n\n\n\n\n\nCode\nts_month.index\n\n\nDatetimeIndex(['2000-01-01', '2000-02-01', '2000-03-01', '2000-04-01',\n               '2000-05-01'],\n              dtype='datetime64[ns]', name='time', freq='MS')\n\n\n\n\nCode\nts_day = pd.read_csv(\n    './book-python-tsa-intro/book-data/2-4-2-time-series-day.csv', \n    index_col=\"time\", \n    parse_dates=True, \n    dtype={'value':'float', }\n)\n\nts_day.index.freq = 'D' # 月初\nts_day\n\n\n\n\n\n\n\n\n\nvalue\n\n\ntime\n\n\n\n\n\n1990-01-01\n1.0\n\n\n1990-01-02\n2.0\n\n\n1990-01-03\n3.0\n\n\n1990-01-04\n4.0\n\n\n1990-01-05\n5.0\n\n\n...\n...\n\n\n1999-12-27\n3648.0\n\n\n1999-12-28\n3649.0\n\n\n1999-12-29\n3650.0\n\n\n1999-12-30\n3651.0\n\n\n1999-12-31\n3652.0\n\n\n\n\n3652 rows × 1 columns\n\n\n\n\n\n2 データ抽出の基本\n\n\nCode\nts_day.head(3)\n\n\n\n\n\n\n\n\n\nvalue\n\n\ntime\n\n\n\n\n\n1990-01-01\n1.0\n\n\n1990-01-02\n2.0\n\n\n1990-01-03\n3.0\n\n\n\n\n\n\n\n\n\nCode\nts_day.tail(3)\n\n\n\n\n\n\n\n\n\nvalue\n\n\ntime\n\n\n\n\n\n1999-12-29\n3650.0\n\n\n1999-12-30\n3651.0\n\n\n1999-12-31\n3652.0\n\n\n\n\n\n\n\n\n\nCode\n# インデックスで抽出\nts_day.loc['1990-01-02']\n\n\nvalue    2.0\nName: 1990-01-02 00:00:00, dtype: float64\n\n\n\n\nCode\n# インデックスで抽出\nts_day.loc['1990-01-02':'1990-01-04']\n\n\n\n\n\n\n\n\n\nvalue\n\n\ntime\n\n\n\n\n\n1990-01-02\n2.0\n\n\n1990-01-03\n3.0\n\n\n1990-01-04\n4.0\n\n\n\n\n\n\n\n\n\nCode\n# 単位を変えて抽出も可能\n# 下記では1990年の2月のデータを抽出する\nts_day.loc['1990-02']\n\n\n\n\n\n\n\n\n\nvalue\n\n\ntime\n\n\n\n\n\n1990-02-01\n32.0\n\n\n1990-02-02\n33.0\n\n\n1990-02-03\n34.0\n\n\n1990-02-04\n35.0\n\n\n1990-02-05\n36.0\n\n\n1990-02-06\n37.0\n\n\n1990-02-07\n38.0\n\n\n1990-02-08\n39.0\n\n\n1990-02-09\n40.0\n\n\n1990-02-10\n41.0\n\n\n1990-02-11\n42.0\n\n\n1990-02-12\n43.0\n\n\n1990-02-13\n44.0\n\n\n1990-02-14\n45.0\n\n\n1990-02-15\n46.0\n\n\n1990-02-16\n47.0\n\n\n1990-02-17\n48.0\n\n\n1990-02-18\n49.0\n\n\n1990-02-19\n50.0\n\n\n1990-02-20\n51.0\n\n\n1990-02-21\n52.0\n\n\n1990-02-22\n53.0\n\n\n1990-02-23\n54.0\n\n\n1990-02-24\n55.0\n\n\n1990-02-25\n56.0\n\n\n1990-02-26\n57.0\n\n\n1990-02-27\n58.0\n\n\n1990-02-28\n59.0\n\n\n\n\n\n\n\n\n\nCode\n# 詳細な日時の指定\n# 分単位で指定している\ntime_idx = pd.date_range(\n    start='2020-01-01',\n    end='2021-01-01',\n    freq='15min',\n    inclusive='left' # 左閉じ\n)\nlong_ts = pd.Series(np.arange(0, len(time_idx), 1), index=time_idx)\nlong_ts\n\n\n2020-01-01 00:00:00        0\n2020-01-01 00:15:00        1\n2020-01-01 00:30:00        2\n2020-01-01 00:45:00        3\n2020-01-01 01:00:00        4\n                       ...  \n2020-12-31 22:45:00    35131\n2020-12-31 23:00:00    35132\n2020-12-31 23:15:00    35133\n2020-12-31 23:30:00    35134\n2020-12-31 23:45:00    35135\nFreq: 15min, Length: 35136, dtype: int32\n\n\n\n\nCode\n# 秒単位でも指定できる\nlong_ts.loc['2020-12-31 01:00:00']\n\n\n35044\n\n\n\n\nCode\n# 時間単位での条件抽出\nlong_ts.loc['2020-12-31 01']\n\n\n2020-12-31 01:00:00    35044\n2020-12-31 01:15:00    35045\n2020-12-31 01:30:00    35046\n2020-12-31 01:45:00    35047\nFreq: 15min, dtype: int32\n\n\n\n\nCode\nlong_ts.loc['2020-12-31']\n\n\n2020-12-31 00:00:00    35040\n2020-12-31 00:15:00    35041\n2020-12-31 00:30:00    35042\n2020-12-31 00:45:00    35043\n2020-12-31 01:00:00    35044\n                       ...  \n2020-12-31 22:45:00    35131\n2020-12-31 23:00:00    35132\n2020-12-31 23:15:00    35133\n2020-12-31 23:30:00    35134\n2020-12-31 23:45:00    35135\nFreq: 15min, Length: 96, dtype: int32\n\n\n\n\nCode\n# 中途半端な時間でもインデックスを使うことができる\nlong_ts.loc['2020-12-13 05':'2020-12-31']\n\n\n2020-12-13 05:00:00    33332\n2020-12-13 05:15:00    33333\n2020-12-13 05:30:00    33334\n2020-12-13 05:45:00    33335\n2020-12-13 06:00:00    33336\n                       ...  \n2020-12-31 22:45:00    35131\n2020-12-31 23:00:00    35132\n2020-12-31 23:15:00    35133\n2020-12-31 23:30:00    35134\n2020-12-31 23:45:00    35135\nFreq: 15min, Length: 1804, dtype: int32\n\n\n\n\nCode\n# 日付を横断した選択\nlong_ts.at_time('10:00:00')\n\n\n2020-01-01 10:00:00       40\n2020-01-02 10:00:00      136\n2020-01-03 10:00:00      232\n2020-01-04 10:00:00      328\n2020-01-05 10:00:00      424\n                       ...  \n2020-12-27 10:00:00    34696\n2020-12-28 10:00:00    34792\n2020-12-29 10:00:00    34888\n2020-12-30 10:00:00    34984\n2020-12-31 10:00:00    35080\nFreq: 1440min, Length: 366, dtype: int32\n\n\n\n\nCode\n# 時間帯を指定した選択\nlong_ts.between_time(start_time='10:00:00', end_time='11:00:00', inclusive='left')\n\n\n2020-01-01 10:00:00       40\n2020-01-01 10:15:00       41\n2020-01-01 10:30:00       42\n2020-01-01 10:45:00       43\n2020-01-02 10:00:00      136\n                       ...  \n2020-12-30 10:45:00    34987\n2020-12-31 10:00:00    35080\n2020-12-31 10:15:00    35081\n2020-12-31 10:30:00    35082\n2020-12-31 10:45:00    35083\nLength: 1464, dtype: int32\n\n\n\n\n3 日時を用いたデータの集計\n\n\nCode\n# 素朴な方法：欲しい粒度での日付を準備する\nts_day[\"month\"] = ts_day.index.month\nts_day.groupby(\"month\").sum()\n\n\n\n\n\n\n\n\n\nvalue\n\n\nmonth\n\n\n\n\n\n1\n514445.0\n\n\n2\n475961.0\n\n\n3\n532797.0\n\n\n4\n524760.0\n\n\n5\n551707.0\n\n\n6\n543060.0\n\n\n7\n570617.0\n\n\n8\n580227.0\n\n\n9\n570660.0\n\n\n10\n599137.0\n\n\n11\n588960.0\n\n\n12\n618047.0\n\n\n\n\n\n\n\n\n\nCode\nts_day.groupby([ts_day.index.year, ts_day.index.month]).sum()\n\n\n\n\n\n\n\n\n\n\nvalue\nmonth\n\n\ntime\ntime\n\n\n\n\n\n\n1990\n1\n496.0\n31\n\n\n2\n1274.0\n56\n\n\n3\n2325.0\n93\n\n\n4\n3165.0\n120\n\n\n5\n4216.0\n155\n\n\n...\n...\n...\n...\n\n\n1999\n8\n108965.0\n248\n\n\n9\n106365.0\n270\n\n\n10\n110856.0\n310\n\n\n11\n108195.0\n330\n\n\n12\n112747.0\n372\n\n\n\n\n120 rows × 2 columns\n\n\n\n\n\n4 PeriodIndexの作成と利用\n\nTimestap:ある時点\nDatetimeIndex:時点の集合\nPeriodINdex:時間の範囲\n\n\n\nCode\nperiod_range = pd.period_range(start='2020-01-01', end='2020-02-01', freq='M')\nperiod_range\n\n\nPeriodIndex(['2020-01', '2020-02'], dtype='period[M]')\n\n\n\n\nCode\ntype(period_range)\n\n\npandas.core.indexes.period.PeriodIndex\n\n\n\n\nCode\n# DatetimeIndexとの変換\ntime_range = pd.date_range(start='2020-01-01', end='2020-02-01', freq='MS')\ntime_range\n\n\nDatetimeIndex(['2020-01-01', '2020-02-01'], dtype='datetime64[ns]', freq='MS')\n\n\n\n\nCode\ntime_range.to_period()\n\n\nPeriodIndex(['2020-01', '2020-02'], dtype='period[M]')\n\n\n\n\nCode\nperiod_range.to_timestamp()\n\n\nDatetimeIndex(['2020-01-01', '2020-02-01'], dtype='datetime64[ns]', freq=None)\n\n\n\n\nCode\n# DataFrameのindex\n# 次の操作でindexがperiod indexに変換される\nts_month_period = ts_month.copy().to_period()\nts_month_period\n\n\n\n\n\n\n\n\n\nsales\n\n\ntime\n\n\n\n\n\n2000-01\n10.0\n\n\n2000-02\n5.0\n\n\n2000-03\n8.0\n\n\n2000-04\n14.0\n\n\n2000-05\n9.0\n\n\n\n\n\n\n\n\n\nCode\nts_month_period.index\n\n\nPeriodIndex(['2000-01', '2000-02', '2000-03', '2000-04', '2000-05'], dtype='period[M]', name='time')\n\n\n\n\nCode\n# 操作\nperiod_range.year\n\n\nIndex([2020, 2020], dtype='int64')\n\n\n\n\nCode\nperiod_range.month\n\n\nIndex([1, 2], dtype='int64')\n\n\n\n\nCode\nts_month_period.loc['2000-04']\n\n\nsales    14.0\nName: 2000-04, dtype: float64\n\n\n\n\nCode\nts_month_period.loc['2000-03':'2000-05']\n\n\n\n\n\n\n\n\n\nsales\n\n\ntime\n\n\n\n\n\n2000-03\n8.0\n\n\n2000-04\n14.0\n\n\n2000-05\n9.0\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "pandasによる日付処理の基本"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch02_時系列データの構造.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch02_時系列データの構造.html",
    "title": "時系列データの構造",
    "section": "",
    "text": "1 はじめに\n時系列データの構造⇒自己相関⇒トレンド⇒季節性⇒外因性⇒ノイズ\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "時系列データの構造"
    ]
  },
  {
    "objectID": "contents/books/10_爆速Python/ch03_並行性_並列性_非同期処理.html",
    "href": "contents/books/10_爆速Python/ch03_並行性_並列性_非同期処理.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport numpy as np\n\n\nx = np.arange(10).reshape((5, 2))\nx\n\n\narray([[0, 1],\n       [2, 3],\n       [4, 5],\n       [6, 7],\n       [8, 9]])\n\n\n\n\nCode\nx.shape\n\n\n(5, 2)\n\n\n\n\nCode\n%timeit -n 100 x.ravel()\n\n\n76.7 ns ± 28.4 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nCode\n%timeit -n 100 x.reshape(-1)\n\n\n259 ns ± 20.8 ns per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\n\nCode\nnp.show_config()\n\n\nBuild Dependencies:\n  blas:\n    detection method: pkgconfig\n    found: true\n    include directory: /c/opt/64/include\n    lib directory: /c/opt/64/lib\n    name: openblas64\n    openblas configuration: USE_64BITINT=1 DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\n    pc file directory: C:/opt/64/lib/pkgconfig\n    version: 0.3.23.dev\n  lapack:\n    detection method: pkgconfig\n    found: true\n    include directory: /c/opt/64/include\n    lib directory: /c/opt/64/lib\n    name: openblas64\n    openblas configuration: USE_64BITINT=1 DYNAMIC_ARCH=1 DYNAMIC_OLDER= NO_CBLAS=\n      NO_LAPACK= NO_LAPACKE= NO_AFFINITY=1 USE_OPENMP= SKYLAKEX MAX_THREADS=2\n    pc file directory: C:/opt/64/lib/pkgconfig\n    version: 0.3.23.dev\nCompilers:\n  c:\n    commands: cl\n    linker: link\n    name: msvc\n    version: 19.29.30152\n  c++:\n    commands: cl\n    linker: link\n    name: msvc\n    version: 19.29.30152\n  cython:\n    commands: cython\n    linker: cython\n    name: cython\n    version: 3.0.2\nMachine Information:\n  build:\n    cpu: x86_64\n    endian: little\n    family: x86_64\n    system: windows\n  host:\n    cpu: x86_64\n    endian: little\n    family: x86_64\n    system: windows\nPython Information:\n  path: C:\\Users\\runneradmin\\AppData\\Local\\Temp\\cibw-run-xj48pggt\\cp312-win_amd64\\build\\venv\\Scripts\\python.exe\n  version: '3.12'\nSIMD Extensions:\n  baseline:\n  - SSE\n  - SSE2\n  - SSE3\n  found:\n  - SSSE3\n  - SSE41\n  - POPCNT\n  - SSE42\n  - AVX\n  - F16C\n  - FMA3\n  - AVX2\n  not found:\n  - AVX512F\n  - AVX512CD\n  - AVX512_SKX\n  - AVX512_CLX\n  - AVX512_CNL\n  - AVX512_ICL\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "爆速Python",
      "Ch03 並行性 並列性 非同期処理"
    ]
  },
  {
    "objectID": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html",
    "href": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html",
    "title": "応用編",
    "section": "",
    "text": "「支援者」が「相談者」から問題の内容をヒアリングする\n「支援者」が「相談者」からデータをもらったり、収集したりする\n「相談者」と「支援者」で検討して、何から解決したいかを決める\n「支援者」が方法を考えて解を求める\n「支援者」が結果をまとめて「相談者」にみてもらう\nおかしなところがあれば適宜戻る",
    "crumbs": [
      "Python",
      "Pythonで学ぶ数理最適化",
      "応用編"
    ]
  },
  {
    "objectID": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#公平にしたい",
    "href": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#公平にしたい",
    "title": "応用編",
    "section": "4.1 公平にしたい",
    "text": "4.1 公平にしたい\n地域によって平均点が異なるので単純な合計値では不公平になる。\nそこで、ここではまず最低得点という制約を加えたモデリングをおこなう。\n\n\nCode\nfrom mip import Model, maximize, xsum\n\nm = Model()\nn = len(data[\"Tokyo\"])\nx = m.add_var_tensor((n, ), \"x\", var_type = \"B\")\n# 東京の得点和\ntokyo = xsum(data[\"Tokyo\"] * x)\n# 大阪の得点和\nosaka = xsum(data[\"Osaka\"] * (1 - x))\n\n# 目的関数\nm.objective = maximize(tokyo + osaka)\n\n# 下限\nm += tokyo &gt;= 53\n\n\nm.verbose = 0\nm.optimize()\n\nif m.status.value == 0:\n    val = x.astype(float, subok = False)\n    tokyo_names = names[val &gt; .5]\n    osaka_names = names[val &lt;= .5]\n    print(f\"東京 {tokyo.x} {tokyo_names[:3]}\")\n    print(f\"大阪 {osaka.x} {osaka_names[:3]}\")\n    print(f\"合計 {tokyo.x + osaka.x}\")\n\n\n東京 64.0 ['神風酒造り' '天空の酔いどれ' '月の輝き']\n大阪 72.0 ['雪の瑞穂酒' '紅葉の美酒' '翡翠の杯']\n合計 136.0",
    "crumbs": [
      "Python",
      "Pythonで学ぶ数理最適化",
      "応用編"
    ]
  },
  {
    "objectID": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#つの尺度で考える",
    "href": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#つの尺度で考える",
    "title": "応用編",
    "section": "4.2 ２つの尺度で考える",
    "text": "4.2 ２つの尺度で考える\nここでは公平性を考えるために、会場間の得点の差が小さくなるようにしながら、得点の合計を最大化することを考える。\nこのような二つの指標で目的変数を考えることを多目的最適化という。\n一般に多目的最適化はトレードオフになるので、ステップを分けて問題を解く。\n\n\n\npicture 9\n\n\n\n\nCode\nfrom mip import Model, maximize, xsum\n\nm = Model()\nn = len(data[\"Tokyo\"])\nx = m.add_var_tensor((n, ), \"x\", var_type = \"B\")\n# 東京の得点和\ntokyo = xsum(data[\"Tokyo\"] * x)\n# 大阪の得点和\nosaka = xsum(data[\"Osaka\"] * (1 - x))\n\n# 目的関数\nm.objective = maximize(tokyo + osaka)\n\n# 下限\nm += tokyo &gt;= 0\ni = 0\nwhile True:\n    i += 1\n    m.verbose = 0\n    m.optimize()\n\n    # 東京の値が大阪よりも大きくなるようにする\n    v1, v2 = tokyo.x, osaka.x\n    print(f\"{i}回目 東京{v1}, 大阪 {v2}, 合計 {v1 + v2}\")\n    if v1 &gt; v2:\n        break\n\n    # 制約条件の変更\n    m.constrs[0].rhs = v1 + 1\n\n\n\nif m.status.value == 0:\n    val = x.astype(float, subok = False)\n    tokyo_names = names[val &gt; .5]\n    osaka_names = names[val &lt;= .5]\n    print(f\"東京 {tokyo.x} {tokyo_names[:3]}\")\n    print(f\"大阪 {osaka.x} {osaka_names[:3]}\")\n    print(f\"合計 {tokyo.x + osaka.x}\")\n\n\n1回目 東京52.0, 大阪 84.0, 合計 136.0\n2回目 東京64.0, 大阪 72.0, 合計 136.0\n3回目 東京66.0, 大阪 69.0, 合計 135.0\n4回目 東京67.0, 大阪 68.0, 合計 135.0\n5回目 東京68.0, 大阪 67.0, 合計 135.0\n東京 68.0 ['神風酒造り' '天空の酔いどれ' '竜神の泡']\n大阪 67.0 ['雪の瑞穂酒' '紅葉の美酒' '翡翠の杯']\n合計 135.0",
    "crumbs": [
      "Python",
      "Pythonで学ぶ数理最適化",
      "応用編"
    ]
  },
  {
    "objectID": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#シフト表をつくるには",
    "href": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#シフト表をつくるには",
    "title": "応用編",
    "section": "5.1 シフト表をつくるには",
    "text": "5.1 シフト表をつくるには\n人×日にちに対して、夜勤、日勤、休日などを割り当てる。\nそのなかで各日ごとや、各人に対して条件を設ける。\n\n\n\npicture 10\n\n\nどのような変数を考えるかについては、0-1変数を使う。\n具体的にはスタッフ\\(i0\\)が日番号\\(i1\\)にシフト\\(i2\\)をするかどうかを表す変数を使う。\n\n\n\npicture 11",
    "crumbs": [
      "Python",
      "Pythonで学ぶ数理最適化",
      "応用編"
    ]
  },
  {
    "objectID": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#まずは作ってみる",
    "href": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#まずは作ってみる",
    "title": "応用編",
    "section": "5.2 まずは作ってみる",
    "text": "5.2 まずは作ってみる\n\n\nCode\nstaffs = [\"安藤\", \"佐藤\", \"高橋\", \"山田\"]\ndays = [0, 1, 2, 3, 4, 5, 6]\nshifts = [\"日\", \"夜\", \"休\"]\nn0, n1, n2 = len(staffs), len(days), len(shifts)\n\nfrom mip import Model, xsum\n\n\nm = Model()\nx = m.add_var_tensor((n0, n1, n2), \"x\", var_type = \"B\")\n\n\n# 制約条件\nfor i0 in range(n0):\n    for i1 in range(n1):\n        # ある人がある日に割り当てられるシフトは1つ\n        m += xsum(x[i0, i1]) == 1 \n        \nm.verbose = 0\nm.optimize()\n\nif m.status.value == 0:\n    val = x.astype(float, subok = False).round().astype(int)\n    result = (np.array([0, 1, 2,]).reshape(1, 1, -1) * val).sum(axis = 2)\n    print(result)\n\n\n\n\n[[2 0 2 0 0 1 0]\n [0 0 2 0 0 2 0]\n [2 2 0 2 0 0 2]\n [2 0 0 0 0 0 1]]",
    "crumbs": [
      "Python",
      "Pythonで学ぶ数理最適化",
      "応用編"
    ]
  },
  {
    "objectID": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#シフト表に必要な条件を考慮",
    "href": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#シフト表に必要な条件を考慮",
    "title": "応用編",
    "section": "5.3 シフト表に必要な条件を考慮",
    "text": "5.3 シフト表に必要な条件を考慮\n\n\n\npicture 12\n\n\n\n\nCode\nm = Model()\nx = m.add_var_tensor((n0, n1, n2), \"x\", var_type = \"B\")\n\n\n# 制約条件\nfor i0 in range(n0):\n    for i1 in range(n1):\n        # ある人がある日に割り当てられるシフトは1つ\n        m += xsum(x[i0, i1]) == 1 \n    m += xsum(x[i0,:,0]) &lt;= 4  # 日勤4日以下\n    m += xsum(x[i0,:,1]) &lt;= 2  # 夜勤2日以下\n    m += xsum(x[i0,:,2]) &lt;= 2  # 休日2日以下\n    \nfor i1 in range(n1):\n    m += xsum(x[:, i1, 0]) == 2 # ある日の日勤は二人\n    m += xsum(x[:, i1, 1]) == 1 # ある日の夜勤は一人\n    \nm.verbose = 0\nm.optimize()\n\n\nif m.status.value == 0:\n    val = x.astype(float, subok = False).round().astype(int)\n    result = (np.array([0, 1, 2,]).reshape(1, 1, -1) * val).sum(axis = 2)\n    print(result)\n\n\n[[0 0 2 0 1 1 2]\n [2 0 1 0 0 2 1]\n [0 2 0 1 2 0 0]\n [1 1 0 2 0 0 0]]\n\n\n\n\nCode\n# る日の日勤、夜勤、休日者の数\nval.sum(axis = 0)\n\n\narray([[2, 1, 1],\n       [2, 1, 1],\n       [2, 1, 1],\n       [2, 1, 1],\n       [2, 1, 1],\n       [2, 1, 1],\n       [2, 1, 1]])",
    "crumbs": [
      "Python",
      "Pythonで学ぶ数理最適化",
      "応用編"
    ]
  },
  {
    "objectID": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#休みの希望を叶える",
    "href": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#休みの希望を叶える",
    "title": "応用編",
    "section": "5.4 休みの希望を叶える",
    "text": "5.4 休みの希望を叶える\n\n\n\npicture 13\n\n\n「可能な限りかなえる」⇒目的変数として希望が実現できる数を目的変数にする。\n\n\n\npicture 14\n\n\n\n\nCode\nfrom mip import Model, maximize, xsum\n\ndef show(result):\n    print(\"    ０１２３４５６\")\n    for i0 in range(n0): # スタッフごと\n        print(f\"{staffs[i0]}: \", end=\"\")\n        for i1 in range(n1): # 日番号ごと\n            print(shifts[result[i0, i1]], end=\"\")\n        print()\n        \nstaffs = [\"安藤\", \"佐藤\", \"高橋\", \"山田\"]\ndays  = [0, 1, 2, 3, 4, 5, 6]\nshifts = [\"日\", \"夜\", \"休\"]\nn0, n1, n2 = len(staffs), len(days), len(shifts)\nwish_days = [ # 休みの希望（追加）\n    [0, 3],\n    [2],\n    [1, 6],\n    [5],\n]\n\nm = Model()\nx = m.add_var_tensor((n0, n1, n2), \"x\", var_type = \"B\")\n\nx_wish_days = []\nfor x_i, wish_day in zip(x, wish_days):\n    for day in wish_day:\n        # 該当スタッフの当該日番号の休みの変数を追加\n        x_wish_days.append(x_i[day, 2])\n        \n# 休みが希望通りの数を指標にする\nm.objective = maximize(xsum(x_wish_days))\n\n# 制約条件\nfor i0 in range(n0):\n    for i1 in range(n1):\n        # ある人がある日に割り当てられるシフトは1つ\n        m += xsum(x[i0, i1]) == 1 \n    m += xsum(x[i0,:,0]) &lt;= 4  # 日勤4日以下\n    m += xsum(x[i0,:,1]) &lt;= 2  # 夜勤2日以下\n    m += xsum(x[i0,:,2]) &lt;= 2  # 休日2日以下\n    \nfor i1 in range(n1):\n    m += xsum(x[:, i1, 0]) == 2 # ある日の日勤は二人\n    m += xsum(x[:, i1, 1]) == 1 # ある日の夜勤は一人\n    \nm.verbose = 0\nm.optimize()\n\nif m.status.value == 0:\n    val = x.astype(float, subok = False).round().astype(int)\n    result = (np.array([0, 1, 2,]).reshape(1, 1, -1) * val).sum(axis = 2)\n    show(result)\n    print(f\"{m.objective_value=}\")\n\n\n    ０１２３４５６\n安藤: 休日日休日夜夜\n佐藤: 日夜休夜日日日\n高橋: 日休夜日夜日休\n山田: 夜日日日休休日\nm.objective_value=6.0",
    "crumbs": [
      "Python",
      "Pythonで学ぶ数理最適化",
      "応用編"
    ]
  },
  {
    "objectID": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#つの応えを出そう",
    "href": "contents/books/09_Pythonで学ぶ数理最適化/ch02_応用編.html#つの応えを出そう",
    "title": "応用編",
    "section": "5.5 2つの応えを出そう",
    "text": "5.5 2つの応えを出そう\n休みの希望の成立数を目的巻数にしているが、条件を満たす解は実は多数あると考えられる。\n複数のシフト表を出すことで、実際に提示するときに議論がしやすくなる。\n\n\n\npicture 15\n\n\n\n\nCode\nval.sum(axis = 0)\n\n\narray([[2, 1, 1],\n       [2, 1, 1],\n       [2, 1, 1],\n       [2, 1, 1],\n       [2, 1, 1],\n       [2, 1, 1],\n       [2, 1, 1]])\n\n\n\n\nCode\nfrom mip import Model, maximize, xsum\n\ndef show(result):\n    print(\"    ０１２３４５６\")\n    for i0 in range(n0): # スタッフごと\n        print(f\"{staffs[i0]}: \", end=\"\")\n        for i1 in range(n1): # 日番号ごと\n            print(shifts[result[i0, i1]], end=\"\")\n        print()\n        \nstaffs = [\"安藤\", \"佐藤\", \"高橋\", \"山田\"]\ndays  = [0, 1, 2, 3, 4, 5, 6]\nshifts = [\"日\", \"夜\", \"休\"]\nn0, n1, n2 = len(staffs), len(days), len(shifts)\nwish_days = [ # 休みの希望（追加）\n    [0, 3],\n    [2],\n    [1, 6],\n    [5],\n]\n\nm = Model()\nx = m.add_var_tensor((n0, n1, n2), \"x\", var_type = \"B\")\n\nx_wish_days = []\nfor x_i, wish_day in zip(x, wish_days):\n    for day in wish_day:\n        # 該当スタッフの当該日番号の休みの変数を追加\n        x_wish_days.append(x_i[day, 2])\n        \n# 休みが希望通りの数を指標にする\nm.objective = maximize(xsum(x_wish_days))\n\n# 制約条件\nfor i0 in range(n0):\n    for i1 in range(n1):\n        # ある人がある日に割り当てられるシフトは1つ\n        m += xsum(x[i0, i1]) == 1 \n    m += xsum(x[i0,:,0]) &lt;= 4  # 日勤4日以下\n    m += xsum(x[i0,:,1]) &lt;= 2  # 夜勤2日以下\n    m += xsum(x[i0,:,2]) &lt;= 2  # 休日2日以下\n\nfor i1 in range(n1):\n    m += xsum(x[:, i1, 0]) == 2 # ある日の日勤は二人\n    m += xsum(x[:, i1, 1]) == 1 # ある日の夜勤は一人\n\nm.verbose = 0\nm.optimize()\n\nif m.status.value == 0:\n    val = x.astype(float, subok = False).round().astype(int)\n    result = (np.array([0, 1, 2,]).reshape(1, 1, -1) * val).sum(axis = 2)\n    show(result)\n    print(f\"{m.objective_value=}\")\n\n\n    ０１２３４５６\n安藤: 休日日休日夜夜\n佐藤: 日夜休夜日日日\n高橋: 日休夜日夜日休\n山田: 夜日日日休休日\nm.objective_value=6.0\n\n\n\n\nCode\nans = x[val &gt; .5]\n\n# 1つ目の解を禁止\nm += xsum(ans) &lt;= len(ans) - 1\n\nm.optimize()\nif m.status.value == 0:\n    val2 = x.astype(float, subok = False).round().astype(int)\n    result2 = (np.array([0, 1, 2,]).reshape(1, 1, -1) * val).sum(axis = 2)\n    show(result2)\n    print(f\"{m.objective_value=}\")\n\n\n\n    ０１２３４５６\n安藤: 休日日休日夜夜\n佐藤: 日夜休夜日日日\n高橋: 日休夜日夜日休\n山田: 夜日日日休休日\nm.objective_value=6.0",
    "crumbs": [
      "Python",
      "Pythonで学ぶ数理最適化",
      "応用編"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html",
    "href": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html",
    "title": "はじめに",
    "section": "",
    "text": "Code\nimport importlib.util\nimport os\n\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    work_dirctory = '/content/drive/Othercomputers/LetsNoteSilver'\n    os.chdir(work_dirctory)\n    print(\"mounted\")\nexcept ModuleNotFoundError as e:\n    print(\"not mounted\")\n\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\npd.set_option('display.float_format', '{:.3f}'.format)\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\n\n\nnot mounted\n本章では、ダイヤモンドデータセットを使って、価格を予測するモデルを構築する。 データセットの属性は次のとおりである。\nCode\ndf = sns.load_dataset(\"diamonds\")\ndf.head()\n\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.230\nIdeal\nE\nSI2\n61.500\n55.000\n326\n3.950\n3.980\n2.430\n\n\n1\n0.210\nPremium\nE\nSI1\n59.800\n61.000\n326\n3.890\n3.840\n2.310\n\n\n2\n0.230\nGood\nE\nVS1\n56.900\n65.000\n327\n4.050\n4.070\n2.310\n\n\n3\n0.290\nPremium\nI\nVS2\n62.400\n58.000\n334\n4.200\n4.230\n2.630\n\n\n4\n0.310\nGood\nJ\nSI2\n63.300\n58.000\n335\n4.340\n4.350\n2.750\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 53940 entries, 0 to 53939\nData columns (total 10 columns):\n #   Column   Non-Null Count  Dtype   \n---  ------   --------------  -----   \n 0   carat    53940 non-null  float64 \n 1   cut      53940 non-null  category\n 2   color    53940 non-null  category\n 3   clarity  53940 non-null  category\n 4   depth    53940 non-null  float64 \n 5   table    53940 non-null  float64 \n 6   price    53940 non-null  int64   \n 7   x        53940 non-null  float64 \n 8   y        53940 non-null  float64 \n 9   z        53940 non-null  float64 \ndtypes: category(3), float64(6), int64(1)\nmemory usage: 3.0 MB\nCode\ndf.select_dtypes(include = [\"float\", \"int\"]).describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncarat\n53940.000\n0.798\n0.474\n0.200\n0.400\n0.700\n1.040\n5.010\n\n\ndepth\n53940.000\n61.749\n1.433\n43.000\n61.000\n61.800\n62.500\n79.000\n\n\ntable\n53940.000\n57.457\n2.234\n43.000\n56.000\n57.000\n59.000\n95.000\n\n\nprice\n53940.000\n3932.800\n3989.440\n326.000\n950.000\n2401.000\n5324.250\n18823.000\n\n\nx\n53940.000\n5.731\n1.122\n0.000\n4.710\n5.700\n6.540\n10.740\n\n\ny\n53940.000\n5.735\n1.142\n0.000\n4.720\n5.710\n6.540\n58.900\n\n\nz\n53940.000\n3.539\n0.706\n0.000\n2.910\n3.530\n4.040\n31.800\nCode\ndf.select_dtypes(include = [\"category\", \"object\"]).describe().T\n\n\n\n\n\n\n\n\n\ncount\nunique\ntop\nfreq\n\n\n\n\ncut\n53940\n5\nIdeal\n21551\n\n\ncolor\n53940\n7\nG\n11292\n\n\nclarity\n53940\n8\nSI1\n13065",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#変数数値",
    "href": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#変数数値",
    "title": "はじめに",
    "section": "1.1 1変数数値",
    "text": "1.1 1変数数値\n\n\nCode\nfig = plt.figure(figsize = (10, 6))\ndf.hist(bins = 20)\nplt.tight_layout()\nplt.show()\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#変数数値-1",
    "href": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#変数数値-1",
    "title": "はじめに",
    "section": "1.2 2変数数値",
    "text": "1.2 2変数数値\n\n\nCode\n# 目的変数と相関が高い変数を調べる\nfig = plt.figure(figsize = (8, 6))\ndf_corr = df.select_dtypes([\"float\", \"int\"]).corr()\nsns.heatmap(\n    df_corr, \n    vmax = 1, \n    vmin = -1, \n    center = 0, \n    annot = True, \n    cmap = \"Blues\"\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 数値の組合わせでプロット作成する\n# 非線形な関係も多いのがわかる\nsns.pairplot(\n    df.select_dtypes([\"float\", \"int\"])\n)",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#カテゴリ変数",
    "href": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#カテゴリ変数",
    "title": "はじめに",
    "section": "1.3 カテゴリ変数",
    "text": "1.3 カテゴリ変数\n\n\nCode\n# ユニーク値を調べる\nlist(\n    print(f\"{column} = {value[0]}\")\n    for column, value in df\n    .select_dtypes([\"category\", \"object\"])\n    .agg(lambda x: [x.value_counts()])\n    .items()\n)\n\n\ncut = cut\nIdeal        21551\nPremium      13791\nVery Good    12082\nGood          4906\nFair          1610\nName: count, dtype: int64\ncolor = color\nG    11292\nE     9797\nF     9542\nH     8304\nD     6775\nI     5422\nJ     2808\nName: count, dtype: int64\nclarity = clarity\nSI1     13065\nVS2     12258\nSI2      9194\nVS1      8171\nVVS2     5066\nVVS1     3655\nIF       1790\nI1        741\nName: count, dtype: int64\n\n\n[None, None, None]\n\n\n\n\nCode\n\nfig = plt.figure(figsize = (10, 6))\nfor i, name in enumerate(df.select_dtypes([\"category\", \"object\"]).columns):\n    ax = plt.subplot(2,2 , i + 1)\n    df[name].value_counts().plot(kind = \"bar\", ax = ax)\n    \nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#前処理",
    "href": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#前処理",
    "title": "はじめに",
    "section": "1.4 前処理",
    "text": "1.4 前処理\nここまでの計算結果を踏まえて、予測モデルの作成に悪影響を及ぼす外れ値データを除外する。 サイズの情報に０があったのでこれは除外する。\n\n\nCode\nfrom functools import reduce\n\nhas_abnormal_size_value = (\n    df\n    .filter(items = [\"x\", \"y\", \"z\"], axis = 1)\n    .apply(lambda v: (v &lt;= 0) | (v &gt;= 10), axis =  0)\n    .sum(axis = 1)\n    .pipe(lambda ser: ser &gt; 0)\n)\nhas_abnormal_size_value.sum()\n\n\n0\n\n\n\n\nCode\n# 外れ値を除外する\ndf = (\n    df\n    .query(\"~ @has_abnormal_size_value\")\n    .reset_index(drop = True)\n)\n\ndf.select_dtypes(include=[\"float\", \"int\"]).describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\ncarat\n53911.000\n0.797\n0.472\n0.200\n0.400\n0.700\n1.040\n3.670\n\n\ndepth\n53911.000\n61.749\n1.432\n43.000\n61.000\n61.800\n62.500\n79.000\n\n\ntable\n53911.000\n57.457\n2.234\n43.000\n56.000\n57.000\n59.000\n95.000\n\n\nprice\n53911.000\n3929.487\n3985.130\n326.000\n949.000\n2401.000\n5321.500\n18823.000\n\n\nx\n53911.000\n5.731\n1.118\n3.730\n4.710\n5.700\n6.540\n9.860\n\n\ny\n53911.000\n5.733\n1.110\n3.680\n4.720\n5.710\n6.540\n9.810\n\n\nz\n53911.000\n3.539\n0.691\n1.070\n2.910\n3.530\n4.040\n6.380",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#評価指標の選択",
    "href": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#評価指標の選択",
    "title": "はじめに",
    "section": "1.5 評価指標の選択",
    "text": "1.5 評価指標の選択\n目的変数priceの分布を確認すると、べき乗則やロングテールと呼ばれるような分布になっている。 このときにはRMSEだと大きな強調して評価されるので、外れ値と小さな誤差を同じ基準で評価するMAEを使うことにする。\nこの部分は少し考え方がよくわかっていない。。\n\n\nCode\nfig = plt.figure(figsize = (6, 4))\ndf[\"price\"].hist(bins = 20, edgecolor = \"white\")\nplt.grid(False)\nplt.show()",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#lasso-回帰モデル",
    "href": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#lasso-回帰モデル",
    "title": "はじめに",
    "section": "2.1 Lasso 回帰モデル",
    "text": "2.1 Lasso 回帰モデル\n\n\nCode\nfrom sklearn.linear_model import Lasso\n\nparams = np.arange(1, 10)\nmae_metrics = []\nfor param in params: \n    model_l1 = Lasso(alpha = param)\n    model_l1.fit(X_tr, y_tr)\n    y_va_pred = model_l1.predict(X_va)\n    mae_metric = mean_absolute_error(y_va, y_va_pred)\n    mae_metrics.append(mae_metric)\n    \nplt.figure(figsize = (8, 4))\nplt.plot(params, mae_metrics)\nplt.xlabel(\"alpha\", fontsize = 14)\nplt.ylabel(\"MAE\", fontsize = 14)\nplt.title(\"MAE vs alpha\", fontsize = 16)\nplt.grid()\nplt.show()\n\n\nc:\\pyenv\\py311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.348e+08, tolerance: 5.462e+07\n  model = cd_fast.enet_coordinate_descent(\n\n\n\n\n\n\n\n\n\n\n\nCode\nmodel = Lasso(alpha = 6)\nmodel.fit(X_train, y_train)\ny_test_pred = model.predict(X_test)\n\n\n\n\nCode\nresiduals = y_test - y_test_pred\n\nfig = plt.figure(figsize = (8, 4))\nplt.scatter(y_test_pred, residuals, s = 3)\nplt.xlabel(\"Predicted values\", fontsize = 14)\nplt.ylabel(\"Residuals\", fontsize = 14)\nplt.title(\"residuals vs predicted values\", fontsize = 16)\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 検証データでの評価\n# 平均的に720ドルの違いがある\ny_va_pred = model.predict(X_va)\nprint(mean_absolute_error(y_va, y_va_pred))\n\n# テストデータ\ny_test_pred = model.predict(X_test)\nprint(mean_absolute_error(y_test, y_test_pred))\n\nlinear_model_coefs = (\n    pd.DataFrame({\n        \"value\":[model.intercept_] + model.coef_.tolist(), \n        \"name\":[\"intercept\"] + model.feature_names_in_.tolist()\n    })\n    .sort_values(by = \"value\", ascending = False)\n)\nlinear_model_coefs\n\nplt.figure(figsize = (8, 4))\nplt.title(\"Regresssion coefficient\")\nplt.bar(range(linear_model_coefs.shape[0]), linear_model_coefs[\"value\"])\nplt.xticks(range(linear_model_coefs.shape[0]), linear_model_coefs[\"name\"], rotation = 90)\nplt.show()\n\n\n710.303690827307\n726.0674410050069",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#optunaを用いたハイパーパラメータチューニング",
    "href": "contents/books/08_handbook_lightgbm/cl04_回帰の予測モデル改善.html#optunaを用いたハイパーパラメータチューニング",
    "title": "はじめに",
    "section": "4.1 Optunaを用いたハイパーパラメータチューニング",
    "text": "4.1 Optunaを用いたハイパーパラメータチューニング\n\n\nCode\n\nparams_base = {\n    \"objective\": \"mae\", \n    \"random_seed\": 1234, \n    \"learning_rate\": .02, \n    \"min_data_in_bin\": 3, \n    \"bagging_freq\": 1, \n    \"bagging_seed\": 0, \n    \"verbose\": -1, \n}\n\ndef objective(trial): \n    params_tuning = {\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 50, 200), \n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 2, 30), \n        \"max_bin\": trial.suggest_int(\"max_bin\", 200, 400), \n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", .8, .95), \n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", .35, .65), \n        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", .01, 1, log = True), \n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", .01, 1, log = True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", .01, 1, log = True),\n    }\n    \n    params_tuning.update(params)\n    lgb_train = lgb.Dataset(X_tr, y_tr)\n    lgb_eval = lgb.Dataset(X_va, y_va)\n    \n    model = lgb.train(\n        params_tuning, \n        lgb_train, \n        num_boost_round = 1000, \n        valid_sets = [lgb_train, lgb_eval], \n        valid_names = [\"train\", \"valid\"], \n        callbacks = [lgb.early_stopping(100), lgb.log_evaluation(500)]\n    )\n    y_va_pred = model.predict(X_va, num_iteration=model.best_iteration)\n    score = mean_absolute_error(y_va, y_va_pred)\n    print(\"- * - \" * 10)\n    return score\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, \n    y, \n    test_size = .2, \n    shuffle = True, \n    random_state = 0\n)\n\n# カテゴリ変数をラベルエンコーディング\nfrom sklearn.preprocessing import LabelEncoder\ncat_cols = [\"cut\", \"color\", \"clarity\"]\n\nfor c in cat_cols:\n    le = LabelEncoder()\n    le.fit(X_train[c])\n    X_train[c] = le.transform(X_train[c])\n    X_test[c] = le.transform(X_test[c])\n    \nfor c in cat_cols:\n    X_train[c] = X_train[c].astype(\"category\")\n    X_test[c] = X_test[c].astype(\"category\") \n\n\n# 検証データとバリーデーション\nX_tr, X_va, y_tr, y_va = train_test_split(\n    X_train, \n    y_train, \n    test_size = .2, \n    shuffle = True, \n    random_state = 0\n)\n    \nimport optuna\nstudy = optuna.create_study(sampler = optuna.samplers.TPESampler(seed = 0), direction = \"minimize\")\nstudy.optimize(objective, n_trials = 200)\n\n\n[I 2024-01-04 14:49:15,250] A new study created in memory with name: no-name-1e7e646f-7232-48dd-8da4-f75d4a1f3f61\n\n\nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[361]   train's l1: 168.729 valid's l1: 250.214\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:49:18,073] Trial 0 finished with value: 250.21391409400061 and parameters: {'num_leaves': 132, 'min_data_in_leaf': 22, 'max_bin': 321, 'bagging_fraction': 0.8817324774495345, 'feature_fraction': 0.4770964398016714, 'min_gain_to_split': 0.19578897201213002, 'lambda_l1': 0.07501954443620121, 'lambda_l2': 0.6074996073425692}. Best is trial 0 with value: 250.21391409400061.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 130.661 valid's l1: 249.112\nEarly stopping, best iteration is:\n[441]   train's l1: 135.121 valid's l1: 248.898\n\n\n[I 2024-01-04 14:49:23,466] Trial 1 finished with value: 248.89774199499587 and parameters: {'num_leaves': 195, 'min_data_in_leaf': 13, 'max_bin': 359, 'bagging_fraction': 0.8793342379629356, 'feature_fraction': 0.5204133683281797, 'min_gain_to_split': 0.70989362574059, 'lambda_l1': 0.013869861245357327, 'lambda_l2': 0.014936835544198456}. Best is trial 1 with value: 248.89774199499587.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 193.668 valid's l1: 252.014\nEarly stopping, best iteration is:\n[756]   train's l1: 180.975 valid's l1: 251.168\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:49:27,744] Trial 2 finished with value: 251.1682188777985 and parameters: {'num_leaves': 53, 'min_data_in_leaf': 26, 'max_bin': 356, 'bagging_fraction': 0.9305018222370228, 'feature_fraction': 0.6435855026698292, 'min_gain_to_split': 0.39656750817710085, 'lambda_l1': 0.08374496868436809, 'lambda_l2': 0.3639639356786389}. Best is trial 1 with value: 248.89774199499587.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 183.759 valid's l1: 248.465\n\n\n[I 2024-01-04 14:49:31,469] Trial 3 finished with value: 248.09634372851536 and parameters: {'num_leaves': 67, 'min_data_in_leaf': 20, 'max_bin': 228, 'bagging_fraction': 0.9417003375574375, 'feature_fraction': 0.5065544965250215, 'min_gain_to_split': 0.06750312521595925, 'lambda_l1': 0.0338151426782029, 'lambda_l2': 0.35356346291488117}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[592]   train's l1: 176.402 valid's l1: 248.096\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 156.974 valid's l1: 250.474\nEarly stopping, best iteration is:\n[471]   train's l1: 160.618 valid's l1: 250.361\n\n\n[I 2024-01-04 14:49:35,221] Trial 4 finished with value: 250.3613253131122 and parameters: {'num_leaves': 118, 'min_data_in_leaf': 18, 'max_bin': 203, 'bagging_fraction': 0.8926453245613816, 'feature_fraction': 0.5336287168167264, 'min_gain_to_split': 0.17134364197119656, 'lambda_l1': 0.7717846862118062, 'lambda_l2': 0.23101522250182369}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 158.458 valid's l1: 250.116\nEarly stopping, best iteration is:\n[452]   train's l1: 161.498 valid's l1: 249.961\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:49:39,319] Trial 5 finished with value: 249.96067468473692 and parameters: {'num_leaves': 104, 'min_data_in_leaf': 14, 'max_bin': 340, 'bagging_fraction': 0.8090338207443906, 'feature_fraction': 0.5500300146337003, 'min_gain_to_split': 0.21941976179005757, 'lambda_l1': 0.02634905974115613, 'lambda_l2': 0.01810725406663109}. Best is trial 3 with value: 248.09634372851536.\n\n\nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:49:41,815] Trial 6 finished with value: 252.75686388504084 and parameters: {'num_leaves': 97, 'min_data_in_leaf': 12, 'max_bin': 314, 'bagging_fraction': 0.865790227019348, 'feature_fraction': 0.6465121514177679, 'min_gain_to_split': 0.015998881492631498, 'lambda_l1': 0.02616697456085447, 'lambda_l2': 0.021019338091154416}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[276]   train's l1: 184.914 valid's l1: 252.757\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 143.176 valid's l1: 251.183\nEarly stopping, best iteration is:\n[480]   train's l1: 144.414 valid's l1: 251.075\n\n\n[I 2024-01-04 14:49:46,356] Trial 7 finished with value: 251.075044773761 and parameters: {'num_leaves': 148, 'min_data_in_leaf': 9, 'max_bin': 293, 'bagging_fraction': 0.8366638388002404, 'feature_fraction': 0.3976908750936559, 'min_gain_to_split': 0.016624564693643644, 'lambda_l1': 0.20542778286285945, 'lambda_l2': 0.018895826938026012}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:49:49,304] Trial 8 finished with value: 251.64466468240585 and parameters: {'num_leaves': 79, 'min_data_in_leaf': 12, 'max_bin': 365, 'bagging_fraction': 0.8145651913689592, 'feature_fraction': 0.6013834722496412, 'min_gain_to_split': 0.015566709318481201, 'lambda_l1': 0.8972612866432508, 'lambda_l2': 0.08655704137361815}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[393]   train's l1: 176.132 valid's l1: 251.645\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 137.99  valid's l1: 250.507\nEarly stopping, best iteration is:\n[800]   train's l1: 122.848 valid's l1: 250.145\n\n\n[I 2024-01-04 14:49:57,996] Trial 9 finished with value: 250.14515247089747 and parameters: {'num_leaves': 197, 'min_data_in_leaf': 19, 'max_bin': 348, 'bagging_fraction': 0.8058781688381481, 'feature_fraction': 0.43484208877292285, 'min_gain_to_split': 0.017393745944806985, 'lambda_l1': 0.0391093317225106, 'lambda_l2': 0.0172764870082179}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 195.554 valid's l1: 251.229\n[1000]  train's l1: 173.411 valid's l1: 249.605\nDid not meet early stopping. Best iteration is:\n[943]   train's l1: 175.143 valid's l1: 249.488\n\n\n[I 2024-01-04 14:50:01,838] Trial 10 finished with value: 249.48816882293096 and parameters: {'num_leaves': 53, 'min_data_in_leaf': 30, 'max_bin': 216, 'bagging_fraction': 0.9394460532541745, 'feature_fraction': 0.3583168079743906, 'min_gain_to_split': 0.04633615565227145, 'lambda_l1': 0.34798264027596815, 'lambda_l2': 0.08310314014919823}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[394]   train's l1: 132.863 valid's l1: 253.448\n\n\n[I 2024-01-04 14:50:06,313] Trial 11 finished with value: 253.44849239038473 and parameters: {'num_leaves': 200, 'min_data_in_leaf': 7, 'max_bin': 263, 'bagging_fraction': 0.9098251531827067, 'feature_fraction': 0.4876152746429839, 'min_gain_to_split': 0.9049911227773816, 'lambda_l1': 0.01035624395150659, 'lambda_l2': 0.18041332104924798}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:50:10,350] Trial 12 finished with value: 251.5387719660545 and parameters: {'num_leaves': 166, 'min_data_in_leaf': 2, 'max_bin': 390, 'bagging_fraction': 0.8571297367400855, 'feature_fraction': 0.544673579378604, 'min_gain_to_split': 0.05903843542415377, 'lambda_l1': 0.010317359922001355, 'lambda_l2': 0.041626708401802846}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[211]   train's l1: 171.691 valid's l1: 251.539\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:50:12,690] Trial 13 finished with value: 251.9364539335419 and parameters: {'num_leaves': 170, 'min_data_in_leaf': 23, 'max_bin': 245, 'bagging_fraction': 0.9167192421770464, 'feature_fraction': 0.4506710731343085, 'min_gain_to_split': 0.9258789248217179, 'lambda_l1': 0.019744509115954174, 'lambda_l2': 0.9415993291148634}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[269]   train's l1: 179.796 valid's l1: 251.863\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 172.5   valid's l1: 249.695\nEarly stopping, best iteration is:\n[430]   train's l1: 178.031 valid's l1: 249.44\n\n\n[I 2024-01-04 14:50:15,543] Trial 14 finished with value: 249.44030979652362 and parameters: {'num_leaves': 78, 'min_data_in_leaf': 16, 'max_bin': 286, 'bagging_fraction': 0.9453591958376131, 'feature_fraction': 0.5192108998385663, 'min_gain_to_split': 0.08119250213072915, 'lambda_l1': 0.050829284704745216, 'lambda_l2': 0.05278815998892531}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:50:19,830] Trial 15 finished with value: 250.8536040025379 and parameters: {'num_leaves': 174, 'min_data_in_leaf': 7, 'max_bin': 397, 'bagging_fraction': 0.8992688886853913, 'feature_fraction': 0.587134085972304, 'min_gain_to_split': 0.44292167603597116, 'lambda_l1': 0.018231994478501216, 'lambda_l2': 0.010813279908977303}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[331]   train's l1: 148.477 valid's l1: 250.854\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 153.638 valid's l1: 249.319\nEarly stopping, best iteration is:\n[557]   train's l1: 149.735 valid's l1: 249.127\n\n\n[I 2024-01-04 14:50:24,618] Trial 16 finished with value: 249.12657495393125 and parameters: {'num_leaves': 135, 'min_data_in_leaf': 20, 'max_bin': 237, 'bagging_fraction': 0.8417910063143887, 'feature_fraction': 0.5802375778365374, 'min_gain_to_split': 0.036252629637577215, 'lambda_l1': 0.17782743533092438, 'lambda_l2': 0.1814173295272962}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 180.834 valid's l1: 251.414\n[1000]  train's l1: 159.118 valid's l1: 250.56\nDid not meet early stopping. Best iteration is:\n[957]   train's l1: 160.275 valid's l1: 250.504\n\n\n[I 2024-01-04 14:50:29,562] Trial 17 finished with value: 250.50445590899483 and parameters: {'num_leaves': 78, 'min_data_in_leaf': 27, 'max_bin': 272, 'bagging_fraction': 0.9189647462801774, 'feature_fraction': 0.45903434228892936, 'min_gain_to_split': 0.1098171111247162, 'lambda_l1': 0.042583257296550894, 'lambda_l2': 0.356692774668329}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 143.83  valid's l1: 249.068\nEarly stopping, best iteration is:\n[467]   train's l1: 146.209 valid's l1: 248.955\n\n\n[I 2024-01-04 14:50:34,114] Trial 18 finished with value: 248.9550384865352 and parameters: {'num_leaves': 151, 'min_data_in_leaf': 16, 'max_bin': 322, 'bagging_fraction': 0.8716312066170828, 'feature_fraction': 0.5071837914163584, 'min_gain_to_split': 0.4468007749771297, 'lambda_l1': 0.015807330543906407, 'lambda_l2': 0.05048835803217476}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:50:37,172] Trial 19 finished with value: 252.10286297890258 and parameters: {'num_leaves': 111, 'min_data_in_leaf': 2, 'max_bin': 374, 'bagging_fraction': 0.8325429400067575, 'feature_fraction': 0.42149367778629837, 'min_gain_to_split': 0.029560405729183267, 'lambda_l1': 0.13091458271582623, 'lambda_l2': 0.031997507243044745}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[348]   train's l1: 163.549 valid's l1: 252.103\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 175.065 valid's l1: 250.818\nEarly stopping, best iteration is:\n[846]   train's l1: 157.495 valid's l1: 249.93\n\n\n[I 2024-01-04 14:50:42,182] Trial 20 finished with value: 249.92985212045812 and parameters: {'num_leaves': 85, 'min_data_in_leaf': 22, 'max_bin': 225, 'bagging_fraction': 0.8885017648065092, 'feature_fraction': 0.5005109137217618, 'min_gain_to_split': 0.09753061386801926, 'lambda_l1': 0.05639214538874004, 'lambda_l2': 0.12491887330793833}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 142.941 valid's l1: 250.298\nEarly stopping, best iteration is:\n[536]   train's l1: 140.879 valid's l1: 250.166\n\n\n[I 2024-01-04 14:50:47,232] Trial 21 finished with value: 250.1658602721037 and parameters: {'num_leaves': 152, 'min_data_in_leaf': 15, 'max_bin': 329, 'bagging_fraction': 0.868258435789388, 'feature_fraction': 0.5102876822733665, 'min_gain_to_split': 0.5053903941351167, 'lambda_l1': 0.015210692986563492, 'lambda_l2': 0.010039807811652276}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:50:50,321] Trial 22 finished with value: 251.97777844993706 and parameters: {'num_leaves': 187, 'min_data_in_leaf': 12, 'max_bin': 310, 'bagging_fraction': 0.8500673029026351, 'feature_fraction': 0.5657577733713315, 'min_gain_to_split': 0.32120884027443325, 'lambda_l1': 0.028947065724439704, 'lambda_l2': 0.06035335795719644}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[211]   train's l1: 168.132 valid's l1: 251.978\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:50:53,891] Trial 23 finished with value: 248.64092128126634 and parameters: {'num_leaves': 151, 'min_data_in_leaf': 17, 'max_bin': 337, 'bagging_fraction': 0.8793919136403086, 'feature_fraction': 0.4795100075058391, 'min_gain_to_split': 0.6621222754479498, 'lambda_l1': 0.01581193261065942, 'lambda_l2': 0.034592807286274535}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[332]   train's l1: 161.463 valid's l1: 248.641\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 139.969 valid's l1: 250.966\nEarly stopping, best iteration is:\n[689]   train's l1: 129.32  valid's l1: 250.606\n\n\n[I 2024-01-04 14:51:00,995] Trial 24 finished with value: 250.60618765624562 and parameters: {'num_leaves': 184, 'min_data_in_leaf': 18, 'max_bin': 375, 'bagging_fraction': 0.9064942629708629, 'feature_fraction': 0.4739241567506467, 'min_gain_to_split': 0.7324251718791496, 'lambda_l1': 0.01273525419811232, 'lambda_l2': 0.027794257938678468}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:51:03,461] Trial 25 finished with value: 250.85934486716704 and parameters: {'num_leaves': 136, 'min_data_in_leaf': 9, 'max_bin': 337, 'bagging_fraction': 0.9301129533240734, 'feature_fraction': 0.40835680299128685, 'min_gain_to_split': 0.6063175909579294, 'lambda_l1': 0.03186821507239008, 'lambda_l2': 0.013952386859597035}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[195]   train's l1: 185.522 valid's l1: 250.859\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 149.085 valid's l1: 249.481\nEarly stopping, best iteration is:\n[654]   train's l1: 140.939 valid's l1: 249.324\n\n\n[I 2024-01-04 14:51:09,697] Trial 26 finished with value: 249.32406553155195 and parameters: {'num_leaves': 162, 'min_data_in_leaf': 25, 'max_bin': 304, 'bagging_fraction': 0.8234117134000855, 'feature_fraction': 0.528296879968715, 'min_gain_to_split': 0.24688004027359095, 'lambda_l1': 0.019877058199912888, 'lambda_l2': 0.027659504570199004}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 137.277 valid's l1: 250.89\nEarly stopping, best iteration is:\n[405]   train's l1: 145.492 valid's l1: 250.663\n\n\n[I 2024-01-04 14:51:14,574] Trial 27 finished with value: 250.66320170782404 and parameters: {'num_leaves': 182, 'min_data_in_leaf': 14, 'max_bin': 267, 'bagging_fraction': 0.8784411771525241, 'feature_fraction': 0.6170190872270966, 'min_gain_to_split': 0.13005010415742133, 'lambda_l1': 0.022810822667222327, 'lambda_l2': 0.12443865861439836}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 182.937 valid's l1: 250.203\n\n\n[I 2024-01-04 14:51:17,797] Trial 28 finished with value: 250.11455247612324 and parameters: {'num_leaves': 68, 'min_data_in_leaf': 20, 'max_bin': 365, 'bagging_fraction': 0.8571290155469604, 'feature_fraction': 0.5622108030530639, 'min_gain_to_split': 0.30512258419876964, 'lambda_l1': 0.06373482080594682, 'lambda_l2': 0.036399534032642275}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[510]   train's l1: 181.888 valid's l1: 250.115\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 154.375 valid's l1: 251.136\nEarly stopping, best iteration is:\n[825]   train's l1: 138.693 valid's l1: 250.816\n\n\n[I 2024-01-04 14:51:24,673] Trial 29 finished with value: 250.81568105494773 and parameters: {'num_leaves': 134, 'min_data_in_leaf': 22, 'max_bin': 281, 'bagging_fraction': 0.8851718480829875, 'feature_fraction': 0.47447273988302097, 'min_gain_to_split': 0.15915988875888157, 'lambda_l1': 0.03572555256703823, 'lambda_l2': 0.4807822652316}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:51:27,752] Trial 30 finished with value: 251.04376947889406 and parameters: {'num_leaves': 125, 'min_data_in_leaf': 17, 'max_bin': 326, 'bagging_fraction': 0.8947044549335247, 'feature_fraction': 0.4936551743068348, 'min_gain_to_split': 0.0721686502558466, 'lambda_l1': 0.013319579299624452, 'lambda_l2': 0.7243799338935034}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[308]   train's l1: 172.632 valid's l1: 251.044\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 147.24  valid's l1: 250.601\nEarly stopping, best iteration is:\n[557]   train's l1: 143.155 valid's l1: 250.544\n\n\n[I 2024-01-04 14:51:32,726] Trial 31 finished with value: 250.544407442113 and parameters: {'num_leaves': 148, 'min_data_in_leaf': 16, 'max_bin': 327, 'bagging_fraction': 0.8712423675351528, 'feature_fraction': 0.5135969501452984, 'min_gain_to_split': 0.6638266788156616, 'lambda_l1': 0.014936396869515334, 'lambda_l2': 0.060910737452065745}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 141.05  valid's l1: 249.847\nEarly stopping, best iteration is:\n[403]   train's l1: 149.292 valid's l1: 249.703\n\n\n[I 2024-01-04 14:51:37,530] Trial 32 finished with value: 249.7027420585827 and parameters: {'num_leaves': 157, 'min_data_in_leaf': 14, 'max_bin': 350, 'bagging_fraction': 0.8788416797158063, 'feature_fraction': 0.45199500086067784, 'min_gain_to_split': 0.3811469562794672, 'lambda_l1': 0.017590111612931832, 'lambda_l2': 0.04489885972641594}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 158.408 valid's l1: 252.173\nEarly stopping, best iteration is:\n[532]   train's l1: 155.77  valid's l1: 251.871\n\n\n[I 2024-01-04 14:51:42,506] Trial 33 finished with value: 251.87091953186004 and parameters: {'num_leaves': 123, 'min_data_in_leaf': 20, 'max_bin': 301, 'bagging_fraction': 0.9019753684080398, 'feature_fraction': 0.48576561514241423, 'min_gain_to_split': 0.5502406300613208, 'lambda_l1': 0.08952723374033626, 'lambda_l2': 0.024511652118902418}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:51:46,484] Trial 34 finished with value: 250.30520499177402 and parameters: {'num_leaves': 141, 'min_data_in_leaf': 10, 'max_bin': 317, 'bagging_fraction': 0.8590109224366697, 'feature_fraction': 0.5448280965039918, 'min_gain_to_split': 0.9790965005819393, 'lambda_l1': 0.010426781267385363, 'lambda_l2': 0.3021205725985494}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[365]   train's l1: 156.3   valid's l1: 250.305\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 160.195 valid's l1: 249.744\nEarly stopping, best iteration is:\n[421]   train's l1: 167.482 valid's l1: 249.581\n\n\n[I 2024-01-04 14:51:50,122] Trial 35 finished with value: 249.58135808842567 and parameters: {'num_leaves': 114, 'min_data_in_leaf': 18, 'max_bin': 201, 'bagging_fraction': 0.9266378740310995, 'feature_fraction': 0.5354414577300035, 'min_gain_to_split': 0.24807376268658157, 'lambda_l1': 0.0231458074291776, 'lambda_l2': 0.014008762147851484}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 167.748 valid's l1: 249.458\nEarly stopping, best iteration is:\n[600]   train's l1: 162.83  valid's l1: 249.291\n\n\n[I 2024-01-04 14:51:53,991] Trial 36 finished with value: 249.29116763218968 and parameters: {'num_leaves': 101, 'min_data_in_leaf': 24, 'max_bin': 340, 'bagging_fraction': 0.8487920008375271, 'feature_fraction': 0.5031547685920289, 'min_gain_to_split': 0.7503950182918488, 'lambda_l1': 0.5886532361815853, 'lambda_l2': 0.08319977770180688}. Best is trial 3 with value: 248.09634372851536.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 164.516 valid's l1: 250.399\n\n\n[I 2024-01-04 14:51:57,902] Trial 37 finished with value: 250.32262409359683 and parameters: {'num_leaves': 91, 'min_data_in_leaf': 13, 'max_bin': 358, 'bagging_fraction': 0.8837459784244214, 'feature_fraction': 0.5239240110127943, 'min_gain_to_split': 0.010086515934441419, 'lambda_l1': 0.014113210227790236, 'lambda_l2': 0.11487676924184388}. Best is trial 3 with value: 248.09634372851536.\n\n\nEarly stopping, best iteration is:\n[514]   train's l1: 163.752 valid's l1: 250.323\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 139.187 valid's l1: 248.239\nEarly stopping, best iteration is:\n[408]   train's l1: 146.249 valid's l1: 247.985\n\n\n[I 2024-01-04 14:52:04,137] Trial 38 finished with value: 247.9849354686687 and parameters: {'num_leaves': 179, 'min_data_in_leaf': 16, 'max_bin': 341, 'bagging_fraction': 0.8638851277375763, 'feature_fraction': 0.46847706735658934, 'min_gain_to_split': 0.3793629342470942, 'lambda_l1': 0.02546677692735338, 'lambda_l2': 0.022828556931285018}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 129.961 valid's l1: 251.06\nEarly stopping, best iteration is:\n[575]   train's l1: 125.458 valid's l1: 250.803\n\n\n[I 2024-01-04 14:52:13,476] Trial 39 finished with value: 250.80316374229784 and parameters: {'num_leaves': 191, 'min_data_in_leaf': 10, 'max_bin': 374, 'bagging_fraction': 0.8628653253874917, 'feature_fraction': 0.3812965898538931, 'min_gain_to_split': 0.1964549825868853, 'lambda_l1': 0.04699284416866659, 'lambda_l2': 0.015763404762510726}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 141.845 valid's l1: 251.812\nEarly stopping, best iteration is:\n[492]   train's l1: 142.603 valid's l1: 251.674\n\n\n[I 2024-01-04 14:52:21,386] Trial 40 finished with value: 251.67414506489177 and parameters: {'num_leaves': 176, 'min_data_in_leaf': 19, 'max_bin': 385, 'bagging_fraction': 0.9471933827528347, 'feature_fraction': 0.43599688479252674, 'min_gain_to_split': 0.33325813937815146, 'lambda_l1': 0.06984020810597, 'lambda_l2': 0.02442694965905298}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 137.479 valid's l1: 251.811\nEarly stopping, best iteration is:\n[512]   train's l1: 136.629 valid's l1: 251.735\n\n\n[I 2024-01-04 14:52:29,196] Trial 41 finished with value: 251.73492357392558 and parameters: {'num_leaves': 195, 'min_data_in_leaf': 16, 'max_bin': 344, 'bagging_fraction': 0.8761728749427744, 'feature_fraction': 0.4651383363093944, 'min_gain_to_split': 0.4359610965020044, 'lambda_l1': 0.026939986865991233, 'lambda_l2': 0.02039678720107524}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 144.91  valid's l1: 248.37\nEarly stopping, best iteration is:\n[450]   train's l1: 148.083 valid's l1: 248.321\n\n\n[I 2024-01-04 14:52:35,502] Trial 42 finished with value: 248.32141899437622 and parameters: {'num_leaves': 160, 'min_data_in_leaf': 17, 'max_bin': 357, 'bagging_fraction': 0.8703793746963912, 'feature_fraction': 0.48461347694867424, 'min_gain_to_split': 0.7410712397968472, 'lambda_l1': 0.023007730225495376, 'lambda_l2': 0.032439046783148}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:52:39,774] Trial 43 finished with value: 251.39844251329075 and parameters: {'num_leaves': 180, 'min_data_in_leaf': 21, 'max_bin': 361, 'bagging_fraction': 0.89325817161555, 'feature_fraction': 0.4871432174781608, 'min_gain_to_split': 0.7515363686183039, 'lambda_l1': 0.034005802446438464, 'lambda_l2': 0.03439069898800271}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[270]   train's l1: 165.665 valid's l1: 251.398\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 184.062 valid's l1: 250.586\n[1000]  train's l1: 162.16  valid's l1: 249.606\nDid not meet early stopping. Best iteration is:\n[995]   train's l1: 162.356 valid's l1: 249.601\n\n\n[I 2024-01-04 14:52:44,523] Trial 44 finished with value: 249.60137827400777 and parameters: {'num_leaves': 62, 'min_data_in_leaf': 18, 'max_bin': 355, 'bagging_fraction': 0.8522815108818905, 'feature_fraction': 0.4391521435682733, 'min_gain_to_split': 0.5286581019536897, 'lambda_l1': 0.02309828294566098, 'lambda_l2': 0.013220147475425935}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[341]   train's l1: 154.323 valid's l1: 250.146\n\n\n[I 2024-01-04 14:52:48,456] Trial 45 finished with value: 250.1464682694806 and parameters: {'num_leaves': 167, 'min_data_in_leaf': 13, 'max_bin': 336, 'bagging_fraction': 0.8404371792598446, 'feature_fraction': 0.47264815989495895, 'min_gain_to_split': 0.05202500809789036, 'lambda_l1': 0.011998079306766319, 'lambda_l2': 0.0231796454259446}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:52:52,121] Trial 46 finished with value: 249.3071785385002 and parameters: {'num_leaves': 161, 'min_data_in_leaf': 11, 'max_bin': 385, 'bagging_fraction': 0.864427666250373, 'feature_fraction': 0.45064645195785213, 'min_gain_to_split': 0.7774823854270912, 'lambda_l1': 0.02022790751305002, 'lambda_l2': 0.06512511642566508}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[309]   train's l1: 158.43  valid's l1: 249.307\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 138.961 valid's l1: 250.66\nEarly stopping, best iteration is:\n[449]   train's l1: 143.133 valid's l1: 250.49\n\n\n[I 2024-01-04 14:52:57,437] Trial 47 finished with value: 250.4896361244068 and parameters: {'num_leaves': 172, 'min_data_in_leaf': 15, 'max_bin': 368, 'bagging_fraction': 0.8274669481534371, 'feature_fraction': 0.48439218785210786, 'min_gain_to_split': 0.6126221657308327, 'lambda_l1': 0.0399024553544704, 'lambda_l2': 0.018988176652188243}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:53:00,929] Trial 48 finished with value: 248.25530484556936 and parameters: {'num_leaves': 144, 'min_data_in_leaf': 17, 'max_bin': 251, 'bagging_fraction': 0.912198574875855, 'feature_fraction': 0.4245859311614965, 'min_gain_to_split': 0.03688304114494748, 'lambda_l1': 0.027372181393744633, 'lambda_l2': 0.21124570356518302}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[353]   train's l1: 162.925 valid's l1: 248.255\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:53:04,264] Trial 49 finished with value: 250.12552274154018 and parameters: {'num_leaves': 142, 'min_data_in_leaf': 19, 'max_bin': 250, 'bagging_fraction': 0.93860331389313, 'feature_fraction': 0.4228570966818336, 'min_gain_to_split': 0.06235039321237215, 'lambda_l1': 0.02837435584394861, 'lambda_l2': 0.1634129372894183}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[358]   train's l1: 166.139 valid's l1: 250.126\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 150.623 valid's l1: 250.135\nEarly stopping, best iteration is:\n[512]   train's l1: 150.163 valid's l1: 250.056\n\n\n[I 2024-01-04 14:53:08,741] Trial 50 finished with value: 250.0555008338255 and parameters: {'num_leaves': 144, 'min_data_in_leaf': 17, 'max_bin': 215, 'bagging_fraction': 0.9190018585664714, 'feature_fraction': 0.38784812173634753, 'min_gain_to_split': 0.02552698992959497, 'lambda_l1': 0.049733614033147515, 'lambda_l2': 0.3033509172848912}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 149.425 valid's l1: 250.166\nEarly stopping, best iteration is:\n[475]   train's l1: 151.48  valid's l1: 250.067\n\n\n[I 2024-01-04 14:53:13,391] Trial 51 finished with value: 250.0668193115747 and parameters: {'num_leaves': 158, 'min_data_in_leaf': 17, 'max_bin': 230, 'bagging_fraction': 0.9143659594226803, 'feature_fraction': 0.3553484087432987, 'min_gain_to_split': 0.03970378506773321, 'lambda_l1': 0.018761362649450636, 'lambda_l2': 0.2414130897638689}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 137.549 valid's l1: 252.719\nEarly stopping, best iteration is:\n[404]   train's l1: 145.238 valid's l1: 252.667\n\n\n[I 2024-01-04 14:53:19,246] Trial 52 finished with value: 252.66741046148817 and parameters: {'num_leaves': 193, 'min_data_in_leaf': 15, 'max_bin': 215, 'bagging_fraction': 0.8887818414581558, 'feature_fraction': 0.4588764251973786, 'min_gain_to_split': 0.02357027172353717, 'lambda_l1': 0.023637791700065573, 'lambda_l2': 0.4717385297408169}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 137.071 valid's l1: 250.707\nEarly stopping, best iteration is:\n[418]   train's l1: 143.92  valid's l1: 250.553\n\n\n[I 2024-01-04 14:53:25,283] Trial 53 finished with value: 250.55319510120077 and parameters: {'num_leaves': 200, 'min_data_in_leaf': 14, 'max_bin': 256, 'bagging_fraction': 0.9361536196159126, 'feature_fraction': 0.49831985753428804, 'min_gain_to_split': 0.8874048810699154, 'lambda_l1': 0.11414794308393804, 'lambda_l2': 0.20124298488521242}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:53:29,377] Trial 54 finished with value: 249.93321616977968 and parameters: {'num_leaves': 166, 'min_data_in_leaf': 19, 'max_bin': 349, 'bagging_fraction': 0.8001814474250672, 'feature_fraction': 0.4172621930471628, 'min_gain_to_split': 0.040518119432069684, 'lambda_l1': 0.03179528702813117, 'lambda_l2': 0.5180314575806758}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[352]   train's l1: 156.811 valid's l1: 249.933\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 147.311 valid's l1: 249.986\nEarly stopping, best iteration is:\n[565]   train's l1: 142.989 valid's l1: 249.912\n\n\n[I 2024-01-04 14:53:38,603] Trial 55 finished with value: 249.91214122610828 and parameters: {'num_leaves': 153, 'min_data_in_leaf': 21, 'max_bin': 241, 'bagging_fraction': 0.8981925907239523, 'feature_fraction': 0.5372477044213086, 'min_gain_to_split': 0.08033151294306631, 'lambda_l1': 0.016741758787073655, 'lambda_l2': 0.011847416430538263}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:53:42,537] Trial 56 finished with value: 251.60026883608768 and parameters: {'num_leaves': 190, 'min_data_in_leaf': 17, 'max_bin': 226, 'bagging_fraction': 0.9227978585733236, 'feature_fraction': 0.5176409770876758, 'min_gain_to_split': 0.04897255044587492, 'lambda_l1': 0.011512322218642266, 'lambda_l2': 0.016899335076097925}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[283]   train's l1: 162.394 valid's l1: 251.6\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:53:46,123] Trial 57 finished with value: 251.21917591911807 and parameters: {'num_leaves': 177, 'min_data_in_leaf': 13, 'max_bin': 333, 'bagging_fraction': 0.9053465627575883, 'feature_fraction': 0.5588810249549871, 'min_gain_to_split': 0.033583349303849944, 'lambda_l1': 0.3484544523386503, 'lambda_l2': 0.36085439193700547}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[270]   train's l1: 162.445 valid's l1: 251.219\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 161.894 valid's l1: 249.203\nEarly stopping, best iteration is:\n[563]   train's l1: 157.656 valid's l1: 248.91\n\n\n[I 2024-01-04 14:53:50,614] Trial 58 finished with value: 248.9104962802268 and parameters: {'num_leaves': 130, 'min_data_in_leaf': 28, 'max_bin': 288, 'bagging_fraction': 0.8712548309119912, 'feature_fraction': 0.4445657973227514, 'min_gain_to_split': 0.38034195652092995, 'lambda_l1': 0.03940273258738886, 'lambda_l2': 0.03960739327049044}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 146.433 valid's l1: 250.169\nEarly stopping, best iteration is:\n[454]   train's l1: 149.981 valid's l1: 250.121\n\n\n[I 2024-01-04 14:53:55,541] Trial 59 finished with value: 250.12052315432507 and parameters: {'num_leaves': 170, 'min_data_in_leaf': 23, 'max_bin': 311, 'bagging_fraction': 0.9107469825870032, 'feature_fraction': 0.4657877241528394, 'min_gain_to_split': 0.09520603372996359, 'lambda_l1': 0.026728362368755795, 'lambda_l2': 0.2777611775346086}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:53:59,528] Trial 60 finished with value: 251.7693522347827 and parameters: {'num_leaves': 185, 'min_data_in_leaf': 5, 'max_bin': 399, 'bagging_fraction': 0.882375221490412, 'feature_fraction': 0.4306868375597421, 'min_gain_to_split': 0.5072163591307811, 'lambda_l1': 0.05837647056807119, 'lambda_l2': 0.14525084112406245}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[280]   train's l1: 150.375 valid's l1: 251.769\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 162.783 valid's l1: 250.022\nEarly stopping, best iteration is:\n[536]   train's l1: 160.372 valid's l1: 249.65\n\n\n[I 2024-01-04 14:54:03,761] Trial 61 finished with value: 249.64977135928768 and parameters: {'num_leaves': 120, 'min_data_in_leaf': 25, 'max_bin': 278, 'bagging_fraction': 0.8687378131288802, 'feature_fraction': 0.44342126027661843, 'min_gain_to_split': 0.3781664415222889, 'lambda_l1': 0.04126372287473671, 'lambda_l2': 0.03903589476536043}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 150.494 valid's l1: 250.152\nEarly stopping, best iteration is:\n[416]   train's l1: 157.739 valid's l1: 250.007\n\n\n[I 2024-01-04 14:54:08,092] Trial 62 finished with value: 250.0065274362485 and parameters: {'num_leaves': 140, 'min_data_in_leaf': 15, 'max_bin': 257, 'bagging_fraction': 0.8730842097074003, 'feature_fraction': 0.47834496431046764, 'min_gain_to_split': 0.14999988696839564, 'lambda_l1': 0.02098169094289273, 'lambda_l2': 0.02824016844752199}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[378]   train's l1: 175.261 valid's l1: 251.091\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:54:11,686] Trial 63 finished with value: 251.0914408199419 and parameters: {'num_leaves': 130, 'min_data_in_leaf': 30, 'max_bin': 294, 'bagging_fraction': 0.8598215267159707, 'feature_fraction': 0.40468323367468473, 'min_gain_to_split': 0.26942027317022926, 'lambda_l1': 0.03601751073383649, 'lambda_l2': 0.04599849274464953}. Best is trial 38 with value: 247.9849354686687.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 164.079 valid's l1: 251.876\nEarly stopping, best iteration is:\n[496]   train's l1: 164.306 valid's l1: 251.836\n\n\n[I 2024-01-04 14:54:16,791] Trial 64 finished with value: 251.83636097740788 and parameters: {'num_leaves': 129, 'min_data_in_leaf': 27, 'max_bin': 232, 'bagging_fraction': 0.8889857699853351, 'feature_fraction': 0.44853731268617936, 'min_gain_to_split': 0.6599958506069021, 'lambda_l1': 0.016469558628734983, 'lambda_l2': 0.05491006606451566}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:54:19,185] Trial 65 finished with value: 250.57409412561648 and parameters: {'num_leaves': 115, 'min_data_in_leaf': 29, 'max_bin': 209, 'bagging_fraction': 0.8761571080949921, 'feature_fraction': 0.5071930422815497, 'min_gain_to_split': 0.8562089772195518, 'lambda_l1': 0.030135853082934972, 'lambda_l2': 0.021852474648210867}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[312]   train's l1: 190.75  valid's l1: 250.574\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 169.321 valid's l1: 251.092\nEarly stopping, best iteration is:\n[672]   train's l1: 158.933 valid's l1: 250.771\n\n\n[I 2024-01-04 14:54:24,787] Trial 66 finished with value: 250.77081073868032 and parameters: {'num_leaves': 106, 'min_data_in_leaf': 28, 'max_bin': 294, 'bagging_fraction': 0.853744050602893, 'feature_fraction': 0.461424963571921, 'min_gain_to_split': 0.44739176344300347, 'lambda_l1': 0.025898174427658276, 'lambda_l2': 0.07215971601269885}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:54:29,165] Trial 67 finished with value: 251.5021472556768 and parameters: {'num_leaves': 146, 'min_data_in_leaf': 12, 'max_bin': 321, 'bagging_fraction': 0.8658723196583835, 'feature_fraction': 0.4307191976353449, 'min_gain_to_split': 0.590925382762307, 'lambda_l1': 0.014158552149332768, 'lambda_l2': 0.03125912169121386}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[372]   train's l1: 153.549 valid's l1: 251.502\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 151.016 valid's l1: 250.687\nEarly stopping, best iteration is:\n[543]   train's l1: 148.638 valid's l1: 250.589\n\n\n[I 2024-01-04 14:54:35,989] Trial 68 finished with value: 250.58930815849988 and parameters: {'num_leaves': 153, 'min_data_in_leaf': 21, 'max_bin': 346, 'bagging_fraction': 0.8433798524309817, 'feature_fraction': 0.4930391476662174, 'min_gain_to_split': 0.01917685771883979, 'lambda_l1': 0.0195495071273504, 'lambda_l2': 0.4098824964007025}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:54:39,177] Trial 69 finished with value: 250.1607809531231 and parameters: {'num_leaves': 137, 'min_data_in_leaf': 16, 'max_bin': 369, 'bagging_fraction': 0.8813370673553869, 'feature_fraction': 0.48109097723896044, 'min_gain_to_split': 0.978205671355497, 'lambda_l1': 0.0784093894382671, 'lambda_l2': 0.02971625529785851}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[327]   train's l1: 170.392 valid's l1: 250.161\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 144.141 valid's l1: 250.243\nEarly stopping, best iteration is:\n[669]   train's l1: 133.803 valid's l1: 249.979\n\n\n[I 2024-01-04 14:54:46,360] Trial 70 finished with value: 249.9792973179026 and parameters: {'num_leaves': 163, 'min_data_in_leaf': 18, 'max_bin': 352, 'bagging_fraction': 0.8709854958181945, 'feature_fraction': 0.5246618291519151, 'min_gain_to_split': 0.3596892235304456, 'lambda_l1': 0.035832084683306197, 'lambda_l2': 0.03976910792323768}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 142.731 valid's l1: 249.775\nEarly stopping, best iteration is:\n[565]   train's l1: 137.761 valid's l1: 249.728\n\n\n[I 2024-01-04 14:54:52,425] Trial 71 finished with value: 249.72817911073722 and parameters: {'num_leaves': 151, 'min_data_in_leaf': 15, 'max_bin': 324, 'bagging_fraction': 0.8618225640749834, 'feature_fraction': 0.509306336015361, 'min_gain_to_split': 0.432969884548429, 'lambda_l1': 0.015465836333656141, 'lambda_l2': 0.8904885110016947}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 188.336 valid's l1: 251.619\n\n\n[I 2024-01-04 14:54:55,742] Trial 72 finished with value: 251.13781099817118 and parameters: {'num_leaves': 51, 'min_data_in_leaf': 14, 'max_bin': 342, 'bagging_fraction': 0.867635470358654, 'feature_fraction': 0.49900415163790385, 'min_gain_to_split': 0.4801177887999686, 'lambda_l1': 0.010012959291867395, 'lambda_l2': 0.04639445228706571}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[678]   train's l1: 178.939 valid's l1: 251.138\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 144.872 valid's l1: 252.145\nEarly stopping, best iteration is:\n[540]   train's l1: 142.216 valid's l1: 252.024\n\n\n[I 2024-01-04 14:55:01,427] Trial 73 finished with value: 252.02368168938506 and parameters: {'num_leaves': 156, 'min_data_in_leaf': 17, 'max_bin': 317, 'bagging_fraction': 0.9435891544639423, 'feature_fraction': 0.47373156223267404, 'min_gain_to_split': 0.29846515933988327, 'lambda_l1': 0.012802733659201716, 'lambda_l2': 0.10474596200572255}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 143.648 valid's l1: 248.855\nEarly stopping, best iteration is:\n[476]   train's l1: 145.373 valid's l1: 248.717\n\n\n[I 2024-01-04 14:55:06,369] Trial 74 finished with value: 248.7169311533382 and parameters: {'num_leaves': 149, 'min_data_in_leaf': 16, 'max_bin': 332, 'bagging_fraction': 0.8865467103300672, 'feature_fraction': 0.6336290066694925, 'min_gain_to_split': 0.7053311948728637, 'lambda_l1': 0.017957678612174736, 'lambda_l2': 0.053548432990865154}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:55:10,167] Trial 75 finished with value: 249.25159007245202 and parameters: {'num_leaves': 148, 'min_data_in_leaf': 18, 'max_bin': 331, 'bagging_fraction': 0.897050719742335, 'feature_fraction': 0.5973741336074612, 'min_gain_to_split': 0.6711667049913296, 'lambda_l1': 0.04495263535974253, 'lambda_l2': 0.03590483507477473}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[365]   train's l1: 158.647 valid's l1: 249.252\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:55:13,720] Trial 76 finished with value: 250.3135255299012 and parameters: {'num_leaves': 131, 'min_data_in_leaf': 20, 'max_bin': 269, 'bagging_fraction': 0.8866040372625258, 'feature_fraction': 0.6184229070427256, 'min_gain_to_split': 0.8116086939431894, 'lambda_l1': 0.02456688086247493, 'lambda_l2': 0.025237363708475614}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[394]   train's l1: 164.163 valid's l1: 250.314\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 151.395 valid's l1: 249.527\nEarly stopping, best iteration is:\n[526]   train's l1: 149.615 valid's l1: 249.407\n\n\n[I 2024-01-04 14:55:19,810] Trial 77 finished with value: 249.4073645969391 and parameters: {'num_leaves': 138, 'min_data_in_leaf': 16, 'max_bin': 361, 'bagging_fraction': 0.8769526133513171, 'feature_fraction': 0.6403868124248584, 'min_gain_to_split': 0.1180940664898308, 'lambda_l1': 0.021111552431719135, 'lambda_l2': 0.07576345111579191}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 151.671 valid's l1: 248.64\nEarly stopping, best iteration is:\n[685]   train's l1: 142.201 valid's l1: 248.263\n\n\n[I 2024-01-04 14:55:25,543] Trial 78 finished with value: 248.26322318266054 and parameters: {'num_leaves': 125, 'min_data_in_leaf': 13, 'max_bin': 307, 'bagging_fraction': 0.9326317692312726, 'feature_fraction': 0.4140174670661608, 'min_gain_to_split': 0.6869311503007131, 'lambda_l1': 0.01815070035925684, 'lambda_l2': 0.0936257508693921}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 187.076 valid's l1: 253.484\n\n\n[I 2024-01-04 14:55:28,090] Trial 79 finished with value: 253.43758711281492 and parameters: {'num_leaves': 57, 'min_data_in_leaf': 11, 'max_bin': 309, 'bagging_fraction': 0.9314988687015779, 'feature_fraction': 0.3703894106101037, 'min_gain_to_split': 0.5770493456902785, 'lambda_l1': 0.011463313580819107, 'lambda_l2': 0.016849307906209997}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[480]   train's l1: 188.929 valid's l1: 253.438\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 167.685 valid's l1: 252.062\nEarly stopping, best iteration is:\n[667]   train's l1: 156.254 valid's l1: 251.566\n\n\n[I 2024-01-04 14:55:32,759] Trial 80 finished with value: 251.56627397206245 and parameters: {'num_leaves': 93, 'min_data_in_leaf': 8, 'max_bin': 356, 'bagging_fraction': 0.9340979490630946, 'feature_fraction': 0.39932696307514864, 'min_gain_to_split': 0.691149277295846, 'lambda_l1': 0.01729871483359858, 'lambda_l2': 0.15044731141256437}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:55:35,795] Trial 81 finished with value: 250.69024379236384 and parameters: {'num_leaves': 124, 'min_data_in_leaf': 12, 'max_bin': 290, 'bagging_fraction': 0.8810562603567236, 'feature_fraction': 0.41335716230832203, 'min_gain_to_split': 0.8387680283394637, 'lambda_l1': 0.018202490237123493, 'lambda_l2': 0.05081302858753658}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[293]   train's l1: 173.606 valid's l1: 250.69\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[331]   train's l1: 152.579 valid's l1: 251.458\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:55:40,479] Trial 82 finished with value: 251.4578613418851 and parameters: {'num_leaves': 181, 'min_data_in_leaf': 13, 'max_bin': 337, 'bagging_fraction': 0.943081644257227, 'feature_fraction': 0.4267437215730898, 'min_gain_to_split': 0.2197313823570854, 'lambda_l1': 0.029607817402509336, 'lambda_l2': 0.09550943955525955}. Best is trial 38 with value: 247.9849354686687.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 135.84  valid's l1: 249.904\nEarly stopping, best iteration is:\n[432]   train's l1: 141.973 valid's l1: 249.529\n\n\n[I 2024-01-04 14:55:46,020] Trial 83 finished with value: 249.52932925626774 and parameters: {'num_leaves': 160, 'min_data_in_leaf': 11, 'max_bin': 248, 'bagging_fraction': 0.9258748851896403, 'feature_fraction': 0.5732951707645729, 'min_gain_to_split': 0.0613510472641891, 'lambda_l1': 0.02187710677368099, 'lambda_l2': 0.019611679739961953}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 178.354 valid's l1: 251.595\n\n\n[I 2024-01-04 14:55:49,190] Trial 84 finished with value: 251.56861829643137 and parameters: {'num_leaves': 69, 'min_data_in_leaf': 16, 'max_bin': 304, 'bagging_fraction': 0.892512777539076, 'feature_fraction': 0.45727636582916376, 'min_gain_to_split': 0.63714039665581, 'lambda_l1': 0.02558311119399844, 'lambda_l2': 0.031997844281058306}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[517]   train's l1: 177.413 valid's l1: 251.569\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 146.442 valid's l1: 251.888\nEarly stopping, best iteration is:\n[680]   train's l1: 136.396 valid's l1: 251.706\n\n\n[I 2024-01-04 14:55:55,359] Trial 85 finished with value: 251.70644749896536 and parameters: {'num_leaves': 144, 'min_data_in_leaf': 14, 'max_bin': 275, 'bagging_fraction': 0.872475557516114, 'feature_fraction': 0.4429083072124641, 'min_gain_to_split': 0.548589154978752, 'lambda_l1': 0.013954319503232233, 'lambda_l2': 0.056723765001123576}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 156.556 valid's l1: 250.128\nEarly stopping, best iteration is:\n[457]   train's l1: 160.195 valid's l1: 249.847\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:55:59,102] Trial 86 finished with value: 249.84699890936838 and parameters: {'num_leaves': 128, 'min_data_in_leaf': 19, 'max_bin': 285, 'bagging_fraction': 0.9498979178124701, 'feature_fraction': 0.46755014311833965, 'min_gain_to_split': 0.7361413340714015, 'lambda_l1': 0.015499703718016128, 'lambda_l2': 0.20604248285042945}. Best is trial 38 with value: 247.9849354686687.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 173.44  valid's l1: 248.898\n\n\n[I 2024-01-04 14:56:02,885] Trial 87 finished with value: 248.82456761772468 and parameters: {'num_leaves': 83, 'min_data_in_leaf': 15, 'max_bin': 379, 'bagging_fraction': 0.9403958145148957, 'feature_fraction': 0.4910493017896031, 'min_gain_to_split': 0.40856328043545914, 'lambda_l1': 0.17534437375445006, 'lambda_l2': 0.01517075044534919}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[508]   train's l1: 172.765 valid's l1: 248.825\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 172.053 valid's l1: 250.106\nEarly stopping, best iteration is:\n[503]   train's l1: 171.553 valid's l1: 250.069\n\n\n[I 2024-01-04 14:56:06,466] Trial 88 finished with value: 250.06866108092123 and parameters: {'num_leaves': 83, 'min_data_in_leaf': 15, 'max_bin': 380, 'bagging_fraction': 0.9394510495645861, 'feature_fraction': 0.49314853677553727, 'min_gain_to_split': 0.4800273796992425, 'lambda_l1': 0.26467563865775584, 'lambda_l2': 0.011069164207374384}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 177.996 valid's l1: 252.176\n[1000]  train's l1: 158.202 valid's l1: 251.496\nDid not meet early stopping. Best iteration is:\n[994]   train's l1: 158.319 valid's l1: 251.487\n\n\n[I 2024-01-04 14:56:11,587] Trial 89 finished with value: 251.48654753781696 and parameters: {'num_leaves': 70, 'min_data_in_leaf': 17, 'max_bin': 366, 'bagging_fraction': 0.9267013991954216, 'feature_fraction': 0.38693656744178695, 'min_gain_to_split': 0.01337832605488843, 'lambda_l1': 0.2160514985532518, 'lambda_l2': 0.01353028410372272}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[352]   train's l1: 145.912 valid's l1: 251.062\n\n\n[I 2024-01-04 14:56:16,305] Trial 90 finished with value: 251.06212337800955 and parameters: {'num_leaves': 189, 'min_data_in_leaf': 13, 'max_bin': 391, 'bagging_fraction': 0.9194206921907717, 'feature_fraction': 0.5481623472559404, 'min_gain_to_split': 0.9125933538197182, 'lambda_l1': 0.12853512239637843, 'lambda_l2': 0.015024579963889248}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 174.788 valid's l1: 249.122\nEarly stopping, best iteration is:\n[799]   train's l1: 160.503 valid's l1: 248.746\n\n\n[I 2024-01-04 14:56:20,871] Trial 91 finished with value: 248.74622065286144 and parameters: {'num_leaves': 74, 'min_data_in_leaf': 16, 'max_bin': 372, 'bagging_fraction': 0.940904920071612, 'feature_fraction': 0.4861611867914935, 'min_gain_to_split': 0.3992063041226488, 'lambda_l1': 0.09509968365796506, 'lambda_l2': 0.02643654722836422}. Best is trial 38 with value: 247.9849354686687.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 175.412 valid's l1: 251.756\n\n\n[I 2024-01-04 14:56:24,020] Trial 92 finished with value: 251.6513535899103 and parameters: {'num_leaves': 75, 'min_data_in_leaf': 16, 'max_bin': 377, 'bagging_fraction': 0.9404365737726692, 'feature_fraction': 0.4898290710778477, 'min_gain_to_split': 0.583413086786081, 'lambda_l1': 0.14529420083477243, 'lambda_l2': 0.021938198092173355}. Best is trial 38 with value: 247.9849354686687.\n\n\nEarly stopping, best iteration is:\n[490]   train's l1: 176.292 valid's l1: 251.651\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 175.166 valid's l1: 250.293\nEarly stopping, best iteration is:\n[444]   train's l1: 179.237 valid's l1: 250.109\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:56:27,842] Trial 93 finished with value: 250.1087602027009 and parameters: {'num_leaves': 73, 'min_data_in_leaf': 14, 'max_bin': 353, 'bagging_fraction': 0.9495170223847873, 'feature_fraction': 0.5325872049340957, 'min_gain_to_split': 0.41411322074544543, 'lambda_l1': 0.08692950017325465, 'lambda_l2': 0.027332183189884895}. Best is trial 38 with value: 247.9849354686687.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 182.181 valid's l1: 247.746\n\n\n[I 2024-01-04 14:56:30,951] Trial 94 finished with value: 247.49181840326082 and parameters: {'num_leaves': 62, 'min_data_in_leaf': 18, 'max_bin': 393, 'bagging_fraction': 0.934585331667364, 'feature_fraction': 0.5177135550901508, 'min_gain_to_split': 0.3387131388468447, 'lambda_l1': 0.17120040101009384, 'lambda_l2': 0.018502931725623104}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[579]   train's l1: 175.968 valid's l1: 247.492\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 181.708 valid's l1: 249.248\n\n\n[I 2024-01-04 14:56:33,953] Trial 95 finished with value: 249.0166614600853 and parameters: {'num_leaves': 64, 'min_data_in_leaf': 18, 'max_bin': 388, 'bagging_fraction': 0.9324750234268026, 'feature_fraction': 0.5136674856499592, 'min_gain_to_split': 0.28500503754063133, 'lambda_l1': 0.19772126538595708, 'lambda_l2': 0.01782243737045975}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[543]   train's l1: 177.369 valid's l1: 249.017\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 184.344 valid's l1: 250.706\n\n\n[I 2024-01-04 14:56:37,477] Trial 96 finished with value: 249.72419131055995 and parameters: {'num_leaves': 63, 'min_data_in_leaf': 19, 'max_bin': 380, 'bagging_fraction': 0.9360858995391556, 'feature_fraction': 0.5034676677441003, 'min_gain_to_split': 0.33809076683711065, 'lambda_l1': 0.31802884085814237, 'lambda_l2': 0.025984508126575368}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[611]   train's l1: 175.858 valid's l1: 249.724\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 186.57  valid's l1: 249.832\nEarly stopping, best iteration is:\n[675]   train's l1: 176.952 valid's l1: 249.137\n\n\n[I 2024-01-04 14:56:41,539] Trial 97 finished with value: 249.1373466511043 and parameters: {'num_leaves': 56, 'min_data_in_leaf': 17, 'max_bin': 392, 'bagging_fraction': 0.9298798299178433, 'feature_fraction': 0.48052916727877615, 'min_gain_to_split': 0.2386862241455436, 'lambda_l1': 0.2164421619592046, 'lambda_l2': 0.021100845377884953}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 174.405 valid's l1: 249.807\n\n\n[I 2024-01-04 14:56:48,771] Trial 98 finished with value: 249.6535043833103 and parameters: {'num_leaves': 79, 'min_data_in_leaf': 20, 'max_bin': 372, 'bagging_fraction': 0.9462939107774153, 'feature_fraction': 0.5202361925942613, 'min_gain_to_split': 0.342159550555219, 'lambda_l1': 0.14967217736142474, 'lambda_l2': 0.03417361236587591}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[488]   train's l1: 175.78  valid's l1: 249.654\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 188.834 valid's l1: 251.886\n\n\n[I 2024-01-04 14:56:51,787] Trial 99 finished with value: 251.72425410158846 and parameters: {'num_leaves': 58, 'min_data_in_leaf': 18, 'max_bin': 397, 'bagging_fraction': 0.9426510935596756, 'feature_fraction': 0.36618044450496384, 'min_gain_to_split': 0.49490822810590657, 'lambda_l1': 0.2584513235662528, 'lambda_l2': 0.012421646431757107}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[481]   train's l1: 190.657 valid's l1: 251.724\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 168.434 valid's l1: 249.015\nEarly stopping, best iteration is:\n[436]   train's l1: 172.384 valid's l1: 248.924\n\n\n[I 2024-01-04 14:56:55,331] Trial 100 finished with value: 248.92427425429713 and parameters: {'num_leaves': 83, 'min_data_in_leaf': 15, 'max_bin': 362, 'bagging_fraction': 0.9225877682434708, 'feature_fraction': 0.4721310667239858, 'min_gain_to_split': 0.188648880316058, 'lambda_l1': 0.17002316584054394, 'lambda_l2': 0.6764914793110878}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 179.225 valid's l1: 249.823\n\n\n[I 2024-01-04 14:56:58,510] Trial 101 finished with value: 249.70070316398883 and parameters: {'num_leaves': 67, 'min_data_in_leaf': 16, 'max_bin': 348, 'bagging_fraction': 0.9364037805088006, 'feature_fraction': 0.4842663627936338, 'min_gain_to_split': 0.7195281634238482, 'lambda_l1': 0.18179822589041, 'lambda_l2': 0.015395368689420123}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[592]   train's l1: 173.742 valid's l1: 249.701\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 183.292 valid's l1: 250.346\n\n\n[I 2024-01-04 14:57:02,035] Trial 102 finished with value: 249.83342674579632 and parameters: {'num_leaves': 60, 'min_data_in_leaf': 17, 'max_bin': 384, 'bagging_fraction': 0.937684854217256, 'feature_fraction': 0.4963635857812554, 'min_gain_to_split': 0.06984304942955603, 'lambda_l1': 0.018467681934921012, 'lambda_l2': 0.018526282285452417}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[671]   train's l1: 174.617 valid's l1: 249.833\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 167.333 valid's l1: 252.248\n\n\n[I 2024-01-04 14:57:06,531] Trial 103 finished with value: 252.0436455562155 and parameters: {'num_leaves': 89, 'min_data_in_leaf': 15, 'max_bin': 372, 'bagging_fraction': 0.928380530374755, 'feature_fraction': 0.5387026487524951, 'min_gain_to_split': 0.4024710753539279, 'lambda_l1': 0.10664800339077712, 'lambda_l2': 0.02388292172948342}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[513]   train's l1: 166.261 valid's l1: 252.044\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 173.67  valid's l1: 249.416\nEarly stopping, best iteration is:\n[415]   train's l1: 179.33  valid's l1: 249.104\n\n\n[I 2024-01-04 14:57:09,317] Trial 104 finished with value: 249.10366425832655 and parameters: {'num_leaves': 75, 'min_data_in_leaf': 14, 'max_bin': 342, 'bagging_fraction': 0.9151710184135586, 'feature_fraction': 0.555109088484512, 'min_gain_to_split': 0.05525549064086035, 'lambda_l1': 0.09648571715115122, 'lambda_l2': 0.016560405433858325}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[375]   train's l1: 146.642 valid's l1: 250.862\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:57:13,830] Trial 105 finished with value: 250.86205216793596 and parameters: {'num_leaves': 174, 'min_data_in_leaf': 13, 'max_bin': 334, 'bagging_fraction': 0.9112746447100628, 'feature_fraction': 0.5034694152945758, 'min_gain_to_split': 0.044805372171620575, 'lambda_l1': 0.9214420585824484, 'lambda_l2': 0.01481627390319592}. Best is trial 94 with value: 247.49181840326082.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 143.222 valid's l1: 250.537\nEarly stopping, best iteration is:\n[401]   train's l1: 150.821 valid's l1: 250.514\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:57:18,212] Trial 106 finished with value: 250.5138341519703 and parameters: {'num_leaves': 166, 'min_data_in_leaf': 19, 'max_bin': 328, 'bagging_fraction': 0.9413722524501744, 'feature_fraction': 0.5297129792743496, 'min_gain_to_split': 0.5367315376995105, 'lambda_l1': 0.4450714139476634, 'lambda_l2': 0.010367746279363847}. Best is trial 94 with value: 247.49181840326082.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 167.092 valid's l1: 249.96\nEarly stopping, best iteration is:\n[542]   train's l1: 164.559 valid's l1: 249.783\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:57:21,776] Trial 107 finished with value: 249.7833818373524 and parameters: {'num_leaves': 98, 'min_data_in_leaf': 17, 'max_bin': 358, 'bagging_fraction': 0.903783329838784, 'feature_fraction': 0.45416360713152387, 'min_gain_to_split': 0.766620338871143, 'lambda_l1': 0.022452430022556986, 'lambda_l2': 0.0432076945957621}. Best is trial 94 with value: 247.49181840326082.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 193.483 valid's l1: 250.866\n\n\n[I 2024-01-04 14:57:24,161] Trial 108 finished with value: 250.50976906364582 and parameters: {'num_leaves': 54, 'min_data_in_leaf': 18, 'max_bin': 339, 'bagging_fraction': 0.9451482389895436, 'feature_fraction': 0.5137341733354087, 'min_gain_to_split': 0.9913107435431426, 'lambda_l1': 0.03285620956947809, 'lambda_l2': 0.24926931153569987}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[555]   train's l1: 189.863 valid's l1: 250.51\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 162.685 valid's l1: 250.312\nEarly stopping, best iteration is:\n[454]   train's l1: 165.626 valid's l1: 250.174\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:57:27,651] Trial 109 finished with value: 250.17361464392383 and parameters: {'num_leaves': 106, 'min_data_in_leaf': 21, 'max_bin': 320, 'bagging_fraction': 0.9340328355383452, 'feature_fraction': 0.4886736011676739, 'min_gain_to_split': 0.029140278455179913, 'lambda_l1': 0.019935473823846094, 'lambda_l2': 0.06536538615775499}. Best is trial 94 with value: 247.49181840326082.\n\n\nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:57:31,668] Trial 110 finished with value: 252.23414487360895 and parameters: {'num_leaves': 155, 'min_data_in_leaf': 16, 'max_bin': 362, 'bagging_fraction': 0.9230994495796144, 'feature_fraction': 0.6387924360095123, 'min_gain_to_split': 0.6159570656333072, 'lambda_l1': 0.01678788514716962, 'lambda_l2': 0.09383239157384529}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[359]   train's l1: 155.139 valid's l1: 252.234\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 160.599 valid's l1: 250.962\nEarly stopping, best iteration is:\n[658]   train's l1: 151.461 valid's l1: 250.856\n\n\n[I 2024-01-04 14:57:36,741] Trial 111 finished with value: 250.85633556996947 and parameters: {'num_leaves': 134, 'min_data_in_leaf': 26, 'max_bin': 220, 'bagging_fraction': 0.8559891642725717, 'feature_fraction': 0.4188575263345835, 'min_gain_to_split': 0.3739905716358471, 'lambda_l1': 0.03877068773241145, 'lambda_l2': 0.029925860768446787}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:57:39,666] Trial 112 finished with value: 248.95974784307262 and parameters: {'num_leaves': 121, 'min_data_in_leaf': 15, 'max_bin': 239, 'bagging_fraction': 0.8782062454191585, 'feature_fraction': 0.40772778202924065, 'min_gain_to_split': 0.4775323494687619, 'lambda_l1': 0.023877236189671094, 'lambda_l2': 0.03712523514958895}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[371]   train's l1: 164.618 valid's l1: 248.96\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 141.604 valid's l1: 250.388\nEarly stopping, best iteration is:\n[661]   train's l1: 131.761 valid's l1: 250.246\n\n\n[I 2024-01-04 14:57:45,638] Trial 113 finished with value: 250.2461727025437 and parameters: {'num_leaves': 149, 'min_data_in_leaf': 12, 'max_bin': 260, 'bagging_fraction': 0.8696182773140307, 'feature_fraction': 0.46934064851648, 'min_gain_to_split': 0.4379782566468774, 'lambda_l1': 0.0553914321132193, 'lambda_l2': 0.12471442960268589}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 137.132 valid's l1: 251.323\nEarly stopping, best iteration is:\n[487]   train's l1: 137.878 valid's l1: 251.205\n\n\n[I 2024-01-04 14:57:50,927] Trial 114 finished with value: 251.20537205074302 and parameters: {'num_leaves': 198, 'min_data_in_leaf': 16, 'max_bin': 298, 'bagging_fraction': 0.8630588206922851, 'feature_fraction': 0.4374227631626794, 'min_gain_to_split': 0.8111155281735318, 'lambda_l1': 0.029018538048180255, 'lambda_l2': 0.39724092489757895}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 182.704 valid's l1: 248.534\n\n\n[I 2024-01-04 14:57:53,405] Trial 115 finished with value: 248.51612258227803 and parameters: {'num_leaves': 66, 'min_data_in_leaf': 14, 'max_bin': 205, 'bagging_fraction': 0.8867768927320587, 'feature_fraction': 0.4459583211345064, 'min_gain_to_split': 0.6579957165972958, 'lambda_l1': 0.012466498269244643, 'lambda_l2': 0.03289431305033875}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[497]   train's l1: 182.833 valid's l1: 248.516\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 179.905 valid's l1: 250.855\nEarly stopping, best iteration is:\n[811]   train's l1: 163.683 valid's l1: 250.178\n\n\n[I 2024-01-04 14:57:57,441] Trial 116 finished with value: 250.17846669140872 and parameters: {'num_leaves': 72, 'min_data_in_leaf': 14, 'max_bin': 207, 'bagging_fraction': 0.8894664906338506, 'feature_fraction': 0.4628956894403763, 'min_gain_to_split': 0.6967966784482668, 'lambda_l1': 0.012451036663013022, 'lambda_l2': 0.02203730685023667}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 185.407 valid's l1: 249.018\n\n\n[I 2024-01-04 14:58:00,695] Trial 117 finished with value: 248.41183613331293 and parameters: {'num_leaves': 65, 'min_data_in_leaf': 17, 'max_bin': 232, 'bagging_fraction': 0.885402069847634, 'feature_fraction': 0.4762431798335603, 'min_gain_to_split': 0.31245300076868454, 'lambda_l1': 0.011623287096281945, 'lambda_l2': 0.03283167419484773}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[671]   train's l1: 174.51  valid's l1: 248.412\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 186.839 valid's l1: 252.225\n\n\n[I 2024-01-04 14:58:03,810] Trial 118 finished with value: 251.82995197329285 and parameters: {'num_leaves': 66, 'min_data_in_leaf': 18, 'max_bin': 221, 'bagging_fraction': 0.8842886697781926, 'feature_fraction': 0.47621018934983783, 'min_gain_to_split': 0.32700400491739656, 'lambda_l1': 0.010945569286470974, 'lambda_l2': 0.047845492683800436}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[622]   train's l1: 178.989 valid's l1: 251.83\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 185.91  valid's l1: 251.487\n[1000]  train's l1: 162.83  valid's l1: 250.838\nDid not meet early stopping. Best iteration is:\n[901]   train's l1: 165.666 valid's l1: 250.766\n\n\n[I 2024-01-04 14:58:07,947] Trial 119 finished with value: 250.76583841554688 and parameters: {'num_leaves': 61, 'min_data_in_leaf': 15, 'max_bin': 234, 'bagging_fraction': 0.894900546204659, 'feature_fraction': 0.4818621749319611, 'min_gain_to_split': 0.26722603488616475, 'lambda_l1': 0.014976128349214921, 'lambda_l2': 0.031606876405659526}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 173.969 valid's l1: 252.399\n\n\n[I 2024-01-04 14:58:10,884] Trial 120 finished with value: 252.33506972689574 and parameters: {'num_leaves': 78, 'min_data_in_leaf': 17, 'max_bin': 201, 'bagging_fraction': 0.8799417652346818, 'feature_fraction': 0.4493176479372039, 'min_gain_to_split': 0.09232453265811677, 'lambda_l1': 0.013227586793373255, 'lambda_l2': 0.026681633961518215}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[532]   train's l1: 171.906 valid's l1: 252.335\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 185.739 valid's l1: 252.048\n\n\n[I 2024-01-04 14:58:13,611] Trial 121 finished with value: 251.93779447208152 and parameters: {'num_leaves': 66, 'min_data_in_leaf': 20, 'max_bin': 227, 'bagging_fraction': 0.8912065454892015, 'feature_fraction': 0.49518180663555805, 'min_gain_to_split': 0.66115149329422, 'lambda_l1': 0.014256157422869486, 'lambda_l2': 0.019592799848330764}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[478]   train's l1: 186.984 valid's l1: 251.938\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 179.823 valid's l1: 250.836\nEarly stopping, best iteration is:\n[645]   train's l1: 168.705 valid's l1: 250.318\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:58:16,963] Trial 122 finished with value: 250.3184260266444 and parameters: {'num_leaves': 71, 'min_data_in_leaf': 17, 'max_bin': 246, 'bagging_fraction': 0.8856662585794733, 'feature_fraction': 0.41426333445445673, 'min_gain_to_split': 0.14159917921156762, 'lambda_l1': 0.01710449923760607, 'lambda_l2': 0.03390651934738554}. Best is trial 94 with value: 247.49181840326082.\n\n\nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:58:19,347] Trial 123 finished with value: 250.58048650499765 and parameters: {'num_leaves': 76, 'min_data_in_leaf': 13, 'max_bin': 214, 'bagging_fraction': 0.8741219810629283, 'feature_fraction': 0.6199540927027839, 'min_gain_to_split': 0.5409259245908115, 'lambda_l1': 0.011138679614553391, 'lambda_l2': 0.042448003067620525}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[382]   train's l1: 184.046 valid's l1: 250.58\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 146.631 valid's l1: 251.168\nEarly stopping, best iteration is:\n[494]   train's l1: 146.698 valid's l1: 251.156\n\n\n[I 2024-01-04 14:58:23,209] Trial 124 finished with value: 251.156345783635 and parameters: {'num_leaves': 177, 'min_data_in_leaf': 16, 'max_bin': 253, 'bagging_fraction': 0.8663591160261164, 'feature_fraction': 0.4252727679922401, 'min_gain_to_split': 0.886215642549602, 'lambda_l1': 0.011856060263474055, 'lambda_l2': 0.025081767074579608}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:58:26,163] Trial 125 finished with value: 250.70174464613038 and parameters: {'num_leaves': 159, 'min_data_in_leaf': 19, 'max_bin': 210, 'bagging_fraction': 0.8754523452986147, 'feature_fraction': 0.5016741976399238, 'min_gain_to_split': 0.5972374036822362, 'lambda_l1': 0.07069957271733289, 'lambda_l2': 0.3076723263574069}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[286]   train's l1: 167.24  valid's l1: 250.702\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[373]   train's l1: 146.076 valid's l1: 253.981\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 14:58:30,338] Trial 126 finished with value: 253.98123584511205 and parameters: {'num_leaves': 169, 'min_data_in_leaf': 12, 'max_bin': 345, 'bagging_fraction': 0.9021341656245929, 'feature_fraction': 0.4872469747012896, 'min_gain_to_split': 0.4615866422450639, 'lambda_l1': 0.013128932203765296, 'lambda_l2': 0.029281769381026463}. Best is trial 94 with value: 247.49181840326082.\n\n\nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:58:33,302] Trial 127 finished with value: 251.73322051878546 and parameters: {'num_leaves': 184, 'min_data_in_leaf': 15, 'max_bin': 242, 'bagging_fraction': 0.8867448935326978, 'feature_fraction': 0.40100000314299333, 'min_gain_to_split': 0.31711641893590964, 'lambda_l1': 0.015958892358391527, 'lambda_l2': 0.5442760223597346}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[238]   train's l1: 171.315 valid's l1: 251.733\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 191.934 valid's l1: 252.094\n\n\n[I 2024-01-04 14:58:36,214] Trial 128 finished with value: 251.8550244115263 and parameters: {'num_leaves': 50, 'min_data_in_leaf': 14, 'max_bin': 368, 'bagging_fraction': 0.8830395649447882, 'feature_fraction': 0.45725640791524363, 'min_gain_to_split': 0.7779594608741716, 'lambda_l1': 0.01939431345674847, 'lambda_l2': 0.023070042166038165}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[640]   train's l1: 182.296 valid's l1: 251.855\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:58:40,550] Trial 129 finished with value: 250.3489383249454 and parameters: {'num_leaves': 163, 'min_data_in_leaf': 17, 'max_bin': 350, 'bagging_fraction': 0.8961279017621276, 'feature_fraction': 0.47666950695146726, 'min_gain_to_split': 0.3951391600499149, 'lambda_l1': 0.02730738329104792, 'lambda_l2': 0.03743874867294142}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[355]   train's l1: 155.962 valid's l1: 250.349\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 136.486 valid's l1: 250.308\n[1000]  train's l1: 115.75  valid's l1: 249.791\nDid not meet early stopping. Best iteration is:\n[984]   train's l1: 116.246 valid's l1: 249.705\n\n\n[I 2024-01-04 14:58:50,398] Trial 130 finished with value: 249.70454529457095 and parameters: {'num_leaves': 194, 'min_data_in_leaf': 18, 'max_bin': 394, 'bagging_fraction': 0.9468995726395042, 'feature_fraction': 0.5083167494864842, 'min_gain_to_split': 0.522132419684433, 'lambda_l1': 0.12186387070134558, 'lambda_l2': 0.012132931456878517}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 146.091 valid's l1: 249.181\nEarly stopping, best iteration is:\n[578]   train's l1: 141.518 valid's l1: 249.057\n\n\n[I 2024-01-04 14:58:55,591] Trial 131 finished with value: 249.0568104608024 and parameters: {'num_leaves': 140, 'min_data_in_leaf': 16, 'max_bin': 379, 'bagging_fraction': 0.8716479171578012, 'feature_fraction': 0.44163229766990203, 'min_gain_to_split': 0.3686851507962205, 'lambda_l1': 0.14201363587144775, 'lambda_l2': 0.03828096527977815}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 152.216 valid's l1: 250.679\nEarly stopping, best iteration is:\n[444]   train's l1: 156.347 valid's l1: 250.39\n\n\n[I 2024-01-04 14:58:59,629] Trial 132 finished with value: 250.39021067089718 and parameters: {'num_leaves': 151, 'min_data_in_leaf': 24, 'max_bin': 223, 'bagging_fraction': 0.8794984756229808, 'feature_fraction': 0.43372368110848847, 'min_gain_to_split': 0.2968552494738383, 'lambda_l1': 0.02164458735548742, 'lambda_l2': 0.03419775649166447}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:59:01,459] Trial 133 finished with value: 253.12675541828912 and parameters: {'num_leaves': 64, 'min_data_in_leaf': 5, 'max_bin': 264, 'bagging_fraction': 0.859826148843883, 'feature_fraction': 0.463914702687051, 'min_gain_to_split': 0.4127571261443004, 'lambda_l1': 0.031067818536872956, 'lambda_l2': 0.042127945816306564}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[298]   train's l1: 200.363 valid's l1: 253.127\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:59:04,493] Trial 134 finished with value: 251.48970802041677 and parameters: {'num_leaves': 146, 'min_data_in_leaf': 14, 'max_bin': 385, 'bagging_fraction': 0.9083907876891837, 'feature_fraction': 0.4464426287115145, 'min_gain_to_split': 0.6299470077811208, 'lambda_l1': 0.16431677455700813, 'lambda_l2': 0.05080738552398487}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[271]   train's l1: 168.782 valid's l1: 251.49\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 183.528 valid's l1: 251.912\nEarly stopping, best iteration is:\n[456]   train's l1: 187.744 valid's l1: 251.662\n\n\n[I 2024-01-04 14:59:06,785] Trial 135 finished with value: 251.66199693947357 and parameters: {'num_leaves': 59, 'min_data_in_leaf': 10, 'max_bin': 230, 'bagging_fraction': 0.8159783744186221, 'feature_fraction': 0.4701457470749345, 'min_gain_to_split': 0.26752907650799507, 'lambda_l1': 0.024731325480938156, 'lambda_l2': 0.11070581208604748}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 191.624 valid's l1: 250.327\nEarly stopping, best iteration is:\n[741]   train's l1: 177.037 valid's l1: 249.592\n\n\n[I 2024-01-04 14:59:10,245] Trial 136 finished with value: 249.59181223390823 and parameters: {'num_leaves': 54, 'min_data_in_leaf': 15, 'max_bin': 315, 'bagging_fraction': 0.8744662953658245, 'feature_fraction': 0.5898430056890517, 'min_gain_to_split': 0.6970655058067564, 'lambda_l1': 0.03640538118270948, 'lambda_l2': 0.03218995841501966}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 161.343 valid's l1: 248.092\n\n\n[I 2024-01-04 14:59:14,879] Trial 137 finished with value: 248.04317891774215 and parameters: {'num_leaves': 110, 'min_data_in_leaf': 13, 'max_bin': 206, 'bagging_fraction': 0.9388498320560991, 'feature_fraction': 0.48997252259036944, 'min_gain_to_split': 0.20533415473055577, 'lambda_l1': 0.018385812417700043, 'lambda_l2': 0.18319704155125865}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[538]   train's l1: 158.372 valid's l1: 248.043\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:59:16,613] Trial 138 finished with value: 251.11241676495854 and parameters: {'num_leaves': 81, 'min_data_in_leaf': 13, 'max_bin': 218, 'bagging_fraction': 0.9399345017522797, 'feature_fraction': 0.49270780626068994, 'min_gain_to_split': 0.1724596331518652, 'lambda_l1': 0.017719317741347402, 'lambda_l2': 0.20793951051345025}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[252]   train's l1: 201.008 valid's l1: 251.112\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 165.51  valid's l1: 250.944\n\n\n[I 2024-01-04 14:59:19,621] Trial 139 finished with value: 250.65538744949382 and parameters: {'num_leaves': 88, 'min_data_in_leaf': 11, 'max_bin': 206, 'bagging_fraction': 0.9338329976145481, 'feature_fraction': 0.5223251214444021, 'min_gain_to_split': 0.8111417072336111, 'lambda_l1': 0.01440043461759758, 'lambda_l2': 0.17354325943605908}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[459]   train's l1: 168.706 valid's l1: 250.655\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 158.278 valid's l1: 251.611\nEarly stopping, best iteration is:\n[875]   train's l1: 142.383 valid's l1: 251.047\n\n\n[I 2024-01-04 14:59:25,994] Trial 140 finished with value: 251.04698790041078 and parameters: {'num_leaves': 111, 'min_data_in_leaf': 16, 'max_bin': 203, 'bagging_fraction': 0.9383306874113676, 'feature_fraction': 0.4823068829767774, 'min_gain_to_split': 0.2247993963953675, 'lambda_l1': 0.020812298851880413, 'lambda_l2': 0.2612079944704039}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 165.396 valid's l1: 251.612\n[1000]  train's l1: 145.703 valid's l1: 250.665\nDid not meet early stopping. Best iteration is:\n[999]   train's l1: 145.739 valid's l1: 250.638\n\n\n[I 2024-01-04 14:59:33,621] Trial 141 finished with value: 250.63794108959837 and parameters: {'num_leaves': 119, 'min_data_in_leaf': 29, 'max_bin': 355, 'bagging_fraction': 0.9303409118130695, 'feature_fraction': 0.4999801388982349, 'min_gain_to_split': 0.36313047839269824, 'lambda_l1': 0.01027041769623813, 'lambda_l2': 0.02719528479802065}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:59:37,987] Trial 142 finished with value: 249.4343387284939 and parameters: {'num_leaves': 127, 'min_data_in_leaf': 12, 'max_bin': 212, 'bagging_fraction': 0.944176846427563, 'feature_fraction': 0.515370868306029, 'min_gain_to_split': 0.30899754839690746, 'lambda_l1': 0.01267576668159671, 'lambda_l2': 0.21914542207962828}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[384]   train's l1: 158.326 valid's l1: 249.434\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:59:41,140] Trial 143 finished with value: 249.519622286848 and parameters: {'num_leaves': 144, 'min_data_in_leaf': 13, 'max_bin': 235, 'bagging_fraction': 0.9356222169037198, 'feature_fraction': 0.4881638814692, 'min_gain_to_split': 0.20123019808522802, 'lambda_l1': 0.015606228310635234, 'lambda_l2': 0.15937319766646302}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[308]   train's l1: 167.825 valid's l1: 249.52\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:59:44,108] Trial 144 finished with value: 249.6094400918304 and parameters: {'num_leaves': 69, 'min_data_in_leaf': 17, 'max_bin': 308, 'bagging_fraction': 0.9414377552472726, 'feature_fraction': 0.4295146212348876, 'min_gain_to_split': 0.5014963474361674, 'lambda_l1': 0.0184368083580013, 'lambda_l2': 0.14169082239421313}. Best is trial 94 with value: 247.49181840326082.\n\n\n[500]   train's l1: 180.098 valid's l1: 249.856\nEarly stopping, best iteration is:\n[402]   train's l1: 187.586 valid's l1: 249.609\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:59:47,829] Trial 145 finished with value: 253.348192378396 and parameters: {'num_leaves': 114, 'min_data_in_leaf': 15, 'max_bin': 364, 'bagging_fraction': 0.8697071294788391, 'feature_fraction': 0.47543129731600536, 'min_gain_to_split': 0.02141374252546936, 'lambda_l1': 0.023046207287368385, 'lambda_l2': 0.3392168782583177}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[353]   train's l1: 171.498 valid's l1: 253.348\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 180.704 valid's l1: 250.635\n\n\n[I 2024-01-04 14:59:51,272] Trial 146 finished with value: 250.58263956146595 and parameters: {'num_leaves': 73, 'min_data_in_leaf': 14, 'max_bin': 332, 'bagging_fraction': 0.8825892078043773, 'feature_fraction': 0.39487563570667117, 'min_gain_to_split': 0.03481408114280661, 'lambda_l1': 0.032805767852574205, 'lambda_l2': 0.018282475672863605}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[501]   train's l1: 180.565 valid's l1: 250.583\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 14:59:54,468] Trial 147 finished with value: 248.6428665297089 and parameters: {'num_leaves': 94, 'min_data_in_leaf': 16, 'max_bin': 373, 'bagging_fraction': 0.8667352620566177, 'feature_fraction': 0.4962524550779911, 'min_gain_to_split': 0.3460283135263791, 'lambda_l1': 0.026925711296214096, 'lambda_l2': 0.07612310865459707}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[353]   train's l1: 183.466 valid's l1: 248.643\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 166.065 valid's l1: 250.332\nEarly stopping, best iteration is:\n[412]   train's l1: 173.583 valid's l1: 249.803\n\n\n[I 2024-01-04 14:59:58,054] Trial 148 finished with value: 249.802555289632 and parameters: {'num_leaves': 93, 'min_data_in_leaf': 18, 'max_bin': 376, 'bagging_fraction': 0.8774818477754576, 'feature_fraction': 0.5039614566571933, 'min_gain_to_split': 0.2665979586440173, 'lambda_l1': 0.25187933425938064, 'lambda_l2': 0.08167465490197161}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:00:01,726] Trial 149 finished with value: 250.89565827055307 and parameters: {'num_leaves': 102, 'min_data_in_leaf': 16, 'max_bin': 370, 'bagging_fraction': 0.9001809750486183, 'feature_fraction': 0.4949558792477762, 'min_gain_to_split': 0.5685325737610398, 'lambda_l1': 0.019952766315230745, 'lambda_l2': 0.07767854391844496}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[375]   train's l1: 172.092 valid's l1: 250.896\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 179.896 valid's l1: 250.144\nEarly stopping, best iteration is:\n[849]   train's l1: 163.198 valid's l1: 249.627\n\n\n[I 2024-01-04 15:00:07,185] Trial 150 finished with value: 249.6273418243545 and parameters: {'num_leaves': 68, 'min_data_in_leaf': 17, 'max_bin': 359, 'bagging_fraction': 0.892165092143445, 'feature_fraction': 0.5095799829793023, 'min_gain_to_split': 0.08411491275213864, 'lambda_l1': 0.01665924566100667, 'lambda_l2': 0.06104710947685137}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:00:10,121] Trial 151 finished with value: 247.77318003627045 and parameters: {'num_leaves': 86, 'min_data_in_leaf': 16, 'max_bin': 383, 'bagging_fraction': 0.8649979641364223, 'feature_fraction': 0.4817486670392335, 'min_gain_to_split': 0.351661759777778, 'lambda_l1': 0.02814569393783445, 'lambda_l2': 0.09115484734822345}. Best is trial 94 with value: 247.49181840326082.\n\n\n[500]   train's l1: 168.764 valid's l1: 247.863\nEarly stopping, best iteration is:\n[407]   train's l1: 177.174 valid's l1: 247.773\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 164.541 valid's l1: 250.733\nEarly stopping, best iteration is:\n[624]   train's l1: 156.286 valid's l1: 250.499\n\n\n[I 2024-01-04 15:00:15,630] Trial 152 finished with value: 250.49908468171873 and parameters: {'num_leaves': 98, 'min_data_in_leaf': 16, 'max_bin': 381, 'bagging_fraction': 0.8630553645033838, 'feature_fraction': 0.4805903168993125, 'min_gain_to_split': 0.3374075480542306, 'lambda_l1': 0.027314088145799657, 'lambda_l2': 0.10356683995186587}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 164.098 valid's l1: 250.502\nEarly stopping, best iteration is:\n[840]   train's l1: 147.879 valid's l1: 249.889\n\n\n[I 2024-01-04 15:00:23,097] Trial 153 finished with value: 249.8886119633343 and parameters: {'num_leaves': 94, 'min_data_in_leaf': 15, 'max_bin': 375, 'bagging_fraction': 0.8657861100776019, 'feature_fraction': 0.4888735972189435, 'min_gain_to_split': 0.10503290156593435, 'lambda_l1': 0.025962264908394695, 'lambda_l2': 0.0924860553034291}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 170.995 valid's l1: 251.975\nEarly stopping, best iteration is:\n[760]   train's l1: 156.306 valid's l1: 251.498\n\n\n[I 2024-01-04 15:00:28,118] Trial 154 finished with value: 251.49764437023114 and parameters: {'num_leaves': 87, 'min_data_in_leaf': 17, 'max_bin': 205, 'bagging_fraction': 0.9379240167334084, 'feature_fraction': 0.49769888464017153, 'min_gain_to_split': 0.39254025151307914, 'lambda_l1': 0.19182087476064508, 'lambda_l2': 0.06794233230323202}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 170.96  valid's l1: 250.405\nEarly stopping, best iteration is:\n[435]   train's l1: 175.308 valid's l1: 250.251\n\n\n[I 2024-01-04 15:00:31,648] Trial 155 finished with value: 250.25095018895524 and parameters: {'num_leaves': 85, 'min_data_in_leaf': 18, 'max_bin': 384, 'bagging_fraction': 0.8571257419293183, 'feature_fraction': 0.4714651356386285, 'min_gain_to_split': 0.24506491770932454, 'lambda_l1': 0.024394636517839432, 'lambda_l2': 0.09092915543510231}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 173.723 valid's l1: 250.295\n\n\n[I 2024-01-04 15:00:35,941] Trial 156 finished with value: 249.7346598592207 and parameters: {'num_leaves': 82, 'min_data_in_leaf': 16, 'max_bin': 367, 'bagging_fraction': 0.851895854933342, 'feature_fraction': 0.48437907108220346, 'min_gain_to_split': 0.44528663784812805, 'lambda_l1': 0.02900524857257388, 'lambda_l2': 0.13155925117538822}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[576]   train's l1: 168.439 valid's l1: 249.735\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 184.577 valid's l1: 251.039\nEarly stopping, best iteration is:\n[778]   train's l1: 171.604 valid's l1: 250.211\n\n\n[I 2024-01-04 15:00:45,955] Trial 157 finished with value: 250.21060309985526 and parameters: {'num_leaves': 62, 'min_data_in_leaf': 19, 'max_bin': 343, 'bagging_fraction': 0.8600772973257325, 'feature_fraction': 0.6079115426268268, 'min_gain_to_split': 0.11899903724164437, 'lambda_l1': 0.01377110562969109, 'lambda_l2': 0.08547931805343581}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:00:50,602] Trial 158 finished with value: 252.7129997464329 and parameters: {'num_leaves': 108, 'min_data_in_leaf': 14, 'max_bin': 325, 'bagging_fraction': 0.8683469649298952, 'feature_fraction': 0.46528497408610425, 'min_gain_to_split': 0.2882774418466176, 'lambda_l1': 0.018431108808647196, 'lambda_l2': 0.06956498909470431}. Best is trial 94 with value: 247.49181840326082.\n\n\nEarly stopping, best iteration is:\n[380]   train's l1: 168.748 valid's l1: 252.713\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 167.875 valid's l1: 250.731\nEarly stopping, best iteration is:\n[429]   train's l1: 173.261 valid's l1: 250.494\n\n\n[I 2024-01-04 15:00:54,115] Trial 159 finished with value: 250.49388535418566 and parameters: {'num_leaves': 90, 'min_data_in_leaf': 15, 'max_bin': 390, 'bagging_fraction': 0.9478651474465936, 'feature_fraction': 0.4786896805191647, 'min_gain_to_split': 0.3511820261157441, 'lambda_l1': 0.011952526576707433, 'lambda_l2': 0.11554711078243335}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 186.952 valid's l1: 251.057\nEarly stopping, best iteration is:\n[662]   train's l1: 179.965 valid's l1: 250.805\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 15:00:57,029] Trial 160 finished with value: 250.80496353100466 and parameters: {'num_leaves': 64, 'min_data_in_leaf': 22, 'max_bin': 386, 'bagging_fraction': 0.865013315102003, 'feature_fraction': 0.4914437229742247, 'min_gain_to_split': 0.887483072136835, 'lambda_l1': 0.16095667839558572, 'lambda_l2': 0.020922945252227043}. Best is trial 94 with value: 247.49181840326082.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 150.129 valid's l1: 250.673\nEarly stopping, best iteration is:\n[511]   train's l1: 149.49  valid's l1: 250.635\n\n\n[I 2024-01-04 15:01:02,310] Trial 161 finished with value: 250.6346992473666 and parameters: {'num_leaves': 133, 'min_data_in_leaf': 17, 'max_bin': 374, 'bagging_fraction': 0.8720273846658164, 'feature_fraction': 0.41226036056326626, 'min_gain_to_split': 0.41644705957076983, 'lambda_l1': 0.03877136764806393, 'lambda_l2': 0.01641885293428038}. Best is trial 94 with value: 247.49181840326082.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:01:06,010] Trial 162 finished with value: 247.42572016119635 and parameters: {'num_leaves': 138, 'min_data_in_leaf': 16, 'max_bin': 285, 'bagging_fraction': 0.8768652682558914, 'feature_fraction': 0.4597140389171417, 'min_gain_to_split': 0.3170777189102552, 'lambda_l1': 0.03396002472637102, 'lambda_l2': 0.014071960871027224}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[394]   train's l1: 157.386 valid's l1: 247.426\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[377]   train's l1: 155.874 valid's l1: 250.58\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 15:01:09,722] Trial 163 finished with value: 250.5795625622893 and parameters: {'num_leaves': 154, 'min_data_in_leaf': 16, 'max_bin': 283, 'bagging_fraction': 0.8756242802475784, 'feature_fraction': 0.4574107618784809, 'min_gain_to_split': 0.3096839426499339, 'lambda_l1': 0.04737132196868192, 'lambda_l2': 0.013877657975683215}. Best is trial 162 with value: 247.42572016119635.\n\n\nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:01:12,783] Trial 164 finished with value: 250.36395056827382 and parameters: {'num_leaves': 137, 'min_data_in_leaf': 17, 'max_bin': 200, 'bagging_fraction': 0.8467501024159305, 'feature_fraction': 0.470569236894148, 'min_gain_to_split': 0.34165311839482404, 'lambda_l1': 0.03451910677720636, 'lambda_l2': 0.01248230520911566}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[326]   train's l1: 171.471 valid's l1: 250.364\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:01:16,905] Trial 165 finished with value: 251.41787440494195 and parameters: {'num_leaves': 142, 'min_data_in_leaf': 15, 'max_bin': 395, 'bagging_fraction': 0.8872550847984648, 'feature_fraction': 0.4638860418732246, 'min_gain_to_split': 0.662803731918857, 'lambda_l1': 0.028792708301565273, 'lambda_l2': 0.014742471644502387}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[359]   train's l1: 160.968 valid's l1: 251.418\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:01:21,382] Trial 166 finished with value: 250.39376034507342 and parameters: {'num_leaves': 147, 'min_data_in_leaf': 18, 'max_bin': 381, 'bagging_fraction': 0.8812313803002702, 'feature_fraction': 0.4767002352919301, 'min_gain_to_split': 0.03115257997439445, 'lambda_l1': 0.021456414840317704, 'lambda_l2': 0.017285738813038423}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[343]   train's l1: 162.312 valid's l1: 250.394\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 172.241 valid's l1: 252.088\nEarly stopping, best iteration is:\n[858]   train's l1: 153.401 valid's l1: 251.351\n\n\n[I 2024-01-04 15:01:26,706] Trial 167 finished with value: 251.3509610837484 and parameters: {'num_leaves': 77, 'min_data_in_leaf': 13, 'max_bin': 216, 'bagging_fraction': 0.8780850319574509, 'feature_fraction': 0.48392955774606733, 'min_gain_to_split': 0.3797765825536852, 'lambda_l1': 0.6067639062950719, 'lambda_l2': 0.02398566835684493}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:01:30,610] Trial 168 finished with value: 249.73229539891614 and parameters: {'num_leaves': 151, 'min_data_in_leaf': 14, 'max_bin': 299, 'bagging_fraction': 0.9319677420827358, 'feature_fraction': 0.6498081331144286, 'min_gain_to_split': 0.7261892887659579, 'lambda_l1': 0.022608388450554528, 'lambda_l2': 0.02011551619134166}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[365]   train's l1: 154.769 valid's l1: 249.732\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 173.416 valid's l1: 251.747\n\n\n[I 2024-01-04 15:01:33,999] Trial 169 finished with value: 251.6314875794779 and parameters: {'num_leaves': 80, 'min_data_in_leaf': 16, 'max_bin': 277, 'bagging_fraction': 0.94396934276416, 'feature_fraction': 0.5040481953340915, 'min_gain_to_split': 0.04216889260923159, 'lambda_l1': 0.015114823160552807, 'lambda_l2': 0.10533610454431483}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[526]   train's l1: 172.382 valid's l1: 251.631\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:01:37,504] Trial 170 finished with value: 251.80344351954858 and parameters: {'num_leaves': 149, 'min_data_in_leaf': 15, 'max_bin': 337, 'bagging_fraction': 0.9256980574945003, 'feature_fraction': 0.45260591501703173, 'min_gain_to_split': 0.014424045320988075, 'lambda_l1': 0.043912975596851936, 'lambda_l2': 0.05583931970799901}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[331]   train's l1: 158.753 valid's l1: 251.803\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 151.655 valid's l1: 249.999\nEarly stopping, best iteration is:\n[452]   train's l1: 154.989 valid's l1: 249.905\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 15:01:41,339] Trial 171 finished with value: 249.90469666568038 and parameters: {'num_leaves': 127, 'min_data_in_leaf': 16, 'max_bin': 295, 'bagging_fraction': 0.869393745428101, 'feature_fraction': 0.44609833626291506, 'min_gain_to_split': 0.278571263544134, 'lambda_l1': 0.03047574275291401, 'lambda_l2': 0.02974275735067851}. Best is trial 162 with value: 247.42572016119635.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 156.71  valid's l1: 251.002\nEarly stopping, best iteration is:\n[405]   train's l1: 164.421 valid's l1: 250.826\n\n\n[I 2024-01-04 15:01:44,974] Trial 172 finished with value: 250.8258421423467 and parameters: {'num_leaves': 131, 'min_data_in_leaf': 17, 'max_bin': 252, 'bagging_fraction': 0.8720934112381963, 'feature_fraction': 0.423920384528167, 'min_gain_to_split': 0.4506275263478653, 'lambda_l1': 0.026239793128009673, 'lambda_l2': 0.03615472414875372}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:01:47,527] Trial 173 finished with value: 251.60834782058734 and parameters: {'num_leaves': 125, 'min_data_in_leaf': 12, 'max_bin': 271, 'bagging_fraction': 0.8667782188780679, 'feature_fraction': 0.46031501106957684, 'min_gain_to_split': 0.33368392238331, 'lambda_l1': 0.03717294315876236, 'lambda_l2': 0.040720924934878354}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[251]   train's l1: 182.115 valid's l1: 251.608\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:01:50,707] Trial 174 finished with value: 250.31464539939657 and parameters: {'num_leaves': 139, 'min_data_in_leaf': 14, 'max_bin': 287, 'bagging_fraction': 0.8744188799228743, 'feature_fraction': 0.4380410991005637, 'min_gain_to_split': 0.4058388426704497, 'lambda_l1': 0.032269544211758906, 'lambda_l2': 0.03241309448555329}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[311]   train's l1: 167.793 valid's l1: 250.315\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 136.601 valid's l1: 249.97\nEarly stopping, best iteration is:\n[494]   train's l1: 136.993 valid's l1: 249.927\n\n\n[I 2024-01-04 15:01:56,237] Trial 175 finished with value: 249.92682837396518 and parameters: {'num_leaves': 188, 'min_data_in_leaf': 16, 'max_bin': 290, 'bagging_fraction': 0.8842697717978859, 'feature_fraction': 0.4922924762633439, 'min_gain_to_split': 0.0723306661113504, 'lambda_l1': 0.02452181367063122, 'lambda_l2': 0.046760502570766756}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 151.722 valid's l1: 250.534\n\n\n[I 2024-01-04 15:02:00,404] Trial 176 finished with value: 250.46833795295862 and parameters: {'num_leaves': 122, 'min_data_in_leaf': 13, 'max_bin': 349, 'bagging_fraction': 0.8802624646096726, 'feature_fraction': 0.45376253449778703, 'min_gain_to_split': 0.3701003797800777, 'lambda_l1': 0.03351211577987117, 'lambda_l2': 0.011153115787596675}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[466]   train's l1: 154.349 valid's l1: 250.468\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 157.854 valid's l1: 250.558\nEarly stopping, best iteration is:\n[461]   train's l1: 160.596 valid's l1: 250.445\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 15:02:04,443] Trial 177 finished with value: 250.44483954285164 and parameters: {'num_leaves': 117, 'min_data_in_leaf': 15, 'max_bin': 304, 'bagging_fraction': 0.9406817651593791, 'feature_fraction': 0.5179569791694056, 'min_gain_to_split': 0.30736306842420713, 'lambda_l1': 0.041555018778770464, 'lambda_l2': 0.027318118098379234}. Best is trial 162 with value: 247.42572016119635.\n\n\nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:02:08,257] Trial 178 finished with value: 250.24498760231444 and parameters: {'num_leaves': 134, 'min_data_in_leaf': 17, 'max_bin': 388, 'bagging_fraction': 0.8629062737631982, 'feature_fraction': 0.49746095558774456, 'min_gain_to_split': 0.5864984672416876, 'lambda_l1': 0.016438117014245503, 'lambda_l2': 0.07400927606429505}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[393]   train's l1: 161.512 valid's l1: 250.245\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 182.806 valid's l1: 249.63\n\n\n[I 2024-01-04 15:02:11,080] Trial 179 finished with value: 249.58605700447265 and parameters: {'num_leaves': 71, 'min_data_in_leaf': 23, 'max_bin': 230, 'bagging_fraction': 0.8885125560499693, 'feature_fraction': 0.48487273414822657, 'min_gain_to_split': 0.5028568570016486, 'lambda_l1': 0.020248026119897348, 'lambda_l2': 0.1833465216422022}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[482]   train's l1: 183.701 valid's l1: 249.586\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 156.078 valid's l1: 250.398\nEarly stopping, best iteration is:\n[469]   train's l1: 158.785 valid's l1: 250.24\n\n\n[I 2024-01-04 15:02:15,168] Trial 180 finished with value: 250.23992591808954 and parameters: {'num_leaves': 144, 'min_data_in_leaf': 27, 'max_bin': 225, 'bagging_fraction': 0.9362066123650771, 'feature_fraction': 0.4710387790897302, 'min_gain_to_split': 0.6273631306040175, 'lambda_l1': 0.027854099893134494, 'lambda_l2': 0.014070845671853495}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 175.287 valid's l1: 251.082\nEarly stopping, best iteration is:\n[419]   train's l1: 181.95  valid's l1: 250.81\n\n\n[I 2024-01-04 15:02:18,000] Trial 181 finished with value: 250.80951406400357 and parameters: {'num_leaves': 75, 'min_data_in_leaf': 15, 'max_bin': 363, 'bagging_fraction': 0.9249513761759067, 'feature_fraction': 0.5727202666889106, 'min_gain_to_split': 0.1601799824052984, 'lambda_l1': 0.10408252777643323, 'lambda_l2': 0.7439220553317485}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 167.988 valid's l1: 249.508\n\n\n[I 2024-01-04 15:02:21,881] Trial 182 finished with value: 249.3110631129208 and parameters: {'num_leaves': 85, 'min_data_in_leaf': 14, 'max_bin': 355, 'bagging_fraction': 0.921583651147675, 'feature_fraction': 0.4783214365441769, 'min_gain_to_split': 0.19039916385246033, 'lambda_l1': 0.17138302120704588, 'lambda_l2': 0.9265749020309945}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[600]   train's l1: 162.44  valid's l1: 249.311\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 169.917 valid's l1: 249.168\n\n\n[I 2024-01-04 15:02:25,686] Trial 183 finished with value: 249.06582831940312 and parameters: {'num_leaves': 83, 'min_data_in_leaf': 16, 'max_bin': 371, 'bagging_fraction': 0.9333184089439035, 'feature_fraction': 0.6308504811107408, 'min_gain_to_split': 0.13224038945657687, 'lambda_l1': 0.1473648567032606, 'lambda_l2': 0.6216842099264523}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[526]   train's l1: 167.992 valid's l1: 249.066\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 183.188 valid's l1: 249.953\nEarly stopping, best iteration is:\n[761]   train's l1: 168.11  valid's l1: 249.467\n\n\n[I 2024-01-04 15:02:30,760] Trial 184 finished with value: 249.4671541920374 and parameters: {'num_leaves': 66, 'min_data_in_leaf': 18, 'max_bin': 360, 'bagging_fraction': 0.9156317811266667, 'feature_fraction': 0.4735128933525085, 'min_gain_to_split': 0.21130356951389268, 'lambda_l1': 0.20114994165231154, 'lambda_l2': 0.0392658876765995}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 173.998 valid's l1: 252.76\nEarly stopping, best iteration is:\n[443]   train's l1: 178.387 valid's l1: 252.359\n\n\n[I 2024-01-04 15:02:33,637] Trial 185 finished with value: 252.35905337278493 and parameters: {'num_leaves': 86, 'min_data_in_leaf': 15, 'max_bin': 243, 'bagging_fraction': 0.9275778013384998, 'feature_fraction': 0.46651952414332837, 'min_gain_to_split': 0.18710382166239822, 'lambda_l1': 0.21812980592938896, 'lambda_l2': 0.015624708091111892}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 178.136 valid's l1: 249.352\nEarly stopping, best iteration is:\n[645]   train's l1: 169.121 valid's l1: 248.932\n\n\n[I 2024-01-04 15:02:37,152] Trial 186 finished with value: 248.93206619004684 and parameters: {'num_leaves': 79, 'min_data_in_leaf': 17, 'max_bin': 280, 'bagging_fraction': 0.9301140124271376, 'feature_fraction': 0.4865963502082907, 'min_gain_to_split': 0.7538162508538646, 'lambda_l1': 0.23606121292562693, 'lambda_l2': 0.4321523615935931}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 164.602 valid's l1: 251.674\nEarly stopping, best iteration is:\n[582]   train's l1: 160.224 valid's l1: 251.476\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 15:02:41,114] Trial 187 finished with value: 251.4762996175302 and parameters: {'num_leaves': 95, 'min_data_in_leaf': 15, 'max_bin': 377, 'bagging_fraction': 0.8779205992752549, 'feature_fraction': 0.4604436612977852, 'min_gain_to_split': 0.2661693882507861, 'lambda_l1': 0.0133519174276913, 'lambda_l2': 0.03522393535970835}. Best is trial 162 with value: 247.42572016119635.\n\n\nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:02:43,624] Trial 188 finished with value: 249.88097802940015 and parameters: {'num_leaves': 91, 'min_data_in_leaf': 16, 'max_bin': 366, 'bagging_fraction': 0.9389526807790968, 'feature_fraction': 0.4793374229049171, 'min_gain_to_split': 0.3590274929965887, 'lambda_l1': 0.062439027824047075, 'lambda_l2': 0.797900317930234}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[336]   train's l1: 181.957 valid's l1: 249.881\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 145.384 valid's l1: 250.642\nEarly stopping, best iteration is:\n[492]   train's l1: 146.387 valid's l1: 250.525\n\n\n[I 2024-01-04 15:02:49,308] Trial 189 finished with value: 250.52539819943138 and parameters: {'num_leaves': 142, 'min_data_in_leaf': 14, 'max_bin': 238, 'bagging_fraction': 0.9123231583100857, 'feature_fraction': 0.5073354200353931, 'min_gain_to_split': 0.23020703481970808, 'lambda_l1': 0.13810195568699196, 'lambda_l2': 0.6695510774298393}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\nEarly stopping, best iteration is:\n[335]   train's l1: 155.849 valid's l1: 249.39\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 15:02:53,828] Trial 190 finished with value: 249.39028250099406 and parameters: {'num_leaves': 156, 'min_data_in_leaf': 13, 'max_bin': 267, 'bagging_fraction': 0.8684131697548491, 'feature_fraction': 0.46848449077476256, 'min_gain_to_split': 0.31658678365111476, 'lambda_l1': 0.18903977378492717, 'lambda_l2': 0.5507867349522573}. Best is trial 162 with value: 247.42572016119635.\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 175.551 valid's l1: 249.713\nEarly stopping, best iteration is:\n[739]   train's l1: 163.099 valid's l1: 249.008\n\n\n[I 2024-01-04 15:02:58,247] Trial 191 finished with value: 249.00789052000718 and parameters: {'num_leaves': 80, 'min_data_in_leaf': 17, 'max_bin': 279, 'bagging_fraction': 0.9310167816866709, 'feature_fraction': 0.4904280251633335, 'min_gain_to_split': 0.7580540499329507, 'lambda_l1': 0.12513113755960384, 'lambda_l2': 0.38557651716229485}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 177.762 valid's l1: 250.622\nEarly stopping, best iteration is:\n[594]   train's l1: 172.027 valid's l1: 250.431\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n[I 2024-01-04 15:03:01,820] Trial 192 finished with value: 250.43111074969602 and parameters: {'num_leaves': 75, 'min_data_in_leaf': 17, 'max_bin': 329, 'bagging_fraction': 0.9360828660144469, 'feature_fraction': 0.4861984151919111, 'min_gain_to_split': 0.8308654187255212, 'lambda_l1': 0.01822501444287623, 'lambda_l2': 0.31333436083047017}. Best is trial 162 with value: 247.42572016119635.\n\n\nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:03:04,419] Trial 193 finished with value: 250.01651084320216 and parameters: {'num_leaves': 77, 'min_data_in_leaf': 18, 'max_bin': 399, 'bagging_fraction': 0.9281731513142994, 'feature_fraction': 0.49845904855397394, 'min_gain_to_split': 0.7024992822543622, 'lambda_l1': 0.3049378543260431, 'lambda_l2': 0.477868250720999}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[343]   train's l1: 191.068 valid's l1: 250.017\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 175.095 valid's l1: 249.34\n\n\n[I 2024-01-04 15:03:07,666] Trial 194 finished with value: 249.31692753970492 and parameters: {'num_leaves': 83, 'min_data_in_leaf': 16, 'max_bin': 273, 'bagging_fraction': 0.9422046155386867, 'feature_fraction': 0.4743056494843865, 'min_gain_to_split': 0.7634907169090223, 'lambda_l1': 0.22763791360094718, 'lambda_l2': 0.02996993440272179}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[509]   train's l1: 173.953 valid's l1: 249.317\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 182.756 valid's l1: 251.889\n[1000]  train's l1: 158.838 valid's l1: 251.107\nDid not meet early stopping. Best iteration is:\n[977]   train's l1: 159.608 valid's l1: 251.037\n\n\n[I 2024-01-04 15:03:11,815] Trial 195 finished with value: 251.0370325059507 and parameters: {'num_leaves': 68, 'min_data_in_leaf': 16, 'max_bin': 210, 'bagging_fraction': 0.9193624363745587, 'feature_fraction': 0.4181099510827295, 'min_gain_to_split': 0.6406588831707636, 'lambda_l1': 0.1590616308434512, 'lambda_l2': 0.013163352154112367}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 162.653 valid's l1: 248.853\nEarly stopping, best iteration is:\n[437]   train's l1: 163.86  valid's l1: 248.779\n\n\n[I 2024-01-04 15:03:15,265] Trial 196 finished with value: 248.77945517444144 and parameters: {'num_leaves': 136, 'min_data_in_leaf': 17, 'max_bin': 257, 'bagging_fraction': 0.8737106207282408, 'feature_fraction': 0.5249507724632081, 'min_gain_to_split': 0.9302512400158699, 'lambda_l1': 0.17998166233627505, 'lambda_l2': 0.42774724795455626}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n\n\n[I 2024-01-04 15:03:18,018] Trial 197 finished with value: 249.77307540512996 and parameters: {'num_leaves': 137, 'min_data_in_leaf': 19, 'max_bin': 372, 'bagging_fraction': 0.8725676496740775, 'feature_fraction': 0.5333837894185348, 'min_gain_to_split': 0.9852238571189288, 'lambda_l1': 0.011093601851032744, 'lambda_l2': 0.018685908076062276}. Best is trial 162 with value: 247.42572016119635.\n\n\nEarly stopping, best iteration is:\n[343]   train's l1: 173.275 valid's l1: 249.773\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 149.624 valid's l1: 248.989\nEarly stopping, best iteration is:\n[466]   train's l1: 152.368 valid's l1: 248.889\n\n\n[I 2024-01-04 15:03:22,177] Trial 198 finished with value: 248.88932167060125 and parameters: {'num_leaves': 135, 'min_data_in_leaf': 17, 'max_bin': 364, 'bagging_fraction': 0.8737174161802679, 'feature_fraction': 0.524724766009316, 'min_gain_to_split': 0.42620809044511215, 'lambda_l1': 0.1768223455419468, 'lambda_l2': 0.2710478694991913}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 153.934 valid's l1: 249.053\nEarly stopping, best iteration is:\n[630]   train's l1: 146.37  valid's l1: 248.886\n\n\n[I 2024-01-04 15:03:27,054] Trial 199 finished with value: 248.88613639666747 and parameters: {'num_leaves': 132, 'min_data_in_leaf': 18, 'max_bin': 262, 'bagging_fraction': 0.8772487349760786, 'feature_fraction': 0.529690879192091, 'min_gain_to_split': 0.4057664939875361, 'lambda_l1': 0.18573851308642703, 'lambda_l2': 0.3267069376843893}. Best is trial 162 with value: 247.42572016119635.\n\n\n- * - - * - - * - - * - - * - - * - - * - - * - - * - - * - \n\n\n\n\nCode\nfrom pprint import pprint\ntrial = study.best_trial\npprint(f\"trial {trial.number}\")\npprint(f\"MAE best: {trial.value:.4f}\")\npprint(trial.params)\n\n\n'trial 162'\n'MAE best: 247.4257'\n{'bagging_fraction': 0.8768652682558914,\n 'feature_fraction': 0.4597140389171417,\n 'lambda_l1': 0.03396002472637102,\n 'lambda_l2': 0.014071960871027224,\n 'max_bin': 285,\n 'min_data_in_leaf': 16,\n 'min_gain_to_split': 0.3170777189102552,\n 'num_leaves': 138}\n\n\nこれを使ってもう一度学習しなおすことで大丈夫です。\n\n\nCode\noptuna.visualization.plot_param_importances(study).show()\n\n\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nFile c:\\pyenv\\py311\\Lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\n      6 with try_import() as _imports:\n----&gt; 7     import plotly\n      8     from plotly import __version__ as plotly_version\n\nModuleNotFoundError: No module named 'plotly'\n\nThe above exception was the direct cause of the following exception:\n\nImportError                               Traceback (most recent call last)\nCell In[82], line 1\n----&gt; 1 optuna.visualization.plot_param_importances(study).show()\n\nFile c:\\pyenv\\py311\\Lib\\site-packages\\optuna\\visualization\\_param_importances.py:190, in plot_param_importances(study, evaluator, params, target, target_name)\n    113 def plot_param_importances(\n    114     study: Study,\n    115     evaluator: BaseImportanceEvaluator | None = None,\n   (...)\n    119     target_name: str = \"Objective Value\",\n    120 ) -&gt; \"go.Figure\":\n    121     \"\"\"Plot hyperparameter importances.\n    122 \n    123     Example:\n   (...)\n    187         A :class:`plotly.graph_objs.Figure` object.\n    188     \"\"\"\n--&gt; 190     _imports.check()\n    191     importances_infos = _get_importances_infos(study, evaluator, params, target, target_name)\n    192     return _get_importances_plot(importances_infos, study)\n\nFile c:\\pyenv\\py311\\Lib\\site-packages\\optuna\\_imports.py:89, in _DeferredImportExceptionContextManager.check(self)\n     87 if self._deferred is not None:\n     88     exc_value, message = self._deferred\n---&gt; 89     raise ImportError(message) from exc_value\n\nImportError: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.\n\n\n\n\n\nCode\nimport joblib\njoblib.dump(trial, \"best-trial.joblib\")\n\n\n['best-trial.joblib']\n\n\n最適なパラメータで再学習をおこなう.\n\n\nCode\nparams = joblib.load(\"best-trial.joblib\").params\nparams.update(params_base)\n\nX_tr, X_va, y_tr, y_va = train_test_split(X_train, y_train, test_size = .2, shuffle = True, )\n\nlgb_train = lgb.Dataset(X_tr, y_tr)\nlgb_eval = lgb.Dataset(X_va, y_va, reference=lgb_train)\nmodel = lgb.train(\n    params, \n    lgb_train, \n    num_boost_round=10_000,\n    valid_sets = [lgb_train, lgb_eval], \n    valid_names = [\"train\", \"valid\"], \n    callbacks = [lgb.early_stopping(100), lgb.log_evaluation(500)], \n\n)\ny_va_pred = model.predict(\n    X_va, \n    num_iteration=model.best_iteration\n)\nscore = mean_absolute_error(y_va, y_va_pred)\nprint(f\"MAE valid: {score: .2f}\")\n\n\nTraining until validation scores don't improve for 100 rounds\n[500]   train's l1: 210.836 valid's l1: 254.237\n[1000]  train's l1: 183.016 valid's l1: 251.676\n[1500]  train's l1: 166.283 valid's l1: 250.733\n[2000]  train's l1: 154.341 valid's l1: 250.346\nEarly stopping, best iteration is:\n[1947]  train's l1: 155.381 valid's l1: 250.313\nMAE valid:  250.31\n\n\n\n\nCode\ny_test_pred = model.predict(X_test, num_iteration=model.best_iteration)\nscore = mean_absolute_error(y_test, y_test_pred)\nprint(f\"MAE valid: {score: .2f}\")\n\n\nMAE valid:  242.05",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl02_データ理解.html",
    "href": "contents/books/08_handbook_lightgbm/cl02_データ理解.html",
    "title": "Setup",
    "section": "",
    "text": "Code\nimport importlib.util\nimport os\n\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    work_dirctory = '/content/drive/Othercomputers/LetsNoteSilver'\n    os.chdir(work_dirctory)\n    print(\"mounted\")\nexcept ModuleNotFoundError as e:\n    print(\"not mounted\")\n\n\nnot mounted\nCode\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\nCode\npd.set_option('display.float_format', '{:.3f}'.format)",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "Setup"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#dataset",
    "href": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#dataset",
    "title": "Setup",
    "section": "0.1 dataset",
    "text": "0.1 dataset\n\n\nCode\ncolumns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\ndf = pd.read_csv(\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n    header=None,\n    names=columns,\n    sep=\"\\s+\")\ndf.head()\n\n\n&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:6: SyntaxWarning: invalid escape sequence '\\s'\nC:\\Users\\suzuk\\AppData\\Local\\Temp\\ipykernel_26716\\1922502560.py:6: SyntaxWarning: invalid escape sequence '\\s'\n  sep=\"\\s+\")\n\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nB\nLSTAT\nMEDV\n\n\n\n\n0\n0.006\n18.000\n2.310\n0\n0.538\n6.575\n65.200\n4.090\n1\n296.000\n15.300\n396.900\n4.980\n24.000\n\n\n1\n0.027\n0.000\n7.070\n0\n0.469\n6.421\n78.900\n4.967\n2\n242.000\n17.800\n396.900\n9.140\n21.600\n\n\n2\n0.027\n0.000\n7.070\n0\n0.469\n7.185\n61.100\n4.967\n2\n242.000\n17.800\n392.830\n4.030\n34.700\n\n\n3\n0.032\n0.000\n2.180\n0\n0.458\n6.998\n45.800\n6.062\n3\n222.000\n18.700\n394.630\n2.940\n33.400\n\n\n4\n0.069\n0.000\n2.180\n0\n0.458\n7.147\n54.200\n6.062\n3\n222.000\n18.700\n396.900\n5.330\n36.200\n\n\n\n\n\n\n\n\n\nCode\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 506 entries, 0 to 505\nData columns (total 14 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   CRIM     506 non-null    float64\n 1   ZN       506 non-null    float64\n 2   INDUS    506 non-null    float64\n 3   CHAS     506 non-null    int64  \n 4   NOX      506 non-null    float64\n 5   RM       506 non-null    float64\n 6   AGE      506 non-null    float64\n 7   DIS      506 non-null    float64\n 8   RAD      506 non-null    int64  \n 9   TAX      506 non-null    float64\n 10  PTRATIO  506 non-null    float64\n 11  B        506 non-null    float64\n 12  LSTAT    506 non-null    float64\n 13  MEDV     506 non-null    float64\ndtypes: float64(12), int64(2)\nmemory usage: 55.5 KB\n\n\n\n\nCode\ndf.shape\n\n\n(506, 14)\n\n\n\n\nCode\ndf.describe()\n\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nB\nLSTAT\nMEDV\n\n\n\n\ncount\n506.000\n506.000\n506.000\n506.000\n506.000\n506.000\n506.000\n506.000\n506.000\n506.000\n506.000\n506.000\n506.000\n506.000\n\n\nmean\n3.614\n11.364\n11.137\n0.069\n0.555\n6.285\n68.575\n3.795\n9.549\n408.237\n18.456\n356.674\n12.653\n22.533\n\n\nstd\n8.602\n23.322\n6.860\n0.254\n0.116\n0.703\n28.149\n2.106\n8.707\n168.537\n2.165\n91.295\n7.141\n9.197\n\n\nmin\n0.006\n0.000\n0.460\n0.000\n0.385\n3.561\n2.900\n1.130\n1.000\n187.000\n12.600\n0.320\n1.730\n5.000\n\n\n25%\n0.082\n0.000\n5.190\n0.000\n0.449\n5.885\n45.025\n2.100\n4.000\n279.000\n17.400\n375.377\n6.950\n17.025\n\n\n50%\n0.257\n0.000\n9.690\n0.000\n0.538\n6.208\n77.500\n3.207\n5.000\n330.000\n19.050\n391.440\n11.360\n21.200\n\n\n75%\n3.677\n12.500\n18.100\n0.000\n0.624\n6.623\n94.075\n5.188\n24.000\n666.000\n20.200\n396.225\n16.955\n25.000\n\n\nmax\n88.976\n100.000\n27.740\n1.000\n0.871\n8.780\n100.000\n12.127\n24.000\n711.000\n22.000\n396.900\n37.970\n50.000\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize = (10, 10))\nsns.heatmap(df.corr(), annot=True)",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "Setup"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#変数eda",
    "href": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#変数eda",
    "title": "Setup",
    "section": "0.2 1変数EDA",
    "text": "0.2 1変数EDA\n\n\nCode\ndf[\"MEDV\"].describe()\n\n\ncount   506.000\nmean     22.533\nstd       9.197\nmin       5.000\n25%      17.025\n50%      21.200\n75%      25.000\nmax      50.000\nName: MEDV, dtype: float64\n\n\n\n\nCode\ndf[\"MEDV\"].hist(bins=50)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# @title 2変数EDA\n\nplt.figure(figsize=(10, 8))\ndf_corr = df.corr()\nsns.heatmap(df_corr, annot = True, vmax = 1, vmin = -1, center = 0, cmap = \"coolwarm\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 散布図\nnum_cols = [\"LSTAT\", \"RM\", \"MEDV\"]\nsns.pairplot(df[num_cols], height=2.5)\n\n\n\n\n\n\n\n\n\n上記をみると線形や非線形の形状が容易に見て取れる。このような探索的な可視化をおこのあう",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "Setup"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#重回帰分析の学習予測評価",
    "href": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#重回帰分析の学習予測評価",
    "title": "Setup",
    "section": "1.1 重回帰分析の学習、予測、評価",
    "text": "1.1 重回帰分析の学習、予測、評価\nデータ、利用するライブラリはこれまでと同様である。\n\n\nCode\nX = df.drop(labels = [\"MEDV\"], axis = 1)\ny = df[\"MEDV\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle = True, test_size=0.2, random_state=0)\n\n# 特徴量の標準化\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nnum_cols = X.select_dtypes(include=[\"float64\", \"int64\"]).columns\n\n# 学習データだけで学習させる\nscaler.fit(X_train[num_cols])\n\n# テストデータについても変換してしまう\nX_train[num_cols] = scaler.transform(X_train[num_cols])\nX_test[num_cols] = scaler.transform(X_test[num_cols])\n\n\n\n\n\nCode\n# カスタムフォーマットを適用（例：小数点以下2桁）\nX_train.describe()\n\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nB\nLSTAT\n\n\n\n\ncount\n404.000\n404.000\n404.000\n404.000\n404.000\n404.000\n404.000\n404.000\n404.000\n404.000\n404.000\n404.000\n404.000\n\n\nmean\n-0.000\n0.000\n-0.000\n0.000\n-0.000\n0.000\n-0.000\n-0.000\n-0.000\n0.000\n0.000\n-0.000\n0.000\n\n\nstd\n1.001\n1.001\n1.001\n1.001\n1.001\n1.001\n1.001\n1.001\n1.001\n1.001\n1.001\n1.001\n1.001\n\n\nmin\n-0.416\n-0.500\n-1.527\n-0.273\n-1.485\n-3.950\n-2.356\n-1.249\n-0.971\n-1.265\n-2.674\n-4.048\n-1.506\n\n\n25%\n-0.407\n-0.500\n-0.858\n-0.273\n-0.929\n-0.597\n-0.832\n-0.810\n-0.625\n-0.743\n-0.485\n0.196\n-0.820\n\n\n50%\n-0.385\n-0.500\n-0.286\n-0.273\n-0.155\n-0.131\n0.318\n-0.279\n-0.509\n-0.474\n0.291\n0.371\n-0.193\n\n\n75%\n-0.003\n0.348\n0.999\n-0.273\n0.653\n0.538\n0.886\n0.660\n1.687\n1.542\n0.793\n0.421\n0.604\n\n\nmax\n10.530\n3.736\n2.379\n3.665\n2.739\n3.572\n1.103\n4.021\n1.687\n1.806\n1.614\n0.430\n3.330\n\n\n\n\n\n\n\n\n\nCode\nX_test.describe()\n\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nB\nLSTAT\n\n\n\n\ncount\n102.000\n102.000\n102.000\n102.000\n102.000\n102.000\n102.000\n102.000\n102.000\n102.000\n102.000\n102.000\n102.000\n\n\nmean\n0.139\n-0.091\n0.009\n-0.003\n-0.051\n-0.123\n-0.080\n0.059\n0.084\n0.157\n-0.015\n-0.118\n-0.036\n\n\nstd\n1.259\n0.935\n0.908\n1.000\n1.034\n1.056\n1.011\n1.064\n1.030\n0.926\n0.936\n1.142\n0.893\n\n\nmin\n-0.415\n-0.500\n-1.487\n-0.273\n-1.424\n-3.515\n-2.160\n-1.271\n-0.971\n-1.048\n-2.674\n-4.022\n-1.480\n\n\n25%\n-0.405\n-0.500\n-0.750\n-0.273\n-0.938\n-0.606\n-1.047\n-0.787\n-0.625\n-0.579\n-0.713\n0.090\n-0.642\n\n\n50%\n-0.386\n-0.500\n-0.175\n-0.273\n-0.312\n-0.209\n0.125\n-0.203\n-0.509\n-0.110\n0.108\n0.360\n-0.159\n\n\n75%\n0.221\n-0.500\n0.999\n-0.273\n0.427\n0.241\n0.932\n0.681\n1.687\n1.542\n0.793\n0.430\n0.411\n\n\nmax\n8.631\n3.101\n1.542\n3.665\n2.739\n3.492\n1.103\n3.340\n1.687\n1.542\n1.249\n0.430\n3.466\n\n\n\n\n\n\n\n\n\nCode\n# モデルの学習\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\nmodel.get_params()\n\n\n{'copy_X': True, 'fit_intercept': True, 'n_jobs': None, 'positive': False}\n\n\n\n\nCode\ny_test_pred = model.predict(X_test)\nprint(f\"RMSE test: {np.sqrt(mean_squared_error(y_test, y_test_pred)) :.2f}\")\n\n\nRMSE test: 5.78\n\n\n\n\nCode\nmodel.coef_, model.intercept_\n\n\n(array([-0.97082019,  1.05714873,  0.03831099,  0.59450642, -1.8551476 ,\n         2.57321942, -0.08761547, -2.88094259,  2.11224542, -1.87533131,\n        -2.29276735,  0.71817947, -3.59245482]),\n 22.611881188118804)\n\n\n\n\nCode\n# 回帰係数の可視化\nimportances = model.coef_\nindices = np.argsort(importances)[::-1]\nplt.figure(figsize=(10, 6))\nplt.title(\"Feature importances\")\nplt.bar(range(X.shape[1]), importances[indices])\nplt.xticks(range(X.shape[1]), X.columns[indices], rotation = 90)\nplt.show()",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "Setup"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#深さ1の回帰木の可視化",
    "href": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#深さ1の回帰木の可視化",
    "title": "Setup",
    "section": "2.1 深さ1の回帰木の可視化",
    "text": "2.1 深さ1の回帰木の可視化\n\n\nCode\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\nimport pandas as pd\nimport numpy as np\nimport graphviz\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nCode\ndf = pd.read_csv(\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n    header=None,\n    names =[\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"],\n    sep=\"\\s+\",\n)\n\nX_train = df.loc[:100, [\"RM\"]]\ny_train = df.loc[:100, \"MEDV\"]\n\nX_test = df.loc[101:, [\"RM\"]]\ny_test = df.loc[101:, \"MEDV\"]\n\n\n&lt;&gt;:5: SyntaxWarning: invalid escape sequence '\\s'\n&lt;&gt;:5: SyntaxWarning: invalid escape sequence '\\s'\nC:\\Users\\suzuk\\AppData\\Local\\Temp\\ipykernel_24336\\4222003498.py:5: SyntaxWarning: invalid escape sequence '\\s'\n  sep=\"\\s+\",\n\n\n\n\nCode\nmodel = DecisionTreeRegressor(criterion = \"squared_error\", max_depth = 1, min_samples_leaf=1, random_state=0)\nmodel.fit(X_train, y_train)\nmodel.get_params()\n\n\n{'ccp_alpha': 0.0,\n 'criterion': 'squared_error',\n 'max_depth': 1,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'random_state': 0,\n 'splitter': 'best'}\n\n\n\n\nCode\n# 予測値\nmodel.predict(X_train)\n\n\narray([20.1675    , 20.1675    , 30.71428571, 30.71428571, 30.71428571,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 20.1675    ,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 20.1675    ,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 20.1675    ,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 20.1675    ,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 30.71428571,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 20.1675    ,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 30.71428571,\n       30.71428571, 30.71428571, 20.1675    , 20.1675    , 20.1675    ,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 20.1675    ,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 20.1675    ,\n       30.71428571, 20.1675    , 30.71428571, 20.1675    , 20.1675    ,\n       20.1675    , 20.1675    , 20.1675    , 30.71428571, 30.71428571,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 20.1675    ,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 20.1675    ,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 20.1675    ,\n       30.71428571, 30.71428571, 20.1675    , 20.1675    , 20.1675    ,\n       30.71428571, 20.1675    , 20.1675    , 30.71428571, 30.71428571,\n       20.1675    , 20.1675    , 20.1675    , 20.1675    , 20.1675    ,\n       30.71428571, 20.1675    , 30.71428571, 30.71428571, 30.71428571,\n       30.71428571])\n\n\n\n\nCode\nfrom sklearn import tree\ndot_data = tree.export_graphviz(\n    model, out_file=None, feature_names=[\"RM\"], class_names=[\"MEDV\"], filled=True, rounded=True, special_characters=True)\ngraphviz.Source(dot_data)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# データと予測値の可視化\nplt.figure(figsize = (8, 4))\nX = X_train.values.flatten()\ny = y_train.values\n\nX_plt = np.linspace(X.min(), X.max(), 100)[:, np.newaxis]\ny_pred = model.predict(X_plt)\n\nplt.scatter(X, y, label = \"Training data\")\nplt.plot(X_plt, y_pred, color = \"red\", label = \"Decision Tree\")\nplt.ylabel(\"MEDV\")\nplt.xlabel(\"RM\")\nplt.legend()\n\n\nc:\\pyenv\\py312\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n  warnings.warn(",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "Setup"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#回帰木の深さと予測値",
    "href": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#回帰木の深さと予測値",
    "title": "Setup",
    "section": "2.2 回帰木の深さと予測値",
    "text": "2.2 回帰木の深さと予測値\n深さ1のときにはレコードを1回だけ2分割して、左葉と右葉の2値を出力する予測モデルを実装した。実務では深さを指定して、指定した深さに到達するまでの２分割を繰り返す。深さが\\(d\\)のときは最大\\(2^d\\)に分割される。\n\n\nCode\nmodel = DecisionTreeRegressor(criterion = \"squared_error\", max_depth = 2, min_samples_leaf=1, random_state=0, ccp_alpha = 0)\nmodel.fit(X_train, y_train)\nmodel.get_params()\n\n\n{'ccp_alpha': 0,\n 'criterion': 'squared_error',\n 'max_depth': 2,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 1,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'random_state': 0,\n 'splitter': 'best'}\n\n\n\n\nCode\n# 予測値\nmodel.predict(X_train)\n\n\narray([22.99642857, 22.99642857, 35.4       , 27.2       , 35.4       ,\n       22.99642857, 18.64423077, 22.99642857, 18.64423077, 18.64423077,\n       22.99642857, 18.64423077, 18.64423077, 18.64423077, 18.64423077,\n       18.64423077, 18.64423077, 18.64423077, 18.64423077, 18.64423077,\n       18.64423077, 18.64423077, 18.64423077, 18.64423077, 18.64423077,\n       18.64423077, 18.64423077, 18.64423077, 22.99642857, 27.2       ,\n       18.64423077, 18.64423077, 18.64423077, 18.64423077, 18.64423077,\n       18.64423077, 18.64423077, 18.64423077, 18.64423077, 27.2       ,\n       35.4       , 27.2       , 22.99642857, 22.99642857, 18.64423077,\n       18.64423077, 18.64423077, 18.64423077, 18.64423077, 18.64423077,\n       18.64423077, 18.64423077, 22.99642857, 18.64423077, 18.64423077,\n       35.4       , 22.99642857, 27.2       , 22.99642857, 18.64423077,\n       18.64423077, 18.64423077, 22.99642857, 27.2       , 35.4       ,\n       22.99642857, 18.64423077, 18.64423077, 18.64423077, 18.64423077,\n       22.99642857, 18.64423077, 18.64423077, 22.99642857, 22.99642857,\n       22.99642857, 22.99642857, 18.64423077, 22.99642857, 18.64423077,\n       27.2       , 27.2       , 22.99642857, 22.99642857, 22.99642857,\n       27.2       , 18.64423077, 18.64423077, 27.2       , 35.4       ,\n       22.99642857, 22.99642857, 22.99642857, 22.99642857, 22.99642857,\n       27.2       , 22.99642857, 35.4       , 35.4       , 35.4       ,\n       27.2       ])\n\n\n\n\nCode\ndot_data = tree.export_graphviz(\n    model, out_file=None, feature_names=[\"RM\"], class_names=[\"MEDV\"], filled=True, rounded=True, special_characters=True)\ngraphviz.Source(dot_data)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# データと予測値の可視化\nplt.figure(figsize = (8, 4))\nX = X_train.values.flatten()\ny = y_train.values\n\nX_plt = np.linspace(X.min(), X.max(), 100)[:, np.newaxis]\ny_pred = model.predict(X_plt)\n\nplt.scatter(X, y, label = \"Training data\")\nplt.plot(X_plt, y_pred, color = \"red\", label = \"Decision Tree\")\nplt.ylabel(\"MEDV\")\nplt.xlabel(\"RM\")\nplt.legend()\nplt.show()\n\n\nc:\\pyenv\\py312\\Lib\\site-packages\\sklearn\\base.py:465: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names\n  warnings.warn(",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "Setup"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#回帰木の学習予測評価",
    "href": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#回帰木の学習予測評価",
    "title": "Setup",
    "section": "2.3 回帰木の学習、予測、評価",
    "text": "2.3 回帰木の学習、予測、評価\n\n\nCode\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport graphviz\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ndf = pd.read_csv(\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data\",\n    header=None,\n    names =[\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"],\n    sep=\"\\\\s+\",\n)\n\nX = df.drop(labels = [\"MEDV\"], axis = 1)\ny = df[\"MEDV\"]\nX.head()\n\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nB\nLSTAT\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296.0\n15.3\n396.90\n4.98\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242.0\n17.8\n396.90\n9.14\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242.0\n17.8\n392.83\n4.03\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222.0\n18.7\n394.63\n2.94\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222.0\n18.7\n396.90\n5.33\n\n\n\n\n\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, shuffle = True, random_state = 0)\n\n\n\n\nCode\nlearn_opts = {\n    \"criterion\": \"squared_error\",\n    \"max_depth\": 4, \n    \"min_samples_leaf\": 10, \n    \"ccp_alpha\": 5, # 葉数に対する正則化の強さ\n}\n\nfrom sklearn.tree import DecisionTreeRegressor\nmodel = DecisionTreeRegressor(\n    **learn_opts\n)\n\nmodel.fit(X_train, y_train)\nmodel.get_params()\n\n\n{'ccp_alpha': 5,\n 'criterion': 'squared_error',\n 'max_depth': 4,\n 'max_features': None,\n 'max_leaf_nodes': None,\n 'min_impurity_decrease': 0.0,\n 'min_samples_leaf': 10,\n 'min_samples_split': 2,\n 'min_weight_fraction_leaf': 0.0,\n 'random_state': None,\n 'splitter': 'best'}\n\n\n\n\nCode\ny_test_pred = model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, y_test_pred))\n\n\n5.950312246327099\n\n\n\n\nCode\nsns.scatterplot(\n    x = y_test, \n    y = y_test_pred\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 木の可視化\nfrom sklearn import tree\ndot_data = tree.export_graphviz(model, out_file = None, rounded = True, feature_names = X.columns, filled = True)\ngraphviz.Source(dot_data, format = \"png\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 特徴量の重要度の可視化\nimportances = model.feature_importances_\nindices = np.argsort(importances)[::-1] # 要素番号を大きい順\nplt.figure(figsize = (8, 4))\nplt.title(\"Feature Importances\")\nplt.bar(range(len(indices)), importances[indices])\nplt.xticks(range(len(indices)), X.columns[indices], rotation = 90)\nplt.show()",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "Setup"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#深さ1のlightgbm回帰の予測値の検証",
    "href": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#深さ1のlightgbm回帰の予測値の検証",
    "title": "Setup",
    "section": "3.1 深さ1のLightGBM回帰の予測値の検証",
    "text": "3.1 深さ1のLightGBM回帰の予測値の検証\n1回ブースティング回帰したときの予測値は次式からなる。 \\(\\hat{y}^0\\)は学習データの全体での平均値である。 二乗誤差で一回だけ更新する例をみるとことで、 アルゴリズムが式のとおり行進出来ていることを確認するというのが目的である\n\\[\n\\hat{y} = \\hat{y}^0 + \\eta w_1(x)\n\\]\n\n\nCode\nprint(f\"samples: {len(y)}\")\npred0 = sum(y) / len(y)\nprint(\"pred0:\", pred0)\n\n\nsamples: 100\npred0: 22.30900000000001\n\n\n\n\nCode\n# 深さ1の回帰木なので左右の部活となる\nthreshold = 6.793\nX_left = X[X&lt;=threshold];\ny_left = y[X&lt;=threshold];\n\n\n# 左の葉を予測\n# - 残差は目的変数の正解値と初期値の差分となる\n# - 葉の重みはobjectiveにmseを指定したため残差の平均値となる\n#   - 自分で書いておいて上の文章がなぜか意味がわからない・・\n#   - なぜmseだと重みが残差の平均値になるのだろうか\nprint(f\"sample_left: {len(y_left)}\")\nresidual_left = y_left - pred0\nweight_left = np.mean(residual_left);\nprint(\"weight_left:\", weight_left)\ny_pred_left = pred0  + .8 * weight_left;\nprint(\"y_pred_left:\", y_pred_left);\n\n\nsample_left: 88\nweight_left: -1.5851363636363758\ny_pred_left: 21.040890909090912",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "Setup"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#lightgbmの学習予測評価",
    "href": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#lightgbmの学習予測評価",
    "title": "Setup",
    "section": "3.2 LightGBMの学習、予測、評価",
    "text": "3.2 LightGBMの学習、予測、評価\n\n\nCode\nX = df.drop([\"MEDV\"], axis = 1)\ny = df[\"MEDV\"]\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size = .2, shuffle = True, random_state = 0\n)\n\n\n\n\nCode\nimport lightgbm as lgb\nlgb_train = lgb.Dataset(X_train, y_train)\n\nparams = {\n    \"objective\": \"mse\", \n    \"num_leaves\": 5, \n    \"seed\": 0, \n    \"verbose\": -1, \n}\n\nmodel = lgb.train(\n    params, \n    lgb_train, \n    num_boost_round = 50, \n    valid_sets = [lgb_train], \n    valid_names = [\"train\"], \n    callbacks = [lgb.log_evaluation(10)]\n)\n\n\n[10]    train's l2: 23.2264\n[20]    train's l2: 11.4353\n[30]    train's l2: 8.26905\n[40]    train's l2: 6.83309\n[50]    train's l2: 5.88687\n\n\n\n\nCode\ny_test_pred = model.predict(X_test)\nnp.sqrt(mean_squared_error(y_test_pred, y_test))\n\n\n4.965761177880827\n\n\n\n\nCode\n# 特徴量の重要度の可視化\nimportances = model.feature_importance(\n    importance_type = \"gain\"\n)\nindices = np.argsort(importances)[::-1]\n\nplt.figure(figsize = (8, 4))\nplt.title(\"Feature Importance\")\nplt.bar(range(len(indices)), importances[indices])\nplt.xticks(range(len(indices)), X.columns[indices], rotation = 90)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 回帰木の可視化\nlgb.plot_tree(model, tree_index = 0, figsize = (20, 20))\n\n\n\n\n\n\n\n\n\n\n\nCode\nlgb.plot_tree(model, tree_index = -1, figsize = (20, 20))",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "Setup"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#shap",
    "href": "contents/books/08_handbook_lightgbm/cl02_データ理解.html#shap",
    "title": "Setup",
    "section": "3.3 SHAP",
    "text": "3.3 SHAP\nSHAPはレコード1件ごとの予測値と平均的な予測値の差分を計算して、 差分を特徴量ごとに分解することで、どの特徴量が予測値に貢献するかを教えてくれる。\n特徴量\\(X\\)をモデルに入力したとき予測値の期待値は\\(\\textrm{E}[\\hat{f}(X)]\\)となる。\nインデックス\\(i\\)の予測値\\(\\hat{f}(x_i)\\)と平均的な予測値\\(\\textrm{E}[\\hat{f}(x)]\\)の差分は特徴量インデックス\\(j\\)で分解できる.\n\\[\n\\hat{f}(x_i)-\\textrm{E}[\\hat{f}(x)]=\\sum_{j=1}^m(\\phi_{ij})\n\\]\n\n\nCode\nimport shap\nexplainer = shap.TreeExplainer(\n    model = model, \n    feature_pertubation = \"tree_path_dependent\"\n)\n\n\n作成したexplainerにテストデータの特徴量\\(X\\)を入力して、SHAP値を計算する。\n\n\nCode\nshap_values = explainer(X_test)\n\n\n\n\nCode\n# 全レコードに対する期待値\nexplainer.expected_value\n\n\n22.611881236511852\n\n\n\n\nCode\n# 予測値\ny_test_pred\n\n\narray([24.33826347, 25.55850304, 22.88538477, 11.58613597, 22.02411837,\n       20.72080442, 21.86359234, 21.10557067, 21.40360298, 19.23261144,\n        9.61777349, 13.6106635 , 14.32141005,  9.76631897, 48.25601687,\n       35.00875917, 21.95652073, 37.80759643, 25.87572833, 22.00650765,\n       23.13945227, 22.36541355, 19.90861776, 23.78192781, 20.77748124,\n       18.71856927, 19.06298637, 16.83366361, 42.81339824, 18.96547221,\n       16.40352523, 16.24413082, 20.68783767, 20.67237901, 24.29716422,\n       16.26813009,  9.76631897, 21.76083107, 16.2511962 , 15.31180178,\n       22.50493327, 20.85719768, 23.72487685, 16.82037207, 23.63069676,\n       22.69888006, 19.02106683, 16.85573098, 15.93691435, 23.13066412,\n       18.16393601, 18.64008031, 21.32675084, 42.5370897 , 15.85521869,\n       19.32749944, 20.24974385, 20.30964921, 18.99113574, 18.75245656,\n       22.33068937, 20.77814028, 32.58636102, 31.77110891, 18.65418033,\n       29.86652849, 15.53331969, 19.5093324 , 17.2016036 , 20.92305328,\n       22.12660381, 23.27809658, 26.15053201, 32.81938365, 25.90559469,\n        9.93819836, 43.24659685, 21.98041858, 23.6518303 , 20.74369915,\n       24.4009425 , 17.80270523, 17.53592739, 42.93458749, 42.71282222,\n       23.13945227, 22.51401249, 16.70501676, 25.60918439, 15.8258483 ,\n       17.9798286 , 11.40325492, 22.59276985, 29.49044256, 21.62153404,\n       21.29432547, 10.59529386, 23.41397781, 14.93005998, 19.50409152,\n       23.2343968 , 20.60667604])\n\n\n\n\nCode\n# 貢献度と期待値の合計が予測値になる\nshap_values[14].base_values + shap_values.values[14].sum()\n\n\n48.25601687152404\n\n\n\n\nCode\ny_test_pred[14]\n\n\n48.25601687152407\n\n\nSHAP値によってどの特徴量の貢献が大きかったのかplots.waterfallメソッドにより可視化可能である。 予測価格が高かった15件目の値を可視化する.\n\n\nCode\nshap.plots.waterfall(shap_values[14])\n\n\n\n\n\n\n\n\n\n\n\nCode\nshap.plots.waterfall(shap_values[10])\n\n\n\n\n\n\n\n\n\n\n\nCode\n# マクロな貢献をみる\n# この値はテストデータの予測値で条件付けられている、ということを忘れないようにする\nshap.plots.bar(shap_values = shap_values)\n\n\n\n\n\n\n\n\n\n\n\nCode\nshap_values.base_values\n\n\narray([[ 0.07616379,  0.        ,  0.22663437, ...,  0.89922106,\n         0.56269372,  2.62651817],\n       [ 0.56348624,  0.        , -0.08134097, ..., -1.14577638,\n         0.73951067,  2.65578769],\n       [ 0.29482469,  0.        , -0.14339472, ...,  1.23283713,\n         0.15968409, -1.09213544],\n       ...,\n       [ 0.29199972,  0.        , -0.10377573, ..., -0.69474318,\n         0.15539382, -0.90536801],\n       [ 0.15960661,  0.        , -0.1468447 , ...,  0.73249154,\n         0.09297993,  1.35855161],\n       [ 0.28207422,  0.        , -0.09163934, ..., -1.2087129 ,\n         0.12557371, -1.00348444]])\n\n\n\n\nCode\nshap_values.values\n\n\narray([[ 0.07616379,  0.        ,  0.22663437, ...,  0.89922106,\n         0.56269372,  2.62651817],\n       [ 0.56348624,  0.        , -0.08134097, ..., -1.14577638,\n         0.73951067,  2.65578769],\n       [ 0.29482469,  0.        , -0.14339472, ...,  1.23283713,\n         0.15968409, -1.09213544],\n       ...,\n       [ 0.29199972,  0.        , -0.10377573, ..., -0.69474318,\n         0.15539382, -0.90536801],\n       [ 0.15960661,  0.        , -0.1468447 , ...,  0.73249154,\n         0.09297993,  1.35855161],\n       [ 0.28207422,  0.        , -0.09163934, ..., -1.2087129 ,\n         0.12557371, -1.00348444]])",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "Setup"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理_Q72.html",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理_Q72.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import (Image)\n\n\n\n0.1 Q84：CMYK色空間\nCMYKもnumpyやPillowで扱うことが可能である. CMYKは印刷でよく使われる色空間である.\nnumpy配列は単純にHxWx4の次元にしておけばよい. データ型はnp.uint8\n\n\nCode\nH, W = 100, 100\ncanvas = np.full((H, W * 5, 4), 0, dtype = np.uint8)\n\nfor i in range(4):\n    o = (i + 1) * W\n    canvas[:, o:(o + W), i] = 255\n\nplt.imshow(Image.fromarray(canvas, \"CMYK\"))\n\n\n\n\n\n\n\n\n\n\n\n0.2 Q85：加法混色，減法混色\nRGBはすべて255にすると白になるが，CMYKは黒になる.\n\n\nCode\nH, W = 600, 700\nc_rgb  = np.full((H, W, 3), 255, dtype = np.uint8)\nc_cmyk = np.full((H, W, 4), 255, dtype = np.uint8)\n\nidy, idx  = np.indices((H, W))\nm_circle1 = np.sqrt((idy - 200) ** 2 + (idx - 200) ** 2) &lt;= 200\nm_circle2 = np.sqrt((idy - 200) ** 2 + (idx - 500) ** 2) &lt;= 200\nm_circle3 = np.sqrt((idy - 400) ** 2 + (idx - 350) ** 2) &lt;= 200\n\nc_rgb [m_circle1, 0] = 0\nc_cmyk[m_circle1, 0] = 0\n\nc_rgb [m_circle2, 1] = 0\nc_cmyk[m_circle2, 1] = 0\n\nc_rgb [m_circle3, 2] = 0\nc_cmyk[m_circle3, 2] = 0\n\n# Kも255になっているのでマスクの部分は０にする\nc_cmyk[m_circle1 +  m_circle2 +  m_circle3 &gt; 0, 3] = 0\n\n\nfig, axs = plt.subplots(1, 2)\naxs[0].imshow(Image.fromarray(c_cmyk, \"CMYK\"))\naxs[1].imshow(c_rgb)\n\n\n\n\n\n\n\n\n\n\n\n0.3 Q86：CMYKのグラデーション\n\n\nCode\nx = np.zeros((256, 256, 4), dtype = np.uint8)\nx[..., 0] = np.arange(256).astype(np.uint8)[:, None]\nx[..., 1] = np.arange(256).astype(np.uint8)\n\n\n\n\nCode\nret = []\n\nfor i in range(3):\n    j  = (i + 1) % 3\n    x = np.zeros((256, 256, 3), dtype = np.uint8)\n    x[..., i] = np.arange(256).astype(np.uint8)[:, None]\n    x[..., j] = np.arange(256).astype(np.uint8)\n    ret.append(x)\n\nplt.imshow(np.concatenate(ret, axis = 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\nret = []\n\nfor i in range(3):\n    j  = (i + 1) % 3\n    x = np.zeros((256, 256, 3), dtype = np.uint8)\n    x[..., i] = np.arange(256).astype(np.uint8)[:, None]\n    x[..., j] = np.arange(256).astype(np.uint8)\n    # Kの部分を追加\n    x = np.concatenate([x, np.zeros((256, 256, 1), dtype = np.uint8)], axis = 2)\n    ret.append(np.array(Image.fromarray(x, \"CMYK\").convert(\"RGB\")))\n\nout = np.concatenate(ret, axis = 1)\nplt.imshow(out)\n\n\n\n\n\n\n\n\n\n\n\n0.4 Q87：CMYKのくすみ\n実際に印刷してみると，RGBから変換したCMYKはくすんだ印象を受ける.\n\n\nCode\nret = []\n\nfor i in range(3):\n    j  = (i + 1) % 3\n    x = np.zeros((256, 256, 3), dtype = np.uint8)\n    x[..., i] = np.arange(256).astype(np.uint8)[:, None]\n    x[..., j] = np.arange(256).astype(np.uint8)\n    ret.append(x)\n\nout = np.concatenate(ret, axis = 1)\nplt.imshow(out)\n\n\n\n\n\n\n\n\n\n\n\nCode\nimg = Image.fromarray(out)\nimg.save(\"out/rgb.jpg\")\n\n\n\n\nCode\ncymk = img.convert(\"CMYK\")\n\n# プロットは問題なくても\n# ビューワーで見てみるとくすんでいる\ncymk.save(\"out/cmyk.jpg\")\nplt.imshow(cymk)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# もとのRGB\nplt.imshow(cymk.convert(\"RGB\"))\n\n\n\n\n\n\n\n\n\n\n\n0.5 Q88：カラープロファイルを埋め込んだ画像\nカラープロファイルがあると，プロットでも，CMYKのくすみを再現できる.\n詳細ははぶくが，ADbeからダウンロードする.\n\n\nCode\nwith open(\"dat/JapanColor2001Coated.icc\", \"rb\") as fp:\n    profile = fp.read()\n\nimg.save(\"out/filename.jpg\",  icc_profile = profile)\n\n\n\n\nCode\ncymk.save(\"out/cmyk_with_profile.jpg\", icc_profile = profile)\n\n\n\n\nCode\nfig = plt.figure(figsize = (16, 16))\nwith Image.open(\"out/cmyk_with_profile.jpg\") as img:\n    plt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (16, 16))\nplt.imshow(out)\n\n\n\n\n\n\n\n\n\n\n\n0.6 Q92：スプライ補間と任意の曲線\n\n\nCode\nimport scipy.interpolate\n\nH, W = 400, 600\nimg = np.full((H, W, 3), 0, dtype = np.uint8)\n\npoints = np.array([\n    [0,   300],\n    [75,  399],\n    [175, 200],\n    [300, 300],\n    [470,  75],\n    [599, 165],\n])\n\nidy, idx = np.indices((H, W))\nfunc  = scipy.interpolate.interp1d(points[:, 0], points[:, 1], kind = \"quadratic\")\ny_new = func(idx)\n\nfor border in [-100, -50, 0, 50, 150]:\n    mask = idy - y_new &gt;= border\n    img[mask, 1:3] += 40\n\n\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n0.7 Q93：二次元の補間\n\n\nCode\nimport scipy.interpolate\n\nH, W = 600, 700\nimg = np.full((H, W, 3), 0, dtype = np.uint8)\nidy, idx = np.indices((H, W))\n\npoints = np.array([\n    [0,   300],\n    [75,  399],\n    [175, 200],\n    [300, 300],\n    [470,  75],\n    [599, 165],\n])\n\n# X, Y, R, G, Bになっている\n# R, G, Bごとに補間を実行すること\ndata = np.array([\n    [0, 0, 0, 0, 0],\n    [0, 599, 0, 0, 0], \n    [699, 0, 0, 0, 0], \n    [699, 599, 0, 0, 0], \n    [200, 200, 255, 0, 0], \n    [500, 200, 0, 255, 0],\n    [300, 400, 0, 0, 255]\n])\n\nret = []\nfor i in range(3):\n    x = data[:, 0]\n    y = data[:, 1]\n    z = data[:, 2+i]\n    func  = scipy.interpolate.interp2d(x, y, z)\n    # メッシュグリッドは関数内でやってくれる\n    new_z = func(np.arange(H), np.arange(W))\n    ret.append(new_z)\n\nimg = np.clip(np.stack(ret, axis = 2), 0, 255).astype(np.uint8)\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n\n0.8 Q95：文字を入れる場所\n画像に著作権表記などの文字を入れたい場合の処理.\n文字は目立つところ，つまり，輝度の差が大きいところに入れる方がよいとする.\n\n\nCode\nfrom PIL import Image, ImageDraw, ImageFont\n\nwith Image.open(\"numpy_book/imgs/pic01.jpg\") as img:\n    pic01   = np.array(img)\n    pic01_l = np.array(img.convert(\"L\"))\n    H, W, _ = pic01.shape\n    plt.imshow(pic01_l, cmap = \"gray\")\n\n\n\n\n\n\n\n\n\n\n\nCode\npic01_l.shape\n\n\n(700, 1050)\n\n\n\n\nCode\nbatch_y, batch_x = 25, 200\noy, ox, orect_kido = 0, 0, 0\nfor ox_c in range(0, W-batch_x, batch_x):\n    for oy_c in range(0, H-batch_y, batch_y):\n        rect_kido = np.mean(pic01_l[oy_c:(oy_c+batch_y), ox_c:(ox_c+batch_x)])\n        if orect_kido &lt; rect_kido:\n            orect_kido = rect_kido\n            oy, ox = oy_c, ox_c\nprint(oy, ox)\n\n\n125 800\n\n\n\n\nCode\ndraw = ImageDraw.Draw(img)\nfont = ImageFont.truetype(\"numpy_book/fonts/M_PLUS_1p/MPLUS1p-Bold.ttf\", 16)\ndraw.text((ox, oy), \"こしあんは美味しい\", font = font, fill = 0)\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n0.9 Q96：なんかダサそうなロゴ\n\n\nCode\nwith Image.open(\"numpy_book/imgs/water.jpg\") as img:\n    water = np.array(img)\n\n\n\n\nCode\ncanvas = Image.fromarray(np.full((3000, 3000), 255, dtype = np.uint8))\ndraw = ImageDraw.Draw(canvas)\nfont = ImageFont.truetype(\"numpy_book/fonts/M_PLUS_1p/MPLUS1p-Bold.ttf\", 180)\ndraw.text((0, 0), \"こしあんは\\n美味しい\", font = font, fill = 0)\nplt.imshow(canvas)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntext_arr = np.array(canvas)\ny_pos, x_pos = np.where(text_arr == 0)\nlx, ly, ux, uy = np.min(x_pos), np.min(y_pos), np.max(x_pos), np.max(y_pos)\nprint(lx, ly, ux, uy)\ntext_arr = text_arr[ly:uy, lx:ux]\nplt.imshow(text_arr, cmap = \"gray\")\n\n\n9 49 887 409\n\n\n\n\n\n\n\n\n\n\n\nCode\nH_t, W_t = uy - ly, ux - lx\ngrad = np.full((H_t, W_t, 3), 255, dtype = np.uint8)\ngrad[..., 0] = np.linspace(0, 255, W_t)[None, :].astype(np.uint8)\nmask = text_arr &gt; 0\ngrad[mask, 2] = 0\nplt.imshow(Image.fromarray(grad, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\nrainbow = np.array(Image.fromarray(grad, \"HSV\").convert(\"RGB\"))\n\nH_w, W_w, _ = water.shape\noy, ox = H_w // 2 - H_t // 2, W_w // 2 - W_t // 2\nwater[oy:oy + H_t, ox:ox + W_t, :] = \\\n     (mask[..., None]) * water[oy:oy + H_t, ox:ox + W_t, :] + \\\n     (1-mask[..., None]) * rainbow\nplt.imshow(water)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 行列のサイズが異なる場合には次のようなpad関数を使えとのこと\n# 入力サイズを合わせるのに使えそうですね\na = [1, 2, 3, 4, 5]\nnp.pad(a, (2, 3), 'constant', constant_values=(4, 6))\n\n\narray([4, 4, 1, 2, 3, 4, 5, 6, 6, 6])\n\n\n\n\n0.10 Q97：ブレンドヲ変えてみる.\n\n\nCode\ndef vividlight(img1, img2):\n    epsilon = 1e-8\n    layer1  = 1 - (1 - img1) / (2 * img2 + epsilon)\n    layer2  = img1 / (1 - 2 * (img2 - .5) + epsilon)\n    mask = img2 &lt; .5\n    out  = mask * layer1 + (1-mask) * layer2\n    out  = np.clip(out, 0, 1)\n    return out\n\ndef overlay(i1, i2):\n    layer_1 = 2 * i1 * i2\n    layer_2 = 1 - 2 * (1- i1) * (1 -i2)\n    mask    = i1 &lt; .5\n    out     = np.clip(mask * layer_1 + (1-mask) * layer_2, 0, 1)\n    return out\n\n\n\n\nCode\nwith Image.open(\"numpy_book/imgs/water.jpg\") as img:\n    water = np.array(img) / 255.\n    H_w, W_w, _ = water.shape\n\ncanvas = Image.fromarray(np.full((3000, 3000), 255, dtype = np.uint8))\ndraw = ImageDraw.Draw(canvas)\nfont = ImageFont.truetype(\"numpy_book/fonts/M_PLUS_1p/MPLUS1p-Bold.ttf\", 180)\ndraw.text((0, 0), \"こしあんは\\n美味しい\", font = font, fill = 0)\ntext_arr = np.array(canvas)\ny_pos, x_pos = np.where(text_arr == 0)\nlx, ly, ux, uy = np.min(x_pos), np.min(y_pos), np.max(x_pos), np.max(y_pos)\ntext_arr = text_arr[ly:uy, lx:ux]\nH_t, W_t = text_arr.shape\n\n\ngrad = np.full((H_t, W_t, 3), 255, dtype = np.uint8)\ngrad[..., 0] = np.linspace(-50, 50, W_t)[None, :].astype(np.uint8)\ngrad[..., 1] = 160\ngrad = np.array(Image.fromarray(grad, \"HSV\").convert(\"RGB\"))\nm = text_arr == 0\ngrad = m[..., None] * grad\ndelta_h = (H_w - H_t) // 2\ndelta_w = (W_w - W_t) // 2\ngrad = np.pad(\n    grad / 255.,\n    # 軸後ごとに，前後にどれだけ足すのかを記述する \n    ((delta_h, delta_h), (delta_w, delta_w), (0, 0,)), \n    \"constant\", \n    constant_values = 0.\n)\n\n\n\n\nCode\no = vividlight(water, grad)\no = overlay(o, o)\n# 文字の部分やpadの部分は０になっている\nm = np.all(grad != 0, axis = 2, keepdims = True)\nv = m * o + (1-m) * water\nplt.imshow(v)\n\n\n\n\n\n\n\n\n\n\n\nCode\n\no.shape\n\n\n(700, 1050, 3)\n\n\n\n\nCode\nwater.shape\n\n\n(700, 1050, 3)\n\n\n\n\n0.11 Q98:写真がないページの判定\n\n\nCode\ndat = np.load(\"numpy_book/data/chap04_q98.npz\")[\"pics\"]\ndat.shape\n\n\n(9, 256, 256, 3)\n\n\n\n\nCode\nfig, axes = plt.subplots(3, 3, figsize = (12,12))\nfor i in np.arange(9):\n    pic = dat[i]\n    sub_area = pic[:30, -30:]\n    is_white = np.all(sub_area == 255)\n    if is_white:\n        sub_area[...] = np.array([255, 255, 0], dtype = np.uint8)\n    axes[i // 3][i % 3].imshow(pic)\n\n\n\n\n\n\n\n\n\n\n\n0.12 Q99：白黒画像の除外.\n\n\nCode\ndat = np.load(\"numpy_book/data/chap04_q99.npz\")[\"pics\"]\ndat.shape\n\n\n(10, 256, 256, 3)\n\n\n\n\nCode\ndef is_wb(arr):\n    hsv = np.array(Image.fromarray(arr).convert(\"HSV\"))\n    return np.all(hsv[..., 1] == 0)\n\n\n\n\nCode\nfig, axes = plt.subplots(4, 3, figsize = (12,12))\nfor i in np.arange(10):\n    pic = dat[i]\n    if not is_wb(pic):\n        axes[i // 3][i % 3].imshow(pic)\n\n\n\n\n\n\n\n\n\n\n\n0.13 Q100：モザイク\nモザイクをかけるには低解像度にしてもとの写真に戻すということをすれば良い.\nPillowは(x, y)の順番であることに注意すること.\n\n\nCode\nwith Image.open(\"numpy_book/imgs/girl01.png\") as img:\n    girl = np.array(img)\n    girl_resized = img.resize((img.width // 15, img.height // 15), Image.NEAREST)\\\n        .resize((img.width, img.height), Image.NEAREST)\n    girl[10:300, 150:500] = np.array(girl_resized)[10:300, 150:500]\n\n\n\n\nCode\nplt.imshow(girl)\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "Q84：CMYK色空間"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html",
    "title": "テンソルと画像処理",
    "section": "",
    "text": "Code\nimport numpy as np\n\n\ndef gray_show(img):\n    plt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n    # plt.axis(\"off\")\n    plt.show()",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#この章で学ぶこと",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#この章で学ぶこと",
    "title": "テンソルと画像処理",
    "section": "1 この章で学ぶこと",
    "text": "1 この章で学ぶこと\n\nnumpy配列でモノクロ画像の描画\nmatplotlibの使い方\nnumpy配列でカラー画像の描画\nnumpy配列で画像を自在に操り，フォトショップの機能を一部実装\n\n章末には100本ノックがある.",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#行列と画像",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#行列と画像",
    "title": "テンソルと画像処理",
    "section": "2 行列と画像",
    "text": "2 行列と画像\n1画素8ビットのカラーチャンネルは，numpy配列の多次元行列として表現することが可能である.\n\n2.1 背景一色の画像\n\n\nCode\nM, N = 300, 400\nimg = np.zeros((M, N), dtype = np.uint8)\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nplt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimg = np.full((M, N), 127, dtype = np.uint8)\nplt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\nplt.show()",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#直線を引く",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#直線を引く",
    "title": "テンソルと画像処理",
    "section": "3 直線を引く",
    "text": "3 直線を引く\n\n\nCode\n# 真っ白なキャンバスを用意して\nimg = np.full((M, N), 255, dtype = np.uint8)\n# 黒い横線を引く\nimg[100, :] = 0\nplt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimg = np.full((M, N), 255, dtype = np.uint8)\nimg[50::50, :] = 0\nimg[:, 50::50] = 0\ngray_show(img)\n\n\n\n\n\n\n\n\n\n\n3.1 斜めの線\n境界 + 適当な幅のデータをプロットする。\n\n\nCode\nfig = plt.figure(figsize = (12, 6))\nfor i, width in enumerate([.5, 1]):\n    img = np.full((M, N), 255, dtype = np.uint8)\n    # mesh_gridのようなイメージ\n    # img.shape点を作成している\n    y_ind, x_ind = np.indices(img.shape)\n    ax = fig.add_subplot(1, 2, i + 1)\n    # 境界からの距離を求めて（横幅）その点をプロット\n    mask = np.abs(x_ind - 300 / 250 * (y_ind -50)) &lt;= width\n    img[mask] = 0\n    ax.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.indices((M, N))\n\n\narray([[[  0,   0,   0, ...,   0,   0,   0],\n        [  1,   1,   1, ...,   1,   1,   1],\n        [  2,   2,   2, ...,   2,   2,   2],\n        ...,\n        [297, 297, 297, ..., 297, 297, 297],\n        [298, 298, 298, ..., 298, 298, 298],\n        [299, 299, 299, ..., 299, 299, 299]],\n\n       [[  0,   1,   2, ..., 397, 398, 399],\n        [  0,   1,   2, ..., 397, 398, 399],\n        [  0,   1,   2, ..., 397, 398, 399],\n        ...,\n        [  0,   1,   2, ..., 397, 398, 399],\n        [  0,   1,   2, ..., 397, 398, 399],\n        [  0,   1,   2, ..., 397, 398, 399]]])",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#matplotlibでの複数グラフの書き方",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#matplotlibでの複数グラフの書き方",
    "title": "テンソルと画像処理",
    "section": "4 matplotlibでの複数グラフの書き方",
    "text": "4 matplotlibでの複数グラフの書き方\n\n\nCode\nx = np.arange(100)\nplt.plot(x, np.log(x))\nplt.show()\n\n\n\n\n\n\n\n\n\nfig.gcaはget current axesの略であり，axesを取得する際に使うことが可能である. fig.get_axesはfigureの中にどのaxesがあるのかを見ることが可能である.\n\n\nCode\nfig = plt.figure()\nfig.get_axes()\n\n\n[]\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n\n\n\nCode\nax = fig.gca()\n\n\n\n\nCode\nfig.get_axes()\n\n\n[&lt;AxesSubplot:&gt;]\n\n\n\n\nCode\nfig.clear() # axesが解消される\nfig.get_axes()\n\n\n[]\n\n\n\n4.1 複数のaxes\n\n\nCode\nfig = plt.figure()\nfor i in range(4):\n    # グラフの配置の形 x, y とグラフの位置\n    ax = fig.add_subplot(2, 2, i + 1)\n    ax.plot(x, np.sin(x / (i + 1)))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# プロットしてもaxesは残っている\nfig.get_axes()\n\n\n[&lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;, &lt;AxesSubplot:&gt;]\n\n\n\n\n4.2 画像をプロット\n\n\nCode\nfig = plt.figure(figsize = (8, 8))\nfor i in range(9):\n    ax = fig.add_subplot(3, 3, i + 1)\n    print(255 * i // 10)\n    img = np.full((400, 400), 255 * i // 10, dtype = np.uint8)\n    ax.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n0\n25\n51\n76\n102\n127\n153\n178\n204\n\n\n\n\n\n\n\n\n\n\n\nCode\n1 // 10\n\n\n0",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#市松模様を作る",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#市松模様を作る",
    "title": "テンソルと画像処理",
    "section": "5 市松模様を作る",
    "text": "5 市松模様を作る\n\n\nCode\nalist = [25, 50, 100]\nfig   = plt.figure(figsize = (16, 8))\nfor l, a in enumerate(alist):\n    img = np.zeros((330, 430), dtype = np.uint8)\n    for i in range(int(np.ceil(img.shape[0] / a))):\n        for j in range(int(np.ceil(img.shape[1] / a))):\n            if (i + j) % 2 == 1:\n                # numpy配列は要素数を超えても問題がない！！！！\n                img[i * a:(i + 1) * a, j * a:(j + 1) * a] = 255\n    ax = fig.add_subplot(1, 3, l + 1)\n    ax.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\nCode\na = np.arange(10)\na[1:12]\n\n\narray([1, 2, 3, 4, 5, 6, 7, 8, 9])",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#グラデーション",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#グラデーション",
    "title": "テンソルと画像処理",
    "section": "6 グラデーション",
    "text": "6 グラデーション\n\n\nCode\nstart_color = 50\nend_color   = 255\nfig = plt.figure(figsize = (14, 8))\n\n# 横方向\nimg = np.zeros((300, 400), np.uint8)\nfor i in range(img.shape[0]):\n    img[i,:] = np.linspace(start_color, end_color, img.shape[1], dtype = img.dtype)\nax = plt.subplot(1, 2, 1)\nax.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n# 縦方向\nimg = np.zeros((300, 400), np.uint8)\nfor i in range(img.shape[1]):\n    img[:, i] = np.linspace(start_color, end_color, img.shape[0], dtype = img.dtype)\nax = plt.subplot(1, 2, 2)\nax.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\nブロードキャストを使った特殊な書き方.\n\n\nCode\nstart_color = 50\nend_color = 255\nfig = plt.figure(figsize=(14, 8))\nimg_shape = (300, 400)# 横方向\nimg = np.broadcast_to(np.linspace(start_color,end_color, img.shape[1],                                  dtype=np.uint8)[None, :], img_shape)\nax = plt.subplot(1, 2, 1)\nax.imshow(img, cmap=\"gray\", vmin=0, vmax=255)# 縦方向\nimg = np.broadcast_to(np.linspace(start_color,end_color, img.shape[0],                                   dtype=np.uint8)[:,None], img_shape)\nax = plt.subplot(1, 2, 2)\nax.imshow(img, cmap=\"gray\", vmin=0, vmax=255)\nplt.show()\n\n\n\n\n\n\n\n\n\n次の部分が本質なのかと思うけど，内容的にはブロードキャストを明示的に実行しているということなのかと思う。その際に，ブロードキャストの対象の形状をブロードキャストする先の形状に合わせていることがわかる.\n\n\nCode\nnp.broadcast_to(\n    np.linspace(start_color,end_color, img.shape[1], dtype=np.uint8)[None, :],  \n    img_shape)\n\n\narray([[ 50,  50,  51, ..., 253, 254, 255],\n       [ 50,  50,  51, ..., 253, 254, 255],\n       [ 50,  50,  51, ..., 253, 254, 255],\n       ...,\n       [ 50,  50,  51, ..., 253, 254, 255],\n       [ 50,  50,  51, ..., 253, 254, 255],\n       [ 50,  50,  51, ..., 253, 254, 255]], dtype=uint8)\n\n\n\n6.1 斜めのグラデーション\n図書を見てもらうとわかるように，斜めのグラデーションを作るには，線形補間の応用で大丈夫である。線形補間の式は同じであるが，行により横軸が変わっていくイメージである。\n\n\nCode\nstart_color = 0\nend_color   = 255\nfig = plt.figure(figsize = (14, 8))\n\nimg1 = np.zeros((300, 400), dtype = np.uint8)\nimg2 = np.zeros_like(img1)\n\nfor i in range(img.shape[0]):\n    left  = start_color + (end_color - start_color) * i / img.shape[0] / 2\n    right = (end_color + start_color) / 2 + (end_color - start_color) * i / img.shape[0] / 2\n    img1[i,:]  = np.linspace(left, right, img1.shape[1])\n    img2[i, :] = np.linspace(left, right, img2.shape[1])[::-1]\n\nax = fig.add_subplot(1, 2, 1)\nax.imshow(img1, cmap = \"gray\", vmin = 0, vmax = 255)\nax = fig.add_subplot(1, 2, 2)\nax.imshow(img2, cmap = \"gray\", vmin = 0, vmax = 255)\nplt.show()",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#円の描画",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#円の描画",
    "title": "テンソルと画像処理",
    "section": "7 円の描画",
    "text": "7 円の描画\n\n\nCode\nimg = np.full((300, 400), 255,  dtype = np.uint8)\ny_ind, x_ind = np.indices(img.shape)\nmask = (x_ind - 250) ** 2 + (y_ind - 150) ** 2 &lt;= 100 ** 2\nimg[mask] = 192\n\ngray_show(img)\n\n\n\n\n\n\n\n\n\n\n7.1 円の境界に色を付ける.\n\n\nCode\nimg = np.full((300, 400), 255,  dtype = np.uint8)\ny_ind, x_ind = np.indices(img.shape)\nv = (x_ind - 250) ** 2 + (y_ind - 150) ** 2\nimg[v &lt;= 100 ** 2] = 192\n\n# 境界の設定\nborder_mask = np.logical_and(v &gt;= 99.5 ** 2, v &lt;= 100.5 ** 2)\nimg[border_mask] = 0\n\ngray_show(img)",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#弾の描画",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#弾の描画",
    "title": "テンソルと画像処理",
    "section": "8 弾の描画",
    "text": "8 弾の描画\n\n\nCode\nimg = np.full((300, 400), 255, dtype = np.uint8)\ny_ind, x_ind = np.indices(img.shape)\n\nv = (x_ind - 200) ** 2 / 150 ** 2 + (y_ind - 150) ** 2 / 75 ** 2\nimg[v &lt;= 1] = 192\n\nborder_mask = np.logical_and(v &gt;= .995 ** 2, v &lt;= 1.005 ** 2)\nimg[border_mask ] = 0\n\ngray_show(img)",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#極座標",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#極座標",
    "title": "テンソルと画像処理",
    "section": "9 極座標",
    "text": "9 極座標\n\n\nCode\ndef cart2pol(x, y):\n    r = np.sqrt(x ** 2 + y ** 2)\n    tehta = np.arctan2(y, x)\n    return (r, theta)\n\ndef pol2cart(r, theta):\n    x = r * np.cos(theta)\n    y = r * np.sin(theta)\n    return (x, y)\n\n\n\n\nCode\nimg = np.full((300, 400), 255, dtype = np.uint8)\ny_ind, x_ind = np.indices(img.shape)\nr, theta = np.sqrt((x_ind - 120) ** 2 + (y_ind - 150) ** 2), np.arctan2(y_ind-150, x_ind-120)\nmask = r &lt;= 100\nimg[mask] = 190\n\ngray_show(img)\n\n\n\n\n\n\n\n\n\n\n9.1 レムニスケート\n\n\nCode\nimg = np.full((300, 400), 255, dtype = np.uint8)\ny_ind, x_ind = np.indices(img.shape)\nr, theta = np.sqrt((x_ind - 200) ** 2 + (y_ind - 150) ** 2), np.arctan2(y_ind-150, x_ind-200)\n\na = 120\nmask = r ** 2 - 2 * a ** 2 * np.cos(2 * theta) &lt;= 0\nimg[mask] = 190\n\ngray_show(img)",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#カラー画像の作り方",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#カラー画像の作り方",
    "title": "テンソルと画像処理",
    "section": "10 カラー画像の作り方",
    "text": "10 カラー画像の作り方\n\n\nCode\nr_ch = np.zeros((300, 400), dtype = np.uint8)\ng_ch = np.zeros_like(r_ch)\nb_ch = np.zeros_like(g_ch)\n\n# 3次元目に新しい軸を追加する. \nimg = np.stack([r_ch, g_ch, b_ch], axis = -1)\nimg.shape\n\n\n(300, 400, 3)\n\n\nあるいは,\n\n\nCode\nimg = np.zeros((300, 400, 3), dtype = np.uint8)\nimg.shape\n\n\n(300, 400, 3)\n\n\n\n\nCode\nfig = plt.figure(figsize = (16, 7))\nfor i in range(3):\n    img = np.zeros((300, 400, 3), dtype = np.uint8)\n    # 3チャンネルのうち1つだけ塗りつぶしを行う. \n    img[..., i] = 255\n    ax = fig.add_subplot(1, 3, i + 1)\n    # カラー画像の場合には範囲指定をする必要がない\n    ax.imshow(img)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nimg[0, 0, :]\n\n\narray([  0,   0, 255], dtype=uint8)",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#カラー画像での矩形塗りつぶし",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#カラー画像での矩形塗りつぶし",
    "title": "テンソルと画像処理",
    "section": "11 カラー画像での矩形塗りつぶし",
    "text": "11 カラー画像での矩形塗りつぶし\n\n\nCode\nimg = np.full((300, 400, 3), 255, dtype = np.uint8)\n# 境界用の矩形\nimg[149:251, 99:301, :] = 0\n# 境界用の矩形の内側を塗りつぶし\n# [None, None, :]はなくてもよい. ただし次元が合っている感が出るのであった方がわかりやすい気がする. \nimg[150:250, 100:300, :] = np.array([64, 230, 209], dtype = np.uint8)[None, None, :]\n\nplt.imshow(img)\nplt.show()",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#テキストの描画",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#テキストの描画",
    "title": "テンソルと画像処理",
    "section": "12 テキストの描画",
    "text": "12 テキストの描画\n\n\nCode\nfrom PIL import Image, ImageDraw, ImageFont\n\nwith Image.new(\"RGB\", (400, 300), color=(255, 255,255)) as canvas:    \n    draw = ImageDraw.Draw(canvas)    \n    font =ImageFont.truetype(\"numpy_book/fonts/M_PLUS_1p/MPLUS1p-Bold.ttf\", 38)    \n    draw.text((30, 30), \"こしあんは美味しい\", font=font, fill=0)\n\n    plt.imshow(canvas)\n    plt.show()\n    \n    text_array = np.array(canvas)\n    print(text_array.dtype, text_array.shape)\n\n\n\n\n\n\n\n\n\nuint8 (300, 400, 3)\n\n\n\n12.1 テキスト画像のトリミング\n\n\nCode\ntext_points = np.where(text_array[:, :, 0] == 0) # 白か黑かしかないのでR=0で判定\ntext_area = [np.min(text_points[0]),np.min(text_points[1]), np.max(text_points[0]),np.max(text_points[1])]\nprint(text_area) # [40, 35, 73, 367]\ntext_array_trimmed =text_array[text_area[0]:text_area[2]+1,text_area[1]:text_area[3]+1, :] # 終端に+1をする\n\nplt.imshow(text_array_trimmed)\nplt.show()\n\n\n[40, 35, 73, 367]\n\n\n\n\n\n\n\n\n\n\n\n12.2 テキストへのグラデーション\n\n\nCode\ngradients = np.empty_like(text_array_trimmed)# (R,G,B) = (255, 51, 51) -&gt; (51, 51, 255)への縦グラデーション\ngradients[:,:,0] = np.linspace(255, 51,gradients.shape[1], dtype=np.uint8)[None, :]\ngradients[:,:,1] = 51\ngradients[:,:,2] = np.linspace(51, 255,gradients.shape[1], dtype=np.uint8)[None, :]\nplt.imshow(gradients)\nplt.show()# テキストのレイヤーマスク\nlayer_mask = (text_array_trimmed[:,:,0] ==0).astype(np.uint8)[:, :, None]# グラデーションをテキストに乗せる\noutput = text_array_trimmed * (1-layer_mask) + gradients * layer_mask\nplt.imshow(output)\nplt.show()",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#pillowとnumpy配列の相互変換",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#pillowとnumpy配列の相互変換",
    "title": "テンソルと画像処理",
    "section": "13 PillowとNumPy配列の相互変換",
    "text": "13 PillowとNumPy配列の相互変換\n\n13.1 Pillow -&gt; NumPyへの変換\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    x = np.array(img)\nprint(x.shape, x.dtype)\nplt.imshow(x)\n\n\n(960, 1280, 3) uint8\n\n\n\n\n\n\n\n\n\n\n\nCode\n# ネガポジ反転\nnegpos = 255 - x\nplt.imshow(negpos)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n13.2 NumPy配列 ⇒ Pillowの変換\n\n\nCode\nx = np.zeros((4000, 6000, 3), dtype = np.uint8)\nx[..., 0] = np.linspace(0, 255, x.shape[1], dtype = np.uint8)[None, :]\nx[..., 1] = 255\nx[..., 2] = np.linspace(255, 0, x.shape[1], dtype = np.uint8)[None, :]\n\nwith Image.fromarray(x) as img:\n    img.save(\"out/color_gradient.png\")\n\n\n\n\nCode\nwith Image.open(\"dat/2019-01-03 11.08.15.jpg\") as img:\n    plt.imshow(img)\n    x = np.array(img)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nx.shape\n\n\n(3024, 4032, 3)\n\n\n\n\nCode\nfig = plt.figure(figsize = (24, 8))\n\nfor i in range(3):\n    v = np.zeros_like(x)\n    v[..., i] = x[..., i]\n    ax = fig.add_subplot(3, 1, 1 + i)\n    ax.imshow(v)",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#演習問題",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理.html#演習問題",
    "title": "テンソルと画像処理",
    "section": "14 演習問題",
    "text": "14 演習問題\n\n14.1 Q1:べた塗りグレースケール\n\n\nCode\nimg1 = np.zeros((128, 128), dtype = np.uint8)\n\nfig = plt.figure(figsize = (24, 8))\ncolors = [0, 128, 192]\nfor i, c in enumerate(colors):\n    v = np.zeros_like(img1)\n    v[...] = c\n    ax = fig.add_subplot(3, 1, 1 + i)\n    ax.imshow(v, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\nCode\nimg1 = np.zeros((512, 512), dtype = np.uint8)\n\nfig = plt.figure(figsize = (24, 8))\ncolors = [0, 128, 192]\nfor i, c in enumerate(colors):\n    v = np.zeros_like(img1)\n    v[...] = c\n    ax = fig.add_subplot(3, 1, 1 + i)\n    ax.imshow(v, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.2 Q2:四角形のぬりつぶし\n\n\nCode\nW, H = 640, 480\ncanvas = np.full((W, H), 255, dtype = np.uint8)\ncanvas[100:(100 + 200), 100:(100 + 100)] = 0\ncanvas[400:(400 + 150), 200:(200 + 250)] = 128\n\nplt.imshow(canvas, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.3 画像の保存\n\n\nCode\n\nplt.imsave(\"out/question03.png\", canvas, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n14.4 円の塗りつぶし\n\n\nCode\nW, H = 640, 480\nimg = np.full((W, H), 255, dtype = np.uint8)\n\n\n\n\nCode\nox, oy = 200, 240\nr = 100\nc = 64\nidy, idx = np.indices(img.shape)\nmask = (idx - ox) ** 2 + (idy - oy) ** 2 &lt;= r ** 2\nimg[mask] = c\n\ngray_show(img)\n\n\n\n\n\n\n\n\n\n\n\n14.5 Q5:平方数の直線\n\n\nCode\nW, H = 640, 480\nimg = np.full((W, H), 255, dtype = np.uint8)\n\n\n\n\nCode\nW, H = 640, 480\nimg = np.full((W, H), 255, dtype = np.uint8)\nN = np.ceil(np.sqrt(H))\nwidth = .5\nfor i in range(1, int(N)-1):\n    oy, ox = i ** 2, i ** 2\n    oyn, oxn = (i + 1) ** 2, (i + 1) ** 2\n    idy, idx = np.indices((oxn - ox, oyn - oy))\n    xhat     = (oxn - ox) / (oyn - oy) * idy + ox\n    mask     = np.abs(xhat - (ox + idx)) &lt;= width\n    img[oy:oyn, ox:oxn] = 255 * np.logical_not(mask)\n\nfig, ax = plt.subplots(1, 1)\nax.set_aspect(\"equal\")\nax.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.6 Q6:斜めの線\n\n\nCode\nW, H = 640, 480\nimg = np.full((H, W), 255, dtype = np.uint8)\nlwd = 3\n\nidy, idx = np.indices(img.shape)\nxhat   = idy / 2\nxslide = range(-450, 601, 50)\n\nfor i in xslide:\n    mask = np.abs(xhat - (idx - i)) &lt;= lwd\n    img[mask] = 0\n\nplt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nidx\n\n\narray([[  0,   1,   2, ..., 637, 638, 639],\n       [  0,   1,   2, ..., 637, 638, 639],\n       [  0,   1,   2, ..., 637, 638, 639],\n       ...,\n       [  0,   1,   2, ..., 637, 638, 639],\n       [  0,   1,   2, ..., 637, 638, 639],\n       [  0,   1,   2, ..., 637, 638, 639]])\n\n\n\n\nCode\nxhat\n\n\narray([[  0. ,   0. ,   0. , ...,   0. ,   0. ,   0. ],\n       [  0.5,   0.5,   0.5, ...,   0.5,   0.5,   0.5],\n       [  1. ,   1. ,   1. , ...,   1. ,   1. ,   1. ],\n       ...,\n       [238.5, 238.5, 238.5, ..., 238.5, 238.5, 238.5],\n       [239. , 239. , 239. , ..., 239. , 239. , 239. ],\n       [239.5, 239.5, 239.5, ..., 239.5, 239.5, 239.5]])\n\n\n\n\n14.7 Q7:同心円\n\n\nCode\nW, H = 640, 480\nimg = np.full((H, W), 255, dtype = np.uint8)\nidy, idx = np.indices(img.shape)\nox, oy = (320, 240)\nr_seq  = range(50, 351, 50)\nlwd    = 0.5\n\ndist = (idy - oy) ** 2 + (idx - ox) ** 2 \n\nfor r in r_seq:\n    mask = np.logical_and(dist &gt;= (r-lwd)**2, dist &lt;= (r+lwd)**2)\n    img[mask] = 0\n\nplt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.8 Q8:アンチエイリアス\n\n\nCode\nW, H = 640, 480\nimg = np.full((H, W), 255, dtype = np.uint8)\nidy, idx = np.indices(img.shape)\nox, oy = (320, 240)\nr_seq  = range(50, 351, 50)\nlwd    = 0.5\n\ndist = (idy - oy) ** 2 + (idx - ox) ** 2 \n\nfor r in r_seq:\n    # 2段階で処理を行う\n    # 少し広い範囲を灰色に塗り\n    # 通常の範囲を黒で塗りつぶす\n    mask = np.logical_and(dist &gt;= (r-1)**2, dist &lt;= (r+1)**2)\n    img[mask] = 128\n    mask = np.logical_and(dist &gt;= (r-.5)**2, dist &lt;= (r+.5)**2)\n    img[mask] = 0\n\nplt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.9 Q9:スーパー楕円\n\n\nCode\nW, H = 640, 480\nox, oy = W // 2, H // 2\nimg = np.full((H, W), 255, dtype = np.uint8)\nidy, idx = np.indices(img.shape)\n\nM = np.linspace(.5, 2, 15)\na = 280\nb = 200\n\nfor i, m in enumerate(M):\n    z = np.abs((idx - ox) / a) ** m + np.abs((idy - oy) / b) ** m\n    mask = np.logical_and(z &gt;= 0.995, z &lt;= 1.005)\n    img[mask] = 0\n\nplt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.10 Q10:カージオイド\n\n\nCode\nW, H = 640, 480\nox, oy = 320, 240\nimg = np.full((H, W), 255, dtype = np.uint8)\nidy, idx = np.indices(img.shape)\n\na = 150\nc = 192\n\ntheta = np.arctan2(idy-oy, idx-ox)\nr     = a * (1 + np.cos(theta))\nd2    = (idy - oy) ** 2 + (idx - ox) ** 2\nmask  = (d2 - r ** 2) &lt;= 0\nimg[mask] = c\n\nplt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.11 Q11:円周率の近似値\n\n\nCode\nr_seq = [100, 200, 500, 1000, 2000, 5000, 10000]\n\nfor r in r_seq:\n    idy, idx = np.indices((r, r))\n    area = r * r\n    mask = idy ** 2 + idx ** 2 &lt;= area\n    pi   = np.sum(mask) / area  * 4\n    print(r, pi, np.pi - pi)\n\n\n100 3.1812 -0.03960734641020691\n200 3.1606 -0.01900734641020696\n500 3.149376 -0.007783346410207059\n1000 3.145544 -0.003951346410207002\n2000 3.143585 -0.0019923464102067356\n5000 3.14238688 -0.0007942264102069885\n10000 3.14199048 -0.000397826410206914\n\n\n\n\n14.12 Q12:積分の数値計算\n\n\nCode\nprint (3 / 8)\n\n\n0.375\n\n\n\n\nCode\na_seq = [100, 200, 500, 1000, 2000, 5000]\ntrue      = 3 / 8\ngrid_size = 10000\nidy, idx  = np.indices((grid_size, grid_size))\nz = np.abs(idy - grid_size / 2) ** (2 / 3) + np.abs(idx - grid_size / 2) ** (2 / 3)\n\nfig = plt.figure(figsize = (12, 12))\nfor i, a in enumerate(a_seq, start = 1):\n    img = np.full((grid_size, grid_size), 255)\n    mask = z &lt;= a ** (2 / 3)\n    img[mask] = 128\n    print(a, \"snorm = \", np.sum(mask) / np.pi / a / a)\n    if a == a_seq[-1]:\n        ax = fig.add_subplot(1, 1, 1)\n        ax.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n100 snorm =  0.3750008769131238\n200 snorm =  0.37497700367166004\n500 snorm =  0.37503652762037637\n1000 snorm =  0.37502920649299415\n2000 snorm =  0.3750070043784328\n5000 snorm =  0.37500106789905546\n\n\n\n\n\n\n\n\n\n\n\n14.13 Q13:オウム貝の螺旋\n\n\nCode\nW, H = 640, 480\nimg = np.full((H, W), 255, dtype = np.uint8)\nidy, idx = np.indices(img.shape)\n\nd2 = (idy - H // 2) ** 2 + (idx - W // 2) ** 2\nthetas  = np.arctan2(idy - H // 2, idx - W // 2)\noffsets = 2 * np.pi * np.arange(10)\n\nfor offset in offsets:\n    r  = 2 ** ((offset + thetas) / 7)\n    lower2 = (r - .5) ** 2\n    upper2 = (r + .5) ** 2\n    mask = np.logical_and(d2 &gt;= lower2, d2 &lt;= upper2)\n    img[mask] = 0\n\n\ngray_show(img)\n\n\n\n\n\n\n\n\n\n\n\nCode\nimg = np.full((480, 640), 255, dtype=np.uint8)\ny_ind, x_ind = np.indices(img.shape)\nx_ind, y_ind = x_ind - 320, y_ind - 240\nr, theta = np.sqrt(x_ind**2 + y_ind**2), np.arctan2(y_ind, x_ind)\nfor i in range(10):\n    v = 2 ** ((theta + 2 * i * np.pi) / 7)\n    # ここの-vってなんだっけ・・・・\n    mask = np.logical_and(v&gt;=r-v-0.5, v&lt;=r-v+0.5)\n    img[mask] = 0\nplt.imshow(img, cmap=\"gray\", vmin=0, vmax=255)\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\narray([0.])\n\n\n\n\nCode\ntheta = np.linspace(0, 20 * np.pi, 10000)\nr     = 2 ** (theta / 7)\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1, projection = \"polar\")\nax.plot(theta, r)\n\n\n\n\n\n\n\n\n\n\n\n14.14 Q14：グラデーション（１）\n\n\nCode\nW, H   = 600, 600\nimg    = np.full((W, H), 255, dtype = np.uint8)\noy, ox = H // 2, W //2\nR      = 255\n\nidy, idx = np.indices(img.shape)\nr = np.sqrt((idx - ox) ** 2 + (idy - oy) ** 2)\nv = np.clip(r, 0, R)\nimg[:,:] = v\n\n\ngray_show(img)\n\n\n\n\n\n\n\n\n\n\n\n14.15 Q14:グラデーション（２）\n\n\nCode\nW, H   = 600, 600\nimg    = np.full((W, H), 255, dtype = np.uint8)\noy, ox = H // 2, W //2\nidy, idx = np.indices(img.shape)\nidy, idx = idy - oy, idx - ox\n\nv = idy ** 2 + idx ** 2\nwhite = v[10, 300]\nblack = v[300, 300]\nv = 255 * (v - black ) / (white - black)\nv = np.clip(v, 0, 255)\nimg[...] = v\n\ngray_show(img)\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.16 Q16:三角形の描画\n\n\nCode\nW, H   = 600, 600\nimg    = np.full((W, H), 255, dtype = np.uint8)\noy, ox = 0, 0\nidy, idx = np.indices(img.shape)\nidy, idx = idy - oy, idx - ox\nfig = plt.figure()\n\nmask1 = idy - (- 2 * idx + 600) &gt;= 0\nimg1 = img.copy()\nimg1[mask1] = 192\nax1 = fig.add_subplot(1, 2, 1)\nax1.imshow(img1, cmap = \"gray\", vmin = 0, vmax = 255)\n\nmask2 = idy - (2 * idx - 600) &gt;= 0\nimg2 = img.copy()\nimg2[mask2] = 192\nax2 = fig.add_subplot(1, 2, 2)\nax2.imshow(img2, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n\n14.17 Q17：三角形の描画\n\n\nCode\nmask = np.logical_and(mask1, mask2)\nimg[mask] = 192\nplt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.18 Q18:三角形の重心\n\n\nCode\ny, x = np.where(mask)\ngy, gx = np.mean(y), np.mean(x)\nprint(gy, gx)\n\n\n399.49888888888887 300.0\n\n\n\n\n14.19 Q19:任意図形の重心\n\n\nCode\nfile_path = \"numpy_book/imgs/dolphin.png\"\nwith Image.open(file_path) as img:\n    plt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n    dolphin = np.array(img)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndolphin.shape\n\n\n(768, 838)\n\n\n\n\nCode\nmask = dolphin &lt; 255\ny, x = np.where(mask)\ngy, gx = np.mean(y), np.mean(x)\nprint(gy, gx)\n\n\n292.2499105593379 421.6434334534775\n\n\n\n\nCode\nidy, idx = np.indices(dolphin.shape)\nmask = np.logical_or(np.abs(idy - gy) &lt;= .5, np.abs(idx -gx) &lt;= .5)\ndolphin[mask] = 0\nplt.imshow(dolphin, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.20 Q20:カラー画像\n\n\nCode\nW, H, C = 600, 400, 3\nimg = np.full((H, W, C), 0, dtype = np.uint8)\n\nimg[:, :201, 0]    = 255\nimg[:, 201:401, 1] = 255\nimg[:, 401:, 2] = 255\n\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n14.21 Q21:イタリアの国旗\n\n\nCode\nW, H, C = 600, 400, 3\nimg = np.full((H, W, C), 0, dtype = np.uint8)\n\nimg[:, :201]     = np.array([0, 98, 33], dtype = np.uint8)[None, None, :]\nimg[:, 201:401]  = np.array([255, 255, 255], dtype = np.uint8)[None, None, :]\nimg[:, 401:]     = np.array([223, 0, 36], dtype = np.uint8)[None, None, :]\n\n\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n14.22 Q22：3階テンソルの乱数\nカラー画像は３次元なので３階テンソルと呼ばれる。\n\n\nCode\nimg = np.random.rand(250, 200, 10)\nimg.shape\n\n\n(250, 200, 10)\n\n\n\n\n14.23 Q23:ホワイトノイズ\n\n\nCode\nimg = np.random.rand(400, 600, 3) * 255\nprint(img[0])\nimg = img.astype(np.uint8)\nprint(img[0])\n\n\n[[237.60024439 180.59405009 120.8702287 ]\n [162.67990876 175.14638539 171.29191215]\n [ 95.22278728 124.35894627 206.65675962]\n ...\n [220.32574568 190.07987688  52.46673445]\n [222.74616695 110.07846665 166.04163626]\n [176.47022485  98.69550035 199.21641395]]\n[[237 180 120]\n [162 175 171]\n [ 95 124 206]\n ...\n [220 190  52]\n [222 110 166]\n [176  98 199]]\n\n\n\n\nCode\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n14.24 Q24:３階テンソルのconcatenate\n\n\nCode\nx = np.concatenate(\n    [\n        np.random.rand(128, 128, 3),\n        np.random.rand(128, 128, 3)\n    ]\n    ,axis=0)\nx.shape\n\n\n(256, 128, 3)\n\n\n\n\nCode\nx = np.concatenate(\n    [\n        np.random.rand(128, 128, 3),\n        np.random.rand(128, 128, 3)\n    ]\n    ,axis=1)\nx.shape\n\n\n(128, 256, 3)\n\n\n\n\nCode\nx = np.concatenate(\n    [\n        np.random.rand(128, 128, 3),\n        np.random.rand(128, 128, 3)\n    ]\n    ,axis=2)\nx.shape\n\n\n(128, 128, 6)\n\n\n\n\nCode\nx = np.concatenate(\n    [\n        np.random.rand(128, 128, 3),\n        np.random.rand(128, 128, 10)\n    ]\n    ,axis=-1)\nx.shape\n\n\n(128, 128, 13)\n\n\n\n\nCode\nx = np.concatenate(\n    [\n        np.random.rand(6, 8, 128),\n        np.random.rand(6, 8, 256), \n        np.random.rand(6, 8, 128)\n    ]\n    ,axis=-1)\nx.shape\n\n\n(6, 8, 512)\n\n\n\n\n14.25 Q25:３階テンソルのstack\n\n\nCode\nx = np.stack(\n    [\n        np.random.rand(6, 8, 128),\n        np.random.rand(6, 8, 128)\n    ]\n    ,axis= 0)\nx.shape\n\n\n(2, 6, 8, 128)\n\n\n\n\nCode\nx = np.stack(\n    [\n        np.random.randn(128, 128, 3) for i in range(5)\n    ], \n    axis = -1)\nx.shape\n\n\n(128, 128, 3, 5)\n\n\n\n\n14.26 Q26:縦方向の画像の結合\n\n\nCode\nfile_path = \"./numpy_book/imgs/flower01.jpg\"\nwith Image.open(file_path) as img:\n    x = np.array(img)\n\ny = np.concatenate([x] * 2, axis = 0)\nplt.imshow(y)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.imshow(np.tile(x, (2, 1, 1)))\n\n\n\n\n\n\n\n\n\n\n\n14.27 Q27：横方向の画像の結合\n\n\nCode\ny = np.concatenate([x] * 2, axis = 1)\nplt.imshow(y)\n\n\n\n\n\n\n\n\n\n\n\n14.28 Q28:日の丸\n\n\nCode\nW = 1500\nH = W * 2 // 3\nD = H * 3 // 5\nimg   = np.full((H, W, 3), 255, dtype = np.uint8)\nwhite = np.array([255, 255, 255])\nred   = np.array([194, 31, 1])\n\noy, ox = H // 2, W //2\nidy, idx = np.indices((H, W))\nidy, idx = idy - oy, idx - ox\n\nr    = np.sqrt(idy ** 2 + idx **2)\nmask = r &lt;= D / 2\n\n\n\n\nCode\nimg[mask, :] = red[None, None, :]\n\n\n\n\nCode\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n14.29 Q29：Z旗\n\n\nCode\nW = 1500\nH = W * 2 // 3\nslope = H / W\nimg   = np.full((H, W, 3), 255, dtype = np.uint8)\nwhite  = np.array([255, 255, 255])\nyellow = np.array([255, 255, 0])\nblue   = np.array([0, 0, 255])\nred    = np.array([194, 31, 1])\nblack  = np.array([0, 0, 0])\n\nidy, idx = np.indices((H, W))\n\nmask1 = idy - (slope * idx) &lt;= 0\nmask2 = idy - (H - slope * idx) &lt;= 0\n\nimg[np.logical_and(mask1, mask2), :] = yellow[None, None, :]\nimg[np.logical_and(mask1, np.logical_not(mask2)), :] = blue[None, None, :]\nimg[np.logical_and(np.logical_not(mask1), mask2), :] = black[None, None, :]\nimg[np.logical_and(np.logical_not(mask1), np.logical_not(mask2)), :] = red[None, None, :]\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n14.30 Q30：セーシャルの国旗\n\n\nCode\nblue   = np.array([0, 47, 108])\nyellow = np.array([255, 209, 65])\nred    = np.array([210, 39, 48])\nwhite  = np.array([255, 255, 255])\ngreen  = np.array([0, 122, 51])\n\n\n\n\nCode\nW = 1200\nH = W // 2\nimg   = np.full((H, W, 3), blue, dtype = np.uint8)\nidy, idx = np.indices((H, W))\n\n\nmask1 = idy - (3 * H / W * idx) &lt;= 0\nimg[mask1, :] = yellow[None, None, :]\nmask2 = idy - (3 * H / W / 2 * idx) &lt;= 0\nimg[mask2, :] = red[None, None, :]\nmask3 = idy - (2 * H / W / 3 * idx) &lt;= 0\nimg[mask3, :] = white[None, None, :]\nmask4 = idy - (H / W / 3 * idx) &lt;= 0\nimg[mask4, :] = green[None, None, :]\n\nplt.imshow(img[::-1])\n\n\n\n\n\n\n\n\n\n\n\n14.31 Q31:画像の合成\n\n\nCode\nfile = \"numpy_book/imgs/flower01.jpg\"\nwith Image.open(file) as img:\n    back = np.array(img)\n\nH, W, _ = back.shape\n\nplt.imshow(back)\n\n\n\n\n\n\n\n\n\n\n\nCode\ngradation = np.full_like(back, 255)\ngradation[..., 0] = np.linspace(255, 128, W)[None, :]\ngradation[..., 1] = 0\ngradation[..., 2] = np.linspace(128, 255, W)[None, :]\nplt.imshow(gradation)\n\n\n\n\n\n\n\n\n\n\n\nCode\nidy, idx = np.indices((H, W))\nidy, idx = idy - H // 2, idx - W //2\nr = np.sqrt(idy ** 2 + idx ** 2)\nmask = r &lt;= H // 2\n\n\n\n\nCode\nmask\n\n\narray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       ...,\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])\n\n\n\n\nCode\nimg = back.copy()\n\n# 3次元でもこれで大丈夫\nimg[mask, :] = gradation[mask, :]\n\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n14.32 Q32:RGBA画像\nAはα値の意味一般に不透明度を扱う　パラメータである.\n\n\nCode\nH, W = 600, 900\ngradation = np.full((H, W, 4), 255, dtype = np.uint8)\ngradation[..., 0] = np.linspace(255, 128, W)[None, :]\ngradation[..., 1] = 0\ngradation[..., 2] = np.linspace(128, 255, W)[None, :]\nplt.imshow(gradation)\n\n\n\n\n\n\n\n\n\n\n\nCode\nidy, idx = np.indices((H, W))\nidy, idx = idy - H // 2, idx - W //2\nr = np.sqrt(idy ** 2 + idx ** 2)\nmask = r &lt;= H // 2\nplt.imshow(mask)\n\n\n\n\n\n\n\n\n\n\n\nCode\ngradation[mask, 3] = 255\ngradation[np.logical_not(mask), 3] = 0\n\n\n\n\nCode\nplt.imshow(gradation)\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith Image.fromarray(gradation) as img:\n    img.save(\"out/gradation.png\")\n\n\n\n\n14.33 Q33:RGBA画像２\n\n\nCode\ngradation[np.logical_not(mask), 3] = 96\nwith Image.fromarray(gradation) as img:\n    img.save(\"out/gradation2.png\")\n    plt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n14.34 Q34：透画像の合成\n\n\nCode\nwith Image.open(\"numpy_book/imgs/heart01.png\") as img:\n    heart = np.array(img)\n\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    flower = np.array(img)\n    \n\n\n\n\nCode\nheart.shape, flower.shape\n\n\n((960, 1280, 4), (960, 1280, 3))\n\n\n\n\nCode\nmask = heart[..., 3] &gt; 0\nflower[mask, :3] = heart[mask, :3]\nplt.imshow(flower)\n\n\n\n\n\n\n\n\n\n\n\n14.35 Q35：クロマキー合成\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    flower = np.array(img)\n\nwith Image.open(\"numpy_book/imgs/girl01_bb.png\") as img:\n    bb = np.array(img)\n    plt.imshow(bb)\n    \n\n\n\n\n\n\n\n\n\n\n\nCode\nbb.shape\n\n\n(697, 636, 3)\n\n\n\n\nCode\n# 青色の判定\nblue = np.array([0, 0, 255])\nmask = np.sqrt(np.sum((bb - blue[None, None, :]) ** 2, axis = 2)) &lt;= 10\n\n\n\n\nCode\nH_bb, W_bb, _ = bb.shape\nH_bb2, W_bb2  = H_bb // 2, W_bb // 2\nH_f, W_f, _ = flower.shape\nH_f2, W_f2 = H_f // 2, W_f //2\nflower_sub = flower[(H_f2 - H_bb2):(H_f2 - H_bb2) + H_bb, (W_f2 - W_bb2):(W_f2 - W_bb2) + W_bb, :]\nflower_sub.shape\n\n\n(697, 636, 3)\n\n\n\n\nCode\nplt.imshow(flower_sub)\n\n\n\n\n\n\n\n\n\n\n\nCode\nbb[mask, :] = flower_sub[mask, :]\nplt.imshow(bb)\n\n\n\n\n\n\n\n\n\n\n\n14.36 Q36：アルファブレンド\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    flower = np.array(img.convert(\"RGBA\")) / 255 * .7\n\nwith Image.open(\"numpy_book/imgs/heart01.png\") as img:\n    heart = np.array(img) / 255 * .3\n\n\n\n\nCode\nflower.shape, heart.shape\n\n\n((960, 1280, 4), (960, 1280, 4))\n\n\n\n\nCode\nimg_out = np.empty_like(flower)\nimg_out[...,  3] = flower[..., 3] + heart[..., 3] * (1 - flower[..., 3])\nimg_out[..., :3] = \\\n    (flower[..., :3] * flower[..., 3][:,:,None] + heart[..., :3] * heart[..., 3][:,:,None] *  (1 - flower[..., 3])[:,:,None]) / \\\n    img_out[..., 3][:,:,None]\nimg_out = (255 * img_out).astype(np.uint8)\nimg_out[img_out[:,:,3] == 0, :3] = 0\n\n\n\n\nCode\nimg_out[..., 3]\n\n\narray([[178, 178, 178, ..., 178, 178, 178],\n       [178, 178, 178, ..., 178, 178, 178],\n       [178, 178, 178, ..., 178, 178, 178],\n       ...,\n       [178, 178, 178, ..., 178, 178, 178],\n       [178, 178, 178, ..., 178, 178, 178],\n       [178, 178, 178, ..., 178, 178, 178]], dtype=uint8)\n\n\n\n\nCode\nplt.imshow(img_out)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef alpha_blend(sa, img_s, img_d):\n    da = 1 - sa\n    img_s = img_s.copy() * sa\n    img_d = img_d.copy() * da\n    img_out = np.empty_like(img_s)\n    img_out[...,  3] = img_s[..., 3] + img_d[..., 3] * (1 - img_s[..., 3])\n    img_out[..., :3] = \\\n        (img_s[..., :3] * img_s[..., 3][:,:,None] + img_d[..., :3] * img_d[..., 3][:,:,None] *  (1 - img_s[..., 3])[:,:,None]) / \\\n        img_out[..., 3][:,:,None]\n    img_out = (255 * img_out).astype(np.uint8)\n    img_out[img_out[:,:,3] == 0, :3] = 0\n    return img_out\n\n\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    flower = np.array(img.convert(\"RGBA\")) / 255 \n\nwith Image.open(\"numpy_book/imgs/heart01.png\") as img:\n    heart = np.array(img) / 255 \n\nfig = plt.figure(figsize = (24,24))\nalphas = np.arange(1, 10) / 10\n\nfor i, alpha in enumerate(alphas, start = 1):\n    img = alpha_blend(alpha, flower, heart)\n    ax  = fig.add_subplot(3, 3, i)\n    ax.imshow(img)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n14.37 Q38：HSV色空間\nRGBとHSVは相互に変換できるらしい． スクラッチでの実装も可能であるが，Pillowを使うとのこと.\nH：色相，S：彩度，Value：明度\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    img_hsv = img.convert(\"HSV\")\n    flower = np.array(img_hsv)\n\nplt.imshow(flower) # HSVの値としてプロットしている\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.imshow(Image.fromarray(flower, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (24, 24))\n\nhs = [10, 25, 50, 100]\nfor i, h in enumerate(hs, start = 1):\n    img = flower.copy()\n    img[..., 0] = img[..., 0] + h\n    ax = fig.add_subplot(2, 2, i)\n    ax.imshow(Image.fromarray(img, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nflower[..., 0] + 100\n\n\narray([[240, 243, 243, ..., 250, 248, 248],\n       [241, 243, 243, ..., 248, 248, 248],\n       [243, 243, 245, ..., 247, 246, 246],\n       ...,\n       [104, 104, 105, ..., 120, 120, 120],\n       [104, 104, 105, ..., 120, 120, 120],\n       [104, 104, 105, ..., 120, 120, 120]], dtype=uint8)\n\n\n\n\n14.38 Q39:HSV色空間\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    img_hsv = img.convert(\"HSV\")\n    flower = np.array(img_hsv)\n\n\n\n\nCode\nfig = plt.figure(figsize = (18, 18))\n\nhs = [-100, -50, 0, 100]\nfor i, h in enumerate(hs, start = 1):\n    img = flower.copy()\n    img[..., 1] = np.clip(img[..., 1].astype(int) + h, 0, 255)\n    ax = fig.add_subplot(2, 2, i)\n    ax.imshow(Image.fromarray(img, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (18, 18))\nhs = [-100, -50, 0, 100]\nfor i, h in enumerate(hs, start = 1):\n    img = flower.copy()\n    img[..., 2] = np.clip(img[..., 2].astype(int) + h, 0, 255)\n    ax = fig.add_subplot(2, 2, i)\n    ax.imshow(Image.fromarray(img, \"HSV\"))\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n14.39 Q41:色相環\n\n\nCode\nimg = np.full((512, 512, 3), 0, dtype = np.uint8)\noy, ox = 512 // 2, 512 // 2\nidy, idx = np.indices((512, 512))\nidy, idx = idy - oy, idx - ox\n\nH = (np.arctan2(idy, idx) + np.pi) / np.pi / 2 * 255\nS = np.sqrt(idx ** 2 + idy ** 2)\nimg[..., 0] = H\nimg[..., 1] = S\n\nfig = plt.figure(figsize = (18, 18))\nVs = [255, 192, 128, 64]\nfor i, v in enumerate(Vs, start = 1):\n    img[..., 2]      = v\n    img[S &gt;= 256, 2] = 0\n    ax = fig.add_subplot(2, 2, i)\n    ax.imshow(Image.fromarray(img, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\n14.40 Q42:補色\n色相環で円周方向の単体にある色を補色という.\n\n\nCode\nnp.random.seed(42)\nfig = plt.figure(figsize = (10, 10))\nimg = np.full((10, 20, 3), 255, dtype = np.uint8)\n\nfor i in range(50):\n    h, *_ = (np.random.rand(1) * 255).astype(np.uint8)\n    img[..., 0] = h\n    img[..., 1] = 255\n    img[:, 10:, 0] += 126\n    ax = fig.add_subplot(10, 5, i + 1)\n    ax.set_axis_off()\n    ax.imshow(Image.fromarray(img, \"HSV\"))\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.random.seed(42)\nfig = plt.figure(figsize = (10, 10))\nimg = np.full((10, 30, 3), 255, dtype = np.uint8)\n\nfor i in range(50):\n    h, *_ = (np.random.rand(1) * 255).astype(np.uint8)\n    img[..., 0] = h\n    img[..., 1] = 255\n    img[:,  0:10, 0] += 25\n    img[:, 20:30, 0] -= 25\n    ax = fig.add_subplot(10, 5, i + 1)\n    ax.set_axis_off()\n    ax.imshow(Image.fromarray(img, \"HSV\"))\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n14.41 Q44:ゲーミングフラワー\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    flower = np.array(img.convert(\"HSV\"))\n\nH, W, _ = flower.shape\nflower[..., 0] = np.linspace(0, 255, W).astype(np.uint8)[None, :]\nplt.imshow(Image.fromarray(flower, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\n14.42 Q45:ゲーミングイルカ\n楕円のグラデーションを作成する.\nもとがモノクロ画像だと，HSVに変換したときに，彩度がすべて０になるとのこと．\nこのため，必要な部分の彩度を上げておく必要がある.\n\n\nCode\nwith Image.open(\"numpy_book/imgs/dolphin.png\") as img:\n    dolphin = np.array(img.convert(\"HSV\"))\n\nH, W, C  = dolphin.shape\noy, ox   = H // 2, W // 2\nidy, idx = np.indices((H, W))\nh        = np.sqrt(((idy-oy)/oy) ** 2 + ((idx - ox) / ox) ** 2) * 255\n\nmask = dolphin[..., 2] &lt;= 250\ndolphin[mask, 0] = h.astype(np.uint8)[mask]\ndolphin[mask, 1] = 255 # すべて０なところを，イルカの部分だけ彩度を上げる\ndolphin[mask, 2] = 255\nplt.imshow(Image.fromarray(dolphin, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\n14.43 Q46:類似色のグラデーション\n\n\nCode\n\n\nrgb_colors  = np.array([\n    [128, 255, 255], \n    [255, 128, 255],\n    [255, 255, 128]\n], dtype = np.uint8)[None, ...]\nwith Image.fromarray(rgb_colors) as img:\n    hsv_colors = np.array(img.convert(\"HSV\"))\n\n\n\n\nCode\nhsv_colors.shape\n\n\n(1, 3, 3)\n\n\n\n\nCode\nfig = plt.figure()\ni = 1\nH, W = 50, 200\nfor h in hsv_colors[0, :, 0]:\n    for gi in [20, 40, 60]:\n        canvas = np.full((H, W, 3), 255, dtype = np.uint8)\n        lower = h - gi\n        upper = lower + 2 * gi\n        canvas[..., 0] = np.linspace(lower, upper, W).astype(np.uint8)[None, :]\n        ax = fig.add_subplot(3, 3, i)\n        ax.set_axis_off()\n        ax.imshow(Image.fromarray(canvas, \"HSV\"))\n        i += 1\n\n\n\n\n\n\n\n\n\n\n\n14.44 Q47：RGBとHSVのグラデーション\n線形補間はHSV領域でおこなった方が発色がよいことがある.\n\n\nCode\nc1 = np.array([30, 144, 255], dtype = np.uint8)\nc2 = np.array([255, 30, 143], dtype = np.uint8)\n\nH, W = 100, 400\ncanvas = np.full((H, W, 3), 255, dtype = np.uint8)\n\nfor i in range(3):\n    canvas[..., i] = np.linspace(c1[i], c2[i], W)[None, :]\n\nplt.imshow(canvas)\n\n\n\n\n\n\n\n\n\n\n\nCode\nrgb = np.stack([c1, c2], axis = 0)[:, None, :]\nwith Image.fromarray(rgb) as img:\n    hsv = np.array(img.convert(\"HSV\"))\n\ncanvas = np.full((H, W, 3), 255, dtype = np.uint8)\ncanvas[..., 0] = np.linspace(hsv[0, 0, 0], hsv[1, 0, 0], W).astype(np.uint8)[None, :]\nplt.imshow(Image.fromarray(canvas, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n14.45 Q48：カラー画像のグレースケール化\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    x = np.array(img.convert(\"HSV\"))\n    x[..., 1] = 0\n    plt.imshow(Image.fromarray(x, \"HSV\"))\n    \n\n\n\n\n\n\n\n\n\n\n\n14.46 Q49:カラー画像のグレースケール化\n\n\nCode\nfig = plt.figure(figsize = (16, 16))\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    x = np.array(img)\n    L  = 0.299 * x[..., 0] + .587 * x[..., 1] + .114 * x[..., 2]\n    Lo = np.array(img.convert(\"L\"))\n    LLo= np.concatenate([L, Lo], axis = 1)\n    plt.imshow(LLo, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.47 Q50:単色カラー化\n\n\nCode\nfig = plt.figure(figsize = (16, 16))\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    x = np.array(img)\n    H, W, c = x.shape\n    c = np.tile(np.full_like(x, 0, dtype = np.uint8), (1, 3, 1))\n    Lv  = .299 * x[..., 0] +  .587 * x[..., 1] + .114 * x[..., 2]\n    for i in range(3):\n        # グレースケールの値を代入すればよい\n        c[:, (i * W):((i+1) * W), i] = Lv.astype(np.uint8)\n    plt.imshow(c, vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.48 Q51:単色カラー化，ダブルトーン\n上述したやり方，つまり(R, G, B) = (L, 0, 0)ではグレー化が正常に動作しない場合がある。\nそこで，輝度をLとしたときの定式化を考える。(R, G, B) = (L, 255, 255)\n以下では，グレー化が失敗する例と，それのダブルトーンによるグレー化を示す.\n\n\nCode\n# グレー化失敗例\n\nfig = plt.figure(figsize = (16, 16))\nwith Image.open(\"numpy_book/imgs/girl01.png\") as img:\n    x = np.array(img)\n    H, W, c = x.shape\n    Lv  = .299 * x[..., 0] + .587 * x[..., 1] + .114 * x[..., 2]\n    c = np.tile(np.full_like(x, 0, dtype = np.uint8), (1, 3, 1))\n    c[..., 3] = 255\n    for i in range(3):\n        # グレースケールの値を代入すればよい\n        o = i * W\n        c[:, o:(o + W), i] = Lv.astype(np.uint8)\n    plt.imshow(c, vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (16, 16))\nwith Image.open(\"numpy_book/imgs/girl01.png\") as img:\n    x = np.array(img)\n    H, W, c = x.shape\n    Lv  = .299 * x[..., 0] + .587 * x[..., 1] + .114 * x[..., 2]\n    c = np.tile(np.full_like(x, 0, dtype = np.uint8), (1, 3, 1))\n    c[..., 3] = 255\n    for i in range(3):\n        # グレースケールの値を代入すればよい\n        o = i * W\n        c[:, o:(o + W), i] = Lv.astype(np.uint8)\n    plt.imshow(c, vmin = 0, vmax = 255)\n\n\n(697, 636, 4)\n\n\n\n\nCode\n# ダブルトーンによるカラー化\nfig = plt.figure(figsize = (16, 16))\nwith Image.open(\"numpy_book/imgs/girl01.png\") as img:\n    x = np.array(img)\n    H, W, c = x.shape\n    L = (.299 * x[..., 0] + .587 * x[..., 1] + .114 * x[..., 2]).astype(np.uint8)\n    c = np.tile(np.full_like(x, 255, dtype = np.uint8), (1, 3, 1))\n    for i in range(3):\n        o = i * W\n        for j in range(1, 3):\n            k = (i + j) % 3\n            c[:, o:(o + W), k] = L\n    plt.imshow(c, vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n14.49 Q52：セピア化\nグレースケール化したときに，白は白になり，黒はセピア色になる処理.\n\n\nCode\n## Q52\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as orig:\n    g = np.array(orig.convert(\"L\")) / 255.0\nimg = np.stack([g*255+(1-g)*107, (g*255)+(1-g)*74, g*255+(1-g)*43], axis=-1).astype(np.uint8)\nplt.imshow(img)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n14.50 Q53：CSSのセピア化\n\n\nCode\ndef generate_sepia(alpha):\n    v = np.array([393,  769,  189,  349, 686,  168,  272,  534,  131]).reshape((3,-1)) / 1000\n    a = np.array([607, -769, -189, -349, 314, -168, -272, -534,  869]).reshape((3,-1)) / 1000\n    return v  + (1 - alpha) * a\n\n\n\n\nCode\nfig = plt.figure(figsize = (16, 16))\nsepia_rate = [.25, .5, .75, 1.]\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    flower = np.array(img)\n\nfor i, alpha in enumerate(sepia_rate, start = 1):\n    sepia = generate_sepia(alpha)\n    img   = np.clip(np.dot(flower, sepia.T), 0, 255).astype(np.uint8)\n    ax    = fig.add_subplot(2, 2, i)\n    ax.imshow(img, vmin = 0, vmax = 255)",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "テンソルと画像処理"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html",
    "title": "numpyの導入",
    "section": "",
    "text": "Numpy配列の作り方\nNumpy配列のデータ型\nデータ型のキャスト",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#学ぶこと",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#学ぶこと",
    "title": "numpyの導入",
    "section": "",
    "text": "Numpy配列の作り方\nNumpy配列のデータ型\nデータ型のキャスト",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#numpyとは",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#numpyとは",
    "title": "numpyの導入",
    "section": "2 NumPyとは",
    "text": "2 NumPyとは",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#numpy配列の初期化",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#numpy配列の初期化",
    "title": "numpyの導入",
    "section": "3 NumPy配列の初期化",
    "text": "3 NumPy配列の初期化\n\n\nCode\nimport numpy as np\n\n\n\n\n\n\nCode\nd = list(range(5))\nprint(d) # Pythonのリスト\n\nnum_array = np.array(d)\nprint(num_array) # numpyの配列\n\n\n[0, 1, 2, 3, 4]\n[0 1 2 3 4]",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#numpy配列のインデックスによるアクセス",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#numpy配列のインデックスによるアクセス",
    "title": "numpyの導入",
    "section": "4 NumPy配列のインデックスによるアクセス",
    "text": "4 NumPy配列のインデックスによるアクセス\n\n\nCode\nx = np.ones(5)\nprint(x)\n\n\n[1. 1. 1. 1. 1.]\n\n\n\n\nCode\nx[2] = 8\nx\n\n\narray([1., 1., 8., 1., 1.])\n\n\nスライスを使った直接の代入も可能である．\n\n\nCode\nx[:2] = 10\nx\n\n\narray([10., 10.,  8.,  1.,  1.])\n\n\nリストのスライスを使った代入は，要素数を変える黒魔術であることに注意．\n基本的には使わないこと.\n\n\nCode\nx = list(range(5))\nx[1:4] = [-1, -1]\nx\n\n\n[0, -1, -1, 4]\n\n\nnumpyの配列はインプレイスな演算もサポートしている.\n\n\nCode\nx = np.arange(5)\nx[:4] += 10\nx[:2] *= 10\nx\n\n\narray([100, 110,  12,  13,   4])",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#numpy配列の計算",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#numpy配列の計算",
    "title": "numpyの導入",
    "section": "5 NumPy配列の計算",
    "text": "5 NumPy配列の計算\n\n\nCode\n# 直接関数を適用することが可能\nnp.exp(np.arange(5))\n\n\narray([ 1.        ,  2.71828183,  7.3890561 , 20.08553692, 54.59815003])",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#ベストプラクティス",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#ベストプラクティス",
    "title": "numpyの導入",
    "section": "6 ベストプラクティス",
    "text": "6 ベストプラクティス\n上記のようにNumPy配列はループを使わない処理が可能である． 数値演算を高速化する場合には，ループ（ピュアなPython）に依存しないようにすることが重要.\n\n\nCode\nfrom mypy.mymodule import TimeExecute\n\n\n\n\nCode\nx = list(range(int(1e7)))\nwith TimeExecute():\n    for i in range(len(x)):\n        x[i] *= 2\n    \n\n\n1,260.662 [msec]\n\n\n\n\nCode\nx = list(range(int(1e7)))\nwith TimeExecute():\n    x * 2\n\n\n262.328 [msec]",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#numpy配列を簡単に呼び出す",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#numpy配列を簡単に呼び出す",
    "title": "numpyの導入",
    "section": "7 NumPy配列を簡単に呼び出す",
    "text": "7 NumPy配列を簡単に呼び出す\nよく使うのはコレらしい.\n\nnp.arange\nnp.ones\nnp.zeros\nnp.ones_like\nnp.zeros_like\nnp.linspace\nnp.random.rand\nnp.random.randn\nnp.full\nnp.empty\nnp.concatenate\n\n\n\nCode\na = np.arange(2)\nb = np.arange(3, 5)\nnp.concatenate([a, b])\n\n\narray([0, 1, 3, 4])\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfig = plt.figure(figsize = (10, 8))\nax  = fig.add_subplot(2, 1, 1)\nax.hist(np.random.rand(100000), bins = 100)\nax  = fig.add_subplot(2, 1, 2)\nax.hist(np.random.randn(100000), bins = 100)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnp.full((3, 2), [3, 2])\n\n\narray([[3, 2],\n       [3, 2],\n       [3, 2]])\n\n\n\n\nCode\n# すべｔ同じ要素の時にもfullを使う\n# np.ones(N) * aなどよりも早い\nnp.full((10,), 5)\n\n\narray([5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n\n\n\n\nCode\n# 領域だけ確保の場合にもnp.zero等を使うより関数を使う. \nnp.empty((3, 2))\n\n\narray([[17.2, 17.2],\n       [17.2, 17.2],\n       [17.2, 24.2]])",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#データ型",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#データ型",
    "title": "numpyの導入",
    "section": "8 データ型",
    "text": "8 データ型\n\n\nCode\n# データ型ごとのバイト数を確認\ndtypes = [np.uint8, np.int32, np.float64, np.object, np.dtype(\"U10\"), np.bool]\nfor dt in dtypes:\n    x = np.ones(3, dtype = dt).tostring()\n    print(dt, len(x) / 3)\n\n\n&lt;class 'numpy.uint8'&gt; 1.0\n&lt;class 'numpy.int32'&gt; 4.0\n&lt;class 'numpy.float64'&gt; 8.0\n&lt;class 'object'&gt; 8.0\n&lt;U10 40.0\n&lt;class 'bool'&gt; 1.0",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#ブーリアンマスク",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#ブーリアンマスク",
    "title": "numpyの導入",
    "section": "9 ブーリアンマスク",
    "text": "9 ブーリアンマスク\nNumPyの配列のスライスは，インデックスだけでなく，ブール型でも可能である.\n\n\nCode\nx = np.arange(5)\nis_even = x % 2 == 0\nx[is_even] += 10\nx\n\n\narray([10,  1, 12,  3, 14])",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#キャスト",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#キャスト",
    "title": "numpyの導入",
    "section": "10 キャスト",
    "text": "10 キャスト\nastypeメソッドを使う.\n\n\nCode\na = np.arange(10, dtype = np.int32)\na.astype(np.uint8)\n\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\n\n\n\n\nCode\na.astype(np.dtype(\"U10\"))\n\n\narray(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], dtype='&lt;U10')\n\n\n\n\nCode\nnp.array([\"10\"]).astype(np.int32)\n\n\narray([10])",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#演習問題",
    "href": "contents/books/07_NumpyInfinity/ch02_numpyの導入.html#演習問題",
    "title": "numpyの導入",
    "section": "11 演習問題",
    "text": "11 演習問題\n\n11.1 question 01\n\n\nCode\n\nimport numpy as np\n\nnp.ones((3,))\n\n\narray([1., 1., 1.])\n\n\n\n\nCode\nnp.zeros((7,))\n\n\narray([0., 0., 0., 0., 0., 0., 0.])\n\n\n\n\n11.2 question 02\n\n\nCode\na = np.ones((3, ), dtype = np.int32)\nprint(a, a.dtype)\n\n\n[1 1 1] int32\n\n\n\n\n11.3 question 03\n\n\nCode\nx = np.zeros(5, dtype = np.int32)\nx[len(x) // 2] += 1\nx\n\n\narray([0, 0, 1, 0, 0])\n\n\n\n\n11.4 question 04\n\n\nCode\nx = np.empty(10, dtype = np.int32)\nis_even = np.arange(len(x)) % 2 == 0\nx[is_even] = 0\nx[np.logical_not(is_even)] = 1\nx\n\n\narray([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\n\n\n\n\n11.5 question 06\n\n\nCode\nN   = 100\nret = np.arange(1, N + 1, dtype = np.object)\nis_fizz = ret % 3 == 0\nis_buzz = ret % 5 == 0\nis_fizz_buzz = np.logical_and(is_fizz, is_buzz)\nfor v, l in zip([\"fizz\", \"buzz\", \"fizz_buzz\"], [is_fizz, is_buzz, is_fizz_buzz]):\n    ret[l] = v\nret\n\n\narray([1, 2, 'fizz', 4, 'buzz', 'fizz', 7, 8, 'fizz', 'buzz', 11, 'fizz',\n       13, 14, 'fizz_buzz', 16, 17, 'fizz', 19, 'buzz', 'fizz', 22, 23,\n       'fizz', 'buzz', 26, 'fizz', 28, 29, 'fizz_buzz', 31, 32, 'fizz',\n       34, 'buzz', 'fizz', 37, 38, 'fizz', 'buzz', 41, 'fizz', 43, 44,\n       'fizz_buzz', 46, 47, 'fizz', 49, 'buzz', 'fizz', 52, 53, 'fizz',\n       'buzz', 56, 'fizz', 58, 59, 'fizz_buzz', 61, 62, 'fizz', 64,\n       'buzz', 'fizz', 67, 68, 'fizz', 'buzz', 71, 'fizz', 73, 74,\n       'fizz_buzz', 76, 77, 'fizz', 79, 'buzz', 'fizz', 82, 83, 'fizz',\n       'buzz', 86, 'fizz', 88, 89, 'fizz_buzz', 91, 92, 'fizz', 94,\n       'buzz', 'fizz', 97, 98, 'fizz', 'buzz'], dtype=object)\n\n\n\n\n11.6 question 07\n\n\nCode\nN  = 100\nret = np.full((N + 1), \"\", dtype = np.object)\nret[2::3] += \"fizz\"\nret[4::5] += \"buzz\"\nret[ret == \"\"] = (1 + np.where(ret == \"\")[0]).astype(np.object)\nret\n\n\narray([1, 2, 'fizz', 4, 'buzz', 'fizz', 7, 8, 'fizz', 'buzz', 11, 'fizz',\n       13, 14, 'fizzbuzz', 16, 17, 'fizz', 19, 'buzz', 'fizz', 22, 23,\n       'fizz', 'buzz', 26, 'fizz', 28, 29, 'fizzbuzz', 31, 32, 'fizz', 34,\n       'buzz', 'fizz', 37, 38, 'fizz', 'buzz', 41, 'fizz', 43, 44,\n       'fizzbuzz', 46, 47, 'fizz', 49, 'buzz', 'fizz', 52, 53, 'fizz',\n       'buzz', 56, 'fizz', 58, 59, 'fizzbuzz', 61, 62, 'fizz', 64, 'buzz',\n       'fizz', 67, 68, 'fizz', 'buzz', 71, 'fizz', 73, 74, 'fizzbuzz', 76,\n       77, 'fizz', 79, 'buzz', 'fizz', 82, 83, 'fizz', 'buzz', 86, 'fizz',\n       88, 89, 'fizzbuzz', 91, 92, 'fizz', 94, 'buzz', 'fizz', 97, 98,\n       'fizz', 'buzz', 101], dtype=object)\n\n\n\n\n11.7 question 08\n\n\nCode\nimport re\nreg = re.compile(\"3\")\nN   = 100\nret = np.arange(1, N + 1, dtype = np.object)\nret[2::3] = \"あほ\"\nfor i in range(N):\n    if reg.search(str(ret[i])):\n        ret[i] = \"あほ\"\n\nret\n\n\narray([1, 2, 'あほ', 4, 5, 'あほ', 7, 8, 'あほ', 10, 11, 'あほ', 'あほ', 14, 'あほ',\n       16, 17, 'あほ', 19, 20, 'あほ', 22, 'あほ', 'あほ', 25, 26, 'あほ', 28, 29,\n       'あほ', 'あほ', 'あほ', 'あほ', 'あほ', 'あほ', 'あほ', 'あほ', 'あほ', 'あほ', 40, 41,\n       'あほ', 'あほ', 44, 'あほ', 46, 47, 'あほ', 49, 50, 'あほ', 52, 'あほ', 'あほ',\n       55, 56, 'あほ', 58, 59, 'あほ', 61, 62, 'あほ', 64, 65, 'あほ', 67, 68,\n       'あほ', 70, 71, 'あほ', 'あほ', 74, 'あほ', 76, 77, 'あほ', 79, 80, 'あほ', 82,\n       'あほ', 'あほ', 85, 86, 'あほ', 88, 89, 'あほ', 91, 92, 'あほ', 94, 95, 'あほ',\n       97, 98, 'あほ', 100], dtype=object)\n\n\n\n\n11.8 question 09\n\n\nCode\nN = 200\nx = np.zeros((N, ), dtype = np.int32)\ny = np.arange(np.floor(np.sqrt(N)).astype(np.int)) ** 2\nx[y] = y\nx\n\n\narray([  0,   1,   0,   0,   4,   0,   0,   0,   0,   9,   0,   0,   0,\n         0,   0,   0,  16,   0,   0,   0,   0,   0,   0,   0,   0,  25,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  36,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  49,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  64,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,  81,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0, 100,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0, 121,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0, 144,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n       169,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n         0,   0,   0,   0,   0])\n\n\n\n\n11.9 question 10\n\n\nCode\nvelocity_km_per_m = (65 - 15) / 60\nx_seq = 15 + velocity_km_per_m * np.arange(61)\nnp.round(x_seq, 2)\n\n\narray([15.  , 15.83, 16.67, 17.5 , 18.33, 19.17, 20.  , 20.83, 21.67,\n       22.5 , 23.33, 24.17, 25.  , 25.83, 26.67, 27.5 , 28.33, 29.17,\n       30.  , 30.83, 31.67, 32.5 , 33.33, 34.17, 35.  , 35.83, 36.67,\n       37.5 , 38.33, 39.17, 40.  , 40.83, 41.67, 42.5 , 43.33, 44.17,\n       45.  , 45.83, 46.67, 47.5 , 48.33, 49.17, 50.  , 50.83, 51.67,\n       52.5 , 53.33, 54.17, 55.  , 55.83, 56.67, 57.5 , 58.33, 59.17,\n       60.  , 60.83, 61.67, 62.5 , 63.33, 64.17, 65.  ])\n\n\n\n\n11.10 question 11\n\n\nCode\n\nx = np.arange(10)\n\nx ** x\n\n\narray([        1,         1,         4,        27,       256,      3125,\n           46656,    823543,  16777216, 387420489], dtype=int32)\n\n\n\n\n11.11 question 12\n\n\nCode\nalpha_arr = np.linspace(0, 2 * np.pi, 50)\nbeta_arr  = np.arange(50)\nx = np.sin(alpha_arr + beta_arr)\ny = np.sin(alpha_arr) * np.cos(beta_arr) + np.cos(alpha_arr) * np.sin(beta_arr)\nx[:10], y[:10]\n\n\n(array([ 0.        ,  0.90365484,  0.77400118, -0.24070502, -0.98017057,\n        -0.59883364,  0.46725577,  0.99904896,  0.38845266, -0.66633031]),\n array([ 0.        ,  0.90365484,  0.77400118, -0.24070502, -0.98017057,\n        -0.59883364,  0.46725577,  0.99904896,  0.38845266, -0.66633031]))\n\n\n\n\nCode\nfor a, b in zip(x, y):\n    if not np.isclose(a, b):\n        print(\"x != y\")\n\n\n\n\n11.12 question 13\n\n\nCode\n# 1円未満は切り捨て\nreturn_fee_rate = .35\n# 1000円未満は四捨五入\nreturn_coupon_rate = .15\n\nfee_arr = np.arange(1000, 30001, 500)\nfee_arr\n\n\narray([ 1000,  1500,  2000,  2500,  3000,  3500,  4000,  4500,  5000,\n        5500,  6000,  6500,  7000,  7500,  8000,  8500,  9000,  9500,\n       10000, 10500, 11000, 11500, 12000, 12500, 13000, 13500, 14000,\n       14500, 15000, 15500, 16000, 16500, 17000, 17500, 18000, 18500,\n       19000, 19500, 20000, 20500, 21000, 21500, 22000, 22500, 23000,\n       23500, 24000, 24500, 25000, 25500, 26000, 26500, 27000, 27500,\n       28000, 28500, 29000, 29500, 30000])\n\n\n\n\nCode\napparent_fee = fee_arr - np.floor(fee_arr * return_fee_rate).astype(np.int)\napparent_fee\n\n\narray([  650,   975,  1300,  1625,  1950,  2275,  2600,  2925,  3250,\n        3576,  3900,  4225,  4550,  4875,  5200,  5525,  5850,  6175,\n        6500,  6826,  7151,  7476,  7800,  8125,  8450,  8775,  9100,\n        9425,  9750, 10075, 10400, 10725, 11050, 11375, 11700, 12025,\n       12350, 12675, 13000, 13326, 13651, 13976, 14301, 14626, 14951,\n       15275, 15600, 15925, 16250, 16575, 16900, 17225, 17550, 17875,\n       18200, 18525, 18850, 19175, 19500])\n\n\n\n\nCode\n x = fee_arr * return_coupon_rate\n z = (x // 1000).astype(np.int)\n y = np.round(1 + x - z * 1000, -3)\n coupen = 1000 * z + y\n coupen\n\n\narray([   0.,    0.,    0.,    0.,    0., 1000., 1000., 1000., 1000.,\n       1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000., 1000.,\n       2000., 2000., 2000., 2000., 2000., 2000., 2000., 2000., 2000.,\n       2000., 2000., 2000., 2000., 2000., 3000., 3000., 3000., 3000.,\n       3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n       4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000., 4000.,\n       4000., 4000., 4000., 4000., 5000.])\n\n\n\n\nCode\napparent_fee - coupen\n\n\narray([  650.,   975.,  1300.,  1625.,  1950.,  1275.,  1600.,  1925.,\n        2250.,  2576.,  2900.,  3225.,  3550.,  3875.,  4200.,  4525.,\n        4850.,  5175.,  4500.,  4826.,  5151.,  5476.,  5800.,  6125.,\n        6450.,  6775.,  7100.,  7425.,  7750.,  8075.,  8400.,  8725.,\n        8050.,  8375.,  8700.,  9025.,  9350.,  9675., 10000., 10326.,\n       10651., 10976., 11301., 11626., 11951., 11275., 11600., 11925.,\n       12250., 12575., 12900., 13225., 13550., 13875., 14200., 14525.,\n       14850., 15175., 14500.])\n\n\n\n\n11.13 question 14\n\n\nCode\na = np.linspace(-1, 1, 9)\n\n\n\n\nCode\nfor x, b, c, d, e in zip(a, np.floor(a), np.ceil(a), np.round(a), a.astype(np.int)):\n    print (f\"{x:&gt;5.2f}, {b:&gt;5.2f}, {c:&gt;5.2f}, {d:&gt;5.2f}, {e:&gt;5.2f}\")\n\n\n-1.00, -1.00, -1.00, -1.00, -1.00\n-0.75, -1.00, -0.00, -1.00,  0.00\n-0.50, -1.00, -0.00, -0.00,  0.00\n-0.25, -1.00, -0.00, -0.00,  0.00\n 0.00,  0.00,  0.00,  0.00,  0.00\n 0.25,  0.00,  1.00,  0.00,  0.00\n 0.50,  0.00,  1.00,  0.00,  0.00\n 0.75,  0.00,  1.00,  1.00,  0.00\n 1.00,  1.00,  1.00,  1.00,  1.00\n\n\n\n\n11.14 question 15\n\n\nCode\nsize = 10\nfor _ in range(5):\n    r = np.random.randn(size)\n    n_positive = np.sum(r &gt;= 0)\n    n_negative = size - n_positive\n    print(f\"p = {n_positive}, n = {n_negative}\")\n\n\np = 5, n = 5\np = 4, n = 6\np = 5, n = 5\np = 5, n = 5\np = 5, n = 5\n\n\n\n\n11.15 question 16\n\n\nCode\nx = np.arange(50)\ny = x // 10\ny\n\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4], dtype=int32)\n\n\n\n\nCode\nfor a in range(5):\n    print(np.sum(y == a))\n\n\n10\n10\n10\n10\n10\n\n\n\n\n11.16 question 17\n\n\nCode\nx = np.array([0, 1, 2, 253, 254, 255], dtype = np.uint8)\n\n\n\n\nCode\nx + 1\n\n\narray([  1,   2,   3, 254, 255,   0], dtype=uint8)\n\n\n\n\nCode\nx - 1\n\n\narray([255,   0,   1, 252, 253, 254], dtype=uint8)\n\n\n\n\nCode\nx * 2\n\n\narray([  0,   2,   4, 250, 252, 254], dtype=uint8)\n\n\n\n\nCode\n254 * 2 % 256\n\n\n252\n\n\n\n\nCode\nx / 2\n\n\narray([  0. ,   0.5,   1. , 126.5, 127. , 127.5])\n\n\n\n\nCode\n(x / 2).dtype\n\n\ndtype('float64')\n\n\n\n\nCode\nx // 2\n\n\narray([  0,   0,   1, 126, 127, 127], dtype=uint8)\n\n\n\n\n11.17 question 18\n\n\nCode\n# 任意の範囲で頭打ちのを作成するには\nnp.clip(np.arange(10), 3, 7)\n\n\narray([3, 3, 3, 3, 4, 5, 6, 7, 7, 7])\n\n\nquestion19からは，問題設定を読み込むのが面倒くさかったので，とりあえず飛ばしておく.",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "numpyの導入"
    ]
  },
  {
    "objectID": "contents/books/17_Pythonで実践する時系列予測の基礎/index.html",
    "href": "contents/books/17_Pythonで実践する時系列予測の基礎/index.html",
    "title": "Pythonで実践する時系列予測の基礎",
    "section": "",
    "text": "1 はじめに\n\nビジネスでの活用を意識した内容\nサポートページ\npythonのバージョンは3.12を使う\n\npmdarimaはpython3.13では動かないので注意\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/books/15_プロになるためのWeb技術入門/01_NOTE.html",
    "href": "contents/books/15_プロになるためのWeb技術入門/01_NOTE.html",
    "title": "HTTPクライアントとHTTPサーバ",
    "section": "",
    "text": "ubuntu on wslで実行する\nnc: HTTPサーバー役のコマンド\ncurl: HTTPクライアント役のコマンド\n\n\n\n\nグループ化コマンド{ ~ }のときには, {の後と}に前にスペースが必要\nWSLでncによりサーバーを立てるときにはnc -q 1 -l -p 8080のように-q 1が必要\n\nこれを使うことがlocalhostにchromeからアクセスできる",
    "crumbs": [
      "note",
      "プロになるためのWeb技術入門",
      "HTTPクライアントとHTTPサーバ"
    ]
  },
  {
    "objectID": "contents/books/15_プロになるためのWeb技術入門/01_NOTE.html#ノート",
    "href": "contents/books/15_プロになるためのWeb技術入門/01_NOTE.html#ノート",
    "title": "HTTPクライアントとHTTPサーバ",
    "section": "",
    "text": "グループ化コマンド{ ~ }のときには, {の後と}に前にスペースが必要\nWSLでncによりサーバーを立てるときにはnc -q 1 -l -p 8080のように-q 1が必要\n\nこれを使うことがlocalhostにchromeからアクセスできる",
    "crumbs": [
      "note",
      "プロになるためのWeb技術入門",
      "HTTPクライアントとHTTPサーバ"
    ]
  },
  {
    "objectID": "contents/books/14_入門統計学/01_NOTE.html",
    "href": "contents/books/14_入門統計学/01_NOTE.html",
    "title": "ノート",
    "section": "",
    "text": "PPDACサイクルはデータ分析の基本的な流れを示すものです。\nPPDACは、問題（Problem）、計画（Plan）、データ（Data）、分析（Analysis）、結論（Conclusion）の頭文字をとったものです。\n\n\n\n\n\n統計を行う際（中略）、まずは「データ収集・保管・運用」から始める必要がある\n\nその際には「個人情報保護」「情報セキュリティ」がキーワードにあげられる。",
    "crumbs": [
      "note",
      "入門統計学",
      "ノート"
    ]
  },
  {
    "objectID": "contents/books/14_入門統計学/01_NOTE.html#ppdacサイクル",
    "href": "contents/books/14_入門統計学/01_NOTE.html#ppdacサイクル",
    "title": "ノート",
    "section": "",
    "text": "PPDACサイクルはデータ分析の基本的な流れを示すものです。\nPPDACは、問題（Problem）、計画（Plan）、データ（Data）、分析（Analysis）、結論（Conclusion）の頭文字をとったものです。",
    "crumbs": [
      "note",
      "入門統計学",
      "ノート"
    ]
  },
  {
    "objectID": "contents/books/14_入門統計学/01_NOTE.html#統計能力向上に向けて",
    "href": "contents/books/14_入門統計学/01_NOTE.html#統計能力向上に向けて",
    "title": "ノート",
    "section": "",
    "text": "統計を行う際（中略）、まずは「データ収集・保管・運用」から始める必要がある\n\nその際には「個人情報保護」「情報セキュリティ」がキーワードにあげられる。",
    "crumbs": [
      "note",
      "入門統計学",
      "ノート"
    ]
  },
  {
    "objectID": "contents/books/13_教員のため教育データ分析/01_NOTE.html",
    "href": "contents/books/13_教員のため教育データ分析/01_NOTE.html",
    "title": "ノート",
    "section": "",
    "text": "PPDACサイクルはデータ分析の基本的な流れを示すものです。\nPPDACは、問題（Problem）、計画（Plan）、データ（Data）、分析（Analysis）、結論（Conclusion）の頭文字をとったものです。\n\n\n\n\n\n統計を行う際（中略）、まずは「データ収集・保管・運用」から始める必要がある\n\nその際には「個人情報保護」「情報セキュリティ」がキーワードにあげられる。",
    "crumbs": [
      "note",
      "教員のための教育データ分析",
      "ノート"
    ]
  },
  {
    "objectID": "contents/books/13_教員のため教育データ分析/01_NOTE.html#ppdacサイクル",
    "href": "contents/books/13_教員のため教育データ分析/01_NOTE.html#ppdacサイクル",
    "title": "ノート",
    "section": "",
    "text": "PPDACサイクルはデータ分析の基本的な流れを示すものです。\nPPDACは、問題（Problem）、計画（Plan）、データ（Data）、分析（Analysis）、結論（Conclusion）の頭文字をとったものです。",
    "crumbs": [
      "note",
      "教員のための教育データ分析",
      "ノート"
    ]
  },
  {
    "objectID": "contents/books/13_教員のため教育データ分析/01_NOTE.html#統計能力向上に向けて",
    "href": "contents/books/13_教員のため教育データ分析/01_NOTE.html#統計能力向上に向けて",
    "title": "ノート",
    "section": "",
    "text": "統計を行う際（中略）、まずは「データ収集・保管・運用」から始める必要がある\n\nその際には「個人情報保護」「情報セキュリティ」がキーワードにあげられる。",
    "crumbs": [
      "note",
      "教員のための教育データ分析",
      "ノート"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/index.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/index.html",
    "title": "Pythonではじめる時系列分析入門",
    "section": "",
    "text": "1 はじめに\n\n古典的な手法から機械学習を用いた方法まで解説している\nサポートページ\nGithub\n\nデータやスクリプトはGithubからダウンロードする必要がある\n\n\n\n\n2 用語\n\nデータ生成過程(Data Generation Process: DGP)\n\n確率過程ともいう\n\n\\(k\\)次自己相関\n\n\\(k\\)個前の事象との相関\n\n\\(k\\)次自己偏相関\n\n\\(k-1\\)次までの自己相関の影響を除いた\\(k\\)個前の事象との相関\n\nトレンド\n\n時系列データの傾向\n\nドラフト\n\nトレンドの変化量\n\nデータの長さ\n\nT個の時系列データ\\(y_{1:T}\\)において欠測がないときのデータサイズ\n\n1次の自己相関\n\n1時点離れただけの関係での相関\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/06_Pythonではじめる数理最適化/ch05_配送計画.html",
    "href": "contents/books/06_Pythonではじめる数理最適化/ch05_配送計画.html",
    "title": "配送計画",
    "section": "",
    "text": "#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.0\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function",
    "crumbs": [
      "Python",
      "Pythonではじめる数理最適化 2nd",
      "配送計画"
    ]
  },
  {
    "objectID": "contents/books/06_Pythonではじめる数理最適化/ch05_配送計画.html#混合整数計画への素朴なモデリング",
    "href": "contents/books/06_Pythonではじめる数理最適化/ch05_配送計画.html#混合整数計画への素朴なモデリング",
    "title": "配送計画",
    "section": "3.1 混合整数計画への素朴なモデリング",
    "text": "3.1 混合整数計画への素朴なモデリング\n\n3.1.1 集合と定数\n省略します\n\n\n3.1.2 決定変数と補助変数\n数理モデルに現れる変数のうち、独立した変数として定義するものを決定変数と呼ぶ。 決定変数に従属して値が決まる変数を補助変数と呼ぶ。\nある日\\(d\\)において地点\\(k_1\\)から\\(k_2\\)への移動の有無を表す決定変数を次のように定義する。\n\\[\nx_{d,k_1, k_2}\\in \\{0, 1\\}\n\\]\n次に\\(d\\)における\\(k\\)に移動する順番を表す補助変数\\(u_{d,k}\\)を定義する。\\(\\mathbb{Z}_{\\ge 0}\\)は 0以上の整数の集合である。\n\\[\nu_{d,k}\\in \\mathbb{Z}_{\\ge 0}\n\\]\n次に、配送日において荷物\\(r\\in R\\)を自社配送するかどうかを表す決定変数を定義する.\n\\[\ny_{d,r}\\in \\{0,1\\}\n\\]\n最期に配送日の残業時間を表す補助変数を定義する。\n\\[\nh_{d} \\in \\mathbb{R}_{\\ge 0}\n\\]\n補助変数は一見すると省略できるようにも見えるが現実の問題を解くことができるモデリングをする場合には必須のテクニックであるう。\n実務では非線形な問題を補助変数により線形問題に帰着させることがよくおこなわれる。\n\n\n3.1.3 制約式と目的関数\n省略\n\n\n3.1.4 サイクルとなる経路\n２つ制約式を用いて、「経路がサイクルをつくる」ということを表すことができる。\n1つ目はある地点から他の地点への移動の合計は、ある地点への他の地点からの移動の合計と同じになる。\n\\[\n\\sum_{k_2\\in K}x_{d,k_2, k_1} = \\sum_{k_2 \\in K}x_{d, k_1, k_2}\n\\]\nもう一つは同じ地点に移動するのは1日に一度までという定義である。\n\\[\n\\sum_{k_2 \\in K}x_{d, k_2, k_1}\\le 1\n\\]\n\n\n3.1.5 配送センターと通るサイクル\nサイクルのうち配送センターを通るものだけに絞るには次の３つの制約をいれる。\nまず、ある日に配送センター\\(p\\)への移動順番は0である。\n\\[\n\\u_{d,p}=0\n\\]\n次に値の上限を制約式として与えることで1番目から順番に番号を付けるようにする。\n\\[\n1\\le u_{d,s} \\le |K|-1\n\\]\n最期に角配送日においてお店間だけのサイクルを禁止する。\\(s_1, s_2 \\in S\\)であり、荷物の配送先を表す集合。 \\(x_{d,s_1, s_2}=1\\)のときは\\(u_{d, s_1}+1 \\le u_{d, s_2}\\)となるうえで、 \\(s_1\\)から\\(s_2\\)への移動があるので、つまりは\\(u_{d, s_1}\\le u_{d, s_2}\\)という制約が生まれる。 これにより地点間でのサイクルは発生しないことになる。\n\\[\nu_{d, s_1}+1 \\le u_{d, s_2}+(|K|-1)(1-x_{d,s_1, s_2})\n\\]\nその他の制約条件は題意から適当に設定することができる。",
    "crumbs": [
      "Python",
      "Pythonではじめる数理最適化 2nd",
      "配送計画"
    ]
  },
  {
    "objectID": "contents/books/06_Pythonではじめる数理最適化/ch03_学校のクラス編成.html",
    "href": "contents/books/06_Pythonではじめる数理最適化/ch03_学校のクラス編成.html",
    "title": "学校のクラス編成",
    "section": "",
    "text": "#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.4\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function\n\n\n1 はじめに\n学校のクラス編成について、問題を定式化して解いてみる。\n大事なことは結果を検証して、必要なモデリングが行えているのかを考察すること。\n条件は次のとおりである。\n\n全生徒をそれぞれ１つのクラスに割り当てる\n各クラスには、３９人以上、４０人以下の生徒が所属する\n各クラスには、男子生徒、女子生徒はそれぞれ２０人以下とする\n各クラスの学力試験の平均点は学年平均点±10点とする\n各クラスにリーダー気質の生徒を2人以上所属させる\n特別支援が必要な生徒は各クラスに一人以下とさせる\n特定ペアの生徒は同一暮らすに割り立てない\n\n\n\n2 実装\nまずは目的関数を決めずに条件を満たす解を求解する。 ダミー変数をLp.Binaryで定義するのがポイントと思われる。\n\n\nCode\nimport pandas as pd\nimport pulp\nfrom pathlib import Path\n\ndata_dir = Path(\"unshare\", \"PyOptBook\", \"3.school\")\n\n# データの読み込み\ns_df = pd.read_csv(data_dir / \"students.csv\")\ns_pair_df = pd.read_csv(data_dir / \"student_pairs.csv\")\n\n# 問題の定義\nprob = pulp.LpProblem(\"school\", pulp.LpMaximize)\n\n\n# 前処理\nS = s_df[\"student_id\"].tolist()\n\nC = list(\"ABCDEFGH\")\n\nSC = [(s, c) for s in S for c in C]\n\nS_male   = [row.student_id for row in s_df.itertuples() if row.gender == 1]\nS_female = [row.student_id for row in s_df.itertuples() if row.gender == 0]\n\nS_leader = [row.student_id for row in s_df.itertuples() if row.leader_flag == 1]\nS_support = [row.student_id for row in s_df.itertuples() if row.support_flag == 1]\nSS = [(row.student_id1, row.student_id2) for row in s_pair_df.itertuples()]\n\nscore = {row.student_id:row.score for row in s_df.itertuples()}\nscore_mean = s_df[\"score\"].mean()\n\n\n\n# 変数の定義\nx = pulp.LpVariable.dicts(\"x\", SC, 0, 1, pulp.LpBinary)\n\n# 制約条件1\nfor s in S:\n    prob += pulp.lpSum(x[(s, c)] for c in C) == 1\n\n# 制約条件2\nfor c in C:\n    prob += pulp.lpSum(x[(s, c)] for s in S) &gt;= 39\n    prob += pulp.lpSum(x[(s, c)] for s in S) &lt;= 40\n\n# 制約条件3\nfor c in C:\n    prob += pulp.lpSum(x[(s, c)] for s in S_male) &lt;= 20\n    prob += pulp.lpSum(x[(s, c)] for s in S_female) &lt;= 20\n    \n# 制約条件4\nfor c in C:\n    prob += pulp.lpSum(score[s] * x[(s, c)] for s in S) &gt;= (score_mean - 10) * pulp.lpSum(x[(s, c)] for s in S)\n    prob += pulp.lpSum(score[s] * x[(s, c)] for s in S) &lt;= (score_mean + 10) * pulp.lpSum(x[(s, c)] for s in S)\n\n# 制約条件5\nfor c in C:\n    prob += pulp.lpSum(x[(s, c)] for s in S_leader) &gt;= 2\n\n# 制約条件6\nfor c in C:\n    prob += pulp.lpSum(x[(s, c)] for s in S_support) &lt;= 1\n\n# 制約条件7\nfor s1, s2 in SS:\n    for c in C:\n        prob += x[(s1, c)] + x[(s2, c)] &lt;= 1\n\n# 目的関数\nstatus = prob.solve()\nprint(f\"Status: {pulp.LpStatus[status]}\")\n#&gt; Status: Optimal\n\n\n結果を確認する。\n\n\nCode\nfor _ in range(10):\n    print(\"- \" * 50)\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\n\n\n\nCode\nC2Ss = {}\nfor c in C:\n    C2Ss[c] = [s for s in S if x[(s, c)].value() == 1]\n\ndef show_result():\n    \"\"\"for文を使って結果を表示することが出来ないので関数化する。\n    \"\"\"\n    for c, Ss in C2Ss.items():\n        print(f\"Class {c}\")\n        print(f\"Number of students: {len(Ss)}\")\n        print(f\"Student: {Ss}\")\n        print(\"- \" * 50)\n\nshow_result()\n#&gt; Class A\n#&gt; Number of students: 40\n#&gt; Student: [2, 9, 19, 39, 42, 63, 65, 71, 79, 83, 85, 88, 99, 109, 111, 123, 126, 136, 138, 145, 148, 165, 168, 173, 177, 179, 180, 193, 199, 206, 224, 233, 237, 240, 246, 264, 267, 291, 292, 298]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class B\n#&gt; Number of students: 40\n#&gt; Student: [11, 15, 48, 50, 70, 82, 89, 91, 102, 104, 113, 114, 120, 121, 124, 127, 146, 149, 159, 167, 170, 172, 176, 190, 203, 212, 213, 220, 222, 231, 238, 245, 263, 270, 275, 276, 283, 287, 290, 317]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class C\n#&gt; Number of students: 40\n#&gt; Student: [1, 3, 14, 23, 25, 27, 31, 33, 41, 49, 53, 54, 58, 73, 93, 97, 98, 122, 152, 156, 160, 171, 187, 210, 211, 217, 219, 227, 236, 242, 250, 254, 258, 260, 268, 273, 277, 278, 301, 318]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class D\n#&gt; Number of students: 40\n#&gt; Student: [10, 13, 16, 36, 37, 43, 51, 56, 59, 61, 67, 68, 75, 84, 92, 108, 128, 134, 139, 140, 158, 161, 175, 188, 192, 198, 200, 205, 218, 221, 225, 241, 252, 255, 256, 257, 261, 266, 293, 316]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class E\n#&gt; Number of students: 40\n#&gt; Student: [4, 5, 21, 22, 24, 38, 69, 72, 78, 87, 96, 105, 106, 115, 129, 132, 141, 143, 150, 154, 164, 166, 183, 184, 189, 195, 196, 197, 243, 244, 247, 249, 269, 271, 288, 300, 305, 309, 310, 315]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class F\n#&gt; Number of students: 39\n#&gt; Student: [12, 17, 45, 46, 62, 74, 76, 80, 95, 100, 101, 103, 107, 110, 112, 116, 119, 125, 130, 135, 151, 153, 169, 178, 182, 185, 202, 209, 230, 232, 239, 272, 281, 282, 284, 289, 303, 304, 306]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class G\n#&gt; Number of students: 40\n#&gt; Student: [7, 18, 28, 32, 35, 44, 52, 55, 57, 60, 64, 66, 81, 117, 118, 131, 147, 155, 181, 186, 194, 204, 207, 208, 214, 216, 223, 226, 228, 229, 235, 262, 265, 279, 280, 296, 299, 308, 312, 313]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class H\n#&gt; Number of students: 39\n#&gt; Student: [6, 8, 20, 26, 29, 30, 34, 40, 47, 77, 86, 90, 94, 133, 137, 142, 144, 157, 162, 163, 174, 191, 201, 215, 234, 248, 251, 253, 259, 274, 285, 286, 294, 295, 297, 302, 307, 311, 314]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\n\n\n\n3 検証\n学力の得点分布をみてみる。これをみると分布形が大きく異なることがわかる。\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfigure = plt.figure(figsize=(10, 10))\nfor i, c in enumerate(C):\n    ax = figure.add_subplot(4, 2, i+1)\n    ax.hist([score[s] for s in C2Ss[c]], bins=10, alpha=0.5, label=c, edgecolor=\"black\")\n    ax.set_title(f\"Class {c}\")\n#&gt; (array([ 6.,  2.,  3.,  3.,  0.,  0.,  2., 12., 11.,  1.]), array([ 88. , 127.7, 167.4, 207.1, 246.8, 286.5, 326.2, 365.9, 405.6,\n#&gt;        445.3, 485. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class A')\n#&gt; (array([ 1.,  4.,  0., 11.,  2.,  5.,  0.,  3.,  5.,  9.]), array([176., 196., 216., 236., 256., 276., 296., 316., 336., 356., 376.]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class B')\n#&gt; (array([ 2., 10.,  4.,  3.,  1.,  0.,  1.,  5., 13.,  1.]), array([240. , 253.9, 267.8, 281.7, 295.6, 309.5, 323.4, 337.3, 351.2,\n#&gt;        365.1, 379. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class C')\n#&gt; (array([ 2.,  1.,  2.,  1.,  8.,  1.,  1.,  9.,  5., 10.]), array([163. , 185.7, 208.4, 231.1, 253.8, 276.5, 299.2, 321.9, 344.6,\n#&gt;        367.3, 390. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class D')\n#&gt; (array([3., 4., 2., 4., 7., 9., 6., 3., 1., 1.]), array([215. , 232.6, 250.2, 267.8, 285.4, 303. , 320.6, 338.2, 355.8,\n#&gt;        373.4, 391. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class E')\n#&gt; (array([10.,  1.,  7.,  2.,  8., 10.,  0.,  0.,  0.,  1.]), array([224. , 245.5, 267. , 288.5, 310. , 331.5, 353. , 374.5, 396. ,\n#&gt;        417.5, 439. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class F')\n#&gt; (array([3., 1., 6., 4., 5., 6., 8., 0., 1., 6.]), array([222. , 239.5, 257. , 274.5, 292. , 309.5, 327. , 344.5, 362. ,\n#&gt;        379.5, 397. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class G')\n#&gt; (array([ 2.,  5.,  3.,  1., 10.,  6.,  8.,  2.,  0.,  2.]), array([200. , 219.9, 239.8, 259.7, 279.6, 299.5, 319.4, 339.3, 359.2,\n#&gt;        379.1, 399. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class H')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4 改善\n成績分布のバラツキを解消するための案として、初期値として成績順のクラス分けをおこなった後に、 初期値からの変更階数を損失として、条件を満たすように修正することを考える。\n\n\nCode\ns_df[\"score_rank\"] = s_df[\"score\"].rank(ascending=False, method = \"first\")\n\nclass_dic = {i:v for i, v in enumerate(C)}\ns_df[\"init_assigned_class\"] = s_df[\"score_rank\"].map(lambda x: x % 8).map(class_dic)\ns_df.head()\n#&gt;    student_id  gender  leader_flag  ...  score  score_rank  init_assigned_class\n#&gt; 0           1       0            0  ...    335       109.0                    F\n#&gt; 1           2       1            0  ...    379        38.0                    G\n#&gt; 2           3       0            0  ...    350        79.0                    H\n#&gt; 3           4       0            0  ...    301       172.0                    E\n#&gt; 4           5       1            0  ...    317       147.0                    D\n#&gt; \n#&gt; [5 rows x 7 columns]\n\n\n初期状態のクラス分けでの学力分布を確認する。\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfigure = plt.figure(figsize=(10, 10))\nfor i, c in enumerate(C):\n    v = s_df.query(\"init_assigned_class == @c\")[\"score\"]\n    ax = figure.add_subplot(4, 2, i+1)\n    ax.hist(v, bins=10, alpha=0.5, label=c, edgecolor=\"black\")\n    ax.set_title(f\"Class {c}\")\n#&gt; (array([1., 0., 2., 4., 5., 6., 7., 7., 4., 3.]), array([128. , 156.9, 185.8, 214.7, 243.6, 272.5, 301.4, 330.3, 359.2,\n#&gt;        388.1, 417. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class A')\n#&gt; (array([1., 1., 2., 7., 7., 9., 7., 4., 1., 1.]), array([123. , 159.2, 195.4, 231.6, 267.8, 304. , 340.2, 376.4, 412.6,\n#&gt;        448.8, 485. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class B')\n#&gt; (array([1., 0., 2., 5., 6., 6., 9., 6., 3., 2.]), array([122. , 153.8, 185.6, 217.4, 249.2, 281. , 312.8, 344.6, 376.4,\n#&gt;        408.2, 440. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class C')\n#&gt; (array([1., 0., 1., 4., 6., 7., 8., 7., 4., 2.]), array([102. , 135.7, 169.4, 203.1, 236.8, 270.5, 304.2, 337.9, 371.6,\n#&gt;        405.3, 439. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class D')\n#&gt; (array([1., 0., 1., 3., 7., 7., 7., 8., 4., 2.]), array([ 98. , 131.9, 165.8, 199.7, 233.6, 267.5, 301.4, 335.3, 369.2,\n#&gt;        403.1, 437. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class E')\n#&gt; (array([1., 0., 1., 3., 5., 8., 7., 8., 5., 2.]), array([ 89. , 123.3, 157.6, 191.9, 226.2, 260.5, 294.8, 329.1, 363.4,\n#&gt;        397.7, 432. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class F')\n#&gt; (array([1., 0., 1., 3., 5., 8., 7., 8., 5., 2.]), array([ 88. , 122.2, 156.4, 190.6, 224.8, 259. , 293.2, 327.4, 361.6,\n#&gt;        395.8, 430. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class G')\n#&gt; (array([1., 1., 3., 5., 6., 5., 6., 6., 4., 2.]), array([160. , 186.1, 212.2, 238.3, 264.4, 290.5, 316.6, 342.7, 368.8,\n#&gt;        394.9, 421. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class H')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n上記の状態から条件を満たすように修正する。\n\n\nCode\n# 問題の定義\nprob = pulp.LpProblem(\"school\", pulp.LpMaximize)\n\n\n# 前処理\nS = s_df[\"student_id\"].tolist()\n\nC = list(\"ABCDEFGH\")\n\nSC = [(s, c) for s in S for c in C]\n\nS_male   = [row.student_id for row in s_df.itertuples() if row.gender == 1]\nS_female = [row.student_id for row in s_df.itertuples() if row.gender == 0]\n\nS_leader = [row.student_id for row in s_df.itertuples() if row.leader_flag == 1]\nS_support = [row.student_id for row in s_df.itertuples() if row.support_flag == 1]\nSS = [(row.student_id1, row.student_id2) for row in s_pair_df.itertuples()]\n\nscore = {row.student_id:row.score for row in s_df.itertuples()}\nscore_mean = s_df[\"score\"].mean()\n\n\n\n# 変数の定義\nx = pulp.LpVariable.dicts(\"x\", SC, 0, 1, pulp.LpBinary)\n\n# 制約条件1\nfor s in S:\n    prob += pulp.lpSum(x[(s, c)] for c in C) == 1\n\n# 制約条件2\nfor c in C:\n    prob += pulp.lpSum(x[(s, c)] for s in S) &gt;= 39\n    prob += pulp.lpSum(x[(s, c)] for s in S) &lt;= 40\n\n# 制約条件3\nfor c in C:\n    prob += pulp.lpSum(x[(s, c)] for s in S_male) &lt;= 20\n    prob += pulp.lpSum(x[(s, c)] for s in S_female) &lt;= 20\n    \n# 制約条件4\nfor c in C:\n    prob += pulp.lpSum(score[s] * x[(s, c)] for s in S) &gt;= (score_mean - 10) * pulp.lpSum(x[(s, c)] for s in S)\n    prob += pulp.lpSum(score[s] * x[(s, c)] for s in S) &lt;= (score_mean + 10) * pulp.lpSum(x[(s, c)] for s in S)\n\n# 制約条件5\nfor c in C:\n    prob += pulp.lpSum(x[(s, c)] for s in S_leader) &gt;= 2\n\n# 制約条件6\nfor c in C:\n    prob += pulp.lpSum(x[(s, c)] for s in S_support) &lt;= 1\n\n# 制約条件7\nfor s1, s2 in SS:\n    for c in C:\n        prob += x[(s1, c)] + x[(s2, c)] &lt;= 1\n\n# 目的関数\ninit_flag = {(s, c):0 for s in S for c in C}\nfor row in s_df.itertuples():\n    init_flag[(row.student_id, row.init_assigned_class)] = 1\nprob += pulp.lpSum(x[(s, c)] * init_flag[(s, c)] for s, c in SC)\n\n\nstatus = prob.solve()\nprint(f\"Status: {pulp.LpStatus[status]}\")\n#&gt; Status: Optimal\n\n\n\n\nCode\nC2Ss = {}\nfor c in C:\n    C2Ss[c] = [s for s in S if x[(s, c)].value() == 1]\n\ndef show_result():\n    \"\"\"for文を使って結果を表示することが出来ないので関数化する。\n    \"\"\"\n    for c, Ss in C2Ss.items():\n        print(f\"Class {c}\")\n        print(f\"Number of students: {len(Ss)}\")\n        print(f\"Student: {Ss}\")\n        print(\"- \" * 50)\n\nshow_result()\n#&gt; Class A\n#&gt; Number of students: 40\n#&gt; Student: [2, 23, 56, 68, 76, 82, 89, 102, 106, 113, 115, 121, 123, 124, 127, 140, 172, 173, 185, 186, 204, 210, 228, 255, 267, 273, 274, 280, 285, 288, 289, 292, 295, 297, 304, 308, 311, 313, 316, 318]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class B\n#&gt; Number of students: 39\n#&gt; Student: [11, 14, 17, 30, 32, 35, 41, 49, 64, 66, 79, 83, 86, 88, 97, 114, 119, 122, 132, 134, 141, 149, 151, 165, 175, 178, 190, 198, 200, 209, 213, 216, 226, 237, 272, 296, 303, 306, 307]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class C\n#&gt; Number of students: 40\n#&gt; Student: [21, 38, 44, 46, 51, 54, 62, 63, 73, 75, 84, 85, 99, 120, 138, 142, 143, 144, 150, 166, 183, 184, 192, 193, 195, 201, 205, 207, 211, 212, 217, 221, 222, 243, 244, 263, 264, 287, 299, 315]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class D\n#&gt; Number of students: 40\n#&gt; Student: [5, 24, 39, 47, 50, 61, 67, 74, 90, 92, 93, 100, 109, 116, 131, 136, 147, 152, 155, 167, 169, 170, 176, 177, 196, 199, 214, 218, 219, 227, 230, 231, 236, 238, 239, 253, 257, 259, 271, 309]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class E\n#&gt; Number of students: 39\n#&gt; Student: [4, 8, 12, 13, 19, 22, 33, 43, 48, 55, 57, 59, 98, 112, 125, 130, 133, 137, 139, 153, 160, 189, 203, 234, 235, 240, 241, 249, 251, 254, 256, 261, 266, 268, 276, 283, 291, 294, 302]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class F\n#&gt; Number of students: 40\n#&gt; Student: [1, 6, 16, 27, 28, 29, 40, 42, 45, 58, 70, 77, 91, 118, 128, 129, 135, 145, 146, 148, 156, 161, 162, 163, 174, 181, 188, 194, 202, 224, 229, 246, 258, 260, 262, 265, 286, 290, 300, 310]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class G\n#&gt; Number of students: 40\n#&gt; Student: [7, 9, 10, 15, 18, 26, 31, 36, 37, 52, 71, 78, 80, 94, 96, 101, 104, 110, 126, 157, 159, 179, 180, 182, 191, 197, 215, 242, 245, 247, 248, 252, 275, 277, 282, 293, 298, 305, 312, 314]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n#&gt; Class H\n#&gt; Number of students: 40\n#&gt; Student: [3, 20, 25, 34, 53, 60, 65, 69, 72, 81, 87, 95, 103, 105, 107, 108, 111, 117, 154, 158, 164, 168, 171, 187, 206, 208, 220, 223, 225, 232, 233, 250, 269, 270, 278, 279, 281, 284, 301, 317]\n#&gt; - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfigure = plt.figure(figsize=(10, 10))\nfor i, c in enumerate(C):\n    ax = figure.add_subplot(4, 2, i+1)\n    ax.hist([score[s] for s in C2Ss[c]], bins=10, alpha=0.5, label=c, edgecolor=\"black\")\n    ax.set_title(f\"Class {c}\")\n    ax.set_ylim(0, 20)\n#&gt; (array([1., 0., 1., 4., 5., 8., 5., 7., 5., 4.]), array([128. , 155.6, 183.2, 210.8, 238.4, 266. , 293.6, 321.2, 348.8,\n#&gt;        376.4, 404. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class A')\n#&gt; (0.0, 20.0)\n#&gt; (array([1., 1., 2., 8., 6., 9., 6., 4., 1., 1.]), array([123. , 159.2, 195.4, 231.6, 267.8, 304. , 340.2, 376.4, 412.6,\n#&gt;        448.8, 485. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class B')\n#&gt; (0.0, 20.0)\n#&gt; (array([1., 0., 3., 4., 5., 6., 9., 6., 4., 2.]), array([122. , 153.8, 185.6, 217.4, 249.2, 281. , 312.8, 344.6, 376.4,\n#&gt;        408.2, 440. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class C')\n#&gt; (0.0, 20.0)\n#&gt; (array([1., 0., 1., 4., 6., 6., 8., 7., 5., 2.]), array([102. , 135.7, 169.4, 203.1, 236.8, 270.5, 304.2, 337.9, 371.6,\n#&gt;        405.3, 439. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class D')\n#&gt; (0.0, 20.0)\n#&gt; (array([1., 2., 4., 7., 5., 7., 6., 4., 2., 1.]), array([176. , 202.1, 228.2, 254.3, 280.4, 306.5, 332.6, 358.7, 384.8,\n#&gt;        410.9, 437. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class E')\n#&gt; (0.0, 20.0)\n#&gt; (array([1., 0., 1., 3., 4., 9., 7., 8., 5., 2.]), array([ 89. , 123.3, 157.6, 191.9, 226.2, 260.5, 294.8, 329.1, 363.4,\n#&gt;        397.7, 432. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class F')\n#&gt; (0.0, 20.0)\n#&gt; (array([2., 0., 0., 4., 6., 7., 7., 8., 4., 2.]), array([ 88. , 122.2, 156.4, 190.6, 224.8, 259. , 293.2, 327.4, 361.6,\n#&gt;        395.8, 430. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class G')\n#&gt; (0.0, 20.0)\n#&gt; (array([1., 1., 3., 4., 6., 5., 6., 7., 5., 2.]), array([160. , 186.1, 212.2, 238.3, 264.4, 290.5, 316.6, 342.7, 368.8,\n#&gt;        394.9, 421. ]), &lt;BarContainer object of 10 artists&gt;)\n#&gt; Text(0.5, 1.0, 'Class H')\n#&gt; (0.0, 20.0)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる数理最適化 2nd",
      "学校のクラス編成"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/index.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/index.html",
    "title": "統計的因果推論の理論と実際",
    "section": "",
    "text": "本稿は共立出版の統計的因果推論の理論と実装に関する勉強ノートです. レベルとしては大学レベルの入門統計学を学習したものとされています. 正誤表があるので内容には注意して読むこと．\n\n前半部分の回帰分析までは理解できたと思うが， 特に回帰不連続デザインの実装部分からよくわかっていないので， 改めて勉強する必要がある．\nまずは処置群の平均処置効果を知りたいときには 「傾向スコアマッチング」をおこない， 平均処置効果を知りたい場合には「傾向スコアからの層別解析」を おこなうことを知っておくことが重要となる． 共分散分析，傾向スコア，操作変数法，回帰不連続デザインなどは 基本的な考え方はわかった． いずれにしてもデータといかに向き合うのかは重要と言える.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/index.html#概要",
    "href": "contents/books/05_統計的因果推論の理論と実際/index.html#概要",
    "title": "統計的因果推論の理論と実際",
    "section": "",
    "text": "本稿は共立出版の統計的因果推論の理論と実装に関する勉強ノートです. レベルとしては大学レベルの入門統計学を学習したものとされています. 正誤表があるので内容には注意して読むこと．\n\n前半部分の回帰分析までは理解できたと思うが， 特に回帰不連続デザインの実装部分からよくわかっていないので， 改めて勉強する必要がある．\nまずは処置群の平均処置効果を知りたいときには 「傾向スコアマッチング」をおこない， 平均処置効果を知りたい場合には「傾向スコアからの層別解析」を おこなうことを知っておくことが重要となる． 共分散分析，傾向スコア，操作変数法，回帰不連続デザインなどは 基本的な考え方はわかった． いずれにしてもデータといかに向き合うのかは重要と言える.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html",
    "title": "回帰不連続デザイン",
    "section": "",
    "text": "傾向スコアでは処置群と統制群の間で重なりがあると仮定している． これは処理の割り付けが確率的である場合であることを意味する． 処理の割り付けが確定的な場合，たとえば入学試験で60以下の場合に受ける補習講義の ような場合にはその効果を傾向スコアで測ることは出来ない．\n回帰不連続デザインはこの場合に使える準実験である． つまり，同じグループ内で全員が一斉に何らかの介入を受けている状況を想定している． ある条件を満たしたときの介入がない状況と， ある状況を満たしていないときの介入がない状況に関する情報がない．\n血圧が高い人に処方する薬があるとする．このとき， ナイーブに推定すると薬を処方されると血圧が高まるという結果になってしまう． これは血圧が高いが薬が処方されていない人がいないという，交絡による．\n\n\nひとことでいうと，共分散分析をおこなうと外挿することになる． 血圧が高いが薬が処方されていない人については， 薬が処方されていない人から推定することになる. 共分散分析による外挿モデルでは，線形モデルを仮定することになるが， 線形モデルの妥当性に関する情報はデータから得ることが出来ない．\n\n\n\n上記の話しは「外挿は必要」であるが「外挿をしたくない」という話しである． そこでデータ全体における平均処置効果であるATEの推定を諦めて， 閾値\\(c\\)前後のみで比較することを考える.\n\\[\nE[Y_i(1)-Y_i(0)|X_i=c]=E[Y_i(1)|X_i=c]-E[Y_i(0)|X_i=c]\n\\]\n式の意味で言えば，別々に回帰モデルをつくって説明変数が\\(c\\)のときの値を比較する， という流れになる． 実際には\\(c\\pm h\\)の範囲を抽出して共分散分析をおこなうことになる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "回帰不連続デザイン"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html#平均処置効果と共分散分析",
    "href": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html#平均処置効果と共分散分析",
    "title": "回帰不連続デザイン",
    "section": "",
    "text": "ひとことでいうと，共分散分析をおこなうと外挿することになる． 血圧が高いが薬が処方されていない人については， 薬が処方されていない人から推定することになる. 共分散分析による外挿モデルでは，線形モデルを仮定することになるが， 線形モデルの妥当性に関する情報はデータから得ることが出来ない．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "回帰不連続デザイン"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html#閾値における局所的な平均処置効果",
    "href": "contents/books/05_統計的因果推論の理論と実際/115_回帰不連続デザインの基礎.html#閾値における局所的な平均処置効果",
    "title": "回帰不連続デザイン",
    "section": "",
    "text": "上記の話しは「外挿は必要」であるが「外挿をしたくない」という話しである． そこでデータ全体における平均処置効果であるATEの推定を諦めて， 閾値\\(c\\)前後のみで比較することを考える.\n\\[\nE[Y_i(1)-Y_i(0)|X_i=c]=E[Y_i(1)|X_i=c]-E[Y_i(0)|X_i=c]\n\\]\n式の意味で言えば，別々に回帰モデルをつくって説明変数が\\(c\\)のときの値を比較する， という流れになる． 実際には\\(c\\pm h\\)の範囲を抽出して共分散分析をおこなうことになる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "回帰不連続デザイン"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html",
    "title": "操作変数法の基礎",
    "section": "",
    "text": "これまでは共変量を調整することで交絡を無視することが出来た． 操作変数法では一定の仮定のもとで共変量による調整が出来ない場合， 効かない場合でも信頼のおける因果推論手法となる.\n操作変数法は，二段階最小二乗法，同時方程式モデリング，内生成とったキーワードでも 取り扱われている．\n\n\n\\(Y\\)を結果変数，\\(X\\)が観測さえれる共変量, \\(U\\)を観測されない共変量，とするときこれらの変数の関係は 次の図で表される構造を想定する. この構造において\\(X\\)から\\(Y\\)への効果を推定したいとする.\nなおこの構造は\\(U-&gt;X-&gt;Y\\)という部分で\\(U\\)が交絡しており， \\(X-&gt;U-&gt;Y\\)という原因\\(X\\)の中間変数\\(U\\)があるわけでないことに注意する. この構造は「アイスの売り上げと水難事故の相関における気温」にあたる．\n\n\nCode\ngrViz(\"\ndigraph dot {\n\ngraph [layout = dot, rankdir = TR]\n\nnode [shape = circle,\n      style = filled,\n      color = grey]\n    Y X U\n\nedge [color = black]\n    U -&gt; {Y X}\n    X -&gt; {Y}\n{rank = max; X; Y}\n\n}\")\n\n\n\n\n\n\nモデル式としては次となる. もし\\(U\\)が観測されるならば， これまでと同じように\\(U\\)をモデルに加えることで， 共分散分析や傾向スコアモデリングにより交絡を取り除くことが出来る． (\\(U\\)で\\(Y\\)と\\(X\\)を条件づけることで適切な解析が行える)\n\\[\nY= \\beta_0 + \\beta_1X + \\beta_2U + \\epsilon\n\\] しかし，ここでは\\(U\\)は観測されない共変量であることしており， 除外変数による偏りが不可避に発生しそうである． このような状況に対して観測できる共変量\\(Z\\)があり次のような構造にあると考える.\n\n\nCode\ngrViz(\"\ndigraph dot {\n\ngraph [layout = dot, rankdir = TR]\n\nnode [shape = circle,\n      style = filled,\n      color = grey]\n    Y X U Z\n\nedge [color = black]\n    U -&gt; {Y X}\n    X -&gt; {Y}\n    Z -&gt; {X}\n{rank = max; X; Y; Z}\n\n}\")\n\n\n\n\n\n\nこのような\\(U\\)に依存しておらず\\(X\\)に影響を与える変数を操作変数と呼ぶ. 操作変数を利用することで観測できない交絡因子\\(U\\)の影響を除いた， \\(X\\)から\\(Y\\)への影響を推定することが出来る.\n\n\n\n内生変数とはモデルの依存関係によって解の定まる変数である． 外生変数とは，モデルの依存関係によらず値が決まる変数である.\n結果変数と中間変数は内生変数である．共変量は外生変数である． 処理の割り付けが無作為でおこなわれているならば処置変数は外生変数である． しかし観察研究では処置変数は内生変数である\n\n\n\n結局のところ操作変数として適切な変数を見つけることは容易でない． 「結局は操作変数が解析者による何らかの介入や突発的な災害や事故の前後といった 変数以外では操作変数法の解析結果はあまり信頼されていない」と言われてる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数法の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#操作変数法のイメージ",
    "href": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#操作変数法のイメージ",
    "title": "操作変数法の基礎",
    "section": "",
    "text": "\\(Y\\)を結果変数，\\(X\\)が観測さえれる共変量, \\(U\\)を観測されない共変量，とするときこれらの変数の関係は 次の図で表される構造を想定する. この構造において\\(X\\)から\\(Y\\)への効果を推定したいとする.\nなおこの構造は\\(U-&gt;X-&gt;Y\\)という部分で\\(U\\)が交絡しており， \\(X-&gt;U-&gt;Y\\)という原因\\(X\\)の中間変数\\(U\\)があるわけでないことに注意する. この構造は「アイスの売り上げと水難事故の相関における気温」にあたる．\n\n\nCode\ngrViz(\"\ndigraph dot {\n\ngraph [layout = dot, rankdir = TR]\n\nnode [shape = circle,\n      style = filled,\n      color = grey]\n    Y X U\n\nedge [color = black]\n    U -&gt; {Y X}\n    X -&gt; {Y}\n{rank = max; X; Y}\n\n}\")\n\n\n\n\n\n\nモデル式としては次となる. もし\\(U\\)が観測されるならば， これまでと同じように\\(U\\)をモデルに加えることで， 共分散分析や傾向スコアモデリングにより交絡を取り除くことが出来る． (\\(U\\)で\\(Y\\)と\\(X\\)を条件づけることで適切な解析が行える)\n\\[\nY= \\beta_0 + \\beta_1X + \\beta_2U + \\epsilon\n\\] しかし，ここでは\\(U\\)は観測されない共変量であることしており， 除外変数による偏りが不可避に発生しそうである． このような状況に対して観測できる共変量\\(Z\\)があり次のような構造にあると考える.\n\n\nCode\ngrViz(\"\ndigraph dot {\n\ngraph [layout = dot, rankdir = TR]\n\nnode [shape = circle,\n      style = filled,\n      color = grey]\n    Y X U Z\n\nedge [color = black]\n    U -&gt; {Y X}\n    X -&gt; {Y}\n    Z -&gt; {X}\n{rank = max; X; Y; Z}\n\n}\")\n\n\n\n\n\n\nこのような\\(U\\)に依存しておらず\\(X\\)に影響を与える変数を操作変数と呼ぶ. 操作変数を利用することで観測できない交絡因子\\(U\\)の影響を除いた， \\(X\\)から\\(Y\\)への影響を推定することが出来る.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数法の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#内生変数と外生変数",
    "href": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#内生変数と外生変数",
    "title": "操作変数法の基礎",
    "section": "",
    "text": "内生変数とはモデルの依存関係によって解の定まる変数である． 外生変数とは，モデルの依存関係によらず値が決まる変数である.\n結果変数と中間変数は内生変数である．共変量は外生変数である． 処理の割り付けが無作為でおこなわれているならば処置変数は外生変数である． しかし観察研究では処置変数は内生変数である",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数法の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#よくない操作変数",
    "href": "contents/books/05_統計的因果推論の理論と実際/113_操作変数法の基礎.html#よくない操作変数",
    "title": "操作変数法の基礎",
    "section": "",
    "text": "結局のところ操作変数として適切な変数を見つけることは容易でない． 「結局は操作変数が解析者による何らかの介入や突発的な災害や事故の前後といった 変数以外では操作変数法の解析結果はあまり信頼されていない」と言われてる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数法の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "よく似たデザインは，文化や政治体制，教育，社会保障などが似た 二つの国を対象にして，ある変数を比較することである． しかし，この方法は完全に一致することはないので，厳密にはどの変数が影響しているのかを 判断することは難しい．\n\n\n\n同じ個体に対して，ある処置を行った場合，行っていない場合の二つの結果を得るのは不可能である．\nこのため実験研究では，無作為に処置の割り付けをした処置群と統制群の二つの群間で， 結果を比較する方法を開発してきた．これにより平均処置効果(ATE)を推定することが可能となった.\n観察研究では無作為の割り付けが困難であるため， 共変量を使ったマッチング方法が開発されて， 事実上同じ個体であると扱うことで因果推論を行うことが出来た．\n交絡因子の検証を行うことで，処置効果を推定することが可能となる． マッチング後にこのような考察を行うには， 繰り返しになるが「未確認の交絡因子がない」という仮定である\n\n\n\n統計的因果推論では，平均処置効果(ATE)，処置群の平均処置効果(ATT)が主な推定対象である.\n\\[\n\\begin{align}\n\\tau_{ATE}&=E[Y_i(1)-Y_i(0)]=E[Y_i(1)]-E[Y_i(0)] \\\\\n\\tau_{ATT}&=E[Y_i(1)-Y_i(0)|T_i=1]=E[Y_i(1)|T_i=1]-E[Y_i(0)|T_i=1]\n\\end{align}\n\\]\nマッチングにおける推定対象は，ATTだけである． なぜなら，処置群における個体に対して，対照群からマッチングする候補を選んでくるため， マッチング後のデータは処置群の個体を中心として構成されているからである．\nまた共分散分析ではATEを推定することは出来るが，ATTを推定することができない． ATTを推定する場合には傾向スコアマッチング，ATEを推定する場合には傾向スコアによる 層化解析，傾向スコアによる重み付け方法を用いる．\n\n\n\n使用するデータは多変量対数正規分布である． このデータの説明変数はテプリッツ行列を相関係数行列して設定している. テプリッツ行列は対称行列であり，斜め成分の値が一致である．\n\n\nCode\ndata11 &lt;- read_csv(\"./causality/data11.csv\", show_col_types = FALSE)\ncor((select(data11, starts_with(\"x\"))))\n#&gt;             x1         x2         x3        x4         x5          x6\n#&gt; x1  1.00000000 0.49291889 0.23515110 0.1042767 0.01814199 -0.01702065\n#&gt; x2  0.49291889 1.00000000 0.50415434 0.2381513 0.08718005  0.02007256\n#&gt; x3  0.23515110 0.50415434 1.00000000 0.5155681 0.21899658  0.09432731\n#&gt; x4  0.10427668 0.23815132 0.51556809 1.0000000 0.49573573  0.28363283\n#&gt; x5  0.01814199 0.08718005 0.21899658 0.4957357 1.00000000  0.53941181\n#&gt; x6 -0.01702065 0.02007256 0.09432731 0.2836328 0.53941181  1.00000000\n\n\n\n\nCode\n# 設定した相関係数行列\ntoeplitz(0.5^(0:5))\n#&gt;         [,1]   [,2]  [,3]  [,4]   [,5]    [,6]\n#&gt; [1,] 1.00000 0.5000 0.250 0.125 0.0625 0.03125\n#&gt; [2,] 0.50000 1.0000 0.500 0.250 0.1250 0.06250\n#&gt; [3,] 0.25000 0.5000 1.000 0.500 0.2500 0.12500\n#&gt; [4,] 0.12500 0.2500 0.500 1.000 0.5000 0.25000\n#&gt; [5,] 0.06250 0.1250 0.250 0.500 1.0000 0.50000\n#&gt; [6,] 0.03125 0.0625 0.125 0.250 0.5000 1.00000\n\n\n\n\nCode\n#ATE\nwith(data11, {\n    print(mean(y1t) - mean(y0t))\n})\n#&gt; [1] 3.755947\n\n\n\n\nCode\n#ATT\nwith(data11, {\n    print(mean(y1t[t1==1]) - mean(y0t[t1==1]))\n})\n#&gt; [1] 2.888651\n\n\n\n\n\nナイーブな比較では\\(t_1\\)の係数が15となっており，ATT，ATEとどちらもも大きく離れていることがわかる．\n\n\nCode\n# ナイーブ\nlm( y3 ~ t1, data = data11) |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1, data = data11)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -62.190 -13.494  -1.613  15.272  69.376 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  26.2678     0.9065   28.98   &lt;2e-16 ***\n#&gt; t1           15.8099     1.4534   10.88   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 22.41 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.106,  Adjusted R-squared:  0.1051 \n#&gt; F-statistic: 118.3 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n共分散分析の結果ではATT値とは当然異なるが， ATE値を見ても異なることがわかる． これは全ての交差項が含まれていないため，正しくモデリングできていないことによる． （もともとのデータで共変量の傾きが処置群と統制群で共通でないものを作成している）．\n\n\nCode\n# 共分散分析\nlm( y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11) |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -18.9525  -3.0228  -0.1244   3.1150  17.4160 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.0519     0.3086   3.409 0.000678 ***\n#&gt; t1            3.4741     0.3266  10.637  &lt; 2e-16 ***\n#&gt; x1            1.3928     0.1841   7.566 8.77e-14 ***\n#&gt; x2           -0.2920     0.1942  -1.504 0.132947    \n#&gt; x3            0.4540     0.1943   2.337 0.019663 *  \n#&gt; x4            7.4736     0.2046  36.531  &lt; 2e-16 ***\n#&gt; x5            9.6599     0.1967  49.101  &lt; 2e-16 ***\n#&gt; x6           11.1501     0.1873  59.545  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4.836 on 992 degrees of freedom\n#&gt; Multiple R-squared:  0.9586, Adjusted R-squared:  0.9583 \n#&gt; F-statistic:  3282 on 7 and 992 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n個体をペアと見なす傾向マッチングでは， 復元抽出であるか，非復元抽出であるのかを決める必要がある. とりあえず復元抽出の方が情報を抽出しやすいということを覚えておく．\n\n\n\n傾向スコアの算出はロジスティックス回帰である必要はない． 個体間の距離が\\(|e_j-e_i|\\)がスカラーで算出するだけでよい．\nマッチング方法も様々ある.\n\n\n\n\n\nCode\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nm.out1 &lt;- matchit(\n    t1 ~ x1 + x2 + x3 + x4 + x5 + x6, \n    data = data11, \n    replace = TRUE, \n    distance = \"glm\", \n    method = \"nearest\"\n)\n\n# マッチング後のデータを抽出\nm.data1 &lt;- match.data(m.out1)\nmodel1  &lt;- lm(y3 ~ t1, data = m.data1, weights = weights)\nmodel2  &lt;- lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = m.data1, weights = weights)\n\n\n\n\nCode\nm.data1\n#&gt; # A tibble: 618 × 12\n#&gt;       y0t   y1t     y3    t1     x1     x2      x3    x4      x5     x6 distance\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 61.6   64.7  64.7       1  0.640 -1.20  -0.0539 1.67   2.11    2.47     0.618\n#&gt;  2 -0.275 -1.97 -0.275     0 -0.473  0.747  0.961  0.956  0.223  -1.06     0.210\n#&gt;  3 46.8   48.3  48.3       1  0.846 -0.855  0.159  1.05   1.29    2.05     0.515\n#&gt;  4 31.0   34.6  34.6       1  0.680 -0.106  0.304  0.362  1.66    0.992    0.427\n#&gt;  5 53.3   54.3  54.3       1  1.70   2.02   2.23   2.88   1.58    1.22     0.481\n#&gt;  6 23.6   20.6  23.6       0 -0.752 -0.567  0.440  1.56   0.686   0.263    0.348\n#&gt;  7 49.5   56.3  49.5       0  2.13   2.11   1.72   1.66   1.27    1.94     0.467\n#&gt;  8 54.5   44.1  54.5       0  1.40  -0.291  1.67   1.58   0.982   2.05     0.549\n#&gt;  9 15.1   30.3  30.3       1  1.48   2.36   0.775  0.918 -0.0192  0.808    0.245\n#&gt; 10 24.4   42.3  42.3       1  0.357  1.47  -0.0248 0.435  1.79    0.733    0.341\n#&gt; # ℹ 608 more rows\n#&gt; # ℹ 1 more variable: weights &lt;dbl&gt;\n\n\n\n\nCode\nmodel1 |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1, data = m.data1, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -75.717 -14.327  -1.654  13.604  83.700 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   39.484      1.464  26.968   &lt;2e-16 ***\n#&gt; t1             2.594      1.845   1.406     0.16    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 22.16 on 616 degrees of freedom\n#&gt; Multiple R-squared:  0.003197,   Adjusted R-squared:  0.001579 \n#&gt; F-statistic: 1.976 on 1 and 616 DF,  p-value: 0.1603\n\n\n\n\nCode\nmodel2 |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = m.data1, \n#&gt;     weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -19.7535  -2.9321  -0.1799   2.7255  19.9425 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.8485     0.5049   3.661 0.000273 ***\n#&gt; t1            2.9174     0.4026   7.247 1.29e-12 ***\n#&gt; x1            1.7283     0.2465   7.010 6.32e-12 ***\n#&gt; x2            1.0956     0.2480   4.417 1.18e-05 ***\n#&gt; x3           -2.1161     0.2437  -8.683  &lt; 2e-16 ***\n#&gt; x4            7.8469     0.2553  30.731  &lt; 2e-16 ***\n#&gt; x5            9.9916     0.2479  40.306  &lt; 2e-16 ***\n#&gt; x6           11.2766     0.2413  46.742  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4.83 on 610 degrees of freedom\n#&gt; Multiple R-squared:  0.9531, Adjusted R-squared:  0.9526 \n#&gt; F-statistic:  1771 on 7 and 610 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n一応作法があるので注意\n\n\n\nVar.Ratioの値が１に近いとバランスが取れている．\n\n\nCode\nsummary(m.out1)\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, \n#&gt;     method = \"nearest\", distance = \"glm\", replace = TRUE)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.3584          0.6022     0.9897    0.1755\n#&gt; x1              0.9655        0.9919         -0.0297     0.7966    0.0210\n#&gt; x2              0.9596        0.9810         -0.0217     0.8841    0.0200\n#&gt; x3              1.1451        0.9162          0.2196     1.0303    0.0616\n#&gt; x4              1.2246        0.8986          0.3429     0.9352    0.1017\n#&gt; x5              1.2874        0.7913          0.4963     1.0065    0.1461\n#&gt; x6              1.2896        0.8382          0.4860     0.9166    0.1376\n#&gt;          eCDF Max\n#&gt; distance   0.3232\n#&gt; x1         0.0540\n#&gt; x2         0.0524\n#&gt; x3         0.1002\n#&gt; x4         0.1797\n#&gt; x5         0.2387\n#&gt; x6         0.2212\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.4369          0.0012     1.0028    0.0014\n#&gt; x1              0.9655        0.9759         -0.0117     0.7990    0.0241\n#&gt; x2              0.9596        0.9644         -0.0049     0.8047    0.0273\n#&gt; x3              1.1451        1.1281          0.0162     0.9851    0.0100\n#&gt; x4              1.2246        1.2465         -0.0231     0.8135    0.0241\n#&gt; x5              1.2874        1.2614          0.0260     1.0968    0.0121\n#&gt; x6              1.2896        1.3208         -0.0336     0.9959    0.0120\n#&gt;          eCDF Max Std. Pair Dist.\n#&gt; distance   0.0129          0.0076\n#&gt; x1         0.0668          1.2125\n#&gt; x2         0.0771          1.2002\n#&gt; x3         0.0386          1.0269\n#&gt; x4         0.0566          0.9933\n#&gt; x5         0.0540          0.6018\n#&gt; x6         0.0488          0.6997\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All            611.       389\n#&gt; Matched (ESS)  167.21     389\n#&gt; Matched        229.       389\n#&gt; Unmatched      382.         0\n#&gt; Discarded        0.         0\n\n\nラブプロットという図を書いてすべてが0.1以下になるように， 共変量や高次項などを調整していく. ラブプロットを記述するライブラリも存在しているが，ここでは スクラッチで実装する．これは後に再利用するためである．\n\n\nCode\ndiffa &lt;- abs(summary(m.out1)$sum.all[,3])\ndiffb &lt;- abs(summary(m.out1)$sum.matched[,3])\ndiff1 &lt;- rev(diffa)\ndiff2 &lt;- rev(diffb)\n\nmaxx    &lt;- max(diff1, diff2)\nlabels0 &lt;- rownames(summary(m.out1)$sum.all)\nlabels1 &lt;- rev(labels0)\n\ndotchart(diff1, xlim = c(0, maxx), labels = c(labels1))\nabline(v = .0, col = 8)\nabline(v = .1, col = 8)\nabline(v = .05, lty = 2, col = 8)\n\npar(new = TRUE)\ndotchart(diff2, xlim = c(0, maxx), labels = c(labels1), \n         pch = 16, xlab = \"Absolute Standardized Mean Difference\")",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#比較政治学におけるよく似たシスステムデザイン",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#比較政治学におけるよく似たシスステムデザイン",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "よく似たデザインは，文化や政治体制，教育，社会保障などが似た 二つの国を対象にして，ある変数を比較することである． しかし，この方法は完全に一致することはないので，厳密にはどの変数が影響しているのかを 判断することは難しい．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#統計的因果推論におけるマッチング",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#統計的因果推論におけるマッチング",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "同じ個体に対して，ある処置を行った場合，行っていない場合の二つの結果を得るのは不可能である．\nこのため実験研究では，無作為に処置の割り付けをした処置群と統制群の二つの群間で， 結果を比較する方法を開発してきた．これにより平均処置効果(ATE)を推定することが可能となった.\n観察研究では無作為の割り付けが困難であるため， 共変量を使ったマッチング方法が開発されて， 事実上同じ個体であると扱うことで因果推論を行うことが出来た．\n交絡因子の検証を行うことで，処置効果を推定することが可能となる． マッチング後にこのような考察を行うには， 繰り返しになるが「未確認の交絡因子がない」という仮定である",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#推定対象",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#推定対象",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "統計的因果推論では，平均処置効果(ATE)，処置群の平均処置効果(ATT)が主な推定対象である.\n\\[\n\\begin{align}\n\\tau_{ATE}&=E[Y_i(1)-Y_i(0)]=E[Y_i(1)]-E[Y_i(0)] \\\\\n\\tau_{ATT}&=E[Y_i(1)-Y_i(0)|T_i=1]=E[Y_i(1)|T_i=1]-E[Y_i(0)|T_i=1]\n\\end{align}\n\\]\nマッチングにおける推定対象は，ATTだけである． なぜなら，処置群における個体に対して，対照群からマッチングする候補を選んでくるため， マッチング後のデータは処置群の個体を中心として構成されているからである．\nまた共分散分析ではATEを推定することは出来るが，ATTを推定することができない． ATTを推定する場合には傾向スコアマッチング，ATEを推定する場合には傾向スコアによる 層化解析，傾向スコアによる重み付け方法を用いる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#使用するデータ",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#使用するデータ",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "使用するデータは多変量対数正規分布である． このデータの説明変数はテプリッツ行列を相関係数行列して設定している. テプリッツ行列は対称行列であり，斜め成分の値が一致である．\n\n\nCode\ndata11 &lt;- read_csv(\"./causality/data11.csv\", show_col_types = FALSE)\ncor((select(data11, starts_with(\"x\"))))\n#&gt;             x1         x2         x3        x4         x5          x6\n#&gt; x1  1.00000000 0.49291889 0.23515110 0.1042767 0.01814199 -0.01702065\n#&gt; x2  0.49291889 1.00000000 0.50415434 0.2381513 0.08718005  0.02007256\n#&gt; x3  0.23515110 0.50415434 1.00000000 0.5155681 0.21899658  0.09432731\n#&gt; x4  0.10427668 0.23815132 0.51556809 1.0000000 0.49573573  0.28363283\n#&gt; x5  0.01814199 0.08718005 0.21899658 0.4957357 1.00000000  0.53941181\n#&gt; x6 -0.01702065 0.02007256 0.09432731 0.2836328 0.53941181  1.00000000\n\n\n\n\nCode\n# 設定した相関係数行列\ntoeplitz(0.5^(0:5))\n#&gt;         [,1]   [,2]  [,3]  [,4]   [,5]    [,6]\n#&gt; [1,] 1.00000 0.5000 0.250 0.125 0.0625 0.03125\n#&gt; [2,] 0.50000 1.0000 0.500 0.250 0.1250 0.06250\n#&gt; [3,] 0.25000 0.5000 1.000 0.500 0.2500 0.12500\n#&gt; [4,] 0.12500 0.2500 0.500 1.000 0.5000 0.25000\n#&gt; [5,] 0.06250 0.1250 0.250 0.500 1.0000 0.50000\n#&gt; [6,] 0.03125 0.0625 0.125 0.250 0.5000 1.00000\n\n\n\n\nCode\n#ATE\nwith(data11, {\n    print(mean(y1t) - mean(y0t))\n})\n#&gt; [1] 3.755947\n\n\n\n\nCode\n#ATT\nwith(data11, {\n    print(mean(y1t[t1==1]) - mean(y0t[t1==1]))\n})\n#&gt; [1] 2.888651",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#ナイーブな比較と共分散分析",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#ナイーブな比較と共分散分析",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "ナイーブな比較では\\(t_1\\)の係数が15となっており，ATT，ATEとどちらもも大きく離れていることがわかる．\n\n\nCode\n# ナイーブ\nlm( y3 ~ t1, data = data11) |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1, data = data11)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -62.190 -13.494  -1.613  15.272  69.376 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  26.2678     0.9065   28.98   &lt;2e-16 ***\n#&gt; t1           15.8099     1.4534   10.88   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 22.41 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.106,  Adjusted R-squared:  0.1051 \n#&gt; F-statistic: 118.3 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\n共分散分析の結果ではATT値とは当然異なるが， ATE値を見ても異なることがわかる． これは全ての交差項が含まれていないため，正しくモデリングできていないことによる． （もともとのデータで共変量の傾きが処置群と統制群で共通でないものを作成している）．\n\n\nCode\n# 共分散分析\nlm( y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11) |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -18.9525  -3.0228  -0.1244   3.1150  17.4160 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.0519     0.3086   3.409 0.000678 ***\n#&gt; t1            3.4741     0.3266  10.637  &lt; 2e-16 ***\n#&gt; x1            1.3928     0.1841   7.566 8.77e-14 ***\n#&gt; x2           -0.2920     0.1942  -1.504 0.132947    \n#&gt; x3            0.4540     0.1943   2.337 0.019663 *  \n#&gt; x4            7.4736     0.2046  36.531  &lt; 2e-16 ***\n#&gt; x5            9.6599     0.1967  49.101  &lt; 2e-16 ***\n#&gt; x6           11.1501     0.1873  59.545  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4.836 on 992 degrees of freedom\n#&gt; Multiple R-squared:  0.9586, Adjusted R-squared:  0.9583 \n#&gt; F-statistic:  3282 on 7 and 992 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#復元によるマッチングと非復元によるマッチング",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#復元によるマッチングと非復元によるマッチング",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "個体をペアと見なす傾向マッチングでは， 復元抽出であるか，非復元抽出であるのかを決める必要がある. とりあえず復元抽出の方が情報を抽出しやすいということを覚えておく．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#距離",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#距離",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "傾向スコアの算出はロジスティックス回帰である必要はない． 個体間の距離が\\(|e_j-e_i|\\)がスカラーで算出するだけでよい．\nマッチング方法も様々ある.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#rによる復元抽出の傾向スコアマッチングattの推定",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#rによる復元抽出の傾向スコアマッチングattの推定",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "Code\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nm.out1 &lt;- matchit(\n    t1 ~ x1 + x2 + x3 + x4 + x5 + x6, \n    data = data11, \n    replace = TRUE, \n    distance = \"glm\", \n    method = \"nearest\"\n)\n\n# マッチング後のデータを抽出\nm.data1 &lt;- match.data(m.out1)\nmodel1  &lt;- lm(y3 ~ t1, data = m.data1, weights = weights)\nmodel2  &lt;- lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = m.data1, weights = weights)\n\n\n\n\nCode\nm.data1\n#&gt; # A tibble: 618 × 12\n#&gt;       y0t   y1t     y3    t1     x1     x2      x3    x4      x5     x6 distance\n#&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 61.6   64.7  64.7       1  0.640 -1.20  -0.0539 1.67   2.11    2.47     0.618\n#&gt;  2 -0.275 -1.97 -0.275     0 -0.473  0.747  0.961  0.956  0.223  -1.06     0.210\n#&gt;  3 46.8   48.3  48.3       1  0.846 -0.855  0.159  1.05   1.29    2.05     0.515\n#&gt;  4 31.0   34.6  34.6       1  0.680 -0.106  0.304  0.362  1.66    0.992    0.427\n#&gt;  5 53.3   54.3  54.3       1  1.70   2.02   2.23   2.88   1.58    1.22     0.481\n#&gt;  6 23.6   20.6  23.6       0 -0.752 -0.567  0.440  1.56   0.686   0.263    0.348\n#&gt;  7 49.5   56.3  49.5       0  2.13   2.11   1.72   1.66   1.27    1.94     0.467\n#&gt;  8 54.5   44.1  54.5       0  1.40  -0.291  1.67   1.58   0.982   2.05     0.549\n#&gt;  9 15.1   30.3  30.3       1  1.48   2.36   0.775  0.918 -0.0192  0.808    0.245\n#&gt; 10 24.4   42.3  42.3       1  0.357  1.47  -0.0248 0.435  1.79    0.733    0.341\n#&gt; # ℹ 608 more rows\n#&gt; # ℹ 1 more variable: weights &lt;dbl&gt;\n\n\n\n\nCode\nmodel1 |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1, data = m.data1, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -75.717 -14.327  -1.654  13.604  83.700 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   39.484      1.464  26.968   &lt;2e-16 ***\n#&gt; t1             2.594      1.845   1.406     0.16    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 22.16 on 616 degrees of freedom\n#&gt; Multiple R-squared:  0.003197,   Adjusted R-squared:  0.001579 \n#&gt; F-statistic: 1.976 on 1 and 616 DF,  p-value: 0.1603\n\n\n\n\nCode\nmodel2 |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = m.data1, \n#&gt;     weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -19.7535  -2.9321  -0.1799   2.7255  19.9425 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.8485     0.5049   3.661 0.000273 ***\n#&gt; t1            2.9174     0.4026   7.247 1.29e-12 ***\n#&gt; x1            1.7283     0.2465   7.010 6.32e-12 ***\n#&gt; x2            1.0956     0.2480   4.417 1.18e-05 ***\n#&gt; x3           -2.1161     0.2437  -8.683  &lt; 2e-16 ***\n#&gt; x4            7.8469     0.2553  30.731  &lt; 2e-16 ***\n#&gt; x5            9.9916     0.2479  40.306  &lt; 2e-16 ***\n#&gt; x6           11.2766     0.2413  46.742  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 4.83 on 610 degrees of freedom\n#&gt; Multiple R-squared:  0.9531, Adjusted R-squared:  0.9526 \n#&gt; F-statistic:  1771 on 7 and 610 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#標準誤差",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#標準誤差",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "一応作法があるので注意",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#バランシングの評価",
    "href": "contents/books/05_統計的因果推論の理論と実際/111_傾向スコアマッチング.html#バランシングの評価",
    "title": "傾向スコアマッチング",
    "section": "",
    "text": "Var.Ratioの値が１に近いとバランスが取れている．\n\n\nCode\nsummary(m.out1)\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, \n#&gt;     method = \"nearest\", distance = \"glm\", replace = TRUE)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.3584          0.6022     0.9897    0.1755\n#&gt; x1              0.9655        0.9919         -0.0297     0.7966    0.0210\n#&gt; x2              0.9596        0.9810         -0.0217     0.8841    0.0200\n#&gt; x3              1.1451        0.9162          0.2196     1.0303    0.0616\n#&gt; x4              1.2246        0.8986          0.3429     0.9352    0.1017\n#&gt; x5              1.2874        0.7913          0.4963     1.0065    0.1461\n#&gt; x6              1.2896        0.8382          0.4860     0.9166    0.1376\n#&gt;          eCDF Max\n#&gt; distance   0.3232\n#&gt; x1         0.0540\n#&gt; x2         0.0524\n#&gt; x3         0.1002\n#&gt; x4         0.1797\n#&gt; x5         0.2387\n#&gt; x6         0.2212\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.4369          0.0012     1.0028    0.0014\n#&gt; x1              0.9655        0.9759         -0.0117     0.7990    0.0241\n#&gt; x2              0.9596        0.9644         -0.0049     0.8047    0.0273\n#&gt; x3              1.1451        1.1281          0.0162     0.9851    0.0100\n#&gt; x4              1.2246        1.2465         -0.0231     0.8135    0.0241\n#&gt; x5              1.2874        1.2614          0.0260     1.0968    0.0121\n#&gt; x6              1.2896        1.3208         -0.0336     0.9959    0.0120\n#&gt;          eCDF Max Std. Pair Dist.\n#&gt; distance   0.0129          0.0076\n#&gt; x1         0.0668          1.2125\n#&gt; x2         0.0771          1.2002\n#&gt; x3         0.0386          1.0269\n#&gt; x4         0.0566          0.9933\n#&gt; x5         0.0540          0.6018\n#&gt; x6         0.0488          0.6997\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All            611.       389\n#&gt; Matched (ESS)  167.21     389\n#&gt; Matched        229.       389\n#&gt; Unmatched      382.         0\n#&gt; Discarded        0.         0\n\n\nラブプロットという図を書いてすべてが0.1以下になるように， 共変量や高次項などを調整していく. ラブプロットを記述するライブラリも存在しているが，ここでは スクラッチで実装する．これは後に再利用するためである．\n\n\nCode\ndiffa &lt;- abs(summary(m.out1)$sum.all[,3])\ndiffb &lt;- abs(summary(m.out1)$sum.matched[,3])\ndiff1 &lt;- rev(diffa)\ndiff2 &lt;- rev(diffb)\n\nmaxx    &lt;- max(diff1, diff2)\nlabels0 &lt;- rownames(summary(m.out1)$sum.all)\nlabels1 &lt;- rev(labels0)\n\ndotchart(diff1, xlim = c(0, maxx), labels = c(labels1))\nabline(v = .0, col = 8)\nabline(v = .1, col = 8)\nabline(v = .05, lty = 2, col = 8)\n\npar(new = TRUE)\ndotchart(diff2, xlim = c(0, maxx), labels = c(labels1), \n         pch = 16, xlab = \"Absolute Standardized Mean Difference\")",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアマッチング"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "共分散分析のモデル次式に示す． ダミー変数を含む重回帰モデルであり， 共変量によるデータの偏りを調整したうえで処置の影響を算出する有力なモデルである．\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1}+\\beta_2T_i+\\epsilon_i\n\\]\n\\[\nT_i =\n\\begin{cases}\n1 \\\\\n0\n\\end{cases}\n\\]\n重回帰モデルであるため共分散分析では，これまでの仮定を満たす必要がある． さらに処置変数によらず共変量が目的変数に与える影響が同じ， つまり\\(\\beta_1\\)が共通という仮定がある.\nこの仮定が満たされない場合には，交互作用項を見込んだモデルを作成する必要がある.\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1}+\\beta_2T_i+\\beta_3X_{i1}T_i+\\epsilon_i\n\\]\nこのモデルを使うことより説明変数が目的変数に与える影響は， \\(\\beta_3\\)だけ処置群間で異なることがわかる.\n\\[\nY_i = \\beta_0 + (\\beta_1+\\beta_3T_i)X_{i1}+\\beta_2T_i+\\epsilon_i\n\\]\nこのようなモデルが必要になるのは, 説明変数で条件づけたときに， 処置の割り付け確率が異なるということが影響している． これにより潜在的結果変数と処置が独立でなくなる.\n\n\n\n\\[\nY_i(T_i=0) = \\beta_0 + \\beta_1X_{i}+\\epsilon_i\n\\]\n\\[\nY_i(T_i=1) = (\\beta_0 + \\beta_2) + (\\beta_1+\\beta_3)X_{i}+\\epsilon_i\n\\]\n\n\n個体別効果は潜在的結果変数\\(Y_i(1)\\)と\\(Y_i(0)\\)の差であるから， これは次式で表されることになる.\n\\[\n\\tau_i=Y_i(1)-Y_i(0)=\\beta_2+\\beta_3X_i\n\\]\nまた実際に推定すべきは個別効果の期待値である． つまり，\\(\\beta_2\\)に\\(\\beta_3\\)と\\(X_i\\)の平均値の積を加えた値である. これは実際に計算してみると共分散分析で求めることが可能となる.\n\\[\n\\tau_{ATE}=E[Y_i(1)-Y_i(0)]\\\\\n\\tau_{ATE}=\\beta_2+\\beta_3E[X_i]\n\\]\n\n\n\n\\(\\tau_{\\text{ATE}}\\)の標準誤差は lmの結果からは求めることが出来ないので注意． 詳細は省略するが\\(var(\\tau_{\\text{ATE}})\\)を求める必要がある.\n\n\n\n\n\n結果変数には影響するが，処置変数とは関連のない共変量は，モデルに入れても偏りに悪影響はない．ただし，モデルの説明量を改善できるため，そのような変数が存在する場合には積極的に活用すること.\n結果変数に影響を与えており，処置変数とも関連のある共変量は偏りに悪影響を及ぼすため，モデルにふくめなければならない．このような変数のことを「交絡因子」と呼ぶ\n処置変数とは関連あるが，結果変数に影響のない共変量は，標準誤差を大きくするため含めない方が望ましい．\n処置変数と結果変数の因果のパスの間に存在する中間辺陬はモデルに入れてはならない\n２以上の共変量間に強い多重共線性があってもそのままモデルに入れてよい\nあまり重要でない共変量はまとめて誤差のとして扱うのが現実的な場合もある.\nもし処置群と統制群とで回帰の傾きが平行でないと考えられるならば，モデルに交互作用項を入れる必要がある\n\n\n\n\n共変量が多変量のときに作業が膨大になる. 平均処置効果，つまり処置があった場合となかった場合の「差」を推定することは可能であるが， 処置群の平均処置効果を推定することが出来ない．\n\n\n\n傾向スコアとは共変量\\(X\\)が与えられたとき， 処置に割り付けられる確率である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#共分散分析の仮定",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#共分散分析の仮定",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "共分散分析のモデル次式に示す． ダミー変数を含む重回帰モデルであり， 共変量によるデータの偏りを調整したうえで処置の影響を算出する有力なモデルである．\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1}+\\beta_2T_i+\\epsilon_i\n\\]\n\\[\nT_i =\n\\begin{cases}\n1 \\\\\n0\n\\end{cases}\n\\]\n重回帰モデルであるため共分散分析では，これまでの仮定を満たす必要がある． さらに処置変数によらず共変量が目的変数に与える影響が同じ， つまり\\(\\beta_1\\)が共通という仮定がある.\nこの仮定が満たされない場合には，交互作用項を見込んだモデルを作成する必要がある.\n\\[\nY_i = \\beta_0 + \\beta_1X_{i1}+\\beta_2T_i+\\beta_3X_{i1}T_i+\\epsilon_i\n\\]\nこのモデルを使うことより説明変数が目的変数に与える影響は， \\(\\beta_3\\)だけ処置群間で異なることがわかる.\n\\[\nY_i = \\beta_0 + (\\beta_1+\\beta_3T_i)X_{i1}+\\beta_2T_i+\\epsilon_i\n\\]\nこのようなモデルが必要になるのは, 説明変数で条件づけたときに， 処置の割り付け確率が異なるということが影響している． これにより潜在的結果変数と処置が独立でなくなる.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#交互作用項のある共分散分析-1",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#交互作用項のある共分散分析-1",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "\\[\nY_i(T_i=0) = \\beta_0 + \\beta_1X_{i}+\\epsilon_i\n\\]\n\\[\nY_i(T_i=1) = (\\beta_0 + \\beta_2) + (\\beta_1+\\beta_3)X_{i}+\\epsilon_i\n\\]\n\n\n個体別効果は潜在的結果変数\\(Y_i(1)\\)と\\(Y_i(0)\\)の差であるから， これは次式で表されることになる.\n\\[\n\\tau_i=Y_i(1)-Y_i(0)=\\beta_2+\\beta_3X_i\n\\]\nまた実際に推定すべきは個別効果の期待値である． つまり，\\(\\beta_2\\)に\\(\\beta_3\\)と\\(X_i\\)の平均値の積を加えた値である. これは実際に計算してみると共分散分析で求めることが可能となる.\n\\[\n\\tau_{ATE}=E[Y_i(1)-Y_i(0)]\\\\\n\\tau_{ATE}=\\beta_2+\\beta_3E[X_i]\n\\]\n\n\n\n\\(\\tau_{\\text{ATE}}\\)の標準誤差は lmの結果からは求めることが出来ないので注意． 詳細は省略するが\\(var(\\tau_{\\text{ATE}})\\)を求める必要がある.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#統制すべき共変量に関するまとめ",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#統制すべき共変量に関するまとめ",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "結果変数には影響するが，処置変数とは関連のない共変量は，モデルに入れても偏りに悪影響はない．ただし，モデルの説明量を改善できるため，そのような変数が存在する場合には積極的に活用すること.\n結果変数に影響を与えており，処置変数とも関連のある共変量は偏りに悪影響を及ぼすため，モデルにふくめなければならない．このような変数のことを「交絡因子」と呼ぶ\n処置変数とは関連あるが，結果変数に影響のない共変量は，標準誤差を大きくするため含めない方が望ましい．\n処置変数と結果変数の因果のパスの間に存在する中間辺陬はモデルに入れてはならない\n２以上の共変量間に強い多重共線性があってもそのままモデルに入れてよい\nあまり重要でない共変量はまとめて誤差のとして扱うのが現実的な場合もある.\nもし処置群と統制群とで回帰の傾きが平行でないと考えられるならば，モデルに交互作用項を入れる必要がある",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#共分散分析の限界",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#共分散分析の限界",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "共変量が多変量のときに作業が膨大になる. 平均処置効果，つまり処置があった場合となかった場合の「差」を推定することは可能であるが， 処置群の平均処置効果を推定することが出来ない．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#傾向スコアと共分散分析の優劣",
    "href": "contents/books/05_統計的因果推論の理論と実際/109_交互作用項のある共分散分析.html#傾向スコアと共分散分析の優劣",
    "title": "交互作用項のある共分散分析",
    "section": "",
    "text": "傾向スコアとは共変量\\(X\\)が与えられたとき， 処置に割り付けられる確率である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "交互作用項のある共分散分析"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html",
    "title": "最小二乗法の仮定と診断1",
    "section": "",
    "text": "最小二乗法による重回帰モデルでは６つの仮定が置かれている． ここでは次に示す３つの仮定について解説する.\n\n誤差項の平均値がゼロ\nパラメータの線形性\n誤差項の条件付き期待値がゼロ\n\n\n\n切片項があれば誤差項がゼロでなくて， 切片項に平均値分を持たせることが出来るので， 通常はこの仮定を満たすことが出来る.\nつまり，回帰係数の不偏性には影響しない.\n\n\n\n最小二乗法による重回帰モデルのパラメータ推定が， 不偏性を持つために必要な条件となる.\n例えば次の二つの式を考える．このうち，１つ目の式は対数変換を 行おうことで適切にモデル化することができる．一方で， 二つ目の式にはそのような変換が存在しない． これはパラメータの線形性と変数の非線形性という少し解釈が難しい話題である.\n\\[\n\\begin{align}\nY_i &= \\beta_0X_1^{\\beta_1}X_2^{\\beta_2}\\exp{\\epsilon_i}\\\\\nY_i &= \\beta_0X_1^{\\beta_1}X_2^{\\beta_2} + \\epsilon_i\n\\end{align}\n\\]\n．．．とはいいつつもデータの非線形性はよいが， パラメータについてはいつも線形であると理解しておけばよさそう.\n\n\n共変量が多変量になる場合には他の共変量を統制した場合の効果， つまり偏回帰係数に興味がある． これは二変量の散布図では適切な関数系を示すことが出来ないため， 成分プラス残差プロットを私用することが推奨されている.\n\n\n\n\n下記の式で表される条件である．これはつまり，統制すべき交絡因子が十分に モデルに含まれていることを指している． しかし，そのことを診断する方法はない.\n\\[\nE[\\epsilon_i|X]=E[\\epsilon_i]=0\n\\]\n統計的因果論の立場で考えると， 観測された共変量の値が同じ個体同士では，処置の割り付けは 無作為になっていると考えて良いという仮定を指す．\n\\[\n\\text{Pr}(T_i|Y_i(1), Y_i(0),X) = \\text{Pr}(T_i|X)\n\\]\nこの仮定を満たすことを考えるためには，できるだけ 多くの変数をモデルに取り入れて必要な共変量を取りこぼす可能性を下げることである． このとき検討する項目は次の２点である.\n\n不要な変数を取り込んだことの影響\n因果関係の間に位置する変数の取り扱い\n\n\n\n説明変数に「多重共線性」が生じていなければ不偏性には問題ない． ただしパラメータの標準誤差が大きくなる.\n\n\nCode\nbeta0 &lt;- 1.\nbeta1 &lt;- 1.5\nbeta2 &lt;- 1.2\ndata &lt;- tibble(\n    x1 = rnorm(n = 100, mean = 1), \n    x2 = rnorm(n = 100, mean = 3), \n    x3 = rnorm(100),\n    y  = 1 + beta1 * x1 + beta2 * x2  + rnorm(100)\n)\n\nmodels &lt;- \n    tibble(\n        formula = c(\n            \"y ~ x1\", \n            \"y ~ x1 + x2\", \n            \"y ~ x1 + x2 + x3\"\n        )\n    ) |&gt; \n    mutate(\n        reg = map(formula, ~ lm(., data = data))\n    )\n\n\n余計な変数を入れても不偏性には影響しない.\n\n\nCode\nmodels$reg \n#&gt; [[1]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;       4.634        1.409  \n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2  \n#&gt;      0.9523       1.5342       1.1703  \n#&gt; \n#&gt; \n#&gt; [[3]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2           x3  \n#&gt;    0.952686     1.533826     1.170225    -0.001641\n\n\nただしパラメータ推定の分散は大きくなる\n\n\nCode\nmodels$reg |&gt; lapply(confint)\n#&gt; [[1]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 4.214445 5.054522\n#&gt; x1          1.132469 1.685788\n#&gt; \n#&gt; [[2]]\n#&gt;                 2.5 %   97.5 %\n#&gt; (Intercept) 0.3004943 1.604135\n#&gt; x1          1.3589493 1.709550\n#&gt; x2          0.9808976 1.359608\n#&gt; \n#&gt; [[3]]\n#&gt;                  2.5 %    97.5 %\n#&gt; (Intercept)  0.2962068 1.6091652\n#&gt; x1           1.3519278 1.7157247\n#&gt; x2           0.9798373 1.3606120\n#&gt; x3          -0.1762091 0.1729263\n\n\n\n\n\n「因果関係の間に位置する変数」の問題を考える。 結論的にいえばそのような変数を含めてはならない.\n\n\nCode\ndata &lt;- tibble(\n    x1 = rnorm(n = 100, mean = 1), \n    x2 = 1 + 1.5 * x1 + rnorm(n = 100), \n    y  = 1 + 1.2 * x1 + 1.6 * x2  + rnorm(100)\n)\n\nmodels &lt;- \n    tibble(\n        formula = c(\n            \"y ~ x1\", \n            \"y ~ x1 + x2\"\n        )\n    ) |&gt; \n    mutate(\n        reg = map(formula, ~ lm(., data = data))\n    )\n\n\n上記のサンプルデータの場合、x1がyに与える影響は \\(1.2 + 1.5 * 1.6 = 3.6\\)なので中間変数であるx2を含まない方がよい推定である ことがわかる.\n\n\nCode\nmodels$reg\n#&gt; [[1]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;       2.682        3.463  \n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2  \n#&gt;      0.8675       1.4104       1.5806\n\n\n\n\nCode\nmodels$reg |&gt; lapply(confint)\n#&gt; [[1]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 2.109008 3.255472\n#&gt; x1          3.085248 3.840094\n#&gt; \n#&gt; [[2]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 0.488317 1.246698\n#&gt; x1          1.085086 1.735793\n#&gt; x2          1.382953 1.778201",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断1"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#誤差項の期待値ゼロ",
    "href": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#誤差項の期待値ゼロ",
    "title": "最小二乗法の仮定と診断1",
    "section": "",
    "text": "切片項があれば誤差項がゼロでなくて， 切片項に平均値分を持たせることが出来るので， 通常はこの仮定を満たすことが出来る.\nつまり，回帰係数の不偏性には影響しない.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断1"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#パラメータにおける線形性",
    "href": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#パラメータにおける線形性",
    "title": "最小二乗法の仮定と診断1",
    "section": "",
    "text": "最小二乗法による重回帰モデルのパラメータ推定が， 不偏性を持つために必要な条件となる.\n例えば次の二つの式を考える．このうち，１つ目の式は対数変換を 行おうことで適切にモデル化することができる．一方で， 二つ目の式にはそのような変換が存在しない． これはパラメータの線形性と変数の非線形性という少し解釈が難しい話題である.\n\\[\n\\begin{align}\nY_i &= \\beta_0X_1^{\\beta_1}X_2^{\\beta_2}\\exp{\\epsilon_i}\\\\\nY_i &= \\beta_0X_1^{\\beta_1}X_2^{\\beta_2} + \\epsilon_i\n\\end{align}\n\\]\n．．．とはいいつつもデータの非線形性はよいが， パラメータについてはいつも線形であると理解しておけばよさそう.\n\n\n共変量が多変量になる場合には他の共変量を統制した場合の効果， つまり偏回帰係数に興味がある． これは二変量の散布図では適切な関数系を示すことが出来ないため， 成分プラス残差プロットを私用することが推奨されている.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断1"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#誤差項の条件付き期待値ゼロ",
    "href": "contents/books/05_統計的因果推論の理論と実際/107_最小二乗法による重回帰モデルの仮定と診断1.html#誤差項の条件付き期待値ゼロ",
    "title": "最小二乗法の仮定と診断1",
    "section": "",
    "text": "下記の式で表される条件である．これはつまり，統制すべき交絡因子が十分に モデルに含まれていることを指している． しかし，そのことを診断する方法はない.\n\\[\nE[\\epsilon_i|X]=E[\\epsilon_i]=0\n\\]\n統計的因果論の立場で考えると， 観測された共変量の値が同じ個体同士では，処置の割り付けは 無作為になっていると考えて良いという仮定を指す．\n\\[\n\\text{Pr}(T_i|Y_i(1), Y_i(0),X) = \\text{Pr}(T_i|X)\n\\]\nこの仮定を満たすことを考えるためには，できるだけ 多くの変数をモデルに取り入れて必要な共変量を取りこぼす可能性を下げることである． このとき検討する項目は次の２点である.\n\n不要な変数を取り込んだことの影響\n因果関係の間に位置する変数の取り扱い\n\n\n\n説明変数に「多重共線性」が生じていなければ不偏性には問題ない． ただしパラメータの標準誤差が大きくなる.\n\n\nCode\nbeta0 &lt;- 1.\nbeta1 &lt;- 1.5\nbeta2 &lt;- 1.2\ndata &lt;- tibble(\n    x1 = rnorm(n = 100, mean = 1), \n    x2 = rnorm(n = 100, mean = 3), \n    x3 = rnorm(100),\n    y  = 1 + beta1 * x1 + beta2 * x2  + rnorm(100)\n)\n\nmodels &lt;- \n    tibble(\n        formula = c(\n            \"y ~ x1\", \n            \"y ~ x1 + x2\", \n            \"y ~ x1 + x2 + x3\"\n        )\n    ) |&gt; \n    mutate(\n        reg = map(formula, ~ lm(., data = data))\n    )\n\n\n余計な変数を入れても不偏性には影響しない.\n\n\nCode\nmodels$reg \n#&gt; [[1]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;       4.634        1.409  \n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2  \n#&gt;      0.9523       1.5342       1.1703  \n#&gt; \n#&gt; \n#&gt; [[3]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2           x3  \n#&gt;    0.952686     1.533826     1.170225    -0.001641\n\n\nただしパラメータ推定の分散は大きくなる\n\n\nCode\nmodels$reg |&gt; lapply(confint)\n#&gt; [[1]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 4.214445 5.054522\n#&gt; x1          1.132469 1.685788\n#&gt; \n#&gt; [[2]]\n#&gt;                 2.5 %   97.5 %\n#&gt; (Intercept) 0.3004943 1.604135\n#&gt; x1          1.3589493 1.709550\n#&gt; x2          0.9808976 1.359608\n#&gt; \n#&gt; [[3]]\n#&gt;                  2.5 %    97.5 %\n#&gt; (Intercept)  0.2962068 1.6091652\n#&gt; x1           1.3519278 1.7157247\n#&gt; x2           0.9798373 1.3606120\n#&gt; x3          -0.1762091 0.1729263\n\n\n\n\n\n「因果関係の間に位置する変数」の問題を考える。 結論的にいえばそのような変数を含めてはならない.\n\n\nCode\ndata &lt;- tibble(\n    x1 = rnorm(n = 100, mean = 1), \n    x2 = 1 + 1.5 * x1 + rnorm(n = 100), \n    y  = 1 + 1.2 * x1 + 1.6 * x2  + rnorm(100)\n)\n\nmodels &lt;- \n    tibble(\n        formula = c(\n            \"y ~ x1\", \n            \"y ~ x1 + x2\"\n        )\n    ) |&gt; \n    mutate(\n        reg = map(formula, ~ lm(., data = data))\n    )\n\n\n上記のサンプルデータの場合、x1がyに与える影響は \\(1.2 + 1.5 * 1.6 = 3.6\\)なので中間変数であるx2を含まない方がよい推定である ことがわかる.\n\n\nCode\nmodels$reg\n#&gt; [[1]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;       2.682        3.463  \n#&gt; \n#&gt; \n#&gt; [[2]]\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1           x2  \n#&gt;      0.8675       1.4104       1.5806\n\n\n\n\nCode\nmodels$reg |&gt; lapply(confint)\n#&gt; [[1]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 2.109008 3.255472\n#&gt; x1          3.085248 3.840094\n#&gt; \n#&gt; [[2]]\n#&gt;                2.5 %   97.5 %\n#&gt; (Intercept) 0.488317 1.246698\n#&gt; x1          1.085086 1.735793\n#&gt; x2          1.382953 1.778201",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断1"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/105_回帰分析の基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/105_回帰分析の基礎.html",
    "title": "回帰分析の基礎",
    "section": "",
    "text": "1 回帰分析の基礎\n共変量Xによって条件付けることで無視可能な割り付けとみなせるとき， 回帰分析によって平均処置効果を推定することができるた． このような条件が満たされない場合には回帰係数は必ずしも因果関係を表さないことに注意する．\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "回帰分析の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html",
    "title": "重要な仮定",
    "section": "",
    "text": "処置の無作為割り付けによって平均処置効果を適切に推定できることを確認した． ここまでは分散分析をはじめとした従来の統計学と同様の論旨である．\nここからは統計的因果推論における重要な仮定と， それを踏まえたうえで無作為割り付けを実行できない観察研究にける統計的因果推論を説明する.\n\n\nStable Unit Treatment Value Assumption\n処置を受ける個体(unit)ごとに処置の値が安定的であるという仮定である． 具体的には次の二つの条件が成り立つ．\n\n相互干渉がない\n個体に対する隠れた処置がない\n\n相互干渉ある場合には相互干渉がなくなるまでunitの単位を変更する. 隠れた処置がない，とはつまり処置以外の属性がランダム化されているという理解で良さそうである． 補習講義を受けた場合でも先生が異なる場合には，A先生の補習講義を受けた，B先生の補習講義を 受けた，補習講義を受けていないなどとして着目した変数以外をランダム化することに努める．\n確率が相互に背反であるとは，集合に重なりがないことであるあため，次式が成立することを指す.\n\\[\nP(A|B)=0\n\\]\n確率が独立であるとは，ある情報で条件づけても確率が変化しないことである．\n\\[\nP(A|B)=P(A)\n\\]\n\n\n\n観測されたデータから母数が一意に満たされていない場合， 標本から母集団への推定ができないということになる． このとき識別性がないという． 特に，ここでは識別性=正値性＋独立性である．\n正値性の条件を課すとunitが処置を受けるかうけないかは， わからない状態，つまりどちらも可能性があるため次の式で表現される.\n\\[\n0&lt;\\textrm{Pr}(T=1) &lt;1\n\\]\n独立性とは処置の割り付けが潜在的結果変数に依存して行われてはいけない，ということである． つまりすべての変量と独立になっており，実験研究ではこの条件が満たされやすい.\n\\[\n{Y(1), Y(0)} \\perp T\n\\]\n改めて観測値\\(Y_i\\)は次の式で表される.\n\\[\nY_i = (1-T_i)Y_i(0) + T_iY_i(1)\n\\]\n\\(Y_i(0), Y_i(1)\\)は潜在的結果変数の組であるため， 個体\\(i\\)に対してはどちらか一方しか観測されない. よって次式により処置効果を求めることは通常不可能である.\n\\[\n\\tau_{ATE}=E[Y_i(1)-Y_i(0)]=E[Y_i(1)]-E[Y_i(0)]\n\\]\n一方でそれぞれの処置で条件付けた場合には平均値の算出が可能であるため， 次式で示したナイーブな推定量を使うことが可能である. しかしこの値は本来推定したいものではない．本来推定したいのは，処置の平均処置効果，処置群の平均処置効果のどちらかである．\n\\[\nE[Y_i|T_i=1]-E[Y_i|T_i=0]\n\\]\n処置の割り付けを表す\\(T_i\\)が潜在的結果変数の組に依存していなければ \\(T_i\\)と\\({Y_i(0), Y_i(1)}\\)とは独立である. 無作為割り付けができる場合には\\(T_I\\)は\\({Y_i(0), Y_i(1)}\\)とも無関係となる. 無作為割り付けの場合には独立性の仮定が満たされることから，常識は\\(T_{ATE}\\)に変換できる.\n\\[\nE[Y_i|T_i=1]=E[Y_i(1)|T_i=1]=E[Y_i(1)]\\\\\nE[Y_i|T_i=0]=E[Y_i(0)|T_i=0]=E[Y_i(0)]\n\\]\n上記から無作為割り付けが出来ていれば，処置平均効果の差分を求めることで，処置効果を推定することが可能となることがわかる. つまり，処置の割り付けの有無をいかに結果変数と独立させるのかが重要である.\n\n\n\n条件BとCを与えたときのAの確率が，条件Cだけを与えたときのAの確率と 一致することを条件付き独立性という．これは\\(A \\perp B|C\\)と各．\n\\[\n\\textrm{Pr}(A|B,C)=\\textrm{Pr}(A|C)\n\\]\nこれは「独立ならば条件付き独立だる」とは限らない．\n統計的因果推論の立場から重要な点は，たとえデータ全体で独立でなかったとしても， 共変量に条件付けた場合には独立とみなし得るという点である．\n\n\n\n観察研究において適切な統計的因果推論を行うためには， 処置群と統制群をいかにして比較可能な状態にするのかが重要である． すなわち比較可能な２つの集団を用意することが出来る.\n共変量とは結果偏す\\(Y_i\\)に影響する変数の中で， 処置の影響を受けていない変数である．この式を\n\\[\nY_i(1), Y_i(0) \\perp T_i|X\n\\]\nまた条件付き正確性は次である.\n\\[\n0 &lt; \\textrm{Pr}(T=1|X) &lt; 1\n\\] 共変量を考慮することで，割り付けと結果変数が独立になる， つまり割り付けによる効果を推定することが可能となる. これは共変量が同じ個体では割り付け確率が同じになる，ということを意味している．\n・・・ということは，共変量が同じであるという条件を加えることで， 推定値を求めることが出来るようになる.\n共変量による条件づけで平均処置効果を算出できるとは，次式の関係があることを指す. \\[\nE[Y_i|T_i=1, X]=E[Y_i(1)|T_i=1, X]=E[Y_i(1)|X]\\\\\nE[Y_i|T_i=0, X]=E[Y_i(0)|T_i=0, X]=E[Y_i(0)|X]\n\\]\nここでは無視可能な割り付けによる算出例を見てみる.\n\n\nCode\npath   &lt;- \"./causality/data03.csv\"\ndata03 &lt;- read_csv(\n    path, \n    locale = locale(encoding = \"UTF-8\"),\n    show_col_types = FALSE\n)\nsummary(data03)\n#&gt;        x1              y3              t1           y0t             y1t       \n#&gt;  Min.   :70.00   Min.   :63.00   Min.   :0.0   Min.   :62.00   Min.   :71.00  \n#&gt;  1st Qu.:73.75   1st Qu.:73.75   1st Qu.:0.0   1st Qu.:66.50   1st Qu.:75.50  \n#&gt;  Median :80.00   Median :77.00   Median :0.5   Median :71.00   Median :81.50  \n#&gt;  Mean   :80.00   Mean   :77.25   Mean   :0.5   Mean   :72.20   Mean   :82.00  \n#&gt;  3rd Qu.:86.25   3rd Qu.:82.00   3rd Qu.:1.0   3rd Qu.:78.75   3rd Qu.:88.75  \n#&gt;  Max.   :90.00   Max.   :91.00   Max.   :1.0   Max.   :82.00   Max.   :92.00\n\n\n\n\nCode\nprint(head(data03))\n#&gt; # A tibble: 6 × 5\n#&gt;      x1    y3    t1   y0t   y1t\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    70    74     1    62    74\n#&gt; 2    70    63     0    63    74\n#&gt; 3    70    73     1    62    73\n#&gt; 4    70    71     1    65    71\n#&gt; 5    70    74     1    63    74\n#&gt; 6    75    67     0    67    77\n\n\n\n\nCode\nwith(data03, {\n    mean(y3[t1==1])-mean(y3[t1==0])\n})\n#&gt; [1] 3.3\n\n\n\n\nCode\n# 真値\nwith(data03, {\n    mean(y1t)-mean(y0t)\n})\n#&gt; [1] 9.8\n\n\n上記の数値の違いからわかることは 「無視可能な割り付け」とはナイーブな推定量によって文字どおりに 「割り付けを無視して解析してよい」ということを意味していない．\n\n\n\n\n\nCode\nmodel1 &lt;- lm(y3 ~ x1 + t1, data = data03)\nsummary(model1)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ x1 + t1, data = data03)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1.84950 -0.54042  0.07711  0.38619  1.18781 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -2.12562    2.27423  -0.935    0.363    \n#&gt; x1           0.93085    0.02704  34.422  &lt; 2e-16 ***\n#&gt; t1           9.81592    0.42757  22.957 3.11e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.8573 on 17 degrees of freedom\n#&gt; Multiple R-squared:  0.9867, Adjusted R-squared:  0.9851 \n#&gt; F-statistic: 629.5 on 2 and 17 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nconfint(model1, level = .95)\n#&gt;                  2.5 %     97.5 %\n#&gt; (Intercept) -6.9238192  2.6725754\n#&gt; x1           0.8737921  0.9878995\n#&gt; t1           8.9138219 10.7180189\n\n\n無視可能な割り付けであれば，共変量による条件づけることで解析が行える. グラフィカルモデルで言えば共変量と処置変数",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#sutva",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#sutva",
    "title": "重要な仮定",
    "section": "",
    "text": "Stable Unit Treatment Value Assumption\n処置を受ける個体(unit)ごとに処置の値が安定的であるという仮定である． 具体的には次の二つの条件が成り立つ．\n\n相互干渉がない\n個体に対する隠れた処置がない\n\n相互干渉ある場合には相互干渉がなくなるまでunitの単位を変更する. 隠れた処置がない，とはつまり処置以外の属性がランダム化されているという理解で良さそうである． 補習講義を受けた場合でも先生が異なる場合には，A先生の補習講義を受けた，B先生の補習講義を 受けた，補習講義を受けていないなどとして着目した変数以外をランダム化することに努める．\n確率が相互に背反であるとは，集合に重なりがないことであるあため，次式が成立することを指す.\n\\[\nP(A|B)=0\n\\]\n確率が独立であるとは，ある情報で条件づけても確率が変化しないことである．\n\\[\nP(A|B)=P(A)\n\\]",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#識別性の条件",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#識別性の条件",
    "title": "重要な仮定",
    "section": "",
    "text": "観測されたデータから母数が一意に満たされていない場合， 標本から母集団への推定ができないということになる． このとき識別性がないという． 特に，ここでは識別性=正値性＋独立性である．\n正値性の条件を課すとunitが処置を受けるかうけないかは， わからない状態，つまりどちらも可能性があるため次の式で表現される.\n\\[\n0&lt;\\textrm{Pr}(T=1) &lt;1\n\\]\n独立性とは処置の割り付けが潜在的結果変数に依存して行われてはいけない，ということである． つまりすべての変量と独立になっており，実験研究ではこの条件が満たされやすい.\n\\[\n{Y(1), Y(0)} \\perp T\n\\]\n改めて観測値\\(Y_i\\)は次の式で表される.\n\\[\nY_i = (1-T_i)Y_i(0) + T_iY_i(1)\n\\]\n\\(Y_i(0), Y_i(1)\\)は潜在的結果変数の組であるため， 個体\\(i\\)に対してはどちらか一方しか観測されない. よって次式により処置効果を求めることは通常不可能である.\n\\[\n\\tau_{ATE}=E[Y_i(1)-Y_i(0)]=E[Y_i(1)]-E[Y_i(0)]\n\\]\n一方でそれぞれの処置で条件付けた場合には平均値の算出が可能であるため， 次式で示したナイーブな推定量を使うことが可能である. しかしこの値は本来推定したいものではない．本来推定したいのは，処置の平均処置効果，処置群の平均処置効果のどちらかである．\n\\[\nE[Y_i|T_i=1]-E[Y_i|T_i=0]\n\\]\n処置の割り付けを表す\\(T_i\\)が潜在的結果変数の組に依存していなければ \\(T_i\\)と\\({Y_i(0), Y_i(1)}\\)とは独立である. 無作為割り付けができる場合には\\(T_I\\)は\\({Y_i(0), Y_i(1)}\\)とも無関係となる. 無作為割り付けの場合には独立性の仮定が満たされることから，常識は\\(T_{ATE}\\)に変換できる.\n\\[\nE[Y_i|T_i=1]=E[Y_i(1)|T_i=1]=E[Y_i(1)]\\\\\nE[Y_i|T_i=0]=E[Y_i(0)|T_i=0]=E[Y_i(0)]\n\\]\n上記から無作為割り付けが出来ていれば，処置平均効果の差分を求めることで，処置効果を推定することが可能となることがわかる. つまり，処置の割り付けの有無をいかに結果変数と独立させるのかが重要である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#独立性と条件付き独立性シンプソンのパラドックス",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#独立性と条件付き独立性シンプソンのパラドックス",
    "title": "重要な仮定",
    "section": "",
    "text": "条件BとCを与えたときのAの確率が，条件Cだけを与えたときのAの確率と 一致することを条件付き独立性という．これは\\(A \\perp B|C\\)と各．\n\\[\n\\textrm{Pr}(A|B,C)=\\textrm{Pr}(A|C)\n\\]\nこれは「独立ならば条件付き独立だる」とは限らない．\n統計的因果推論の立場から重要な点は，たとえデータ全体で独立でなかったとしても， 共変量に条件付けた場合には独立とみなし得るという点である．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#共変量の役割",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#共変量の役割",
    "title": "重要な仮定",
    "section": "",
    "text": "観察研究において適切な統計的因果推論を行うためには， 処置群と統制群をいかにして比較可能な状態にするのかが重要である． すなわち比較可能な２つの集団を用意することが出来る.\n共変量とは結果偏す\\(Y_i\\)に影響する変数の中で， 処置の影響を受けていない変数である．この式を\n\\[\nY_i(1), Y_i(0) \\perp T_i|X\n\\]\nまた条件付き正確性は次である.\n\\[\n0 &lt; \\textrm{Pr}(T=1|X) &lt; 1\n\\] 共変量を考慮することで，割り付けと結果変数が独立になる， つまり割り付けによる効果を推定することが可能となる. これは共変量が同じ個体では割り付け確率が同じになる，ということを意味している．\n・・・ということは，共変量が同じであるという条件を加えることで， 推定値を求めることが出来るようになる.\n共変量による条件づけで平均処置効果を算出できるとは，次式の関係があることを指す. \\[\nE[Y_i|T_i=1, X]=E[Y_i(1)|T_i=1, X]=E[Y_i(1)|X]\\\\\nE[Y_i|T_i=0, X]=E[Y_i(0)|T_i=0, X]=E[Y_i(0)|X]\n\\]\nここでは無視可能な割り付けによる算出例を見てみる.\n\n\nCode\npath   &lt;- \"./causality/data03.csv\"\ndata03 &lt;- read_csv(\n    path, \n    locale = locale(encoding = \"UTF-8\"),\n    show_col_types = FALSE\n)\nsummary(data03)\n#&gt;        x1              y3              t1           y0t             y1t       \n#&gt;  Min.   :70.00   Min.   :63.00   Min.   :0.0   Min.   :62.00   Min.   :71.00  \n#&gt;  1st Qu.:73.75   1st Qu.:73.75   1st Qu.:0.0   1st Qu.:66.50   1st Qu.:75.50  \n#&gt;  Median :80.00   Median :77.00   Median :0.5   Median :71.00   Median :81.50  \n#&gt;  Mean   :80.00   Mean   :77.25   Mean   :0.5   Mean   :72.20   Mean   :82.00  \n#&gt;  3rd Qu.:86.25   3rd Qu.:82.00   3rd Qu.:1.0   3rd Qu.:78.75   3rd Qu.:88.75  \n#&gt;  Max.   :90.00   Max.   :91.00   Max.   :1.0   Max.   :82.00   Max.   :92.00\n\n\n\n\nCode\nprint(head(data03))\n#&gt; # A tibble: 6 × 5\n#&gt;      x1    y3    t1   y0t   y1t\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    70    74     1    62    74\n#&gt; 2    70    63     0    63    74\n#&gt; 3    70    73     1    62    73\n#&gt; 4    70    71     1    65    71\n#&gt; 5    70    74     1    63    74\n#&gt; 6    75    67     0    67    77\n\n\n\n\nCode\nwith(data03, {\n    mean(y3[t1==1])-mean(y3[t1==0])\n})\n#&gt; [1] 3.3\n\n\n\n\nCode\n# 真値\nwith(data03, {\n    mean(y1t)-mean(y0t)\n})\n#&gt; [1] 9.8\n\n\n上記の数値の違いからわかることは 「無視可能な割り付け」とはナイーブな推定量によって文字どおりに 「割り付けを無視して解析してよい」ということを意味していない．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#回帰分析と共分散分析",
    "href": "contents/books/05_統計的因果推論の理論と実際/103_統計的因果推論における重要な仮定.html#回帰分析と共分散分析",
    "title": "重要な仮定",
    "section": "",
    "text": "Code\nmodel1 &lt;- lm(y3 ~ x1 + t1, data = data03)\nsummary(model1)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ x1 + t1, data = data03)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1.84950 -0.54042  0.07711  0.38619  1.18781 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) -2.12562    2.27423  -0.935    0.363    \n#&gt; x1           0.93085    0.02704  34.422  &lt; 2e-16 ***\n#&gt; t1           9.81592    0.42757  22.957 3.11e-14 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.8573 on 17 degrees of freedom\n#&gt; Multiple R-squared:  0.9867, Adjusted R-squared:  0.9851 \n#&gt; F-statistic: 629.5 on 2 and 17 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nconfint(model1, level = .95)\n#&gt;                  2.5 %     97.5 %\n#&gt; (Intercept) -6.9238192  2.6725754\n#&gt; x1           0.8737921  0.9878995\n#&gt; t1           8.9138219 10.7180189\n\n\n無視可能な割り付けであれば，共変量による条件づけることで解析が行える. グラフィカルモデルで言えば共変量と処置変数",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重要な仮定"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html",
    "title": "基礎の基礎",
    "section": "",
    "text": "因果関係は，原因と結果の関係である．因果関係は目に見えないため，因果推論とは 因果を推し測って考えるということである． 統計的因果推論とは，そのような因果推論をデータに基づいて行うことである.\n因果関係には「効果をもたらした原因」と「原因がもたらす効果」がある． 前者を判定するにはフィールドワークなどの観察により 事象の背景知識を把握することが必要である． 一方で後者はデータにもとづいて定量的に行われることになる． つまり統計的因果推論では「原因Aが結果Bにもたらす効果」を取り扱うことになる．\nなお統計的因果推論では原因のことを「処置」という．\n\n\n\n本書ではRubinの潜在的結果変数の枠組みで統計的因果推論を行う.\n大切だと思うのは常に交絡因子が存在していることであり， 交絡因子について思慮続けることである. 性差が原因だとして，その性差とは具体的には何を表しているのだろうか？ その性差がなぜ結果Bに影響を与えるのだろうか？\n操作無くして因果なしの考えに基づけば，性差は操作ができないものである． つまりある個人が別の性別，性認識での結果は存在しない． 性差と結果の間になにか中間変数があるはずである． 本署の中では性差が治療結果の違いに影響を与えるのは，処置方針という中間変数が存在しているため，という事例が記載されていた.\n\n\n\nグレンジャー因果は予測に役立つという意味であり， 統計的因果推論とは異なるものであることに注意する．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "基礎の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#統計的因果推論とは",
    "href": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#統計的因果推論とは",
    "title": "基礎の基礎",
    "section": "",
    "text": "因果関係は，原因と結果の関係である．因果関係は目に見えないため，因果推論とは 因果を推し測って考えるということである． 統計的因果推論とは，そのような因果推論をデータに基づいて行うことである.\n因果関係には「効果をもたらした原因」と「原因がもたらす効果」がある． 前者を判定するにはフィールドワークなどの観察により 事象の背景知識を把握することが必要である． 一方で後者はデータにもとづいて定量的に行われることになる． つまり統計的因果推論では「原因Aが結果Bにもたらす効果」を取り扱うことになる．\nなお統計的因果推論では原因のことを「処置」という．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "基礎の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#反事実モデル",
    "href": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#反事実モデル",
    "title": "基礎の基礎",
    "section": "",
    "text": "本書ではRubinの潜在的結果変数の枠組みで統計的因果推論を行う.\n大切だと思うのは常に交絡因子が存在していることであり， 交絡因子について思慮続けることである. 性差が原因だとして，その性差とは具体的には何を表しているのだろうか？ その性差がなぜ結果Bに影響を与えるのだろうか？\n操作無くして因果なしの考えに基づけば，性差は操作ができないものである． つまりある個人が別の性別，性認識での結果は存在しない． 性差と結果の間になにか中間変数があるはずである． 本署の中では性差が治療結果の違いに影響を与えるのは，処置方針という中間変数が存在しているため，という事例が記載されていた.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "基礎の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#グレンジャー因果",
    "href": "contents/books/05_統計的因果推論の理論と実際/101_統計的因果推論の基礎の基礎.html#グレンジャー因果",
    "title": "基礎の基礎",
    "section": "",
    "text": "グレンジャー因果は予測に役立つという意味であり， 統計的因果推論とは異なるものであることに注意する．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "基礎の基礎"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch07_回帰分析とシミュレーション.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch07_回帰分析とシミュレーション.html",
    "title": "ch07 回帰分析",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch07 回帰分析"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch05_統計検定の論理とエラー確率のコントロール - コピー (2).html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch05_統計検定の論理とエラー確率のコントロール - コピー (2).html",
    "title": "ch05 統計検定の論理",
    "section": "",
    "text": "検定はあらかじめ\\(\\alpha\\)と\\(\\beta\\)を設定しておき、 その確率にタイプⅠ、タイプⅡエラー 確率が抑えられるように手続きを設計することが重要である。\n重要なのは低い\\(\\alpha\\)と\\(\\beta\\)を設定することではなく、 推論のエラー確率が定めた\\(\\alpha\\)と\\(\\beta\\)を下回るように、 手続きを設定することである。",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch05 統計検定の論理"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch05_統計検定の論理とエラー確率のコントロール - コピー (2).html#t検定の等分散の仮定からの逸脱",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch05_統計検定の論理とエラー確率のコントロール - コピー (2).html#t検定の等分散の仮定からの逸脱",
    "title": "ch05 統計検定の論理",
    "section": "2.1 \\(t\\)検定の等分散の仮定からの逸脱",
    "text": "2.1 \\(t\\)検定の等分散の仮定からの逸脱\n等分散が満たされないときに、 エラー確率が制御できないことを確認する。\n\n\nCode\nn &lt;- c(40, 20) # サンプルサイズを変えた設定\nsigma1 &lt;- 1\nsigma2 &lt;- c(0.2, 0.5, 0.75, 1, 1.5, 2, 5) # 母標準偏差のパターン\np &lt;- length(sigma2)\niter &lt;- 10000 # シミュレーション回数\nalpha &lt;- 0.05 # 有意水準\nmu &lt;- c(0, 0) # 母平均が等しい設定にする\n# 結果を格納するオブジェクト\npvalue &lt;- array(NA, dim = c(p, iter))\n## シミュレーション\nset.seed(1234)\nfor (i in 1:p) {\n  for (j in 1:iter) {\n    Y1 &lt;- rnorm(n[1], mu[1], sigma1)\n    Y2 &lt;- rnorm(n[2], mu[2], sigma2[i])\n    result &lt;- t.test(Y1, Y2, var.equal = TRUE)\n    pvalue[i, j] &lt;- result$p.value\n  }\n}\n# 結果を格納するオブジェクト\ntype1error_ttest &lt;- rep(0, p)\n\n\nfor (i in 1:p) {\n  type1error_ttest[i] &lt;- mean(pvalue[i, ] &lt; alpha)\n}\n\ntype1error_ttest |&gt; \n  plot(\n    type = \"b\", \n    xaxt = \"n\", \n    ylim = c(0, .2), \n    xlab = \"郡2の母標準偏差\"\n  )\n\naxis(1, at = 1:p, labels = sigma2)\nabline(h = alpha, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\nCode\npvalue[6,] |&gt; hist()\n\n\n\n\n\n\n\n\n\n一方でWeltch検定は等分散を仮定していないので、 制御出来ていることがわかる。\n\n\nCode\nn &lt;- c(40, 20) # サンプルサイズをかえる\nsigma1 &lt;- 1\nsigma2 &lt;- c(.2, .5, .75, 1, 1.5, 2, 5) # 母標準偏差のパターン\np &lt;- length(sigma2)\niter &lt;- 10000\nalpah &lt;- .05\nmu &lt;- c(0, 0)\n\nset.seed(1234)\npvalue &lt;- array(NA, dim = c(p, iter))\nfor (i in 1:p) {\n  for (j in 1:iter) {\n    Y1 &lt;- rnorm(n[1], mu[1], sigma1)\n    Y2 &lt;- rnorm(n[2], mu[2], sigma2[i])\n    result &lt;- t.test(Y1, Y2, var.equal = FALSE)\n    pvalue[i, j] &lt;- result$p.value\n  }\n}\n\ntype1error_welch &lt;- rep(0, p)\nfor (i in 1:p) {\n  type1error_welch[i] &lt;- mean(pvalue[i, ] &lt; alpha)\n}\n\ntype1error_welch |&gt; \n  plot(\n    type = \"b\", \n    xaxt = \"n\", \n    ylim = c(0, .2), \n    xlab = \"群2の母標準偏差\"\n  )\naxis(1, at = 1:p, labels = sigma2)\nabline(h = alpha, lty = 2)",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch05 統計検定の論理"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch03_乱数生成シミュレーションの基礎.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch03_乱数生成シミュレーションの基礎.html",
    "title": "ch03 乱数生成シミュレーションの基礎",
    "section": "",
    "text": "「仮にもっと多くのデータがあったなら正確にわかったかもしれないこと」を 推測するには、データの生成モデルを考える必用がある。 「どのようなエータが観測されいあｙすいかは、確率法則に従う」という仮定を 億ことで検討がおこなえる。 これは仮定であるので、本ッにそうであるかどうかはわからない。\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch03 乱数生成シミュレーションの基礎"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch01_本書の狙い.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch01_本書の狙い.html",
    "title": "ch01 本書のねらい",
    "section": "",
    "text": "1 シミュレーションでわかること\n「相関係数が0.5のときの２変量確率変数をN個観測したときに実際に得られる相関係数」 についてシミュレーションをおこなう。これを観ると、サンプルサイズが２５では、非常にバラツキが大きいことがわかる。\n\n\nCode\nrho &lt;- .5\nn &lt;- 25\niter &lt;- 1000\n\nr &lt;- rep(0, iter)\n\nset.seed(123)\nfor (i in 1:iter) {\n  Y1 &lt;- rnorm(n, 0, 1)\n  Y2 &lt;- Y1 * rho + rnorm(n, 0, (1 - rho^2)^.5)\n  r[i] &lt;- cor(Y1, Y2)\n}\n\n\nhist(r)\n\n\n\n\n\n\n\n\n\nサンプルサイズを増やすと推定値のバラツキが小さくなることがわかる。\n\n\nCode\nrho &lt;- .5\nn &lt;- 100\niter &lt;- 1000\n\nr &lt;- rep(0, iter)\n\nset.seed(123)\nfor (i in 1:iter) {\n  Y1 &lt;- rnorm(n, 0, 1)\n  Y2 &lt;- Y1 * rho + rnorm(n, 0, (1 - rho^2)^.5)\n  r[i] &lt;- cor(Y1, Y2)\n}\n\n\nhist(r)\n\n\n\n\n\n\n\n\n\nさらに検定もおこなう. ここでは無相関検定\\(\\rho = 0\\)をおこなう。 具体的には無相関検定をおこなってp値を求める。求めたp値が、0.05を下回る 回数の割合を求める.\nこの結果を見ると仮定が正しいときに、p値が0.05を下回る確率は5%であることがわかる。\n\n\nCode\nrho &lt;- .0\nn &lt;- 25\niter &lt;- 10000\n\nr &lt;- rep(0, iter)\n\nset.seed(123)\nfor (i in 1:iter) {\n  Y1 &lt;- rnorm(n, 0, 1)\n  Y2 &lt;- Y1 * rho + rnorm(n, 0, (1 - rho^2)^.5)\n  r[i] &lt;- cor.test(Y1, Y2)$p.value\n}\n\nifelse(r &lt;= .05, 1, 0) |&gt; mean()\n#&gt; [1] 0.0488\n\n\n検定が適切におこなわれれば、p値は一様分布にしたがう。\n\n\nCode\nhist(r)\n\n\n\n\n\n\n\n\n\nまた、正しく検定がおこわれればp値はサンプルサイズに依らない。\n\n\nCode\nrho &lt;- .0\nn &lt;- 2500\niter &lt;- 10000\n\nr &lt;- rep(0, iter)\n\nset.seed(123)\nfor (i in 1:iter) {\n  Y1 &lt;- rnorm(n, 0, 1)\n  Y2 &lt;- Y1 * rho + rnorm(n, 0, (1 - rho^2)^.5)\n  r[i] &lt;- cor.test(Y1, Y2)$p.value\n}\n\nifelse(r &lt;= .05, 1, 0) |&gt; mean()\n#&gt; [1] 0.0486\n\n\n誤った尾統計分析としてあるのは、検定において データの検定結果を見てから「も少しで有意になるからサンプルサイズをもう少し大きくしよう」とデータを足すことです。\nこれが問題となることをシミュレーションで確認する。このシミュレーションから、後からデータを追加することは、p値のバイアスを生むことがわかる。\n\n\nCode\n## 設定と準備\nrho &lt;- 0\nn &lt;- 25\niter &lt;- 10000\nalpha &lt;- 0.05\n\n# 結果を格納するオブジェクト\np &lt;- rep(0, iter)\n\n## シミュレーション\nset.seed(123)\nfor (i in 1:iter) {\n  # 最初のデータ\n  Y1 &lt;- rnorm(n, 0, 1)\n  Y2 &lt;- Y1 * rho + rnorm(n, 0, (1 - rho^2)^0.5)\n  p[i] &lt;- cor.test(Y1, Y2)$p.value\n  # データ追加\n  count &lt;- 0\n  ## p値が5％を下回るか、データが当初の倍になるまで増やし続ける\n  while (p[i] &gt;= alpha && count &lt; n * 2) {\n    # 有意ではなかったとき、それぞれの変数に1つデータを追加\n    Y1_add &lt;- rnorm(1, 0, 1)\n    Y1 &lt;- c(Y1, Y1_add)\n    Y2 &lt;- c(Y2, Y1_add * rho + rnorm(1, 0, (1 - rho^2)^0.5))\n    p[i] &lt;- cor.test(Y1, Y2)$p.value\n    count &lt;- count + 1\n  }\n}\n\nifelse(p &lt; .05, 1, 0) |&gt; mean()\n#&gt; [1] 0.1791\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch01 本書のねらい"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch11_結論.html",
    "href": "contents/books/02_因果推論ミックステープ/ch11_結論.html",
    "title": "ch11 結論",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch11 結論"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch09_差分の差デザイン.html",
    "href": "contents/books/02_因果推論ミックステープ/ch09_差分の差デザイン.html",
    "title": "ch09 差分の差デザイン",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch09 差分の差デザイン"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch07_操作変数.html",
    "href": "contents/books/02_因果推論ミックステープ/ch07_操作変数.html",
    "title": "ch07 操作変数",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch07 操作変数"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch05_マッチングと層別化.html",
    "href": "contents/books/02_因果推論ミックステープ/ch05_マッチングと層別化.html",
    "title": "ch05 マッチングと層別化",
    "section": "",
    "text": "Warning\n\n\n\n全然理解出来ていないので、もう一度読み込むこと。 考え方はわかったが、数式の部分でわかっていない。 誤植がありそうなので別の図書を見た方がよいかもしれない。",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch05 マッチングと層別化"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch05_マッチングと層別化.html#識別のための仮定",
    "href": "contents/books/02_因果推論ミックステープ/ch05_マッチングと層別化.html#識別のための仮定",
    "title": "ch05 マッチングと層別化",
    "section": "1.1 識別のための仮定",
    "text": "1.1 識別のための仮定\n交絡因子があるときに因果効果を推定するためには、 CIAが成り立つこと、処置の確率が各層で0より大きく1より小さいこと。",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch05 マッチングと層別化"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html",
    "href": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html",
    "title": "ch03 非巡回的有向グラフ",
    "section": "",
    "text": "DAG記法では因果関係は一方向にしか進まない\nDAGでは需要供給のような同時性を扱うことはできない\n因果関係には2つの生じ方がある\n\n直接働く場合\n第三の変数を媒介する場合\n\nこれをバックドアパスという\n第三の変数が未観測のときバックドアが開いてるという\nバックドアがある状態では因果不明である\nこのような第三因子を交絡因子ともいう\n\n\nDAGはデータが関係あるということを示している\n\n\n\nバックドアとなる変数が観測されているときに、正しく推定ができるのかを確認してみる。次の例は、xがyに直接影響を与える場合と、xがzを経由してyに影響を与えることがわかる。両方とも観測できていれば、それぞれがyに与える影響を正しく推定出来てることがわかる。\n\n\nCode\ndataset &lt;- tibble(\n    x = rnorm(100), \n    z = 2 * x + rnorm(100), \n    y = 1 + x + 3 * z + rnorm(100) \n) \n\nfit &lt;- lm(y ~ x + z, data = dataset)\n\nfit |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x + z, data = dataset)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.3660 -0.4878 -0.1002  0.5022  2.2313 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.86739    0.08772   9.888 2.34e-16 ***\n#&gt; x            0.72964    0.19595   3.724  0.00033 ***\n#&gt; z            3.10085    0.08720  35.560  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.8768 on 97 degrees of freedom\n#&gt; Multiple R-squared:  0.9857, Adjusted R-squared:  0.9854 \n#&gt; F-statistic:  3345 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nxがzを経由してyに影響を与えている分もあるので、zを見込まないとバイアスが入ることがわかる。\n\n\nCode\ndataset &lt;- tibble(\n    x = rnorm(100), \n    z = 2 * x + rnorm(100), \n    y = 1 + x + 3 * z + rnorm(100) \n) \n\nfit &lt;- lm(y ~ x, data = dataset)\n\nfit |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, data = dataset)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -5.8603 -1.9672 -0.1174  1.6625  7.3820 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.4611     0.2786   5.245 9.03e-07 ***\n#&gt; x             6.9687     0.2685  25.950  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.776 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.873,  Adjusted R-squared:  0.8717 \n#&gt; F-statistic: 673.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nバックドアパスが開いているときに、回帰分析をすると、 推定値の符号を反転させるほどの深刻な問題が引き起こされる。 このためデータ分析の際には、バックドアパスを閉じることにする。\nバックドアを閉じる方法は次の２つである。\n\n交絡因子を条件付ける(マッチングなど)\n合流点を作りバックドアを閉じる\n\nすべてのバックドアパスが閉じているときに、そのリサーチデザインはバックドア基準を満たすということになる。\nバックドアの合流点があるときに、その合流点を制御することで、バックドアパスが閉じられることになる。アウトカムとなる変数について、関心がある変数以外の合流点があるときにはその合流点を制御するだけで、バックドア基準を満たすことができる。　\n合流点バイアスの考え方は非常に難しいわ。\n\n\nCode\nlibrary(stargazer)\n\ntb &lt;- tibble(\n    female  = ifelse(runif(10000) &gt; .5, 1, 0), \n    ability = rnorm(10000), \n    discrimination = female, \n    occupation = 1 + 2 * ability + 0 * female - 2 * discrimination  + rnorm(10000), \n    wage       = 1 - 1 * discrimination  + 1 * occupation + 2 * ability + rnorm(10000)\n)\n\nlm_1 &lt;- lm(wage ~ female, tb)\nlm_2 &lt;- lm(wage ~ female + occupation, tb)\nlm_3 &lt;- lm(wage ~ female + occupation + ability, tb)\n\n\n# warningが出るが問題ない\n# chunkで実行する場合にだけwarningが発生してくる\nstargazer(lm_1, lm_2, lm_3, type = \"text\", omit.stat = \"all\", column.labels = c(\"a\", \"b\", \"c\"))\n#&gt; \n#&gt; ========================================\n#&gt;                 Dependent variable:     \n#&gt;            -----------------------------\n#&gt;                        wage             \n#&gt;                a         b         c    \n#&gt;               (1)       (2)       (3)   \n#&gt; ----------------------------------------\n#&gt; female     -3.044***  0.549*** -1.053***\n#&gt;             (0.085)   (0.029)   (0.028) \n#&gt;                                         \n#&gt; occupation            1.795*** 0.989*** \n#&gt;                       (0.006)   (0.010) \n#&gt;                                         \n#&gt; ability                        2.024*** \n#&gt;                                 (0.022) \n#&gt;                                         \n#&gt; Constant    2.008***  0.213*** 1.016*** \n#&gt;             (0.059)   (0.020)   (0.017) \n#&gt;                                         \n#&gt; ========================================\n#&gt; ========================================\n#&gt; Note:        *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nDAGにおいて合流点をコントロールすると、つまり今回の場合には職業をコントロールすると、 差別と賃金の間に、関係全体を歪めるほど強力活誤ったバックドアパスが 開いてしまう。職業と能力をコントロールした場合にのみ、 賃金に対するジェンダーの直接的な因果効果を分離することができるのである。\nバックドアとDAGのノート",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch03 非巡回的有向グラフ"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html#シミュレーション1",
    "href": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html#シミュレーション1",
    "title": "ch03 非巡回的有向グラフ",
    "section": "",
    "text": "バックドアとなる変数が観測されているときに、正しく推定ができるのかを確認してみる。次の例は、xがyに直接影響を与える場合と、xがzを経由してyに影響を与えることがわかる。両方とも観測できていれば、それぞれがyに与える影響を正しく推定出来てることがわかる。\n\n\nCode\ndataset &lt;- tibble(\n    x = rnorm(100), \n    z = 2 * x + rnorm(100), \n    y = 1 + x + 3 * z + rnorm(100) \n) \n\nfit &lt;- lm(y ~ x + z, data = dataset)\n\nfit |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x + z, data = dataset)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -2.3660 -0.4878 -0.1002  0.5022  2.2313 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.86739    0.08772   9.888 2.34e-16 ***\n#&gt; x            0.72964    0.19595   3.724  0.00033 ***\n#&gt; z            3.10085    0.08720  35.560  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.8768 on 97 degrees of freedom\n#&gt; Multiple R-squared:  0.9857, Adjusted R-squared:  0.9854 \n#&gt; F-statistic:  3345 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nxがzを経由してyに影響を与えている分もあるので、zを見込まないとバイアスが入ることがわかる。\n\n\nCode\ndataset &lt;- tibble(\n    x = rnorm(100), \n    z = 2 * x + rnorm(100), \n    y = 1 + x + 3 * z + rnorm(100) \n) \n\nfit &lt;- lm(y ~ x, data = dataset)\n\nfit |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, data = dataset)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -5.8603 -1.9672 -0.1174  1.6625  7.3820 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   1.4611     0.2786   5.245 9.03e-07 ***\n#&gt; x             6.9687     0.2685  25.950  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.776 on 98 degrees of freedom\n#&gt; Multiple R-squared:  0.873,  Adjusted R-squared:  0.8717 \n#&gt; F-statistic: 673.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch03 非巡回的有向グラフ"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html#バックドア基準",
    "href": "contents/books/02_因果推論ミックステープ/ch03_非巡回的有向グラフ.html#バックドア基準",
    "title": "ch03 非巡回的有向グラフ",
    "section": "",
    "text": "バックドアパスが開いているときに、回帰分析をすると、 推定値の符号を反転させるほどの深刻な問題が引き起こされる。 このためデータ分析の際には、バックドアパスを閉じることにする。\nバックドアを閉じる方法は次の２つである。\n\n交絡因子を条件付ける(マッチングなど)\n合流点を作りバックドアを閉じる\n\nすべてのバックドアパスが閉じているときに、そのリサーチデザインはバックドア基準を満たすということになる。\nバックドアの合流点があるときに、その合流点を制御することで、バックドアパスが閉じられることになる。アウトカムとなる変数について、関心がある変数以外の合流点があるときにはその合流点を制御するだけで、バックドア基準を満たすことができる。　\n合流点バイアスの考え方は非常に難しいわ。\n\n\nCode\nlibrary(stargazer)\n\ntb &lt;- tibble(\n    female  = ifelse(runif(10000) &gt; .5, 1, 0), \n    ability = rnorm(10000), \n    discrimination = female, \n    occupation = 1 + 2 * ability + 0 * female - 2 * discrimination  + rnorm(10000), \n    wage       = 1 - 1 * discrimination  + 1 * occupation + 2 * ability + rnorm(10000)\n)\n\nlm_1 &lt;- lm(wage ~ female, tb)\nlm_2 &lt;- lm(wage ~ female + occupation, tb)\nlm_3 &lt;- lm(wage ~ female + occupation + ability, tb)\n\n\n# warningが出るが問題ない\n# chunkで実行する場合にだけwarningが発生してくる\nstargazer(lm_1, lm_2, lm_3, type = \"text\", omit.stat = \"all\", column.labels = c(\"a\", \"b\", \"c\"))\n#&gt; \n#&gt; ========================================\n#&gt;                 Dependent variable:     \n#&gt;            -----------------------------\n#&gt;                        wage             \n#&gt;                a         b         c    \n#&gt;               (1)       (2)       (3)   \n#&gt; ----------------------------------------\n#&gt; female     -3.044***  0.549*** -1.053***\n#&gt;             (0.085)   (0.029)   (0.028) \n#&gt;                                         \n#&gt; occupation            1.795*** 0.989*** \n#&gt;                       (0.006)   (0.010) \n#&gt;                                         \n#&gt; ability                        2.024*** \n#&gt;                                 (0.022) \n#&gt;                                         \n#&gt; Constant    2.008***  0.213*** 1.016*** \n#&gt;             (0.059)   (0.020)   (0.017) \n#&gt;                                         \n#&gt; ========================================\n#&gt; ========================================\n#&gt; Note:        *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01\n\n\nDAGにおいて合流点をコントロールすると、つまり今回の場合には職業をコントロールすると、 差別と賃金の間に、関係全体を歪めるほど強力活誤ったバックドアパスが 開いてしまう。職業と能力をコントロールした場合にのみ、 賃金に対するジェンダーの直接的な因果効果を分離することができるのである。\nバックドアとDAGのノート",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch03 非巡回的有向グラフ"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch01_導入.html",
    "href": "contents/books/02_因果推論ミックステープ/ch01_導入.html",
    "title": "ch01 導入",
    "section": "",
    "text": "1 はじめに\n\n本書は因果推論のプログラム的な部分を補間する内容である\n扱っている内容\n\n潜在アウトカムモデル\n実験デザイン\nマッチング\n操作変数法\n回帰不連続デザイン\nパネルデータ\nDAG：非巡回的有向グラフ\n\n扱っていない内容\n\n合成コントロール\nグラフィカルモデル\n\n\n\n\n2 最適化はすべてを内生化する\n\n\n\n\n\n\nCaution\n\n\n\n「因果を発見した」というあらゆる主張には、その正当化のために事前の知識が必要不可欠である。\n\n\n\n\n\n\n\n\nCaution\n\n\n\n信頼できる、価値のある研究をするためには、特定の意図した結果を求めるのではなく、 方法論に則って正しく研究をおこなうことが、より重要でえある。\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch01 導入"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch12_パネルデータ.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch12_パネルデータ.html",
    "title": "ch12 パネルデータ",
    "section": "",
    "text": "1 Setup\n\n\n2 固定効果モデル\n1997年から2019年の47都道府県別の失業率と自殺死亡率のデータを使う.\n\npref: 都道府県\nyear: 年度\nsuicide: 自殺率\nunemp: 完全失業率\n\n上記のデータに対して、失業などの経済的な不安定が心身の 疲弊を招き、最悪の場合には自殺に至るという考える。\n\n\nCode\npath_to_file &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap12/prefecture.csv\")\nprefdata &lt;- read_csv(path_to_file, show_col_types = FALSE)\nprefdata |&gt; \n    head() |&gt; \n    paged_table()\n\n\n\n  \n\n\n\n\n\nCode\nprefdata |&gt; \n    lm(suicide ~ -1 + unemp + pref, data = _) |&gt; \n    tidy() |&gt; \n    select(term, estimate) |&gt; \n    filter(term == \"unemp\")\n#&gt; # A tibble: 1 × 2\n#&gt;   term  estimate\n#&gt;   &lt;chr&gt;    &lt;dbl&gt;\n#&gt; 1 unemp     3.07\n\n\n上記の結果は失業率が１％上昇すると、１０万人あたりの自殺者が３人増えるという結果である。\n次に、被説明変数と説明変数からそれぞれの都道府県別平均を引いた変数を作成して、 within推定を実施する。\n\n\nCode\nprefdata |&gt; \n    group_by(pref) |&gt; \n    mutate(\n        suicidebar = mean(suicide), \n        unempbar = mean(unemp), \n        suicide2 = suicide - suicidebar, \n        unemp2 = unemp - unempbar\n    ) |&gt; \n    lm(suicide2 ~ -1 + unemp2, data = _) |&gt; \n    tidy() |&gt; \n    select(term, estimate)\n#&gt; # A tibble: 1 × 2\n#&gt;   term   estimate\n#&gt;   &lt;chr&gt;     &lt;dbl&gt;\n#&gt; 1 unemp2     3.07\n\n\n上記の結果は、都道府県について固定したモデルである。 ここでさらに、時間について固定した、２方向固定効果モデルについて検討する。\n\n\nCode\nlibrary(fixest)\nprefdata |&gt; \n    feols(suicide ~ unemp | pref + year)\n#&gt; OLS estimation, Dep. Var.: suicide\n#&gt; Observations: 1,081 \n#&gt; Fixed-effects: pref: 47,  year: 23\n#&gt; Standard-errors: Clustered (pref) \n#&gt;       Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; unemp 0.770868   0.298724 2.58054 0.013121 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.76097     Adj. R2: 0.8622  \n#&gt;                 Within R2: 0.022751\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch12 パネルデータ"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch10_回帰不連続デザイン.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch10_回帰不連続デザイン.html",
    "title": "ch10 回帰不連続デザイン",
    "section": "",
    "text": "1 Setup\n\n\n2 はじめに\n「テストの点数が60未満を対象に強制した補講」の学習に与える効果を考える。 ランダムなトリートメントではないためこれまでの比較は困難であるが、 60点前後の人達については実際にはランダムに近い状態にある。 ここではその情報を使って処理を行う。\nもう少し詳しくいうとある変数\\(Z_i\\)がわかればトリートメントの状態が 判断できるというデータセットについて考える。\n「ぎりぎり補講にならなかった学生」と「ぎりぎり補講になった学生」の比較なら、 補講の効果を判断しやすいはずという考えである。\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch10 回帰不連続デザイン"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch08_ランダム化実験.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch08_ランダム化実験.html",
    "title": "ch08 ランダム化実験",
    "section": "",
    "text": "1 Setup\n\n\n2 はじめに\nまず本当に独立な説明変数であったら、その変数が含まれているのかどうか係数に影響しないことを確認する。 下記から変数にバイアスが生じていないことが確認できる。\n\n\nCode\nn  &lt;- 100\nbeta0 &lt;- 0.5\nbeta1 &lt;- 1\nbeta2 &lt;- 2\n\nresult &lt;- replicate(300, {\n    x1 &lt;- rnorm(n, sd = .8)\n    x2 &lt;- rnorm(n, sd = 1.5) + 3\n    y &lt;- beta0 + beta1 * x1 + beta2 * x2 + rnorm(n)\n    coef(lm(y ~ x1))[2]\n})\n\nhist(result)\n\n\n\n\n\n\n\n\n\n\n\nCode\nsummary(result)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt; -0.2883  0.7491  1.0277  1.0248  1.3080  2.6383\n\n\nただし、定数項は別なので注意すること。\n\n\nCode\nn  &lt;- 100\nbeta0 &lt;- 0.5\nbeta1 &lt;- 1\nbeta2 &lt;- 2\n\nresult &lt;- replicate(300, {\n    x1 &lt;- rnorm(n, sd = .8)\n    x2 &lt;- rnorm(n, sd = 1.5) + 3\n    y &lt;- beta0 + beta1 * x1 + beta2 * x2 + rnorm(n)\n    coef(lm(y ~ x1))[1]\n})\n\nhist(result)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch08 ランダム化実験"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch06_相関関係と因果関係.html#疑似相関",
    "href": "contents/books/01_Rによる実証分析_2e/ch06_相関関係と因果関係.html#疑似相関",
    "title": "ch06 相関関係と因果関係",
    "section": "2.1 疑似相関",
    "text": "2.1 疑似相関\n下記のグラフを見ると、ゲーム時間が少ないほど、得点が高いという負の相関が見られる。\nただし本当にこれを因果関係とみるのかは別の問題である。なぜなら、家庭環境などの交絡情報が含まれている可能性が高いためである。 また、実際には点そのものではなく、勉強時間などへの影響をみるべきである。そうでないと、 ゲームの時間を減らしただけで点があがるのかということになる。\nつまり、家庭環境や勉強時間を条件付けた上で、ゲームの時間がテストの点に与える影響を見る必要がある。\n\n\nCode\npath &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap06/video_game.csv\")\nvideodata &lt;- read_csv(path, show_col_types = FALSE)\nvideodata\n#&gt; # A tibble: 500 × 2\n#&gt;    grade hours\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1    81   0  \n#&gt;  2    47   2.4\n#&gt;  3    59   2  \n#&gt;  4    47   2.2\n#&gt;  5    22   2.3\n#&gt;  6    49   1.7\n#&gt;  7    78   0  \n#&gt;  8    59   0  \n#&gt;  9    42   2.2\n#&gt; 10    43   1.6\n#&gt; # ℹ 490 more rows\n\n\n\n\nCode\nvideodata |&gt; \n    ggplot(aes(hours, grade)) + \n    geom_point()",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch06 相関関係と因果関係"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch06_相関関係と因果関係.html#同時性",
    "href": "contents/books/01_Rによる実証分析_2e/ch06_相関関係と因果関係.html#同時性",
    "title": "ch06 相関関係と因果関係",
    "section": "2.2 同時性",
    "text": "2.2 同時性\n同時性とは「２つの事柄について、お互いがお互いの原因であり同時に結果である」という状態である。 需要と供給で言えば、ニーズが高まれば多く供給されるが、多く供給されることでニーズが減るという、関係性にある。",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch06 相関関係と因果関係"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch04_回帰分析の基礎.html#気温と電力使用量",
    "href": "contents/books/01_Rによる実証分析_2e/ch04_回帰分析の基礎.html#気温と電力使用量",
    "title": "ch04 回帰分析の基礎",
    "section": "2.1 気温と電力使用量",
    "text": "2.1 気温と電力使用量\n\n\nCode\npath &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap03/temperature.csv\")\ntempdata  &lt;- \n    read_csv(path, show_col_types = FALSE) |&gt; \n    filter(between(strftime(date), strftime(\"2014/8/1\"), strftime(\"2014/8/31\")))\nhead(tempdata)\n#&gt; # A tibble: 6 × 5\n#&gt;   date     time    elec  prec  temp\n#&gt;   &lt;chr&gt;    &lt;time&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 2014/8/1 00:00   3193     0  27.9\n#&gt; 2 2014/8/1 01:00   2960     0  27.9\n#&gt; 3 2014/8/1 02:00   2807     0  27.1\n#&gt; 4 2014/8/1 03:00   2748     0  26.8\n#&gt; 5 2014/8/1 04:00   2735     0  26.9\n#&gt; 6 2014/8/1 05:00   2736     0  27.3\n\n\n\n\nCode\nsummaries &lt;- tempdata |&gt; skim() |&gt; partition()\n\nmap(summaries, paged_table)\n#&gt; $character\n#&gt;   skim_variable n_missing complete_rate min max empty n_unique whitespace\n#&gt; 1          date         0             1   8   9     0       31          0\n#&gt; \n#&gt; $difftime\n#&gt;   skim_variable n_missing complete_rate    min        max     median n_unique\n#&gt; 1          time         0             1 0 secs 82800 secs 41400 secs       24\n#&gt; \n#&gt; $numeric\n#&gt;   skim_variable n_missing complete_rate        mean          sd     p0      p25\n#&gt; 1          elec         0             1 3398.486559 714.3446713 2213.0 2823.500\n#&gt; 2          prec         0             1    0.141129   0.9451167    0.0    0.000\n#&gt; 3          temp         0             1   27.668414   3.5454533   19.8   25.675\n#&gt;      p50      p75   p100  hist\n#&gt; 1 3342.5 3871.500 4980.0 ▆▇▇▃▃\n#&gt; 2    0.0    0.000   17.0 ▇▁▁▁▁\n#&gt; 3   28.0   30.125   35.5 ▃▂▇▅▂\n\n\nまずはデータをプロットしてみる。\n\n\nCode\ntempdata |&gt; \n    filter(between(strftime(date), strftime(\"2014/8/1\"), strftime(\"2014/8/31\"))) |&gt; \n    ggplot(aes(x = time, y = temp)) +\n    geom_point() + \n    xlab(\"時刻\") + \n    ylab(\"気温\") + \n    ggtitle(\"2014年8月における時刻と気温の関係\")\n\n\n\n\n\n\n\n\n\nノンパラメトリック回帰として時刻ごとの平均値を求める。\n\n\nCode\ntempdata |&gt; \n    group_by(time) |&gt; \n    summarise(across(c(temp), .fns = list(mean = mean, sd = sd, max = max, min = min)))\n#&gt; # A tibble: 24 × 5\n#&gt;    time   temp_mean temp_sd temp_max temp_min\n#&gt;    &lt;time&gt;     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 00:00       26.4    2.75     29.3     20.1\n#&gt;  2 01:00       26.1    2.73     29.3     19.8\n#&gt;  3 02:00       25.9    2.69     28.7     19.8\n#&gt;  4 03:00       25.8    2.61     28.3     20.2\n#&gt;  5 04:00       25.7    2.67     28.5     19.9\n#&gt;  6 05:00       25.8    2.76     28.6     20.1\n#&gt;  7 06:00       26.4    3.10     29.8     19.9\n#&gt;  8 07:00       27.3    3.41     31.1     20.2\n#&gt;  9 08:00       28.1    3.54     32.1     20.5\n#&gt; 10 09:00       29.0    3.80     33.5     20.9\n#&gt; # ℹ 14 more rows\n\n\n\n\nCode\ntempdata |&gt; \n    filter(between(strftime(date), strftime(\"2014/8/1\"), strftime(\"2014/8/31\"))) |&gt; \n    ggplot(aes(x = time, y = temp)) + \n    geom_point() + \n    stat_summary(geom = \"line\", fun = \"mean\") + \n    xlab(\"時間\") + \n    ylab(\"気温\") + \n    coord_cartesian()\n\n\n\n\n\n\n\n\n\n相関係数を見てみると、電力と気温には強い正の関係があることがわかる。\n\n\nCode\nlibrary(correlation)\ncorrelation(tempdata)\n#&gt; # Correlation Matrix (pearson-method)\n#&gt; \n#&gt; Parameter1 | Parameter2 |     r |         95% CI | t(742) |         p\n#&gt; ---------------------------------------------------------------------\n#&gt; elec       |       prec | -0.08 | [-0.15, -0.01] |  -2.20 | 0.028*   \n#&gt; elec       |       temp |  0.72 | [ 0.68,  0.75] |  28.25 | &lt; .001***\n#&gt; prec       |       temp | -0.16 | [-0.23, -0.09] |  -4.31 | &lt; .001***\n#&gt; \n#&gt; p-value adjustment method: Holm (1979)\n#&gt; Observations: 744\n\n\n\n\nCode\ntempdata |&gt; \n    ggplot(aes(temp, elec)) + \n    geom_point()\n\n\n\n\n\n\n\n\n\nノンパラメトリック回帰として、気温ごとの平均値を算出する。\n\n\nCode\ntempdata |&gt; \n    transmute(temp = round(temp), elec, time = hour(time)) |&gt; \n    ggplot(aes(x = temp, y = elec)) +\n    geom_point(aes(color = time)) + \n    stat_summary(geom= \"line\", fun = \"mean\", color = \"red\") + \n    theme(panel.grid.minor = element_blank()) + \n    scale_color_viridis_b(\n        breaks = c(0, 6, 12, 18, 24), \n        limits = c(0, 24)\n    )",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch04 回帰分析の基礎"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch02_統計の基礎知識.html#section",
    "href": "contents/books/01_Rによる実証分析_2e/ch02_統計の基礎知識.html#section",
    "title": "ch02 統計の基礎知識",
    "section": "3.1 2.1",
    "text": "3.1 2.1\n\n\nCode\nx &lt;- runif(100)\nprint(mean(x))\n#&gt; [1] 0.4884829\nprint(var(x))\n#&gt; [1] 0.08050623\nprint(sd(x))\n#&gt; [1] 0.2837362\n\n\n\n\nCode\nparams &lt;- tibble(\n    n_sample =  c(1:1000), \n    n_simulate = 1000\n)\nresult &lt;- \n    params |&gt; \n    mutate(res = map2(n_sample, n_simulate, \\(x, y) replicate(y, mean(runif(x))))) |&gt; \n    mutate(mean = map_dbl(res, mean), sd = map_dbl(res, sd))\n\nresult |&gt; \n    ggplot(aes(x = n_sample, y = mean)) + \n    geom_line() + \n    scale_x_log10() + \n    scale_y_continuous(\n        breaks = 0:10 * .1, \n        limits = c(0, 1)) + \n    geom_ribbon(\n        aes(ymin = mean - sd, ymax = mean + sd), fill = \"pink\", alpha = .5) + \n    theme(\n        panel.grid.minor = element_blank()\n    )",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch02 統計の基礎知識"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch02_統計の基礎知識.html#section-1",
    "href": "contents/books/01_Rによる実証分析_2e/ch02_統計の基礎知識.html#section-1",
    "title": "ch02 統計の基礎知識",
    "section": "3.2 2.2",
    "text": "3.2 2.2\n\n\nCode\nlibrary(correlation)\n#&gt; Warning: パッケージ 'correlation' はバージョン 4.3.3 の R の下で造られました\n\ndf &lt;- tibble(\n    x = runif(100), \n    y = rnorm(100, 0, 1), \n    z = 1.3 * x - .7 * y\n)\n\nuv &lt;- \n    combn(syms(c(\"x\", \"y\", \"z\")), 2, simplify = FALSE)\n\ngraphs &lt;-\n    uv |&gt; \n    map(\\(x) ggplot(df, aes(!!x[[1]], !!x[[2]])) + geom_point()) |&gt; \n    list_modify(ncol = 1)\n\ndo.call(grid.arrange, graphs)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf |&gt; \n    correlation()\n#&gt; # Correlation Matrix (pearson-method)\n#&gt; \n#&gt; Parameter1 | Parameter2 |     r |         95% CI |  t(98) |         p\n#&gt; ---------------------------------------------------------------------\n#&gt; x          |          y |  0.07 | [-0.12,  0.27] |   0.73 | 0.464    \n#&gt; x          |          z |  0.43 | [ 0.26,  0.58] |   4.74 | &lt; .001***\n#&gt; y          |          z | -0.87 | [-0.91, -0.81] | -17.26 | &lt; .001***\n#&gt; \n#&gt; p-value adjustment method: Holm (1979)\n#&gt; Observations: 100\n\n\n標本サイズと相関係数の関係を確認する。\n\n\nCode\nlibrary(correlation)\n\nparams &lt;- tibble(\n    n_sample =  c(10:1000), \n    n_simulate = 100\n)\nresult &lt;- \n    params |&gt; \n    mutate(res1 = map2(\n        n_sample, \n        n_simulate, \n        \\(x, y) replicate(\n            y, {\n                xx = runif(x)\n                yy = rnorm(x, 0, 1)\n                zz = 1.3 * xx - .7 * yy\n                c(\"xy\" = cor(xx, yy), \"yz\" = cor(yy, zz), \"zx\" = cor(zz, xx))\n            }))) |&gt; \n    mutate(res1 = map(res1, \\(x) as_tibble(t(x)))) |&gt; \n    mutate(res1 = map(res1, \\(x) summarise(x, across(everything(), .fns = list(mean = mean, sd = sd))))) |&gt; \n    unnest(res1)\n\n\n\n\nCode\ncols &lt;- c(\"xy\", \"yz\", \"zx\")\ngraphs &lt;- \n    map(cols, \\(x) {\n        u &lt;- sym(glue::glue(\"{x}_mean\"))\n        v &lt;- sym(glue::glue(\"{x}_sd\"))\n        result |&gt; \n            ggplot(aes(x = n_sample, y = !!u)) + \n            geom_path() + \n            geom_ribbon(\n                aes(ymin = !!u - !!v, ymax = !!u + !!v), fill = \"pink\", alpha = .5)\n    }) |&gt; \n    list_modify(ncol = 1)\n\ndo.call(grid.arrange, graphs)",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch02 統計の基礎知識"
    ]
  },
  {
    "objectID": "contents/libs/Python/or-tools/tutorial.html",
    "href": "contents/libs/Python/or-tools/tutorial.html",
    "title": "チュートリアル",
    "section": "",
    "text": "1 線形計画問題\n\n\nCode\nfrom ortools.linear_solver import pywraplp\n\n# ソルバーの作成\nsolver = pywraplp.Solver.CreateSolver('GLOP')  # GLOP = LP用ソルバー\n\n# 変数の定義\nx = solver.NumVar(0, 1, 'x')\ny = solver.NumVar(0, 2, 'y')\n\n# 制約式: x + y ≤ 2\nsolver.Add(x + y &lt;= 2)\n\n# 目的関数: x + y の最大化\nsolver.Maximize(x + y)\n\nstatus = solver.Solve()\n\nif status == pywraplp.Solver.OPTIMAL:\n    print('解:', x.solution_value(), y.solution_value())\n    print('目的関数値:', solver.Objective().Value())\nelse:\n    print('最適解が見つかりませんでした。')\n\n\n解: 0.0 2.0\n目的関数値: 2.0\n\n\n\n\n2 ナーススケジューリング問題\n\n\nCode\nfrom ortools.sat.python import cp_model\n\n# パラメータ\nnum_nurses = 3\nnum_days = 3\nnum_shifts = 3  # 0: 早番, 1: 遅番, 2: 夜勤\n\nmodel = cp_model.CpModel()\n\n# 変数定義: shifts[n][d][s] → ナースnが日dのシフトsを担当するか（0 or 1）\nshifts = {}\nfor n in range(num_nurses):\n    for d in range(num_days):\n        for s in range(num_shifts):\n            shifts[n, d, s] = model.NewBoolVar(f'shift_n{n}_d{d}_s{s}')\n\n# 制約1: 各シフトにはちょうど1人のナース\nfor d in range(num_days):\n    for s in range(num_shifts):\n        model.AddExactlyOne(shifts[n, d, s] for n in range(num_nurses))\n\n# 制約2: 各ナースは1日に1シフトまで\nfor n in range(num_nurses):\n    for d in range(num_days):\n        model.AddAtMostOne(shifts[n, d, s] for s in range(num_shifts))\n\n# ソルバー\nsolver = cp_model.CpSolver()\nstatus = solver.Solve(model)\n\n# 結果表示\nif status == cp_model.OPTIMAL or status == cp_model.FEASIBLE:\n    shift_name = ['早番', '遅番', '夜勤']\n    for d in range(num_days):\n        print(f'\\n【{d+1}日目】')\n        for s in range(num_shifts):\n            for n in range(num_nurses):\n                if solver.Value(shifts[n, d, s]):\n                    print(f'  {shift_name[s]} → ナース{n+1}')\nelse:\n    print('解が見つかりませんでした。')\n\n\n\n【1日目】\n  早番 → ナース2\n  遅番 → ナース1\n  夜勤 → ナース3\n\n【2日目】\n  早番 → ナース2\n  遅番 → ナース1\n  夜勤 → ナース3\n\n【3日目】\n  早番 → ナース2\n  遅番 → ナース1\n  夜勤 → ナース3\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/libs/R/tidyverse/index.html",
    "href": "contents/libs/R/tidyverse/index.html",
    "title": "はじめに",
    "section": "",
    "text": "1 はじめに\ntidyverseはメタパッケージです。\n個別のパッケージについての解説HPもありますが、全体の開発動向については、公式サイトのブログが１番の情報源です。\nブログはライブラリーでフィルタすることができます。\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "tidyverse",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/tidyverse/01_blog.html",
    "href": "contents/libs/R/tidyverse/01_blog.html",
    "title": "01 Blog",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/tidyverse\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggfittext\",\n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "tidyverse",
      "01 Blog"
    ]
  },
  {
    "objectID": "contents/libs/R/tidyverse/01_blog.html#dplyr-1.1.0-pick-reframe-and-arrange",
    "href": "contents/libs/R/tidyverse/01_blog.html#dplyr-1.1.0-pick-reframe-and-arrange",
    "title": "01 Blog",
    "section": "1.1 dplyr 1.1.0: pick, reframe, and arrange",
    "text": "1.1 dplyr 1.1.0: pick, reframe, and arrange\n\n1.1.1 pick\n\n\nCode\ndf &lt;- tibble(\n  x_1 = c(1, 3, 2, 1, 2), \n  x_2 = 6:10, \n  w_4 = 11:15, \n  y_2 = c(5, 2, 4, 0, 6)\n)\n\ndf |&gt;\n  summarise(\n    n_x = ncol(across(starts_with(\"x\"))),\n    n_y = ncol(across(starts_with(\"y\")))\n  )\n#&gt; # A tibble: 1 × 2\n#&gt;     n_x   n_y\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     2     1\n\n\nacrossは関数の適用対象となカラムに関心があるので名前が少し不自然である。 pickはカラムを選択していることが自然にわかるものである。\n\n\nCode\ndf |&gt; \n  summarise(\n    n_x = ncol(pick(starts_with(\"x\"))), \n    n_y = ncol(pick(starts_with(\"y\")))\n  )\n#&gt; # A tibble: 1 × 2\n#&gt;     n_x   n_y\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     2     1\n\n\npickはたとえばdense_rankで複数のカラムを使うときに有効である。\n\n\nCode\ndf |&gt; \n  mutate(\n    rank1 = dense_rank(x_1), \n    rank2 = dense_rank(pick(x_1, y_2))\n  )\n#&gt; # A tibble: 5 × 6\n#&gt;     x_1   x_2   w_4   y_2 rank1 rank2\n#&gt;   &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1     1     6    11     5     1     2\n#&gt; 2     3     7    12     2     3     5\n#&gt; 3     2     8    13     4     2     3\n#&gt; 4     1     9    14     0     1     1\n#&gt; 5     2    10    15     6     2     4\n\n\n.fnsを使わないacrossを廃止することはまだないが、今後はpickが代替手段となる.\n\n\n1.1.2 reframe\n集約関数を適用したときに複数行を返すことができるのは有用であると認識されているが、 summariseでおこなうのは適当でないと考えられる。\n\n\nCode\ndf &lt;- tibble(\n  g = c(1, 1, 1, 2, 2),\n  x = c(4, 3, 6, 2, 8),\n  y = c(5, 1, 2, 8, 9)\n)\n\ndf\n#&gt; # A tibble: 5 × 3\n#&gt;       g     x     y\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     4     5\n#&gt; 2     1     3     1\n#&gt; 3     1     6     2\n#&gt; 4     2     2     8\n#&gt; 5     2     8     9\n\n\nsd(x)とするところが、xとしてしまって値がリサイクルされている。 この場合、summariseで集約していないので、警告が発生する。\n\n\nCode\ndf |&gt;\n  summarise(\n    x_average = mean(x),\n    x_sd = x, # Oops\n    .by = g\n  )\n#&gt; Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\n#&gt; dplyr 1.1.0.\n#&gt; ℹ Please use `reframe()` instead.\n#&gt; ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n#&gt;   always returns an ungrouped data frame and adjust accordingly.\n#&gt; # A tibble: 5 × 3\n#&gt;       g x_average  x_sd\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1      4.33     4\n#&gt; 2     1      4.33     3\n#&gt; 3     1      4.33     6\n#&gt; 4     2      5        2\n#&gt; 5     2      5        8\n#&gt; Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\n#&gt; dplyr 1.1.0.\n#&gt; ℹ Please use `reframe()` instead.\n#&gt; ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n#&gt;   always returns an ungrouped data frame and adjust accordingly.\n#&gt; # A tibble: 5 × 3\n#&gt;       g x_average  x_sd\n#&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1      4.33     4\n#&gt; 2     1      4.33     3\n#&gt; 3     1      4.33     6\n#&gt; 4     2      5        2\n#&gt; 5     2      5        8\n\n\nそこで、reframeを使う。また、enframe、deframeも使う。\n\nenframe: Takes a vector, returns a data frame\ndeframe: Takes a data frame, returns a vector\nreframe: Takes a data frame, returns a data frame\n\n次の例では、グループ化した状態で、カラムxを引数としてquantile_dfが実行されている。 group_by -&gt; map -&gt; bind_rowsの流れを非常に簡潔に記述できていることがわかる。\n\n\nCode\nquantile_df &lt;- function(x, probs = c(0.25, 0.5, 0.75)) {\n  tibble(\n    value = quantile(x, probs, na.rm = TRUE),\n    prob = probs\n  )\n}\ndf |&gt;\n  reframe(quantile_df(x), .by = g)\n#&gt; # A tibble: 6 × 3\n#&gt;       g value  prob\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1   3.5  0.25\n#&gt; 2     1   4    0.5 \n#&gt; 3     1   5    0.75\n#&gt; 4     2   3.5  0.25\n#&gt; 5     2   5    0.5 \n#&gt; 6     2   6.5  0.75\n#&gt; # A tibble: 6 × 3\n#&gt;       g value  prob\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1   3.5  0.25\n#&gt; 2     1   4    0.5 \n#&gt; 3     1   5    0.75\n#&gt; 4     2   3.5  0.25\n#&gt; 5     2   5    0.5 \n#&gt; 6     2   6.5  0.75\n\n\nacrossと組み合わせることで処理を拡張、つまり複数の列に対して処理がおこなえるようになる。\nこれはつまり、横持ち状態のデータフレームに対しても処理が簡潔におこなえるということになる。\n\n\nCode\ndf |&gt;\n  reframe(\n    across(x:y, quantile_df), \n    .by = g\n  )\n#&gt; # A tibble: 6 × 3\n#&gt;       g x$value $prob y$value $prob\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     3.5  0.25    1.5   0.25\n#&gt; 2     1     4    0.5     2     0.5 \n#&gt; 3     1     5    0.75    3.5   0.75\n#&gt; 4     2     3.5  0.25    8.25  0.25\n#&gt; 5     2     5    0.5     8.5   0.5 \n#&gt; 6     2     6.5  0.75    8.75  0.75\n\n\nデータをunpackすることも可能である。\n\n\nCode\ndf |&gt; \n  reframe(\n    across(x:y, quantile_df, .unpack = TRUE), \n    .by = g\n  )\n#&gt; # A tibble: 6 × 5\n#&gt;       g x_value x_prob y_value y_prob\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1     1     3.5   0.25    1.5    0.25\n#&gt; 2     1     4     0.5     2      0.5 \n#&gt; 3     1     5     0.75    3.5    0.75\n#&gt; 4     2     3.5   0.25    8.25   0.25\n#&gt; 5     2     5     0.5     8.5    0.5 \n#&gt; 6     2     6.5   0.75    8.75   0.75\n\n\nenframeについても試してみる。 stack, unstackと同じ処理のようにみえる。\n\n\nCode\nenframe(1:3)\n#&gt; # A tibble: 3 × 2\n#&gt;    name value\n#&gt;   &lt;int&gt; &lt;int&gt;\n#&gt; 1     1     1\n#&gt; 2     2     2\n#&gt; 3     3     3\nenframe(c(a = 5, b = 7))\n#&gt; # A tibble: 2 × 2\n#&gt;   name  value\n#&gt;   &lt;chr&gt; &lt;dbl&gt;\n#&gt; 1 a         5\n#&gt; 2 b         7\nenframe(list(one = 1, two = 2:3, three = 4:6))\n#&gt; # A tibble: 3 × 2\n#&gt;   name  value    \n#&gt;   &lt;chr&gt; &lt;list&gt;   \n#&gt; 1 one   &lt;dbl [1]&gt;\n#&gt; 2 two   &lt;int [2]&gt;\n#&gt; 3 three &lt;int [3]&gt;\ndeframe(enframe(3:1))\n#&gt; 1 2 3 \n#&gt; 3 2 1\ndeframe(tibble(a = 1:3))\n#&gt; [1] 1 2 3\ndeframe(tibble(a = as.list(1:3)))\n#&gt; [[1]]\n#&gt; [1] 1\n#&gt; \n#&gt; [[2]]\n#&gt; [1] 2\n#&gt; \n#&gt; [[3]]\n#&gt; [1] 3\n\n\n\n\n1.1.3 arrange\nパス",
    "crumbs": [
      "R",
      "tidyverse",
      "01 Blog"
    ]
  },
  {
    "objectID": "contents/libs/R/purrr/index.html",
    "href": "contents/libs/R/purrr/index.html",
    "title": "purrr",
    "section": "",
    "text": "1 はじめに\n\nリストのデータハンドリングであるpurrrパッケージを使う\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/libs/R/pointblank/index.html",
    "href": "contents/libs/R/pointblank/index.html",
    "title": "pointblank",
    "section": "",
    "text": "データのバリデーションができるponintblankパッケージを使う\nレポートを作成することも可能である\n公式サイト",
    "crumbs": [
      "R",
      "pointblank",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/pointblank/index.html#agent-ベースで処理",
    "href": "contents/libs/R/pointblank/index.html#agent-ベースで処理",
    "title": "pointblank",
    "section": "2.1 agent ベースで処理",
    "text": "2.1 agent ベースで処理\n\n\nCode\nlibrary(pointblank)\n#&gt; Warning: パッケージ 'pointblank' はバージョン 4.5.1 の R の下で造られました\n\n# Generate a simple `action_levels` object to\n# set the `warn` state if a validation step\n# has a single 'fail' test unit\nal &lt;- action_levels(warn_at = 1)\n\n# Create a pointblank `agent` object, with the\n# tibble as the target table. Use three validation\n# functions, then, `interrogate()`. The agent will\n# then have some useful intel.\nagent &lt;- \n  dplyr::tibble(\n    a = c(5, 7, 6, 5, NA, 7),\n    b = c(6, 1, 0, 6,  0, 7)\n  ) %&gt;%\n  create_agent(\n    label = \"A very *simple* example.\",\n    actions = al\n  ) %&gt;%\n  col_vals_between(\n    columns = a,\n    left = 1,\n    right = 9,\n    na_pass = TRUE\n  ) %&gt;%\n  col_vals_lt(\n    columns = c, 12,\n    preconditions = ~ . %&gt;% dplyr::mutate(c = a + b)\n  ) %&gt;%\n  col_is_numeric(columns = c(a, b)) %&gt;%\n  interrogate()",
    "crumbs": [
      "R",
      "pointblank",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/pointblank/index.html#direct",
    "href": "contents/libs/R/pointblank/index.html#direct",
    "title": "pointblank",
    "section": "2.2 direct",
    "text": "2.2 direct\n\n\nCode\ntryCatch({\n  dplyr::tibble(\n    a = c(5, 7, 6, 5, NA, 7),\n    b = c(6, 1, 0, 6,  0, 7)\n  ) %&gt;%\n  col_vals_between(\n    columns = a,\n    left = 1,\n    right = 9,\n    na_pass = TRUE\n  ) %&gt;%\n  col_vals_lt(\n    columns = c,\n    value = 12,\n    preconditions = ~ . %&gt;% dplyr::mutate(c = a + b)\n  ) %&gt;%\n  col_is_numeric(columns = c(a, b))\n}, error = \\(e) print(e))\n#&gt; &lt;simpleError: Exceedance of failed test units where values in `c` should have been &lt; `12`.\n#&gt; The `col_vals_lt()` validation failed beyond the absolute threshold level (1).\n#&gt; * failure level (2) &gt;= failure threshold (1)&gt;\n\n\n\nエラーではなくワーニングにすることもできる\n\n\n\nCode\n# The `warn_on_fail()` function is a nice\n# shortcut for `action_levels(warn_at = 1)`;\n# it works great in this data checking workflow\n# (and the threshold can still be adjusted)\ndplyr::tibble(\n    a = c(5, 7, 6, 5, NA, 7),\n    b = c(6, 1, 0, 6,  0, 7)\n  ) %&gt;%\n  col_vals_between(\n    columns = a,\n    left = 1,\n    right = 9,\n    na_pass = TRUE,\n    actions = warn_on_fail()\n  ) %&gt;%\n  col_vals_lt(\n    columns = c,\n    value = 12,\n    preconditions = ~ . %&gt;% dplyr::mutate(c = a + b),\n    actions = warn_on_fail()\n  ) %&gt;%\n  col_is_numeric(\n    columns = c(a, b),\n    actions = warn_on_fail()\n  )\n#&gt; Warning: Exceedance of failed test units where values in `c` should have been &lt; `12`.\n#&gt; The `col_vals_lt()` validation failed beyond the absolute threshold level (1).\n#&gt; * failure level (2) &gt;= failure threshold (1)\n#&gt; # A tibble: 6 × 2\n#&gt;       a     b\n#&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     5     6\n#&gt; 2     7     1\n#&gt; 3     6     0\n#&gt; 4     5     6\n#&gt; 5    NA     0\n#&gt; 6     7     7",
    "crumbs": [
      "R",
      "pointblank",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/pointblank/index.html#table-information",
    "href": "contents/libs/R/pointblank/index.html#table-information",
    "title": "pointblank",
    "section": "2.3 table information",
    "text": "2.3 table information\n\n\nCode\n\n# Create a pointblank `informant` object, with the\n# tibble as the target table. Use a few information\n# functions and end with `incorporate()`. The informant\n# will then show you information about the tibble.\ninformant &lt;- \n  dplyr::tibble(\n    a = c(5, 7, 6, 5, NA, 7),\n    b = c(6, 1, 0, 6,  0, 7)\n  ) %&gt;%\n  create_informant(\n    label = \"A very *simple* example.\",\n    tbl_name = \"example_tbl\"\n  ) %&gt;%\n  info_tabular(\n    description = \"This two-column table is nothing all that\n    interesting, but, it's fine for examples on **GitHub**\n    `README` pages. Column names are `a` and `b`. ((Cool stuff))\"\n  ) %&gt;%\n  info_columns(\n    columns = a,\n    info = \"This column has an `NA` value. [[Watch out!]]&lt;&lt;color: red;&gt;&gt;\"\n  ) %&gt;%\n  info_columns(\n    columns = a,\n    info = \"Mean value is `{a_mean}`.\"\n  ) %&gt;%\n  info_columns(\n    columns = b,\n    info = \"Like column `a`. The lowest value is `{b_lowest}`.\"\n  ) %&gt;%\n  info_columns(\n    columns = b,\n    info = \"The highest value is `{b_highest}`.\"\n  ) %&gt;%\n  info_snippet(\n    snippet_name = \"a_mean\",\n    fn = ~ . %&gt;% .$a %&gt;% mean(na.rm = TRUE) %&gt;% round(2)\n  ) %&gt;%\n  info_snippet(snippet_name = \"b_lowest\", fn = snip_lowest(\"b\")) %&gt;%\n  info_snippet(snippet_name = \"b_highest\", fn = snip_highest(\"b\")) %&gt;%\n  info_section(\n    section_name = \"further information\", \n    `examples and documentation` = \"Examples for how to use the\n    `info_*()` functions (and many more) are available at the\n    [**pointblank** site](https://rstudio.github.io/pointblank/).\"\n  ) %&gt;%\n  incorporate()",
    "crumbs": [
      "R",
      "pointblank",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/pointblank/index.html#reportの作成",
    "href": "contents/libs/R/pointblank/index.html#reportの作成",
    "title": "pointblank",
    "section": "2.4 reportの作成",
    "text": "2.4 reportの作成\n\n記述統計,　変数ごとの集計,　相関関係など色々な情報が得られる\n数値型には便利のような気がするが、文字列や因子型は集計してくれない？\n\n\n\nCode\n\niris |&gt; \n  scan_data()\n\n\n\n\n\n        \n    \n    \n  \n\n\n\n\n\n\nOverview of iris\n\n\n\n\n\nOverview\n\n\nReproducibility\n\n\n\n\n\nTable Overview\n\n\n\n\n\n\nColumns\n5\n\n\nRows\n150\n\n\nNAs\n0\n\n\nDuplicate Rows\n1 (0.67%)\n\n\n\n\n\n\n\nColumn Types\n\n\n\n\n\n\nnumeric\n4\n\n\nfactor\n1\n\n\n\n\n\n\n\n\n\nReproducibility Information\n\n\n\n\n\n\n\n\n\n\nScan Build Time\n2025-07-26 17:43:50\n\n\npointblank Version\n0.12.2\n\n\nR Version\nR version 4.5.0 (2025–04–11 ucrt)How About a Twenty–Six\n\n\nOperating System\nx86_64-w64-mingw32\n\n\n\n\n\n\n\n\n\n\n\n\nVariables\n\n\n\n\n\n\n\n\nSepal.Length\n\n\nnumeric\n\n\n\n\n\n\n\n\n\n\n\nDistinct\n35\n\n\nNAs\n0\n\n\nInf/-Inf\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean\n5.84\n\n\nMinimum\n4.3\n\n\nMaximum\n7.9\n\n\n\n\n\n\n\nToggle details\n\n\n\n\n\nStatistics\n\n\nCommon Values\n\n\nMax/Min Slices\n\n\n\n\n\nQuantile Statistics\n\n\n\n\n\n\nMinimum\n4.30\n\n\n5th Percentile\n4.60\n\n\nQ1\n5.10\n\n\nMedian\n5.80\n\n\nQ3\n6.40\n\n\n95th Percentile\n7.25\n\n\nMaximum\n7.90\n\n\nRange\n3.60\n\n\nIQR\n1.30\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\n\n\n\n\nMean\n5.84\n\n\nVariance\n0.69\n\n\nStandard Deviation\n0.83\n\n\nCoefficient of Variation\n0.14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n5\n10\n6.7%\n\n\n5.1\n9\n6.0%\n\n\n6.3\n9\n6.0%\n\n\n5.7\n8\n5.3%\n\n\n6.7\n8\n5.3%\n\n\n5.5\n7\n4.7%\n\n\n5.8\n7\n4.7%\n\n\n6.4\n7\n4.7%\n\n\n4.9\n6\n4.0%\n\n\nOther Values (73)\n73\n48.7%\n\n\n\n\n\n\n\n\n\nMaximum Values\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n5.0\n10\n6.67%\n\n\n5.1\n9\n6.00%\n\n\n6.3\n9\n6.00%\n\n\n5.7\n8\n5.33%\n\n\n6.7\n8\n5.33%\n\n\n5.5\n7\n4.67%\n\n\n5.8\n7\n4.67%\n\n\n6.4\n7\n4.67%\n\n\n4.9\n6\n4.00%\n\n\n5.4\n6\n4.00%\n\n\n\n\n\n\n\nMinimum Values\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n4.3\n1\n0.67%\n\n\n4.5\n1\n0.67%\n\n\n5.3\n1\n0.67%\n\n\n7.0\n1\n0.67%\n\n\n7.1\n1\n0.67%\n\n\n7.3\n1\n0.67%\n\n\n7.4\n1\n0.67%\n\n\n7.6\n1\n0.67%\n\n\n7.9\n1\n0.67%\n\n\n4.7\n2\n1.33%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSepal.Width\n\n\nnumeric\n\n\n\n\n\n\n\n\n\n\n\nDistinct\n23\n\n\nNAs\n0\n\n\nInf/-Inf\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean\n3.06\n\n\nMinimum\n2\n\n\nMaximum\n4.4\n\n\n\n\n\n\n\nToggle details\n\n\n\n\n\nStatistics\n\n\nCommon Values\n\n\nMax/Min Slices\n\n\n\n\n\nQuantile Statistics\n\n\n\n\n\n\nMinimum\n2.00\n\n\n5th Percentile\n2.34\n\n\nQ1\n2.80\n\n\nMedian\n3.00\n\n\nQ3\n3.30\n\n\n95th Percentile\n3.80\n\n\nMaximum\n4.40\n\n\nRange\n2.40\n\n\nIQR\n0.50\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\n\n\n\n\nMean\n3.06\n\n\nVariance\n0.19\n\n\nStandard Deviation\n0.44\n\n\nCoefficient of Variation\n0.14\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n3\n26\n17.3%\n\n\n2.8\n14\n9.3%\n\n\n3.2\n13\n8.7%\n\n\n3.4\n12\n8.0%\n\n\n3.1\n11\n7.3%\n\n\n2.9\n10\n6.7%\n\n\n2.7\n9\n6.0%\n\n\n2.5\n8\n5.3%\n\n\n3.3\n6\n4.0%\n\n\nOther Values (35)\n35\n23.3%\n\n\n\n\n\n\n\n\n\nMaximum Values\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n3.0\n26\n17.33%\n\n\n2.8\n14\n9.33%\n\n\n3.2\n13\n8.67%\n\n\n3.4\n12\n8.00%\n\n\n3.1\n11\n7.33%\n\n\n2.9\n10\n6.67%\n\n\n2.7\n9\n6.00%\n\n\n2.5\n8\n5.33%\n\n\n3.3\n6\n4.00%\n\n\n3.5\n6\n4.00%\n\n\n\n\n\n\n\nMinimum Values\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n2.0\n1\n0.67%\n\n\n4.0\n1\n0.67%\n\n\n4.1\n1\n0.67%\n\n\n4.2\n1\n0.67%\n\n\n4.4\n1\n0.67%\n\n\n3.9\n2\n1.33%\n\n\n2.2\n3\n2.00%\n\n\n2.4\n3\n2.00%\n\n\n3.7\n3\n2.00%\n\n\n2.3\n4\n2.67%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPetal.Length\n\n\nnumeric\n\n\n\n\n\n\n\n\n\n\n\nDistinct\n43\n\n\nNAs\n0\n\n\nInf/-Inf\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean\n3.76\n\n\nMinimum\n1\n\n\nMaximum\n6.9\n\n\n\n\n\n\n\nToggle details\n\n\n\n\n\nStatistics\n\n\nCommon Values\n\n\nMax/Min Slices\n\n\n\n\n\nQuantile Statistics\n\n\n\n\n\n\nMinimum\n1.00\n\n\n5th Percentile\n1.30\n\n\nQ1\n1.60\n\n\nMedian\n4.35\n\n\nQ3\n5.10\n\n\n95th Percentile\n6.10\n\n\nMaximum\n6.90\n\n\nRange\n5.90\n\n\nIQR\n3.50\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\n\n\n\n\nMean\n3.76\n\n\nVariance\n3.12\n\n\nStandard Deviation\n1.77\n\n\nCoefficient of Variation\n0.47\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n1.4\n13\n8.7%\n\n\n1.5\n13\n8.7%\n\n\n4.5\n8\n5.3%\n\n\n5.1\n8\n5.3%\n\n\n1.3\n7\n4.7%\n\n\n1.6\n7\n4.7%\n\n\n5.6\n6\n4.0%\n\n\n4\n5\n3.3%\n\n\n4.7\n5\n3.3%\n\n\nOther Values (73)\n73\n48.7%\n\n\n\n\n\n\n\n\n\nMaximum Values\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n1.4\n13\n8.67%\n\n\n1.5\n13\n8.67%\n\n\n4.5\n8\n5.33%\n\n\n5.1\n8\n5.33%\n\n\n1.3\n7\n4.67%\n\n\n1.6\n7\n4.67%\n\n\n5.6\n6\n4.00%\n\n\n4.0\n5\n3.33%\n\n\n4.7\n5\n3.33%\n\n\n4.9\n5\n3.33%\n\n\n\n\n\n\n\nMinimum Values\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n1.0\n1\n0.67%\n\n\n1.1\n1\n0.67%\n\n\n3.0\n1\n0.67%\n\n\n3.6\n1\n0.67%\n\n\n3.7\n1\n0.67%\n\n\n3.8\n1\n0.67%\n\n\n6.3\n1\n0.67%\n\n\n6.4\n1\n0.67%\n\n\n6.6\n1\n0.67%\n\n\n6.9\n1\n0.67%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPetal.Width\n\n\nnumeric\n\n\n\n\n\n\n\n\n\n\n\nDistinct\n22\n\n\nNAs\n0\n\n\nInf/-Inf\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean\n1.2\n\n\nMinimum\n0.1\n\n\nMaximum\n2.5\n\n\n\n\n\n\n\nToggle details\n\n\n\n\n\nStatistics\n\n\nCommon Values\n\n\nMax/Min Slices\n\n\n\n\n\nQuantile Statistics\n\n\n\n\n\n\nMinimum\n0.10\n\n\n5th Percentile\n0.20\n\n\nQ1\n0.30\n\n\nMedian\n1.30\n\n\nQ3\n1.80\n\n\n95th Percentile\n2.30\n\n\nMaximum\n2.50\n\n\nRange\n2.40\n\n\nIQR\n1.50\n\n\n\n\n\n\n\nDescriptive Statistics\n\n\n\n\n\n\nMean\n1.20\n\n\nVariance\n0.58\n\n\nStandard Deviation\n0.76\n\n\nCoefficient of Variation\n0.64\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n0.2\n29\n19.3%\n\n\n1.3\n13\n8.7%\n\n\n1.5\n12\n8.0%\n\n\n1.8\n12\n8.0%\n\n\n1.4\n8\n5.3%\n\n\n2.3\n8\n5.3%\n\n\n0.3\n7\n4.7%\n\n\n0.4\n7\n4.7%\n\n\n1\n7\n4.7%\n\n\nOther Values (41)\n41\n27.3%\n\n\n\n\n\n\n\n\n\nMaximum Values\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n0.2\n29\n19.33%\n\n\n1.3\n13\n8.67%\n\n\n1.5\n12\n8.00%\n\n\n1.8\n12\n8.00%\n\n\n1.4\n8\n5.33%\n\n\n2.3\n8\n5.33%\n\n\n0.3\n7\n4.67%\n\n\n0.4\n7\n4.67%\n\n\n1.0\n7\n4.67%\n\n\n2.0\n6\n4.00%\n\n\n\n\n\n\n\nMinimum Values\n\n\n\n\n\n\nValue\nCount\nFrequency\n\n\n\n\n0.5\n1\n0.67%\n\n\n0.6\n1\n0.67%\n\n\n1.7\n2\n1.33%\n\n\n1.1\n3\n2.00%\n\n\n2.2\n3\n2.00%\n\n\n2.4\n3\n2.00%\n\n\n2.5\n3\n2.00%\n\n\n1.6\n4\n2.67%\n\n\n0.1\n5\n3.33%\n\n\n1.2\n5\n3.33%\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpecies\n\n\nfactor\n\n\n\n\n\n\n\n\n\n\n\nDistinct\n3\n\n\nNAs\n0\n\n\nInf/-Inf\n0\n\n\n\n\n\n\n \n\n\n\n\n\n\nInteractions\n\n\n\n\n\n\n\n\n\n\nCorrelations\n\n\n\n\n\nPearson\n\n\nKendall\n\n\nSpearman\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMissing Values\n\n\n\n\n\n\n\n\n\n\nSample\n\n\n\n\n\n\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n1\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n2\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n3\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n6..145\n\n\n\n\n\n\n\n146\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n147\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n148\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n149\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n150\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n&gt;",
    "crumbs": [
      "R",
      "pointblank",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/index.html",
    "href": "contents/libs/R/plotly/index.html",
    "title": "はじめに",
    "section": "",
    "text": "1 はじめに\nplotlyに関するノートです。quartoはplotlyの出力をサポートしているため, インタラクティブなグラフを作成することが出来ます。 ただしRStudioでないとエディター外で作成されてしまいます。\n\nPlotly R Open Source Graphing Library\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "plotly",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/01_fundamentals.html",
    "href": "contents/libs/R/plotly/01_fundamentals.html",
    "title": "01 Fundamentals",
    "section": "",
    "text": "#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.0\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function\n\n\n\nCode\nplot(iris)\n\n\n\n\n\n\n\n\n\n\n1 The Figure Data Structure in R\n\n\nCode\nlibrary(plotly) \n\nfig &lt;- plot_ly() %&gt;% \n  add_lines(x = c(\"a\",\"b\",\"c\"), y = c(1,3,2))%&gt;% \n  layout(\n    title=\"sample figure\", \n    xaxis = list(title = 'x'), \n    yaxis = list(title = 'y'), \n    plot_bgcolor = \"#c7daec\"\n  ) \n\nfig\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "plotly",
      "01 Fundamentals"
    ]
  },
  {
    "objectID": "contents/libs/R/legendry/index.html",
    "href": "contents/libs/R/legendry/index.html",
    "title": "legendry",
    "section": "",
    "text": "Code\nlibrary(legendry)\nlibrary(tidyverse)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 はじめに\nlegendryでは凡例や軸スタイルを調整することが可能となる\n\n\n2 Example\n\n\nCode\nlibrary(legendry)\n\nbase &lt;- ggplot(mpg, aes(displ, hwy, colour = cty)) +\n  geom_point() +\n  labs(\n    x = \"Engine displacement\",\n    y = \"Highway miles per gallon\",\n    col = \"City miles\\nper gallon\"\n  ) +\n  theme(axis.line = element_line())\n\n\n\n\nCode\n# A partial guide to display a bracket\nefficient_bracket &lt;- primitive_bracket(\n  # Keys determine what is displayed\n  key = key_range_manual(start = 25, end = Inf, name = \"Efficient\"),\n  bracket = \"square\",\n  # We want vertical text\n  theme = theme(\n    legend.text = element_text(angle = 90, hjust = 0.5),\n    axis.text.y.left = element_text(angle = 90, hjust = 0.5)\n  )\n)\n\nbase + guides(y = guide_axis_stack(\"axis\", efficient_bracket))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "legendry",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/igraph/r_interface.html",
    "href": "contents/libs/R/igraph/r_interface.html",
    "title": "R インターフェイス",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/igraph\")\nCode\nbox::use(\n  igraph[..., V, E],\n  ggplot2[...],\n  cowplot[...], \n  showtext[showtext_auto], \n  sysfonts[font_add_google],\n)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "igraph",
      "R インターフェイス"
    ]
  },
  {
    "objectID": "contents/libs/R/igraph/r_interface.html#selecting-vertices",
    "href": "contents/libs/R/igraph/r_interface.html#selecting-vertices",
    "title": "R インターフェイス",
    "section": "7.1 Selecting vertices",
    "text": "7.1 Selecting vertices\n\n\nCode\n# 最も辺が多い頂点\nwhich.max(degree(g))\n#&gt; Claire \n#&gt;      3\n\n\n1を除いた奇数個のエッジを持つ頂点を探す.\n\n\nCode\ngraph &lt;- graph.full(10)\nonly_odd_vertices &lt;- which(V(graph) %% 2 == 1)\nlength(only_odd_vertices)\n#&gt; [1] 5\n\n\n属性を使いフィルターすることも可能である。\n\n\nCode\nV(g)[age &lt; 30]\n#&gt; + 4/7 vertices, named, from 220d792:\n#&gt; [1] Alice  Claire Frank  Esther",
    "crumbs": [
      "R",
      "igraph",
      "R インターフェイス"
    ]
  },
  {
    "objectID": "contents/libs/R/igraph/r_interface.html#selecting-edges",
    "href": "contents/libs/R/igraph/r_interface.html#selecting-edges",
    "title": "R インターフェイス",
    "section": "7.2 Selecting edges",
    "text": "7.2 Selecting edges\n.toや.fromを使うことで、エッジのフィルターが可能となる。\n\n\nCode\nE(g)[.from(\"Alice\")]\n#&gt; + 3/9 edges from 220d792 (vertex names):\n#&gt; [1] Alice--Bob    Alice--Claire Alice--Frank\n\n\n\n\nCode\nE(g)[.from(3)]\n#&gt; + 4/9 edges from 220d792 (vertex names):\n#&gt; [1] Alice --Claire Claire--Frank  Claire--Dennis Claire--Esther\n\n\n%--%オペレータを使うことでエッジを頂点グループで指定することができる。 頂点グループの指定方法はcでもいいし、色々と使うことができる。\n\n\nCode\nE(g)[c(1, 2) %--% V(g)]\n#&gt; + 3/9 edges from 220d792 (vertex names):\n#&gt; [1] Alice--Bob    Alice--Claire Alice--Frank\n\n\n%--%はname属性でも動かすことができる.\n\n\nCode\nV(g)$gender &lt;-  c(\"f\", \"m\", \"f\", \"m\", \"m\", \"f\", \"m\")\nmen &lt;- V(g)[gender == \"m\"]$name\nwomen &lt;- V(g)[gender == \"f\"]$name\nE(g)[men %--% women]\n#&gt; + 5/9 edges from 220d792 (vertex names):\n#&gt; [1] Alice --Bob    Alice --Frank  Claire--Frank  Claire--Dennis Dennis--Esther",
    "crumbs": [
      "R",
      "igraph",
      "R インターフェイス"
    ]
  },
  {
    "objectID": "contents/libs/R/igraph/r_interface.html#write_graphread_graph",
    "href": "contents/libs/R/igraph/r_interface.html#write_graphread_graph",
    "title": "R インターフェイス",
    "section": "10.1 write_graph/read_graph",
    "text": "10.1 write_graph/read_graph\n\n\nCode\ng &lt;- make_ring(10)\nfile &lt;- tempfile(fileext = \".txt\")\nwrite_graph(g, file, \"edgelist\")\n\n\nplot(g)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot(read_graph(file, \"edgelist\", directed = FALSE))",
    "crumbs": [
      "R",
      "igraph",
      "R インターフェイス"
    ]
  },
  {
    "objectID": "contents/libs/R/gt/index.html",
    "href": "contents/libs/R/gt/index.html",
    "title": "gt",
    "section": "",
    "text": "1 はじめに\n\n表を体裁を整えられるgtパッケージを使う\n公式サイト\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/libs/R/ggtext/working.html",
    "href": "contents/libs/R/ggtext/working.html",
    "title": "ggtext",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/ggtext\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "ggtext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggtext/working.html#markdown-in-theme-elements",
    "href": "contents/libs/R/ggtext/working.html#markdown-in-theme-elements",
    "title": "ggtext",
    "section": "2.1 Markdown in theme elements",
    "text": "2.1 Markdown in theme elements\nthemeでelementを指定することで、html要素を使うことが可能となる。 たとえば&lt;br&gt;で改行したり、iタグでCSSを当てたりすることができる.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(glue)\n\ndata &lt;- tibble(\n  bactname = c(\"Staphylococcaceae\", \"Moraxella\", \"Streptococcus\", \"Acinetobacter\"),\n  OTUname = c(\"OTU 1\", \"OTU 2\", \"OTU 3\", \"OTU 4\"),\n  value = c(-0.5, 0.5, 2, 3)\n)\n\ndata %&gt;% mutate(\n  color = c(\"#009E73\", \"#D55E00\", \"#0072B2\", \"#000000\"),\n  name = glue(\"&lt;i style='color:{color}'&gt;{bactname}&lt;/i&gt; ({OTUname})\"),\n  name = fct_reorder(name, value)\n)  %&gt;%\n  ggplot(aes(value, name, fill = color)) + \n  geom_col(alpha = 0.5) + \n  scale_fill_identity() +\n  labs(caption = \"Example posted on **stackoverflow.com**&lt;br&gt;(using made-up data)\") +\n  theme(\n    axis.text.y = element_markdown(),\n    plot.caption = element_markdown(lineheight = 1.2)\n  )\n\n\n\n\n\n\n\n\n\nimgタグを使うことも可能である。\n\n\nCode\nlabels &lt;- c(\n  setosa = \"&lt;img src='unnamed-chunk-6-1.png'\n    width='100' /&gt;&lt;br&gt;*I. setosa*\",\n  virginica = \"&lt;img src='unnamed-chunk-4-1.png'\n    width='100' /&gt;&lt;br&gt;*I. virginica*\",\n  versicolor = \"&lt;img src='unnamed-chunk-5-1.png'\n    width='100' /&gt;&lt;br&gt;*I. versicolor*\"\n)\n\nggplot(iris, aes(Species, Sepal.Width)) +\n  geom_boxplot() +\n  scale_x_discrete(\n    name = NULL,\n    labels = labels\n  ) +\n  theme(\n    axis.text.x = element_markdown(color = \"black\", size = 11)\n  )\n\n\n\n\n\n\n\n\n\ngeom_textbox_simpleなどでは、直接マークダウン要素を使うことが可能である。\n\n\nCode\nggplot(mtcars, aes(disp, mpg)) + \n  geom_point() +\n  labs(\n    title = \"&lt;b&gt;Fuel economy vs. engine displacement&lt;/b&gt;&lt;br&gt;\n    &lt;span style = 'font-size:10pt'&gt;Lorem ipsum *dolor sit amet,*\n    consectetur adipiscing elit, **sed do eiusmod tempor incididunt** ut\n    labore et dolore magna aliqua. &lt;span style = 'color:red;'&gt;Ut enim\n    ad minim veniam,&lt;/span&gt; quis nostrud exercitation ullamco laboris nisi\n    ut aliquip ex ea commodo consequat.&lt;/span&gt;\",\n    x = \"displacement (in&lt;sup&gt;3&lt;/sup&gt;)\",\n    y = \"Miles per gallon (mpg)&lt;br&gt;&lt;span style = 'font-size:8pt'&gt;A measure of\n    the car's fuel efficiency.&lt;/span&gt;\"\n  ) +\n  theme(\n    plot.title.position = \"plot\",\n    plot.title = element_textbox_simple(\n      size = 13,\n      lineheight = 1,\n      padding = margin(5.5, 5.5, 5.5, 5.5),\n      margin = margin(0, 0, 5.5, 0),\n      fill = \"cornsilk\"\n    ),\n    axis.title.x = element_textbox_simple(\n      width = NULL,\n      padding = margin(4, 4, 4, 4),\n      margin = margin(4, 0, 0, 0),\n      linetype = 1,\n      r = grid::unit(8, \"pt\"),\n      fill = \"azure1\"\n    ),\n    axis.title.y = element_textbox_simple(\n      hjust = 0,\n      orientation = \"left-rotated\",\n      minwidth = unit(1, \"in\"),\n      maxwidth = unit(2, \"in\"),\n      padding = margin(4, 4, 2, 4),\n      margin = margin(0, 0, 2, 0),\n      fill = \"lightsteelblue1\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nfacetのラベルにも使うことが可能である。\n\n\nCode\n\nlibrary(cowplot)\n\nggplot(mpg, aes(cty, hwy)) + \n  geom_point() +\n  facet_wrap(~class) +\n  theme_half_open(12) +\n  background_grid() +\n  theme(\n    strip.background = element_blank(),\n    strip.text = element_textbox(\n      size = 12,\n      color = \"white\", fill = \"#5D729D\", box.color = \"#4A618C\",\n      halign = 0.5, linetype = 1, r = unit(5, \"pt\"), width = unit(1, \"npc\"),\n      padding = margin(2, 0, 1, 0), margin = margin(3, 3, 3, 3)\n    )\n  )\n\n\n\n\n\n\n\n\n\nCode\n#&gt; Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n#&gt; Please use the `linewidth` argument instead.\n#&gt; Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n#&gt; Please use the `linewidth` argument instead.",
    "crumbs": [
      "R",
      "ggtext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggtext/working.html#geoms",
    "href": "contents/libs/R/ggtext/working.html#geoms",
    "title": "ggtext",
    "section": "2.2 Geoms",
    "text": "2.2 Geoms\ngeom_richtextを使うとテキストボックスの加工が色々と可能となる。\n\n\nCode\ndf &lt;- tibble(\n  label = c(\n    \"Some text **in bold.**\",\n    \"Linebreaks&lt;br&gt;Linebreaks&lt;br&gt;Linebreaks\",\n    \"*x*&lt;sup&gt;2&lt;/sup&gt; + 5*x* + *C*&lt;sub&gt;*i*&lt;/sub&gt;\",\n    \"Some &lt;span style='color:blue'&gt;blue text **in bold.**&lt;/span&gt;&lt;br&gt;And *italics text.*&lt;br&gt;\n    And some &lt;span style='font-size:18pt; color:black'&gt;large&lt;/span&gt; text.\"\n  ),\n  x = c(.2, .1, .5, .9),\n  y = c(.8, .4, .1, .5),\n  hjust = c(0.5, 0, 0, 1),\n  vjust = c(0.5, 1, 0, 0.5),\n  angle = c(0, 0, 45, -45),\n  color = c(\"black\", \"blue\", \"black\", \"red\"),\n  fill = c(\"cornsilk\", \"white\", \"lightblue1\", \"white\")\n)\n\n\nggplot(df) +\n  aes(\n    x, y, label = label, angle = angle, color = color, fill = fill,\n    hjust = hjust, vjust = vjust\n  ) +\n  geom_richtext() +\n  geom_point(color = \"black\", size = 2) +\n  scale_color_identity() +\n  scale_fill_identity() +\n  xlim(0, 1) + ylim(0, 1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df) +\n  aes(\n    x, y, label = label, angle = angle, color = color,\n    hjust = hjust, vjust = vjust\n  ) +\n  geom_richtext(\n    fill = NA, label.color = NA, # remove background and outline\n    label.padding = grid::unit(rep(0, 4), \"pt\") # remove padding\n  ) +\n  geom_point(color = \"black\", size = 2) +\n  scale_color_identity() +\n  xlim(0, 1) + ylim(0, 1)\n\n\n\n\n\n\n\n\n\ngeom_textboxは、word wrapさせることも可能である。ただし、回転角度についてはサポートされれていない。\n\n\nCode\n\ndf &lt;- tibble(\n  label = rep(\"Lorem ipsum dolor **sit amet,** consectetur adipiscing elit,\n    sed do *eiusmod tempor incididunt* ut labore et dolore magna\n    aliqua.\", 2),\n  x = c(0, .6),\n  y = c(1, .6),\n  hjust = c(0, 0),\n  vjust = c(1, 0),\n  orientation = c(\"upright\", \"right-rotated\"),\n  color = c(\"black\", \"blue\"),\n  fill = c(\"cornsilk\", \"white\")\n)\n\nggplot(df) +\n  aes(\n    x, y, label = label, color = color, fill = fill,\n    hjust = hjust, vjust = vjust,\n    orientation = orientation\n  ) +\n  geom_textbox(width = unit(0.4, \"npc\")) +\n  geom_point(color = \"black\", size = 2) +\n  scale_discrete_identity(aesthetics = c(\"color\", \"fill\", \"orientation\")) +\n  xlim(0, 1) + ylim(0, 1)",
    "crumbs": [
      "R",
      "ggtext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggplot2/references.html",
    "href": "contents/libs/R/ggplot2/references.html",
    "title": "references",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\n\n\n\n\nCode\nlibrary(legendry)\nlibrary(tidyverse)\nlibrary(showtext)\n\nproject_dir &lt;- here::here()\ncur_dir     &lt;- file.path(project_dir, \"contents/libs/R/ggplot2\")\n\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 Extract tick information from guides\nget_guide_data builds a plot and extracts inforamtion from guide keys.\n\n\nCode\n# A standard plot\np &lt;- ggplot(mtcars) +\n  aes(mpg, disp, colour = drat, size = drat) +\n  geom_point() +\n  facet_wrap(vars(cyl), scales = \"free_x\")\n\n\nplot(p)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Guide information for legends\nget_guide_data(p, \"size\")\n#&gt;       size .value .label\n#&gt; 1 2.662822    3.0    3.0\n#&gt; 2 3.919819    3.5    3.5\n#&gt; 3 4.779645    4.0    4.0\n#&gt; 4 5.477285    4.5    4.5\n\n\n\n\nCode\n# Note that legend guides can be merged\nmerged &lt;- p + guides(colour = \"legend\")\nget_guide_data(merged, \"size\")\n#&gt;    colour .value .label     size\n#&gt; 1 #1A3855    3.0    3.0 2.662822\n#&gt; 2 #28557C    3.5    3.5 3.919819\n#&gt; 3 #3874A5    4.0    4.0 4.779645\n#&gt; 4 #4894D0    4.5    4.5 5.477285\n\n\n\n\nCode\n\n# Guide information for positions\nget_guide_data(p, \"x\", panel = 2)\n#&gt;           x .value .label y\n#&gt; 1 0.0959596     18     18 0\n#&gt; 2 0.3484848     19     19 0\n#&gt; 3 0.6010101     20     20 0\n#&gt; 4 0.8535354     21     21 0\n\n\n\n\nCode\n# Coord polar doesn't support proper guides, so we get a list\npolar &lt;- p + coord_polar()\nget_guide_data(polar, \"theta\", panel = 2)\n#&gt; $theta.range\n#&gt; [1] 17.8 21.4\n#&gt; \n#&gt; $theta.major\n#&gt; [1] 18 19 20 21\n#&gt; \n#&gt; $theta.minor\n#&gt; [1] 18.0 18.5 19.0 19.5 20.0 20.5 21.0\n#&gt; \n#&gt; $theta.labels\n#&gt; [1] \"18\" \"19\" \"20\" \"21\"\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "ggplot2",
      "references"
    ]
  },
  {
    "objectID": "contents/libs/R/ggplot2/articles.html",
    "href": "contents/libs/R/ggplot2/articles.html",
    "title": "articles",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\n\n\n\n\nCode\nlibrary(legendry)\nlibrary(tidyverse)\nlibrary(showtext)\n\nproject_dir &lt;- here::here()\ncur_dir     &lt;- file.path(project_dir, \"contents/libs/R/ggplot2\")\n\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "ggplot2",
      "articles"
    ]
  },
  {
    "objectID": "contents/libs/R/ggfittext/working.html",
    "href": "contents/libs/R/ggfittext/working.html",
    "title": "ggfittext",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/ggfit\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggfittext\",\n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "ggfittext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggfittext/working.html#fitting-text-inside-a-box",
    "href": "contents/libs/R/ggfittext/working.html#fitting-text-inside-a-box",
    "title": "ggfittext",
    "section": "2.1 Fitting text inside a box",
    "text": "2.1 Fitting text inside a box\n\n\nCode\n\nggplot(animals, aes(x = type, y = flies, label = animal)) +\n  geom_tile(fill = \"white\", colour = \"black\") +\n  geom_fit_text()",
    "crumbs": [
      "R",
      "ggfittext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggfittext/working.html#reflowing-text",
    "href": "contents/libs/R/ggfittext/working.html#reflowing-text",
    "title": "ggfittext",
    "section": "2.2 Reflowing text",
    "text": "2.2 Reflowing text\n\n\nCode\nggplot(animals, aes(x = type, y = flies, label = animal)) +\n  geom_tile(fill = \"white\", colour = \"black\") +\n  geom_fit_text(reflow = TRUE)",
    "crumbs": [
      "R",
      "ggfittext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggfittext/working.html#growing-text",
    "href": "contents/libs/R/ggfittext/working.html#growing-text",
    "title": "ggfittext",
    "section": "2.3 Growing text",
    "text": "2.3 Growing text\n\n\nCode\n\nggplot(animals, aes(x = type, y = flies, label = animal)) +\n  geom_tile(fill = \"white\", colour = \"black\") +\n  geom_fit_text(reflow = TRUE, grow = TRUE)",
    "crumbs": [
      "R",
      "ggfittext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggfittext/working.html#placing-text",
    "href": "contents/libs/R/ggfittext/working.html#placing-text",
    "title": "ggfittext",
    "section": "2.4 Placing text",
    "text": "2.4 Placing text\n\n\nCode\nggplot(animals, aes(x = type, y = flies, label = animal)) +\n  geom_tile(fill = \"white\", colour = \"black\") +\n  geom_fit_text(place = \"topleft\", reflow = TRUE)",
    "crumbs": [
      "R",
      "ggfittext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggfittext/working.html#bar-plots",
    "href": "contents/libs/R/ggfittext/working.html#bar-plots",
    "title": "ggfittext",
    "section": "2.5 Bar plots",
    "text": "2.5 Bar plots\ngeom_bar_textを使うだけで簡単に作成することができてしまう。\n\n\nCode\n\nggplot(altitudes, aes(x = craft, y = altitude, label = altitude)) +\n  geom_col() +\n  geom_bar_text()\n\n\n\n\n\n\n\n\n\n\n\nCode\n\nggplot(beverages, aes(x = beverage, y = proportion, label = ingredient,\n                    fill = ingredient)) +\n  geom_col(position = \"stack\") +\n  geom_bar_text(position = \"stack\", reflow = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(beverages, aes(x = beverage, y = proportion, label = ingredient,\n                    fill = ingredient)) +\n  geom_col(position = \"dodge\") +\n  geom_bar_text(position = \"dodge\", grow = TRUE, reflow = TRUE, \n                place = \"left\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nリッチなテクストを配置することも可能である。\n\n\nCode\nggplot(animals_rich, aes(x = type, y = flies, label = animal)) +\n  geom_tile(fill = \"white\", colour = \"black\") +\n  geom_fit_text(reflow = TRUE, grow = TRUE, rich = TRUE)",
    "crumbs": [
      "R",
      "ggfittext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggfittext/working.html#specifying-the-box-limits",
    "href": "contents/libs/R/ggfittext/working.html#specifying-the-box-limits",
    "title": "ggfittext",
    "section": "2.6 Specifying the box limits",
    "text": "2.6 Specifying the box limits\n\n\nCode\nggplot(presidential, aes(ymin = start, ymax = end, x = party, label = name)) +\n  geom_fit_text(grow = TRUE) +\n  geom_errorbar(alpha = 0.5)",
    "crumbs": [
      "R",
      "ggfittext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggfittext/working.html#experimental-feaure-text-inpolar-coordinates",
    "href": "contents/libs/R/ggfittext/working.html#experimental-feaure-text-inpolar-coordinates",
    "title": "ggfittext",
    "section": "2.7 Experimental feaure: text inpolar coordinates",
    "text": "2.7 Experimental feaure: text inpolar coordinates\n\n\nCode\np &lt;- ggplot(gold, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, \n                 fill = linenumber, label = line)) +\n  coord_polar() +\n  geom_rect() +\n  scale_fill_gradient(low = \"#fee391\", high = \"#238443\")\n\np + geom_fit_text(min.size = 0, grow = TRUE)",
    "crumbs": [
      "R",
      "ggfittext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggfittext/working.html#other-useful-arguments",
    "href": "contents/libs/R/ggfittext/working.html#other-useful-arguments",
    "title": "ggfittext",
    "section": "2.8 Other useful arguments",
    "text": "2.8 Other useful arguments\ncontrastパラメータは自動でテキストの色を反転させることができる。\n\n\nCode\n\nggplot(animals, aes(x = type, y = flies, fill = mass, label = animal)) +\n  geom_tile() +\n  geom_fit_text(reflow = TRUE, grow = TRUE, contrast = TRUE)",
    "crumbs": [
      "R",
      "ggfittext",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/geoarrow/index.html",
    "href": "contents/libs/R/geoarrow/index.html",
    "title": "geoarrow",
    "section": "",
    "text": "1 はじめに\n\ngeoparquetが扱えるgeoarrowパッケージについて\n安定して使うにはまだまだかな？\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/libs/R/duckdb/working.html",
    "href": "contents/libs/R/duckdb/working.html",
    "title": "duckdb",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\nCode\ncur_dir &lt;- here::here(\"contents/libs/R/duckdb\")\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "duckdb",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/duckdb/working.html#startup-shutdown",
    "href": "contents/libs/R/duckdb/working.html#startup-shutdown",
    "title": "duckdb",
    "section": "2.1 Startup & Shutdown",
    "text": "2.1 Startup & Shutdown\nduckdbのファイル拡張子は、.dbか.duckdbである。 :memory:とするとin-memory databaseとして使うことが可能である。この場合、Rの終了と合わせてデータが破棄される。 read-onlyパラメータを使うことで書き込み制限ができる。\n\n\nCode\n# to start an in-memory database\ncon &lt;- dbConnect(duckdb())\n# or\ncon &lt;- dbConnect(duckdb(), dbdir = \":memory:\")\n# to use a database file (not shared between processes)\ncon &lt;- dbConnect(duckdb(), dbdir = \"my-db.duckdb\", read_only = FALSE)\n\n\n\n\nCode\n# to use a database file (shared between processes)\ncon &lt;- dbConnect(duckdb(), dbdir = \"my-db.duckdb\", read_only = TRUE)",
    "crumbs": [
      "R",
      "duckdb",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/duckdb/working.html#duckdb-version",
    "href": "contents/libs/R/duckdb/working.html#duckdb-version",
    "title": "duckdb",
    "section": "2.2 duckdb version",
    "text": "2.2 duckdb version\n\n\nCode\ndbGetQuery(\n  con, \n  str_glue(\"\n    SELECT version()    \n  \")\n)\n#&gt;   \"version\"()\n#&gt; 1      v1.2.2",
    "crumbs": [
      "R",
      "duckdb",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/duckdb/working.html#querying",
    "href": "contents/libs/R/duckdb/working.html#querying",
    "title": "duckdb",
    "section": "2.3 Querying",
    "text": "2.3 Querying\n\ndbExecuteはCREATE TABLEやUPDATEのように返値を期待しないクエリに対して使う。\ndbGetQueryはSELECTのように返値を期待するクエリに対して使う。\n\n\n\nCode\ncon &lt;- dbConnect(duckdb(\":memory:\"))\n\n\n\n\nCode\ndbExecute(con, \"CREATE TABLE items (item VARCHAR, value DECIMAL(10, 2), count INTEGER)\")\n#&gt; [1] 0\ndbExecute(con, \"INSERT INTO items VALUES ('jeans', 20.0, 1), ('hammer', 42.2, 2)\")\n#&gt; [1] 2\nres &lt;- dbGetQuery(con, \"SELECT * FROM items\")\nprint(res)\n#&gt;     item value count\n#&gt; 1  jeans  20.0     1\n#&gt; 2 hammer  42.2     2",
    "crumbs": [
      "R",
      "duckdb",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/duckdb/working.html#efficient-transfer",
    "href": "contents/libs/R/duckdb/working.html#efficient-transfer",
    "title": "duckdb",
    "section": "2.4 Efficient Transfer",
    "text": "2.4 Efficient Transfer\ndbWriteTableを使うことでRのデータフレームをduckdbに書き込むことができる。\n\n\nCode\ndbWriteTable(con, \"iris_table\", iris)\nres &lt;- dbGetQuery(con, \"SELECT * FROM iris_table LIMIT 1\")\nprint(res)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n\n\n\n\nCode\niris_tibble &lt;- as_tibble(iris)\ndbWriteTable(con, \"iris_tibble_table\", iris_tibble)\nres &lt;- dbGetQuery(con, \"SELECT * FROM iris_tibble_table LIMIT 1\")\nprint(res)\n#&gt;   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#&gt; 1          5.1         3.5          1.4         0.2  setosa\n\n\n\n\nCode\ndbDisconnect(con)",
    "crumbs": [
      "R",
      "duckdb",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/duckdb/working.html#dbplyr",
    "href": "contents/libs/R/duckdb/working.html#dbplyr",
    "title": "duckdb",
    "section": "2.5 dbplyr",
    "text": "2.5 dbplyr\n\n\nCode\ncon &lt;- dbConnect(duckdb())\nduckdb_register(con, \"flights\", nycflights13::flights)\n\n\ntbl(con, \"flights\") |&gt;\n  group_by(dest) |&gt;\n  summarise(delay = mean(dep_time, na.rm = TRUE)) |&gt;\n  collect()\n#&gt; # A tibble: 105 × 2\n#&gt;    dest  delay\n#&gt;    &lt;chr&gt; &lt;dbl&gt;\n#&gt;  1 SLC   1455.\n#&gt;  2 PIT   1324.\n#&gt;  3 DCA   1394.\n#&gt;  4 GSO   1395.\n#&gt;  5 GRR   1324.\n#&gt;  6 PSE   1463.\n#&gt;  7 CAE   1860.\n#&gt;  8 LGA     NA \n#&gt;  9 ATL   1293.\n#&gt; 10 PBI   1335.\n#&gt; # ℹ 95 more rows\n\n\ndbplyrを使っているときは、csvやparquertのファイル群をdbplyr::tbl関数を使うことで読み込むことが可能である。\n\n\nCode\nwrite_csv(mtcars, \"mtcars.csv\")\n\n\nquery &lt;- \n  tbl(con, \"mtcars.csv\", check_from = FALSE) |&gt; \n  group_by(cyl) |&gt; \n  summarise(across(disp:wt, .fns = mean)) \n#&gt; Warning: The `check_from` argument of `tbl_sql()` is deprecated as of dbplyr 2.5.0.\n#&gt; ℹ The deprecated feature was likely used in the dbplyr package.\n#&gt;   Please report the issue at &lt;https://github.com/tidyverse/dbplyr/issues&gt;.\n\nquery\n#&gt; Warning: Missing values are always removed in SQL aggregation functions.\n#&gt; Use `na.rm = TRUE` to silence this warning\n#&gt; This warning is displayed once every 8 hours.\n#&gt; # Source:   SQL [?? x 5]\n#&gt; # Database: DuckDB v1.2.2 [114012@Windows 10 x64:R 4.5.0/:memory:]\n#&gt;     cyl  disp    hp  drat    wt\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     4  105.  82.6  4.07  2.29\n#&gt; 2     8  353. 209.   3.23  4.00\n#&gt; 3     6  183. 122.   3.59  3.12\n\n\n\n\nCode\ncollect(query)\n#&gt; # A tibble: 3 × 5\n#&gt;     cyl  disp    hp  drat    wt\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     8  353. 209.   3.23  4.00\n#&gt; 2     6  183. 122.   3.59  3.12\n#&gt; 3     4  105.  82.6  4.07  2.29\n\n\nparquetファイルに書き出すことも可能である。\n\n\nCode\ndbExecute(con, \"COPY flights TO 'dataset' (FORMAT PARQUET, PARTITION_BY (year, month), OVERWRITE_OR_IGNORE)\")\n#&gt; [1] 336776\n\n\nparquetに書き出したファイルにを対象にしてインメモリで集計する。読み出しを含めて爆速で集計できるのがわかる。\n\n\nCode\nbench::mark(\n  duckdb = tbl(con, \"read_parquet('dataset/**/*.parquet', hive_partitioning = true)\") |&gt;\n    filter(month == 3) |&gt;\n    summarise(delay = mean(dep_time, na.rm = TRUE)), \n  dplyr = nycflights13::flights |&gt; \n    filter(month == 3) |&gt; \n    summarise(delay = mean(dep_time, na.rm = TRUE)), \n  check = FALSE\n)\n#&gt; # A tibble: 2 × 6\n#&gt;   expression      min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt; &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 duckdb      93.19ms   97.3ms      9.53   385.4KB     6.36\n#&gt; 2 dplyr        6.51ms   13.4ms     69.4     10.4MB     7.17\n\n\n\n\nCode\ndbDisconnect(con)",
    "crumbs": [
      "R",
      "duckdb",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/corrr/working.html",
    "href": "contents/libs/R/corrr/working.html",
    "title": "corrr",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/corr\")\n\n\n\n\nCode\nlibrary(corrr)\nlibrary(tidyverse)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 はじめに\ncorrrは相関係数をデータフレームでハンドリングできるパッケージである。\ncorrrが公式サイトである。\ncorrelateで相関係数を計算して、データフレームとしての相関係数行列を各種APIでハンドリングするのが主な流れである。\nNAが含まれる場合は、correlateの引数にuse = \"pairwise.complete.obs\"を指定する。 デフォルトでは指定されている。\n\n\nCode\ncor_df &lt;- \n  iris |&gt; \n  select(where(is.numeric)) |&gt; \n  correlate() \n#&gt; Correlation computed with\n#&gt; • Method: 'pearson'\n#&gt; • Missing treated using: 'pairwise.complete.obs'\n\n\ncor_df\n#&gt; # A tibble: 4 × 5\n#&gt;   term         Sepal.Length Sepal.Width Petal.Length Petal.Width\n#&gt;   &lt;chr&gt;               &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;\n#&gt; 1 Sepal.Length       NA          -0.118        0.872       0.818\n#&gt; 2 Sepal.Width        -0.118      NA           -0.428      -0.366\n#&gt; 3 Petal.Length        0.872      -0.428       NA           0.963\n#&gt; 4 Petal.Width         0.818      -0.366        0.963      NA\n\n\n\n\nCode\ncor_df |&gt; \n  # 下△成分だけを取り出す\n  shave() |&gt; \n  # ロングフォーマットに変換する\n  stretch()# |&gt; \n#&gt; # A tibble: 16 × 3\n#&gt;    x            y                 r\n#&gt;    &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt;\n#&gt;  1 Sepal.Length Sepal.Length NA    \n#&gt;  2 Sepal.Length Sepal.Width  -0.118\n#&gt;  3 Sepal.Length Petal.Length  0.872\n#&gt;  4 Sepal.Length Petal.Width   0.818\n#&gt;  5 Sepal.Width  Sepal.Length NA    \n#&gt;  6 Sepal.Width  Sepal.Width  NA    \n#&gt;  7 Sepal.Width  Petal.Length -0.428\n#&gt;  8 Sepal.Width  Petal.Width  -0.366\n#&gt;  9 Petal.Length Sepal.Length NA    \n#&gt; 10 Petal.Length Sepal.Width  NA    \n#&gt; 11 Petal.Length Petal.Length NA    \n#&gt; 12 Petal.Length Petal.Width   0.963\n#&gt; 13 Petal.Width  Sepal.Length NA    \n#&gt; 14 Petal.Width  Sepal.Width  NA    \n#&gt; 15 Petal.Width  Petal.Length NA    \n#&gt; 16 Petal.Width  Petal.Width  NA\n  # ggplot(aes(x = x, y = y, fill = r)) +\n  # geom_tile() +\n  # geom_text(aes(label = round(r, 2))) +\n  # scale_fill_viridis_c() +\n  # theme_minimal() +\n  # theme(\n  #   axis.text.x = element_text(angle = 45, hjust = 1)\n  # )\n\n\n\n\nCode\ncor_df |&gt; \n  # 特定の変数に着目する\n  focus(Sepal.Length)\n#&gt; # A tibble: 3 × 2\n#&gt;   term         Sepal.Length\n#&gt;   &lt;chr&gt;               &lt;dbl&gt;\n#&gt; 1 Sepal.Width        -0.118\n#&gt; 2 Petal.Length        0.872\n#&gt; 3 Petal.Width         0.818\n\n\n\n\nCode\nx &lt;- \n  mtcars |&gt;\n  correlate() |&gt; \n  # 強い順に並び変える\n  rearrange()\n#&gt; Correlation computed with\n#&gt; • Method: 'pearson'\n#&gt; • Missing treated using: 'pairwise.complete.obs'\nx |&gt; \n  # 綺麗な見た目にして表示\n  fashion()\n#&gt;    term  mpg   vs drat   am gear qsec carb   hp   wt disp  cyl\n#&gt; 1   mpg       .66  .68  .60  .48  .42 -.55 -.78 -.87 -.85 -.85\n#&gt; 2    vs  .66       .44  .17  .21  .74 -.57 -.72 -.55 -.71 -.81\n#&gt; 3  drat  .68  .44       .71  .70  .09 -.09 -.45 -.71 -.71 -.70\n#&gt; 4    am  .60  .17  .71       .79 -.23  .06 -.24 -.69 -.59 -.52\n#&gt; 5  gear  .48  .21  .70  .79      -.21  .27 -.13 -.58 -.56 -.49\n#&gt; 6  qsec  .42  .74  .09 -.23 -.21      -.66 -.71 -.17 -.43 -.59\n#&gt; 7  carb -.55 -.57 -.09  .06  .27 -.66       .75  .43  .39  .53\n#&gt; 8    hp -.78 -.72 -.45 -.24 -.13 -.71  .75       .66  .79  .83\n#&gt; 9    wt -.87 -.55 -.71 -.69 -.58 -.17  .43  .66       .89  .78\n#&gt; 10 disp -.85 -.71 -.71 -.59 -.56 -.43  .39  .79  .89       .90\n#&gt; 11  cyl -.85 -.81 -.70 -.52 -.49 -.59  .53  .83  .78  .90\n\n\n\n\nCode\nx |&gt; \n  shave() |&gt; \n  rplot()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndatasets::airquality %&gt;% \n  correlate() %&gt;% \n  network_plot(min_cor = .2)\n#&gt; Correlation computed with\n#&gt; • Method: 'pearson'\n#&gt; • Missing treated using: 'pairwise.complete.obs'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "corrr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html",
    "href": "contents/libs/R/assertr/index.html",
    "title": "assertr",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/assertr\")\nsetwd(cur_dir)\nCode\nbox::use(\n  assertr[...],\n  igraph[...],\n  ggplot2[...],\n  cowplot[...], \n  showtext[showtext_auto], \n  sysfonts[font_add_google],\n)\n\nlibrary(assertr)\nlibrary(stringr)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html#concrete-data-errors",
    "href": "contents/libs/R/assertr/index.html#concrete-data-errors",
    "title": "assertr",
    "section": "2.1 Concrete data errors",
    "text": "2.1 Concrete data errors\n\n\nCode\nhead(mtcars)\n#&gt;                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#&gt; Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#&gt; Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#&gt; Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n#&gt; Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\n\nCode\nour.data &lt;- mtcars\nour.data$mpg[5] &lt;- our.data$mpg[5] * -1\nour.data[4:6,]\n#&gt;                     mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#&gt; Hornet 4 Drive     21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#&gt; Hornet Sportabout -18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n#&gt; Valiant            18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nこのまま計算すると何か間違えている・・・？負の値を入り込ましているのでデータ気付かないうちに計算を間違う。\n\n\nCode\nlibrary(dplyr)\n\nour.data %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg.mpg=mean(mpg))\n#&gt; # A tibble: 3 × 2\n#&gt;     cyl avg.mpg\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1     4    26.7\n#&gt; 2     6    19.7\n#&gt; 3     8    12.4",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html#enter-assertr",
    "href": "contents/libs/R/assertr/index.html#enter-assertr",
    "title": "assertr",
    "section": "2.2 enter assertr",
    "text": "2.2 enter assertr\n次のようなverifyを使うことで計算が失敗するようになる。\n\n\nCode\nlibrary(assertr)\n\ntryCatch({\nour.data |&gt; \n  verify(mpg &gt;= 0) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg.mpg = mean(mpg))\n\n}, error = \\(e) print(e))\n#&gt; verification [mpg &gt;= 0] failed! (1 failure)\n#&gt; \n#&gt;     verb redux_fn predicate column index value\n#&gt; 1 verify       NA  mpg &gt;= 0     NA     5    NA\n#&gt; \n#&gt; &lt;simpleError: assertr stopped execution&gt;\n\n\nassert関数を使うと次のように記述することができる。結果をみると行単位で評価結果を得られてることがわかる。\n\n\nCode\ntryCatch({\n\nour.data |&gt; \n  assert(within_bounds(0, Inf), mpg) |&gt; \n  group_by(cyl) |&gt; \n  summarise(avg.mpg = mean(mpg))\n}, error = \\(e) print(e))  \n#&gt; Column 'mpg' violates assertion 'within_bounds(0, Inf)' 1 time\n#&gt;     verb redux_fn             predicate column index value\n#&gt; 1 assert       NA within_bounds(0, Inf)    mpg     5 -18.7\n#&gt; \n#&gt; &lt;simpleError: assertr stopped execution&gt;\n\n\ndplyrのselect関数が使える。\n\n\nCode\nlibrary(assertr)\n\ntryCatch({\n\nour.data %&gt;%\n  assert(within_bounds(0,Inf, include.lower=FALSE), -mpg) %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg.mpg=mean(mpg))\n}, error = \\(e) print(e))\n#&gt; Column 'vs' violates assertion 'within_bounds(0, Inf, include.lower = FALSE)' 18 times\n#&gt;     verb redux_fn                                    predicate column index\n#&gt; 1 assert       NA within_bounds(0, Inf, include.lower = FALSE)     vs     1\n#&gt; 2 assert       NA within_bounds(0, Inf, include.lower = FALSE)     vs     2\n#&gt; 3 assert       NA within_bounds(0, Inf, include.lower = FALSE)     vs     5\n#&gt; 4 assert       NA within_bounds(0, Inf, include.lower = FALSE)     vs     7\n#&gt; 5 assert       NA within_bounds(0, Inf, include.lower = FALSE)     vs    12\n#&gt;   value\n#&gt; 1     0\n#&gt; 2     0\n#&gt; 3     0\n#&gt; 4     0\n#&gt; 5     0\n#&gt;   [omitted 13 rows]\n#&gt; \n#&gt; \n#&gt; Column 'am' violates assertion 'within_bounds(0, Inf, include.lower = FALSE)' 19 times\n#&gt;     verb redux_fn                                    predicate column index\n#&gt; 1 assert       NA within_bounds(0, Inf, include.lower = FALSE)     am     4\n#&gt; 2 assert       NA within_bounds(0, Inf, include.lower = FALSE)     am     5\n#&gt; 3 assert       NA within_bounds(0, Inf, include.lower = FALSE)     am     6\n#&gt; 4 assert       NA within_bounds(0, Inf, include.lower = FALSE)     am     7\n#&gt; 5 assert       NA within_bounds(0, Inf, include.lower = FALSE)     am     8\n#&gt;   value\n#&gt; 1     0\n#&gt; 2     0\n#&gt; 3     0\n#&gt; 4     0\n#&gt; 5     0\n#&gt;   [omitted 14 rows]\n#&gt; \n#&gt; \n#&gt; &lt;simpleError: assertr stopped execution&gt;",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html#verify-vs.-assert",
    "href": "contents/libs/R/assertr/index.html#verify-vs.-assert",
    "title": "assertr",
    "section": "2.3 verify vs. assert",
    "text": "2.3 verify vs. assert\n式を引数にとるか、述語関数やカラムを引数にとるかが異なる。 またverifyは引数全体に対する評価であり、全体を評価してから違反がないかを考えている。 このため行単位での問題の特定が出来ない。",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html#asserts-predicates",
    "href": "contents/libs/R/assertr/index.html#asserts-predicates",
    "title": "assertr",
    "section": "2.4 asserts predicates",
    "text": "2.4 asserts predicates\n述語関数には次のようなものがある。\n\nnot_naはNAじゃないことを確認する\nwithin_boundsは数値が与えられた範囲に含まれているかを確認する\nin_setは集合のメンバーであるかどうかを確認する\nis_uniqはユニークかどうかを確認する",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html#cusotm-predicates",
    "href": "contents/libs/R/assertr/index.html#cusotm-predicates",
    "title": "assertr",
    "section": "2.5 cusotm predicates",
    "text": "2.5 cusotm predicates\n\n\nCode\nnot.empty.p &lt;- function(x) if(x==\"\") return(FALSE)\n\n\nほかには\n\n\nCode\nseven.digit.p &lt;- function(x) nchar(x)==7",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html#enter-insist-and-predicate-generators",
    "href": "contents/libs/R/assertr/index.html#enter-insist-and-predicate-generators",
    "title": "assertr",
    "section": "2.6 enter insist and predicate generators",
    "text": "2.6 enter insist and predicate generators\nたぶんだけどSQLのWindows関数のように自分の行以外の情報が評価に必要なものに対して有効、ということだと思う。\n\n\nCode\nmtcars %&gt;%\n  insist(within_n_sds(3), mpg) %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg.mpg=mean(mpg))\n#&gt; # A tibble: 3 × 2\n#&gt;     cyl avg.mpg\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1     4    26.7\n#&gt; 2     6    19.7\n#&gt; 3     8    15.1\n\n\n\n\nCode\nmtcars %&gt;%\n  insist(within_n_sds(10), mpg:carb) %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg.mpg=mean(mpg))\n#&gt; # A tibble: 3 × 2\n#&gt;     cyl avg.mpg\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1     4    26.7\n#&gt; 2     6    19.7\n#&gt; 3     8    15.1\n\n\nお試しの計算です。\n\n\nCode\nexample.vector &lt;- c(7.4, 7.1, 7.2, 72.1)\nwithin_n_sds(2)(example.vector)(example.vector)\n#&gt; [1] TRUE TRUE TRUE TRUE",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html#row-wise-assertions-and-row-reduction-functions",
    "href": "contents/libs/R/assertr/index.html#row-wise-assertions-and-row-reduction-functions",
    "title": "assertr",
    "section": "2.7 row-wise assertions and row reduction functions",
    "text": "2.7 row-wise assertions and row reduction functions\n\n\nCode\nexample.data &lt;- data.frame(x=c(8, 9, 6, 5, 9, 5, 6, 7,\n                             8, 9, 6, 5, 5, 6, 7),\n                         y=c(82, 91, 61, 49, 40, 49, 57,\n                             74, 78, 90, 61, 49, 51, 62, 68))\n(example.data)\n#&gt;    x  y\n#&gt; 1  8 82\n#&gt; 2  9 91\n#&gt; 3  6 61\n#&gt; 4  5 49\n#&gt; 5  9 40\n#&gt; 6  5 49\n#&gt; 7  6 57\n#&gt; 8  7 74\n#&gt; 9  8 78\n#&gt; 10 9 90\n#&gt; 11 6 61\n#&gt; 12 5 49\n#&gt; 13 5 51\n#&gt; 14 6 62\n#&gt; 15 7 68\n\n\n\n\nCode\nplot(example.data$x, example.data$y, xlab=\"\", ylab=\"\")\n\n\n\n\n\n\n\n\n\n多変量の検証にはマハラノビス距離が使える。\n\n\nCode\nmaha_dist(example.data)\n#&gt;  [1]  1.28106379  3.10992407  0.25081851  1.35993969 12.81898913  1.35993969\n#&gt;  [7]  0.26181283  0.47714597  0.87804987  2.95741956  0.25081851  1.35993969\n#&gt; [13]  1.29208587  0.28235776  0.05969507\n\n\n\n\nCode\nmaha_dist(example.data) %&gt;% hist(main=\"\", xlab=\"\")\n\n\n\n\n\n\n\n\n\nマハラノビス距離を評価に組み込む。\n\n\nCode\ntryCatch({\n\nexample.data %&gt;%\n  insist_rows(maha_dist, within_n_mads(3), dplyr::everything())\n}, error = \\(e) print(e))\n#&gt; Data frame row reduction 'maha_dist' violates predicate 'within_n_mads(3)' 1 time\n#&gt;          verb  redux_fn        predicate               column index    value\n#&gt; 1 insist_rows maha_dist within_n_mads(3) ~dplyr::everything()     5 12.81899\n#&gt; \n#&gt; &lt;simpleError: assertr stopped execution&gt;",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html#success-error-and-defect-functions",
    "href": "contents/libs/R/assertr/index.html#success-error-and-defect-functions",
    "title": "assertr",
    "section": "2.8 success, error, and defect functions",
    "text": "2.8 success, error, and defect functions\n成功・失敗時の処理の挙動を制御できる。",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html#obligatory-assertions",
    "href": "contents/libs/R/assertr/index.html#obligatory-assertions",
    "title": "assertr",
    "section": "2.9 Obligatory assertions",
    "text": "2.9 Obligatory assertions\n独立でないルールを適用することがある。\n\n\nCode\ntryCatch({\n\nmtcars_without_am &lt;- mtcars %&gt;% \n  dplyr::select(-am)\nmtcars_without_am %&gt;% \n  verify(has_all_names(\"am\", \"vs\"), error_fun = error_append) %&gt;% \n  assert(in_set(0, 1), am, vs, error_fun = error_report)\n}, error = \\(e) print(e))\n#&gt; &lt;error/vctrs_error_subscript_oob&gt;\n#&gt; Error in `dplyr::select()`:\n#&gt; ! Can't subset columns that don't exist.\n#&gt; ✖ Column `am` doesn't exist.\n#&gt; ---\n#&gt; Backtrace:\n#&gt;     ▆\n#&gt;  1. ├─base::tryCatch(...)\n#&gt;  2. │ └─base (local) tryCatchList(expr, classes, parentenv, handlers)\n#&gt;  3. │   └─base (local) tryCatchOne(expr, names, parentenv, handlers[[1L]])\n#&gt;  4. │     └─base (local) doTryCatch(return(expr), name, parentenv, handler)\n#&gt;  5. ├─... %&gt;% ...\n#&gt;  6. └─assertr::assert(., in_set(0, 1), am, vs, error_fun = error_report)\n#&gt;  7.   ├─dplyr::select(data, !!!(keeper.vars))\n#&gt;  8.   └─dplyr:::select.data.frame(data, !!!(keeper.vars))\n\n\n上記はverifyと関係ないところでエラーが出ている。 その場合には次のようにする。\n\n\nCode\ntryCatch({\n\nmtcars_without_am &lt;- mtcars %&gt;% \n  dplyr::select(-am)\nmtcars_without_am %&gt;% \n  verify(has_all_names(\"am\", \"vs\"), obligatory=TRUE, error_fun=error_append) %&gt;% \n  assert(in_set(0, 1), am, vs, defect_fun=defect_report)\n}, error = \\(e) print(e))\n#&gt; assert: verification [in_set(0, 1)] omitted due to data defect! Columns passed to assertion: am vs",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html#combining-chains-of-assertions",
    "href": "contents/libs/R/assertr/index.html#combining-chains-of-assertions",
    "title": "assertr",
    "section": "2.10 combining chains of assertions",
    "text": "2.10 combining chains of assertions\nパイプラインだと1つの処理が失敗するとそこから先の処理は実施されない。 一方で、複数のチェックをリポートしてもらいたいときがある。\n\n\nCode\nmtcars %&gt;%\n  chain_start %&gt;%\n  verify(nrow(mtcars) &gt; 10) %&gt;%\n  verify(mpg &gt; 0) %&gt;%\n  insist(within_n_sds(4), mpg) %&gt;%\n  assert(in_set(0,1), am, vs) %&gt;%\n  chain_end %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg.mpg=mean(mpg))\n#&gt; # A tibble: 3 × 2\n#&gt;     cyl avg.mpg\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1     4    26.7\n#&gt; 2     6    19.7\n#&gt; 3     8    15.1",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/assertr/index.html#advanced-creating-your-own-predicate-generators-for-insist",
    "href": "contents/libs/R/assertr/index.html#advanced-creating-your-own-predicate-generators-for-insist",
    "title": "assertr",
    "section": "2.11 advanced: creating your own predicate generators for insist",
    "text": "2.11 advanced: creating your own predicate generators for insist\n\n\nCode\nwithin_3_iqrs &lt;- function(a_vector){\n  the_median &lt;- median(a_vector)\n  the_iqr &lt;- IQR(a_vector)\n  within_bounds((the_median-the_iqr*3), (the_median+the_iqr*3))\n}\nmtcars %&gt;%\n  insist(within_3_iqrs, mpg) %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(avg.mpg=mean(mpg))\n#&gt; # A tibble: 3 × 2\n#&gt;     cyl avg.mpg\n#&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1     4    26.7\n#&gt; 2     6    19.7\n#&gt; 3     8    15.1",
    "crumbs": [
      "R",
      "assertr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/argparse/index.html",
    "href": "contents/libs/R/argparse/index.html",
    "title": "argparse",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/argparse\")\nsetwd(cur_dir)\n\n\n\n\nCode\nbox::use(\n  argparse[...],\n  assertr[...],\n  igraph[...],\n  ggplot2[...],\n  cowplot[...], \n  showtext[showtext_auto], \n  sysfonts[font_add_google],\n)\n\nlibrary(assertr)\nlibrary(argparse)\nlibrary(stringr)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 はじめに\nargparseを使うことで、コマンドラインから使いやすくなる。\n\nCode\nglue::glue(\"- {objects('package:argparse')}\") |&gt; \n  cat(sep = \"\\n\")\n\n\nArgumentParser\n\nlibrary(argparse)\n\n# パーサーを作成\nparser &lt;- ArgumentParser(description = \"Test application\")\n\n# 引数を追加\nparser$add_argument(\"number\", type = \"integer\", help = \"Number to square\")\n\n# 引数を解析\nargs &lt;- parser$parse_args()\n\n# 計算と出力\nresult &lt;- args$number^2\ncat(\"The square of\", args$number, \"is\", result, \"\\n\")\n\n\nCode\nrscript ./main.R 5\n#&gt; \n#&gt; Some OSX users know that OSX is really Unix [...], others think that OSX is\n#&gt; cooler Windows, and they have ontological problems with non-Apple phenomena and\n#&gt; constructs.\n#&gt;    -- Roger Bivand\n#&gt;       R-SIG-Geo (May 2012)\n#&gt; \n#&gt;  警告メッセージ: \n#&gt;  パッケージ 'argparse' はバージョン 4.3.3 の R の下で造られました  \n#&gt; The square of 5 is 25\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "argparse",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html",
    "href": "contents/libs/R/DBI/working.html",
    "title": "DBI",
    "section": "",
    "text": "Code\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(showtext)\nlibrary(arrow)\nlibrary(nanoarrow)\nlibrary(odbc)\nlibrary(DBI)\nlibrary(RSQLite)\ndotenv::load_dot_env(file.path(here::here(), \".env\"))\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html#prepare",
    "href": "contents/libs/R/DBI/working.html#prepare",
    "title": "DBI",
    "section": "3.1 Prepare",
    "text": "3.1 Prepare\n\n\nCode\ncon &lt;- dbConnect(RSQLite::SQLite())\n\ndata &lt;- data.frame(\n  a = 1:3,\n  b = 4.5,\n  c = \"five\"\n)\n\ndbWriteTable(con, \"tbl\", data)",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html#read-all-rows-from-table",
    "href": "contents/libs/R/DBI/working.html#read-all-rows-from-table",
    "title": "DBI",
    "section": "3.2 Read all rows from table",
    "text": "3.2 Read all rows from table\n\n\nCode\nstream &lt;- dbReadTableArrow(con, \"tbl\")\nstream\n#&gt; &lt;nanoarrow_array_stream struct&lt;a: int32, b: double, c: string&gt;&gt;\n#&gt;  $ get_schema:function ()  \n#&gt;  $ get_next  :function (schema = x$get_schema(), validate = TRUE)  \n#&gt;  $ release   :function ()\n\n\n\n\nCode\nstream$get_schema()\n#&gt; &lt;nanoarrow_schema struct&gt;\n#&gt;  $ format    : chr \"+s\"\n#&gt;  $ name      : chr \"\"\n#&gt;  $ metadata  : list()\n#&gt;  $ flags     : int 0\n#&gt;  $ children  :List of 3\n#&gt;   ..$ a:&lt;nanoarrow_schema int32&gt;\n#&gt;   .. ..$ format    : chr \"i\"\n#&gt;   .. ..$ name      : chr \"a\"\n#&gt;   .. ..$ metadata  : list()\n#&gt;   .. ..$ flags     : int 2\n#&gt;   .. ..$ children  : list()\n#&gt;   .. ..$ dictionary: NULL\n#&gt;   ..$ b:&lt;nanoarrow_schema double&gt;\n#&gt;   .. ..$ format    : chr \"g\"\n#&gt;   .. ..$ name      : chr \"b\"\n#&gt;   .. ..$ metadata  : list()\n#&gt;   .. ..$ flags     : int 2\n#&gt;   .. ..$ children  : list()\n#&gt;   .. ..$ dictionary: NULL\n#&gt;   ..$ c:&lt;nanoarrow_schema string&gt;\n#&gt;   .. ..$ format    : chr \"u\"\n#&gt;   .. ..$ name      : chr \"c\"\n#&gt;   .. ..$ metadata  : list()\n#&gt;   .. ..$ flags     : int 2\n#&gt;   .. ..$ children  : list()\n#&gt;   .. ..$ dictionary: NULL\n#&gt;  $ dictionary: NULL\n\n\n\n\nCode\nas_tibble(stream)\n#&gt; # A tibble: 3 × 3\n#&gt;       a     b c    \n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n#&gt; 1     1   4.5 five \n#&gt; 2     2   4.5 five \n#&gt; 3     3   4.5 five",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html#run-queries",
    "href": "contents/libs/R/DBI/working.html#run-queries",
    "title": "DBI",
    "section": "3.3 Run queries",
    "text": "3.3 Run queries\n\n\nCode\nstream &lt;- dbGetQueryArrow(con, \"SELECT COUNT(*) AS n FROM tbl WHERE a &lt; 3\")\nstream\n#&gt; &lt;nanoarrow_array_stream struct&lt;n: int32&gt;&gt;\n#&gt;  $ get_schema:function ()  \n#&gt;  $ get_next  :function (schema = x$get_schema(), validate = TRUE)  \n#&gt;  $ release   :function ()\n\n\nファイルに書き出す。\n\n\nCode\npath &lt;- tempfile(fileext = \".parquet\")\narrow::write_parquet(arrow::as_record_batch_reader(stream), path)\narrow::read_parquet(path)\n#&gt; # A tibble: 1 × 1\n#&gt;       n\n#&gt;   &lt;int&gt;\n#&gt; 1     2",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html#prepared-queries",
    "href": "contents/libs/R/DBI/working.html#prepared-queries",
    "title": "DBI",
    "section": "3.4 Prepared queries",
    "text": "3.4 Prepared queries\nクエリに使うパラメータをデータフレームから渡すことができる。\n\n\nCode\nparams &lt;- data.frame(a = 3L)\nstream &lt;- dbGetQueryArrow(con, \"SELECT $a AS batch, * FROM tbl WHERE a &lt; $a\", params = params)\nas.data.frame(stream)\n#&gt;   batch a   b    c\n#&gt; 1     3 1 4.5 five\n#&gt; 2     3 2 4.5 five\n\n\nパラメータが不足するときには最期の値がrecursiveする。\n\n\nCode\nparams &lt;- data.frame(a = c(2L, 4L))\n# Equivalent to dbBind()\nstream &lt;- dbGetQueryArrow(con, \"SELECT $a AS batch, * FROM tbl WHERE a &lt; $a\", params = params)\nas.data.frame(stream)\n#&gt;   batch a   b    c\n#&gt; 1     2 1 4.5 five\n#&gt; 2     4 1 4.5 five\n#&gt; 3     4 2 4.5 five\n#&gt; 4     4 3 4.5 five",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html#manual-flow",
    "href": "contents/libs/R/DBI/working.html#manual-flow",
    "title": "DBI",
    "section": "3.5 Manual flow",
    "text": "3.5 Manual flow\ndbSendQueryArrowを使うとクエリを送付し、dbFetchArrowで結果を取得する。 これらはdbBindArrowを使うことでもできる。処理後は必ずdbClearResultを使いクリアすることになる。\n\n\nCode\nrs &lt;- dbSendQueryArrow(con, \"SELECT $a AS batch, * FROM tbl WHERE a &lt; $a\")\n\nin_arrow &lt;- nanoarrow::as_nanoarrow_array(data.frame(a = 1L))\ndbBindArrow(rs, in_arrow)\nas.data.frame(dbFetchArrow(rs))\n#&gt; [1] batch a     b     c    \n#&gt; &lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\nCode\nin_arrow &lt;- nanoarrow::as_nanoarrow_array(data.frame(a = 2L))\ndbBindArrow(rs, in_arrow)\nas.data.frame(dbFetchArrow(rs))\n#&gt;   batch a   b    c\n#&gt; 1     2 1 4.5 five\n\n\n\n\nCode\nin_arrow &lt;- nanoarrow::as_nanoarrow_array(data.frame(a = 3L))\ndbBindArrow(rs, in_arrow)\nas.data.frame(dbFetchArrow(rs))\n#&gt;   batch a   b    c\n#&gt; 1     3 1 4.5 five\n#&gt; 2     3 2 4.5 five\n\n\n\n\nCode\nin_arrow &lt;- nanoarrow::as_nanoarrow_array(data.frame(a = 1:4L))\ndbBindArrow(rs, in_arrow)\nas.data.frame(dbFetchArrow(rs))\n#&gt;   batch a   b    c\n#&gt; 1     2 1 4.5 five\n#&gt; 2     3 1 4.5 five\n#&gt; 3     3 2 4.5 five\n#&gt; 4     4 1 4.5 five\n#&gt; 5     4 2 4.5 five\n#&gt; 6     4 3 4.5 five",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html#writing-data",
    "href": "contents/libs/R/DBI/working.html#writing-data",
    "title": "DBI",
    "section": "3.6 Writing Data",
    "text": "3.6 Writing Data\n\n\nCode\nstream &lt;- dbGetQueryArrow(con, \"SELECT * FROM tbl WHERE a &lt; 3\")\n#&gt; Warning: Closing open result set, pending rows\ndbWriteTableArrow(con, \"tbl_new\", stream)\ndbReadTable(con, \"tbl_new\")\n#&gt;   a   b    c\n#&gt; 1 1 4.5 five\n#&gt; 2 2 4.5 five",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html#appending-data",
    "href": "contents/libs/R/DBI/working.html#appending-data",
    "title": "DBI",
    "section": "3.7 Appending Data",
    "text": "3.7 Appending Data\n\n\nCode\nstream &lt;- dbGetQueryArrow(con, \"SELECT * FROM tbl WHERE a &lt; 3\")\ndbCreateTableArrow(con, \"tbl_split\", stream)\ndbAppendTableArrow(con, \"tbl_split\", stream)\n#&gt; [1] 2\n\n\n\n\nCode\nstream &lt;- dbGetQueryArrow(con, \"SELECT * FROM tbl WHERE a &gt;= 3\")\ndbAppendTableArrow(con, \"tbl_split\", stream)\n#&gt; [1] 1\n\n\n\n\nCode\ndbReadTable(con, \"tbl_split\")\n#&gt;   a   b    c\n#&gt; 1 1 4.5 five\n#&gt; 2 2 4.5 five\n#&gt; 3 3 4.5 five",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html#disconnect",
    "href": "contents/libs/R/DBI/working.html#disconnect",
    "title": "DBI",
    "section": "3.8 disconnect",
    "text": "3.8 disconnect\n\n\nCode\ndbDisconnect(con)",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html#connect",
    "href": "contents/libs/R/DBI/working.html#connect",
    "title": "DBI",
    "section": "4.1 Connect",
    "text": "4.1 Connect\n\n\nCode\ncon &lt;- dbConnect(\n  odbc::odbc(), \n  Driver = \"SnowflakeDSIIDriver\",\n  Server = paste0(Sys.getenv(\"SNOW_ACCOUNT\"), \".snowflakecomputing.com\"), \n  UID = Sys.getenv(\"USER\"), \n  PWD = Sys.getenv(\"PASSWORD\"), \n  warehouse = Sys.getenv(\"SNOW_WAREHOUSE\"), \n  role = Sys.getenv(\"ROLE\")\n)",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html#run-query",
    "href": "contents/libs/R/DBI/working.html#run-query",
    "title": "DBI",
    "section": "4.2 run query",
    "text": "4.2 run query\n\n\nCode\ndbGetQuery(con, \"SELECT * FROM SZK_DEV.PUBLIC.JAPANESE LIMIT 5\")\n#&gt;               日本語 START_DATE     START_TIMESTAMP\n#&gt; 1 日本語のカラムです 2024-01-01 2024-06-10 16:17:26\n#&gt; 2 日本語のカラムです 2024-01-01 2024-06-10 16:18:02",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/working.html#disconnect-1",
    "href": "contents/libs/R/DBI/working.html#disconnect-1",
    "title": "DBI",
    "section": "4.3 disconnect",
    "text": "4.3 disconnect\n\n\nCode\ndbDisconnect(con)",
    "crumbs": [
      "R",
      "DBI",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ComplexHeatmap/working.html",
    "href": "contents/libs/R/ComplexHeatmap/working.html",
    "title": "ComplexHeatmap",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/ComplexHeatmap\")\n\n\n\n\nCode\nlibrary(ComplexHeatmap)\nlibrary(showtext)\nlibrary(dplyr)\nlibrary(ggplot2)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 はじめに\nComplexHeatmapで美しいモザイクプロットを作成することができる。\n\n\n2 Example\n\n\nCode\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(\"ComplexHeatmap\")\nlibrary(ComplexHeatmap)\n\n\n\n\nCode\n# サンプルデータの生成\nset.seed(123)\ndata &lt;- matrix(rnorm(100), nrow=10, ncol=10)\n\n# ヒートマップの作成\nHeatmap(data, \n        name = \"my_heatmap\", \n        cluster_rows = TRUE, \n        cluster_columns = TRUE,\n        show_row_dend = TRUE, \n        show_column_dend = TRUE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "ComplexHeatmap",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/sqlite3/03_spatial.html",
    "href": "contents/libs/Python/sqlite3/03_spatial.html",
    "title": "Spatialite",
    "section": "",
    "text": "#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.0\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function\n\n\n1 はじめに\n\nsqliteにはその空間データ拡張であるspatialiteがある\n\n\n\n2 Quick Example\n\n\nCode\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns\nimport os\n\n# データ接続\ncon = sqlite3.connect(os.path.join(r.cur_dir, \"spatialite.db\"))\n\n# 拡張機能を有効化\ncon.enable_load_extension(True)\ncon.load_extension(\"mod_spatialite\")\n\ncur = con.cursor()\nret = cur.execute(\"SELECT AsText(ST_GeomFromText('POINTZ(1.0 2.0 3.0)'))\").fetchall()\n\nprint(ret)\n#&gt; [('POINT Z(1 2 3)',)]\n\n# 接続を閉じる\ncon.close()\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "sqlite3",
      "Spatialite"
    ]
  },
  {
    "objectID": "contents/libs/Python/sqlite3/01_theta_join.html",
    "href": "contents/libs/Python/sqlite3/01_theta_join.html",
    "title": "θ結合",
    "section": "",
    "text": "#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.0\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function\n\n\n1 はじめに\n\npandasやpolarsではθ結合が非効率である\nSQLiteやduckdbを使ってθ結合をするのがよい\n\n\n\n2 Quick Example\n\n\nCode\nimport duckdb\nimport pandas as pd\n\n# サンプルデータ\ndata = {\n    'employee_id': [1, 2, 3, 4, 5],\n    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'manager_id': [None, 1, 1, 2, 3],  # 上司ID\n    'age': [50, 40, 38, 35, 30]  # 年齢\n}\ndf = pd.DataFrame(data)\n\n# θ結合SQL（部下の年齢が上司より5歳以上若い）\nquery = \"\"\"\nSELECT e1.employee_id AS employee_id, e1.name AS employee_name, e1.age AS emp_age,\n       e1.manager_id, e2.name AS manager_name, e2.age AS mgr_age\nFROM employees e1\nJOIN employees e2 ON e1.manager_id = e2.employee_id\nWHERE e1.age &lt;= e2.age - 5\n\"\"\"\n\n# 実行\ndf_sql = pd.read_sql(query, conn)\n\n# 結果表示\nprint(df_sql)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "sqlite3",
      "θ結合"
    ]
  },
  {
    "objectID": "contents/libs/Python/plotnine/notes.html",
    "href": "contents/libs/Python/plotnine/notes.html",
    "title": "ノート",
    "section": "",
    "text": "#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.0\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function",
    "crumbs": [
      "Python",
      "plotnine",
      "ノート"
    ]
  },
  {
    "objectID": "contents/libs/Python/plotnine/notes.html#maps",
    "href": "contents/libs/Python/plotnine/notes.html#maps",
    "title": "ノート",
    "section": "2.1 Maps",
    "text": "2.1 Maps\n\n\nCode\nfrom plotnine import *\nimport geodatasets\nimport geopandas as gp\n\nchicago = gp.read_file(geodatasets.get_path(\"geoda.chicago_commpop\"))\ngroceries = gp.read_file(geodatasets.get_path(\"geoda.groceries\"))\n\n\n\n\nCode\n\n(\n    ggplot(chicago)\n    + geom_map() \n    + coord_fixed()\n).show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n(\n    ggplot()\n    + geom_map(data=chicago, fill=None)\n    + geom_map(data=groceries.to_crs(chicago.crs), color=\"green\")\n    + theme_void()\n    + coord_fixed()\n).show()",
    "crumbs": [
      "Python",
      "plotnine",
      "ノート"
    ]
  },
  {
    "objectID": "contents/libs/Python/pandera/index.html",
    "href": "contents/libs/Python/pandera/index.html",
    "title": "union pandera",
    "section": "",
    "text": "Code\ncur_dir &lt;- here::here(\"contents/libs/Python/pandera\")\n\nshowtext::showtext_auto()\nsysfonts::font_add_google(\"Noto Sans JP\")\n\npacman::p_load(reticulate)\n#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.0\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function",
    "crumbs": [
      "Python",
      "pandera",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/pandera/index.html#quick-start",
    "href": "contents/libs/Python/pandera/index.html#quick-start",
    "title": "union pandera",
    "section": "2.1 Quick Start",
    "text": "2.1 Quick Start\n\n\nCode\n\nimport pandas as pd\nimport pandera as pa\n#&gt; C:\\pyenv\\py312\\Lib\\site-packages\\pyspark\\pandas\\__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow&gt;=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n#&gt;   warnings.warn(\n\n# data to validate\ndf = pd.DataFrame({\n    \"column1\": [1, 4, 0, 10, 9],\n    \"column2\": [-1.3, -1.4, -2.9, -10.1, -20.4],\n    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n})\n\n# define schema\nschema = pa.DataFrameSchema({\n    \"column1\": pa.Column(int, checks=pa.Check.le(10)),\n    \"column2\": pa.Column(float, checks=pa.Check.lt(-1.2)),\n    \"column3\": pa.Column(str, checks=[\n        pa.Check.str_startswith(\"value_\"),\n        # define custom checks as functions that take a series as input and\n        # outputs a boolean or boolean Series\n        pa.Check(lambda s: s.str.split(\"_\", expand=True).shape[1] == 2)\n    ]),\n})\n\nvalidated_df = schema(df)\nprint(validated_df)\n#&gt;    column1  column2  column3\n#&gt; 0        1     -1.3  value_1\n#&gt; 1        4     -1.4  value_2\n#&gt; 2        0     -2.9  value_3\n#&gt; 3       10    -10.1  value_2\n#&gt; 4        9    -20.4  value_1\n\n\n型の定義には、pythonのビルトインも使えるしpanderaで用意されているものも使える。\n\n\nCode\nschema = pa.DataFrameSchema({\n    # built-in python types\n    \"int_column\": pa.Column(int),\n    \"float_column\": pa.Column(float),\n    \"str_column\": pa.Column(str),\n\n    # pandas dtype string aliases\n    \"int_column2\": pa.Column(\"int64\"),\n    \"float_column2\": pa.Column(\"float64\"),\n    # pandas &gt; 1.0.0 support native \"string\" type\n    \"str_column2\": pa.Column(\"str\"),\n\n    # pandera DataType\n    \"int_column3\": pa.Column(pa.Int),\n    \"float_column3\": pa.Column(pa.Float),\n    \"str_column3\": pa.Column(pa.String),\n})\nschema\n#&gt; &lt;Schema DataFrameSchema(columns={'int_column': &lt;Schema Column(name=int_column, type=DataType(int64))&gt;, 'float_column': &lt;Schema Column(name=float_column, type=DataType(float64))&gt;, 'str_column': &lt;Schema Column(name=str_column, type=DataType(str))&gt;, 'int_column2': &lt;Schema Column(name=int_column2, type=DataType(int64))&gt;, 'float_column2': &lt;Schema Column(name=float_column2, type=DataType(float64))&gt;, 'str_column2': &lt;Schema Column(name=str_column2, type=DataType(str))&gt;, 'int_column3': &lt;Schema Column(name=int_column3, type=DataType(int64))&gt;, 'float_column3': &lt;Schema Column(name=float_column3, type=DataType(float64))&gt;, 'str_column3': &lt;Schema Column(name=str_column3, type=DataType(str))&gt;}, checks=[], index=None, coerce=False, dtype=None, strict=False, name=None, ordered=False, unique_column_names=Falsemetadata='None, unique_column_names=False, add_missing_columns=False)&gt;",
    "crumbs": [
      "Python",
      "pandera",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/pandera/index.html#data-frame-models",
    "href": "contents/libs/Python/pandera/index.html#data-frame-models",
    "title": "union pandera",
    "section": "2.2 Data Frame Models",
    "text": "2.2 Data Frame Models\ndataclassやpydanticを使ったAPIも用意されている。ノードごとに制御するというよりも、データフレーム全体に対してスキーマを設定し検証するという流れのように見える。\n\n\nCode\nfrom pandera.typing import Series\n\nclass Schema(pa.DataFrameModel):\n\n    column1: int = pa.Field(le=10)\n    column2: float = pa.Field(lt=-1.2)\n    column3: str = pa.Field(str_startswith=\"value_\")\n\n    @pa.check(\"column3\")\n    def column_3_check(cls, series: Series[str]) -&gt; Series[bool]:\n        \"\"\"Check that column3 values have two elements after being split with '_'\"\"\"\n        return series.str.split(\"_\", expand=True).shape[1] == 2\n\n\nSchema.validate(df)\n#&gt;    column1  column2  column3\n#&gt; 0        1     -1.3  value_1\n#&gt; 1        4     -1.4  value_2\n#&gt; 2        0     -2.9  value_3\n#&gt; 3       10    -10.1  value_2\n#&gt; 4        9    -20.4  value_1",
    "crumbs": [
      "Python",
      "pandera",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/pandera/index.html#informative-errors",
    "href": "contents/libs/Python/pandera/index.html#informative-errors",
    "title": "union pandera",
    "section": "2.3 Informative Errors",
    "text": "2.3 Informative Errors\n\n\nCode\nsimple_schema = pa.DataFrameSchema({\n    \"column1\": pa.Column(\n        int, pa.Check(lambda x: 0 &lt;= x &lt;= 10, element_wise=True,\n                   error=\"range checker [0, 10]\"))\n})\n\n# validation rule violated\nfail_check_df = pd.DataFrame({\n    \"column1\": [-20, 5, 10, 30],\n})\n\ntry:\n  simple_schema(fail_check_df)\nexcept Exception as e:\n  print(e)\n#&gt; Column 'column1' failed element-wise validator number 0: &lt;Check &lt;lambda&gt;: range checker [0, 10]&gt; failure cases: -20, 30\n\n\nカラムがないという情報も出力してくれる。\n\n\nCode\n# column name mis-specified\nwrong_column_df = pd.DataFrame({\n   \"foo\": [\"bar\"] * 10,\n   \"baz\": [1] * 10\n})\n\ntry:\n  simple_schema.validate(wrong_column_df)\nexcept Exception as e: \n  print(e)\n#&gt; column 'column1' not in dataframe. Columns in dataframe: ['foo', 'baz']",
    "crumbs": [
      "Python",
      "pandera",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/pandera/index.html#checking-column-properties",
    "href": "contents/libs/Python/pandera/index.html#checking-column-properties",
    "title": "union pandera",
    "section": "3.1 Checking column properties",
    "text": "3.1 Checking column properties\n\n\nCode\nimport pandera as pa\n\ncheck_lt_10 = pa.Check(lambda s: s &lt;= 10)\n\nschema = pa.DataFrameSchema({\"column1\": pa.Column(int, check_lt_10)})\n\ntry:\n  schema.validate(pd.DataFrame({\"column1\": range(10)}))\nexcept Exception as e:\n  print(e)\n#&gt;    column1\n#&gt; 0        0\n#&gt; 1        1\n#&gt; 2        2\n#&gt; 3        3\n#&gt; 4        4\n#&gt; 5        5\n#&gt; 6        6\n#&gt; 7        7\n#&gt; 8        8\n#&gt; 9        9\n\n\nチェックする情報を複数設定することも可能である。\n\n\nCode\nschema = pa.DataFrameSchema({\n    \"column2\": pa.Column(str, [\n        pa.Check(lambda s: s.str.startswith(\"value\")),\n        pa.Check(lambda s: s.str.split(\"_\", expand=True).shape[1] == 2)\n    ]),\n})",
    "crumbs": [
      "Python",
      "pandera",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/pandera/index.html#built-in-checks",
    "href": "contents/libs/Python/pandera/index.html#built-in-checks",
    "title": "union pandera",
    "section": "3.2 Built-in checks",
    "text": "3.2 Built-in checks\n\n\nCode\nimport pandera as pa\nfrom pandera import Column, Check, DataFrameSchema\n\nschema = DataFrameSchema({\n    \"small_values\": Column(float, Check.less_than(100)),\n    \"one_to_three\": Column(int, Check.isin([1, 2, 3])),\n    \"phone_number\": Column(str, Check.str_matches(r'^[a-z0-9-]+$')),\n})",
    "crumbs": [
      "Python",
      "pandera",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/pandera/index.html#vectorized-vs.-element-wise-checks",
    "href": "contents/libs/Python/pandera/index.html#vectorized-vs.-element-wise-checks",
    "title": "union pandera",
    "section": "3.3 Vectorized vs. Element-wise checks",
    "text": "3.3 Vectorized vs. Element-wise checks\nデフォルトでは、Checkオブジェクトはpd.Seriesで動作する。\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\nschema = pa.DataFrameSchema({\n    \"a\": pa.Column(\n        int,\n        checks=[\n            # a vectorized check that returns a bool\n            pa.Check(lambda s: s.mean() &gt; 5, element_wise=False),\n\n            # a vectorized check that returns a boolean series\n            pa.Check(lambda s: s &gt; 0, element_wise=False),\n\n            # an element-wise check that returns a bool\n            pa.Check(lambda x: x &gt; 0, element_wise=True),\n        ]\n    ),\n})\ndf = pd.DataFrame({\"a\": [4, 4, 5, 6, 6, 7, 8, 9]})\nschema.validate(df)\n#&gt;    a\n#&gt; 0  4\n#&gt; 1  4\n#&gt; 2  5\n#&gt; 3  6\n#&gt; 4  6\n#&gt; 5  7\n#&gt; 6  8\n#&gt; 7  9",
    "crumbs": [
      "Python",
      "pandera",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/pandera/index.html#column-check-groups",
    "href": "contents/libs/Python/pandera/index.html#column-check-groups",
    "title": "union pandera",
    "section": "3.4 Column Check Groups",
    "text": "3.4 Column Check Groups\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\nschema = pa.DataFrameSchema({\n    \"height_in_feet\": pa.Column(\n        float, [\n            # groupby as a single column\n            pa.Check(\n                lambda g: g[False].mean() &gt; 6,\n                groupby=\"age_less_than_20\"),\n\n            # define multiple groupby columns\n            pa.Check(\n                lambda g: g[(True, \"F\")].sum() == 9.1,\n                groupby=[\"age_less_than_20\", \"sex\"]),\n\n            # groupby as a callable with signature:\n            # (DataFrame) -&gt; DataFrameGroupBy\n            pa.Check(\n                lambda g: g[(False, \"M\")].median() == 6.75,\n                groupby=lambda df: (\n                    df.assign(age_less_than_15=lambda d: d[\"age\"] &lt; 15)\n                    .groupby([\"age_less_than_15\", \"sex\"]))),\n        ]),\n    \"age\": pa.Column(int, pa.Check(lambda s: s &gt; 0)),\n    \"age_less_than_20\": pa.Column(bool),\n    \"sex\": pa.Column(str, pa.Check(lambda s: s.isin([\"M\", \"F\"])))\n})\n\ndf = (\n    pd.DataFrame({\n        \"height_in_feet\": [6.5, 7, 6.1, 5.1, 4],\n        \"age\": [25, 30, 21, 18, 13],\n        \"sex\": [\"M\", \"M\", \"F\", \"F\", \"F\"]\n    })\n    .assign(age_less_than_20=lambda x: x[\"age\"] &lt; 20)\n)\n\nschema.validate(df)\n#&gt;    height_in_feet  age sex  age_less_than_20\n#&gt; 0             6.5   25   M             False\n#&gt; 1             7.0   30   M             False\n#&gt; 2             6.1   21   F             False\n#&gt; 3             5.1   18   F              True\n#&gt; 4             4.0   13   F              True",
    "crumbs": [
      "Python",
      "pandera",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/pandera/index.html#wide-checks",
    "href": "contents/libs/Python/pandera/index.html#wide-checks",
    "title": "union pandera",
    "section": "3.5 Wide Checks",
    "text": "3.5 Wide Checks\n横持ちになっているtidyではないデータに対しても対応しているとのこと。\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\n\ndf = pd.DataFrame({\n    \"height\": [5.6, 6.4, 4.0, 7.1],\n    \"group\": [\"A\", \"B\", \"A\", \"B\"],\n})\n\nschema = pa.DataFrameSchema({\n    \"height\": pa.Column(\n        float,\n        pa.Check(lambda g: g[\"A\"].mean() &lt; g[\"B\"].mean(), groupby=\"group\")\n    ),\n    \"group\": pa.Column(str)\n})\n\nschema.validate(df)\n#&gt;    height group\n#&gt; 0     5.6     A\n#&gt; 1     6.4     B\n#&gt; 2     4.0     A\n#&gt; 3     7.1     B\n\n\n\n\nCode\ndf = pd.DataFrame({\n    \"height_A\": [5.6, 4.0],\n    \"height_B\": [6.4, 7.1],\n})\n\nschema = pa.DataFrameSchema(\n    columns={\n        \"height_A\": pa.Column(float),\n        \"height_B\": pa.Column(float),\n    },\n    # define checks at the DataFrameSchema-level\n    checks=pa.Check(\n        lambda df: df[\"height_A\"].mean() &lt; df[\"height_B\"].mean()\n    )\n)\n\nschema.validate(df)\n#&gt;    height_A  height_B\n#&gt; 0       5.6       6.4\n#&gt; 1       4.0       7.1",
    "crumbs": [
      "Python",
      "pandera",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/pandera/index.html#raise-warnings-instead-of-error-on-chekc-failure",
    "href": "contents/libs/Python/pandera/index.html#raise-warnings-instead-of-error-on-chekc-failure",
    "title": "union pandera",
    "section": "3.6 Raise Warnings Instead of Error on Chekc Failure",
    "text": "3.6 Raise Warnings Instead of Error on Chekc Failure\n\n\nCode\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport pandera as pa\n\nfrom scipy.stats import normaltest\n\n\nnp.random.seed(1000)\n\ndf = pd.DataFrame({\n    \"var1\": np.random.normal(loc=0, scale=1, size=1000),\n    \"var2\": np.random.uniform(low=0, high=10, size=1000),\n})\n\nnormal_check = pa.Hypothesis(\n    test=normaltest,\n    samples=\"normal_variable\",\n    # null hypotheses: sample comes from a normal distribution. The\n    # relationship function checks if we cannot reject the null hypothesis,\n    # i.e. the p-value is greater or equal to alpha.\n    relationship=lambda stat, pvalue, alpha=0.05: pvalue &gt;= alpha,\n    error=\"normality test\",\n    raise_warning=True,\n)\n\nschema = pa.DataFrameSchema(\n    columns={\n        \"var1\": pa.Column(checks=normal_check),\n        \"var2\": pa.Column(checks=normal_check),\n    }\n)\n\n# catch and print warnings\nwith warnings.catch_warnings(record=True) as caught_warnings:\n    warnings.simplefilter(\"always\")\n    validated_df = schema(df)\n    for warning in caught_warnings:\n        print(warning.message)\n#&gt; Column 'var2' failed series or dataframe validator 0: &lt;Check normaltest: normality test&gt;",
    "crumbs": [
      "Python",
      "pandera",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/pandera/index.html#column-validation",
    "href": "contents/libs/Python/pandera/index.html#column-validation",
    "title": "union pandera",
    "section": "4.1 Column Validation",
    "text": "4.1 Column Validation\n\nNULL値を許すのか\n重複を許すのか\n型を強制するのか\n\n\n4.1.1 Null Values in Columns\nデフォルトは許容しない。\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport pandera as pa\n\nfrom pandera import Check, Column, DataFrameSchema\n\ndf = pd.DataFrame({\"column1\": [5, 1, np.nan]})\n\nnon_null_schema = DataFrameSchema({\n    \"column1\": Column(float, Check(lambda x: x &gt; 0))\n})\n\ntry:\n  non_null_schema.validate(df)\nexcept Exception as e:\n  print(e)\n#&gt; non-nullable series 'column1' contains null values:\n#&gt; 2   NaN\n#&gt; Name: column1, dtype: float64\n\n\nNULLを許容する。\n\n\nCode\nnull_schema = DataFrameSchema({\n    \"column1\": Column(float, Check(lambda x: x &gt; 0), nullable=True)\n})\n\nprint(null_schema.validate(df))\n#&gt;    column1\n#&gt; 0      5.0\n#&gt; 1      1.0\n#&gt; 2      NaN\n\n\n\n\n4.1.2 Coercing Types on Columns\n型変換を強制するとヴァリデーションの前に変換をおこなう。\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\nfrom pandera import Column, DataFrameSchema\n\ndf = pd.DataFrame({\"column1\": [1, 2, 3]})\nschema = DataFrameSchema({\"column1\": Column(str, coerce=True)})\n\nvalidated_df = schema.validate(df)\nassert isinstance(validated_df.column1.iloc[0], str)\n\n\n型変換の強制はNULL値を含む時の処理は含まれていない。\n\n\nCode\ndf = pd.DataFrame({\"column1\": [1., 2., 3, np.nan]})\nschema = DataFrameSchema({\n    \"column1\": Column(int, coerce=True, nullable=True)\n})\n\ntry:\n  validated_df = schema.validate(df)\nexcept Exception as e:\n  print(e)\n#&gt; Error while coercing 'column1' to type int64: Could not coerce &lt;class 'pandas.core.series.Series'&gt; data_container into type int64:\n#&gt;    index  failure_case\n#&gt; 0      3           NaN\n\n\nこのような場合にはより抽象的なデータ型を使うことにする。\n\n\nCode\nschema_object = DataFrameSchema({\n    \"column1\": Column(object, coerce=True, nullable=True)\n})\nschema_float = DataFrameSchema({\n    \"column1\": Column(float, coerce=True, nullable=True)\n})\n\nprint(schema_object.validate(df).dtypes)\n#&gt; column1    object\n#&gt; dtype: object\nprint(schema_float.validate(df).dtypes)\n#&gt; column1    float64\n#&gt; dtype: object\n\n\n\n\n4.1.3 Required Columns\n必須カラムを作ることができる。\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\nfrom pandera import Column, DataFrameSchema\n\ndf = pd.DataFrame({\"column2\": [\"hello\", \"pandera\"]})\nschema = DataFrameSchema({\n    \"column1\": Column(int, required=False),\n    \"column2\": Column(str)\n})\n\nvalidated_df = schema.validate(df)\nprint(validated_df)\n#&gt;    column2\n#&gt; 0    hello\n#&gt; 1  pandera\n\n\n\n\nCode\nschema = DataFrameSchema({\n    \"column1\": Column(int),\n    \"column2\": Column(str),\n})\n\ntry:\n  schema.validate(df)\nexcept Exception as e:\n  print(e)\n#&gt; column 'column1' not in dataframe. Columns in dataframe: ['column2']\n\n\n\n\n4.1.4 Ordered Columns Stand-alone Column Validation\nカラム単位でバリデーションが行える。\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\ndf = pd.DataFrame({\n    \"column1\": [1, 2, 3],\n    \"column2\": [\"a\", \"b\", \"c\"],\n})\n\ncolumn1_schema = pa.Column(int, name=\"column1\")\ncolumn2_schema = pa.Column(str, name=\"column2\")\n\n# pass the dataframe as an argument to the Column object callable\ndf = column1_schema(df)\nvalidated_df = column2_schema(df)\n\n# or explicitly use the validate method\ndf = column1_schema.validate(df)\nvalidated_df = column2_schema.validate(df)\n\n# use the DataFrame.pipe method to validate two columns\nvalidated_df = df.pipe(column1_schema).pipe(column2_schema)\n\n\n\n\n4.1.5 Column Regex Pattern Matching\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport pandera as pa\n\ncategories = [\"A\", \"B\", \"C\"]\n\nnp.random.seed(100)\n\ndataframe = pd.DataFrame({\n    \"cat_var_1\": np.random.choice(categories, size=100),\n    \"cat_var_2\": np.random.choice(categories, size=100),\n    \"num_var_1\": np.random.uniform(0, 10, size=100),\n    \"num_var_2\": np.random.uniform(20, 30, size=100),\n})\n\nschema = pa.DataFrameSchema({\n    \"num_var_.+\": pa.Column(\n        float,\n        checks=pa.Check.greater_than_or_equal_to(0),\n        regex=True,\n    ),\n    \"cat_var_.+\": pa.Column(\n        pa.Category,\n        checks=pa.Check.isin(categories),\n        coerce=True,\n        regex=True,\n    ),\n})\n\nprint(schema.validate(dataframe).head())\n#&gt;   cat_var_1 cat_var_2  num_var_1  num_var_2\n#&gt; 0         A         A   6.804147  24.743304\n#&gt; 1         A         C   3.684308  22.774633\n#&gt; 2         A         C   5.911288  28.416588\n#&gt; 3         C         A   4.790627  21.951250\n#&gt; 4         C         B   4.504166  28.563142\n\n\n\n\n4.1.6 Handling DataFrame Columns not in the Schema\nスキーマで指定されていないカラムはチェックがされないのがデフォルトである。 strict=trueを指定するとスキーマで指定しないことがエラーとなる\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\nfrom pandera import Column, DataFrameSchema\n\nschema = DataFrameSchema(\n    {\"column1\": Column(int)},\n    strict=True)\n\ndf = pd.DataFrame({\"column2\": [1, 2, 3]})\n\ntry:\n  schema.validate(df)\nexcept Exception as e:\n  print(e)\n#&gt; column 'column2' not in DataFrameSchema {'column1': &lt;Schema Column(name=column1, type=DataType(int64))&gt;}\n\n\nもしくは指定していないカラムをドロップさせることができうｒ。\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\nfrom pandera import Column, DataFrameSchema\n\ndf = pd.DataFrame({\"column1\": [\"drop\", \"me\"],\"column2\": [\"keep\", \"me\"]})\nschema = DataFrameSchema({\"column2\": Column(str)}, strict='filter')\n\nvalidated_df = schema.validate(df)\nprint(validated_df)\n#&gt;   column2\n#&gt; 0    keep\n#&gt; 1      me\n\n\n\n\n4.1.7 Validation the order of the columns\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\nschema = pa.DataFrameSchema(\n    columns={\"a\": pa.Column(int), \"b\": pa.Column(int)}, ordered=True\n)\ndf = pd.DataFrame({\"b\": [1], \"a\": [1]})\n\n\ntry:\n  print(schema.validate(df))\nexcept Exception as e:\n  print(e)\n#&gt; column 'b' out-of-order\n\n\n\n\n4.1.8 validating the join uniquness of columns\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\nschema = pa.DataFrameSchema(\n    columns={col: pa.Column(int) for col in [\"a\", \"b\", \"c\"]},\n    unique=[\"a\", \"c\"],\n)\ndf = pd.DataFrame.from_records([\n    {\"a\": 1, \"b\": 2, \"c\": 3},\n    {\"a\": 1, \"b\": 2, \"c\": 3},\n])\n\ntry:\n  schema.validate(df)\nexcept Exception as e:\n  print(e)\n#&gt; columns '('a', 'c')' not unique:\n#&gt;    a  c\n#&gt; 0  1  3\n#&gt; 1  1  3\n\n\nゆにーくのコントールも可能である。\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\nschema = pa.DataFrameSchema(\n    columns={col: pa.Column(int) for col in [\"a\", \"b\", \"c\"]},\n    unique=[\"a\", \"c\"],\n    report_duplicates = \"exclude_first\",\n)\ndf = pd.DataFrame.from_records([\n    {\"a\": 1, \"b\": 2, \"c\": 3},\n    {\"a\": 1, \"b\": 2, \"c\": 3},\n])\n\ntry:\n  schema.validate(df)\nexcept Exception as e:\n  print(e)\n#&gt; columns '('a', 'c')' not unique:\n#&gt;    a  c\n#&gt; 1  1  3\n\n\n\n\n4.1.9 Adding missin columns\nadd_missing_columns=Trueを追加することで、NULL値でカラムを追加することができる。\n\n\nCode\nimport pandas as pd\nimport pandera as pa\n\nschema = pa.DataFrameSchema(\n    columns={\n        \"a\": pa.Column(int),\n        \"b\": pa.Column(int, default=1),\n        \"c\": pa.Column(float, nullable=True),\n    },\n    add_missing_columns=True,\n    coerce=True,\n)\ndf = pd.DataFrame({\"a\": [1, 2, 3]})\nprint(schema.validate(df))\n#&gt;    a  b   c\n#&gt; 0  1  1 NaN\n#&gt; 1  2  1 NaN\n#&gt; 2  3  1 NaN",
    "crumbs": [
      "Python",
      "pandera",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/duckdb/01_theta_join.html",
    "href": "contents/libs/Python/duckdb/01_theta_join.html",
    "title": "チュートリアル",
    "section": "",
    "text": "#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.4\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function\n\n\n1 はじめに\n\n\n2 theta join\n\n\nCode\nimport duckdb\nimport pandas as pd\n\n# サンプルデータ\ndata = {\n    'employee_id': [1, 2, 3, 4, 5],\n    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'manager_id': [None, 1, 1, 2, 3],  # 上司ID\n    'age': [50, 40, 38, 35, 30]  # 年齢\n}\ndf = pd.DataFrame(data)\n\n# DuckDBに接続\ncon = duckdb.connect(database=':memory:')\n\n# データフレームを DuckDB にロード\ncon.register('employees', df)\n#&gt; &lt;duckdb.duckdb.DuckDBPyConnection object at 0x0000025047AA3E70&gt;\n\n# θ結合SQL（部下の年齢が上司より5歳以上若い）\nquery = \"\"\"\nSELECT e1.employee_id AS employee_id, e1.name AS employee_name, e1.age AS emp_age,\n       e1.manager_id, e2.name AS manager_name, e2.age AS mgr_age\nFROM employees e1\nJOIN employees e2 ON e1.manager_id = e2.employee_id\nWHERE e1.age &lt;= e2.age - 5\n\"\"\"\n\n# クエリを実行\nresult = con.execute(query).fetchdf()\n\n# 結果を表示\nprint(result)\n#&gt;    employee_id employee_name  emp_age  manager_id manager_name  mgr_age\n#&gt; 0            3       Charlie       38         1.0        Alice       50\n#&gt; 1            4         David       35         2.0          Bob       40\n#&gt; 2            5           Eve       30         3.0      Charlie       38\n#&gt; 3            2           Bob       40         1.0        Alice       50\n\n# 接続を閉じる\ncon.close()\n\n#&gt; (10, 20)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "duckdb",
      "チュートリアル"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html",
    "title": "23 Arrow",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/r4ds2e/Import/\")\nCode\nlibrary(tidyverse)\nlibrary(arrow)\nlibrary(dbplyr)\nlibrary(duckdb)\nlibrary(here)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "ALL",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html#advantages",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html#advantages",
    "title": "23 Arrow",
    "section": "4.1 Advantages",
    "text": "4.1 Advantages\n\nParquetは効率的なエンコードでファイルサイズを小さくする\nCSVに持たせることが出来ない情報を持たせることができる\nカラム指向である\nチャンク化されていても,異なるチャンクを同時扱うことが出来る",
    "crumbs": [
      "ALL",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html#partitioning",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html#partitioning",
    "title": "23 Arrow",
    "section": "4.2 Partitioning",
    "text": "4.2 Partitioning\n\nデータが巨大になるほど1つのファイルで管理するのが困難となる\nチャンク化は20MB以下になることと2GB以上になることを避ける\nChekoutYearを軸にチャンク化をおこなう\nISBNがint64やnullだと出力が出来ないのでここでは文字列として出力する\n\n\n\nCode\npq_path &lt;- here(cur_dir, \"data/seattle-library-checkouts\")\nseattle_csv |&gt; \n    group_by(CheckoutYear) |&gt; \n    write_dataset(path = pq_path, format = \"parquet\")\n\n\n生成したデータを確認する.\n\n\nCode\ntibble(\n    files = list.files(pq_path, recursive = TRUE), \n    size_MB = file.size(here(pq_path, files)) / 1024^2\n)\n#&gt; # A tibble: 18 × 2\n#&gt;    files                            size_MB\n#&gt;    &lt;chr&gt;                              &lt;dbl&gt;\n#&gt;  1 CheckoutYear=2005/part-0.parquet    109.\n#&gt;  2 CheckoutYear=2006/part-0.parquet    164.\n#&gt;  3 CheckoutYear=2007/part-0.parquet    178.\n#&gt;  4 CheckoutYear=2008/part-0.parquet    195.\n#&gt;  5 CheckoutYear=2009/part-0.parquet    214.\n#&gt;  6 CheckoutYear=2010/part-0.parquet    222.\n#&gt;  7 CheckoutYear=2011/part-0.parquet    239.\n#&gt;  8 CheckoutYear=2012/part-0.parquet    249.\n#&gt;  9 CheckoutYear=2013/part-0.parquet    269.\n#&gt; 10 CheckoutYear=2014/part-0.parquet    282.\n#&gt; 11 CheckoutYear=2015/part-0.parquet    294.\n#&gt; 12 CheckoutYear=2016/part-0.parquet    300.\n#&gt; 13 CheckoutYear=2017/part-0.parquet    304.\n#&gt; 14 CheckoutYear=2018/part-0.parquet    292.\n#&gt; 15 CheckoutYear=2019/part-0.parquet    288.\n#&gt; 16 CheckoutYear=2020/part-0.parquet    151.\n#&gt; 17 CheckoutYear=2021/part-0.parquet    229.\n#&gt; 18 CheckoutYear=2022/part-0.parquet    241.",
    "crumbs": [
      "ALL",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html#using-dplyr-with-arrow",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html#using-dplyr-with-arrow",
    "title": "23 Arrow",
    "section": "4.3 Using dplyr with arrow",
    "text": "4.3 Using dplyr with arrow\n\n\nCode\nseattle_pq &lt;- open_dataset(pq_path)\nseattle_pq\n#&gt; FileSystemDataset with 18 Parquet files\n#&gt; UsageClass: string\n#&gt; CheckoutType: string\n#&gt; MaterialType: string\n#&gt; CheckoutMonth: int64\n#&gt; Checkouts: int64\n#&gt; Title: string\n#&gt; ISBN: string\n#&gt; Creator: string\n#&gt; Subjects: string\n#&gt; Publisher: string\n#&gt; PublicationYear: string\n#&gt; CheckoutYear: int32\n\n\ndplyrの構文がそのまま使える. collectを実行したときに, コードが実行される.\n\n\nCode\nquery &lt;- \n    seattle_pq |&gt; \n    filter(CheckoutYear &gt;= 2008, MaterialType == \"BOOK\") |&gt; \n    group_by(CheckoutYear, CheckoutMonth) |&gt; \n    summarise(TotalCheckouts = sum(Checkouts)) |&gt; \n    arrange(CheckoutYear, CheckoutMonth)\nquery\n#&gt; FileSystemDataset (query)\n#&gt; CheckoutYear: int32\n#&gt; CheckoutMonth: int64\n#&gt; TotalCheckouts: int64\n#&gt; \n#&gt; * Grouped by CheckoutYear\n#&gt; * Sorted by CheckoutYear [asc], CheckoutMonth [asc]\n#&gt; See $.data for the source Arrow object\n\n\n前述したように, collectで評価される.\n\n\nCode\nquery |&gt; collect()\n#&gt; # A tibble: 178 × 3\n#&gt; # Groups:   CheckoutYear [15]\n#&gt;    CheckoutYear CheckoutMonth TotalCheckouts\n#&gt;           &lt;int&gt;         &lt;int&gt;          &lt;int&gt;\n#&gt;  1         2008             1         343384\n#&gt;  2         2008             2         315806\n#&gt;  3         2008             3         291710\n#&gt;  4         2008             4         341452\n#&gt;  5         2008             5         328072\n#&gt;  6         2008             6         356621\n#&gt;  7         2008             7         388959\n#&gt;  8         2008             8         363506\n#&gt;  9         2008             9         353561\n#&gt; 10         2008            10         354779\n#&gt; # ℹ 168 more rows",
    "crumbs": [
      "ALL",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html#performance",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html#performance",
    "title": "23 Arrow",
    "section": "4.4 Performance",
    "text": "4.4 Performance\nデータが絞られるほどCSVよりも高速に動作する.\n\n\nCode\nbench::mark(\n    {\n        seattle_csv |&gt; \n        filter(CheckoutYear &gt;= 2008, MaterialType == \"BOOK\") |&gt; \n        group_by(CheckoutYear, CheckoutMonth) |&gt; \n        summarise(TotalCheckouts = sum(Checkouts)) |&gt; \n        arrange(CheckoutYear, CheckoutMonth) |&gt; \n        collect()\n    }, \n    {\n        seattle_pq |&gt; \n        filter(CheckoutYear &gt;= 2008, MaterialType == \"BOOK\") |&gt; \n        group_by(CheckoutYear, CheckoutMonth) |&gt; \n        summarise(TotalCheckouts = sum(Checkouts)) |&gt; \n        arrange(CheckoutYear, CheckoutMonth) |&gt; \n        collect()\n    }, \n    check = FALSE, \n    max_iterations = 1\n)\n#&gt; Warning: Some expressions had a GC in every iteration; so filtering is\n#&gt; disabled.\n#&gt; # A tibble: 2 × 6\n#&gt;   expression                             min median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt;                           &lt;bch&gt; &lt;bch:&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 { collect(arrange(summarise(group_b… 18.3s  18.3s    0.0547     218KB    0    \n#&gt; 2 { collect(arrange(summarise(group_b…  2.1s   2.1s    0.475      218KB    0.475",
    "crumbs": [
      "ALL",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/r4ds2e/Import/ch23_arrow.html#advance",
    "href": "contents/website/r4ds2e/Import/ch23_arrow.html#advance",
    "title": "23 Arrow",
    "section": "4.5 Advance",
    "text": "4.5 Advance\nparquetはカラム指向なので同じカラム指向のリレーショナルデータベースであるDuckDBと非常に相性がよい. DuckDBはカラム指向でデータの高速処理を指向して開発されたデータベースである. サーバーレスで動作する.\n\n\nCode\nseattle_pq |&gt; \n  to_duckdb() |&gt;\n  filter(CheckoutYear &gt;= 2008, MaterialType == \"BOOK\") |&gt;\n  group_by(CheckoutYear) |&gt;\n  summarize(TotalCheckouts = sum(Checkouts)) |&gt;\n  arrange(desc(CheckoutYear)) |&gt;\n  collect() |&gt; \n  system.time()\n#&gt; Warning: Missing values are always removed in SQL aggregation functions.\n#&gt; Use `na.rm = TRUE` to silence this warning\n#&gt; This warning is displayed once every 8 hours.\n#&gt;    user  system elapsed \n#&gt;    1.44    0.21    2.69",
    "crumbs": [
      "ALL",
      "R for Data Science 2e",
      "Import",
      "23 Arrow"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch12_choropleths.html",
    "href": "contents/website/leaflet4r/chapters/ch12_choropleths.html",
    "title": "Choropleths",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Choropleths"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch12_choropleths.html#basic-stats-map",
    "href": "contents/website/leaflet4r/chapters/ch12_choropleths.html#basic-stats-map",
    "title": "Choropleths",
    "section": "1.1 Basic stats map",
    "text": "1.1 Basic stats map\n\n\nCode\nm &lt;- leaflet(states) %&gt;%\n  setView(-96, 37.8, 4) %&gt;%\n  addProviderTiles(\"MapBox\", options = providerTileOptions(\n    id = \"mapbox.light\",\n    accessToken = Sys.getenv('MAPBOX_ACCESS_TOKEN')))\n\nm\n\n\n\n\n\n\n\n\nCode\nm %&gt;% addPolygons()",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Choropleths"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch10_Legends.html",
    "href": "contents/website/leaflet4r/chapters/ch10_Legends.html",
    "title": "Legends",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")\n\n\n\n1 Lgends\n\n\nCode\ncountries &lt;- sf::st_read(\"https://rstudio.github.io/leaflet/json/countries.geojson\")\n#&gt; Reading layer `countries' from data source \n#&gt;   `https://rstudio.github.io/leaflet/json/countries.geojson' \n#&gt;   using driver `GeoJSON'\n#&gt; Simple feature collection with 177 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513\n#&gt; Geodetic CRS:  WGS 84\n\n\n\n\nCode\nmap &lt;- leaflet(countries) %&gt;% addTiles()\n\n\naddLegend関数を使うことで凡例を追加することができる.\n\n\nCode\npal &lt;- colorNumeric(\n  palette = \"YlGnBu\",\n  domain = countries$gdp_md_est\n)\nmap %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~pal(gdp_md_est)\n  ) %&gt;%\n  addLegend(\n      \"bottomright\", \n      pal = pal, \n      values = ~gdp_md_est,\n      title = \"Est. GDP (2010)\",\n      labFormat = labelFormat(prefix = \"$\"),\n      opacity = 1\n  )\n\n\n\n\n\n\n異なる凡例を使うのも簡単である.\n\n\nCode\nqpal &lt;- colorQuantile(\"RdYlBu\", countries$gdp_md_est, n = 5)\nmap %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~qpal(gdp_md_est)\n  ) %&gt;%\n  addLegend(pal = qpal, values = ~gdp_md_est, opacity = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Legends"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch08_shiny_integration.html",
    "href": "contents/website/leaflet4r/chapters/ch08_shiny_integration.html",
    "title": "Shiny Integration",
    "section": "",
    "text": "Quartoのプロジェクトの中に置いていると shinyを動かすことができないんだけど、 基本的にはleafletでもshinyで使えますという話し.\nまた, Proxyとしてマウスの位置やクリック位置などを検出することができるので, それを使えばリッチなWebアプリが作成できますという主旨のことが書いてある。\n作るときに忘れず参照することが大事である.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Shiny Integration"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch06_geojson.html",
    "href": "contents/website/leaflet4r/chapters/ch06_geojson.html",
    "title": "Working with GeoJSON and TopoJSON",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Working with GeoJSON and TopoJSON"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch06_geojson.html#styling-raw-geojson-topojson",
    "href": "contents/website/leaflet4r/chapters/ch06_geojson.html#styling-raw-geojson-topojson",
    "title": "Working with GeoJSON and TopoJSON",
    "section": "1.1 Styling raw GeoJSON / TopoJSON",
    "text": "1.1 Styling raw GeoJSON / TopoJSON\nGeoJSONなどはいくつかの方法でスタイルを直接変更することができる.\n\n\nCode\nlibrary(jsonlite)\n#&gt; \n#&gt; Attaching package: 'jsonlite'\n#&gt; The following object is masked from 'package:purrr':\n#&gt; \n#&gt;     flatten\n#&gt; The following objects are masked from 'package:rjson':\n#&gt; \n#&gt;     fromJSON, toJSON\n#&gt; The following object is masked from 'package:shiny':\n#&gt; \n#&gt;     validate\n\n# From http://data.okfn.org/data/datasets/geo-boundaries-world-110m\ngeojson &lt;- \n    readLines(\n        \"https://rstudio.github.io/leaflet/json/countries.geojson\", \n        warn = FALSE) %&gt;%\n  paste(collapse = \"\\n\") %&gt;%\n  fromJSON(simplifyVector = FALSE)\n\n# Default styles for all features\ngeojson$style = list(\n  weight = 1,\n  color = \"#555555\",\n  opacity = 1,\n  fillOpacity = 0.8\n)\n\n# Gather GDP estimate from all countries\ngdp_md_est &lt;- sapply(geojson$features, function(feat) {\n  feat$properties$gdp_md_est\n})\n# Gather population estimate from all countries\npop_est &lt;- sapply(geojson$features, function(feat) {\n  max(1, feat$properties$pop_est)\n})\n\n# Color by per-capita GDP using quantiles\npal &lt;- colorQuantile(\"Greens\", gdp_md_est / pop_est)\n# Add a properties$style list to each feature\ngeojson$features &lt;- lapply(geojson$features, function(feat) {\n  feat$properties$style &lt;- list(\n    fillColor = pal(\n      feat$properties$gdp_md_est / max(1, feat$properties$pop_est)\n    )\n  )\n  feat\n})\n\n# Add the now-styled GeoJSON object to the map\nleaflet() %&gt;% addGeoJSON(geojson)",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Working with GeoJSON and TopoJSON"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html",
    "href": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html",
    "title": "Popups and Shapes",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Popups and Shapes"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html#customizing-marker-labels",
    "href": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html#customizing-marker-labels",
    "title": "Popups and Shapes",
    "section": "2.1 Customizing Marker Labels",
    "text": "2.1 Customizing Marker Labels\n\n\nCode\n# Change Text Size and text Only and also a custom CSS\nleaflet() %&gt;% addTiles() %&gt;% setView(-118.456554, 34.09, 13) %&gt;%\n  addMarkers(\n    lng = -118.456554, lat = 34.105,\n    label = \"Default Label\",\n    labelOptions = labelOptions(noHide = T)) %&gt;%\n  addMarkers(\n    lng = -118.456554, lat = 34.095,\n    label = \"Label w/o surrounding box\",\n    labelOptions = labelOptions(noHide = T, textOnly = TRUE)) %&gt;%\n  addMarkers(\n    lng = -118.456554, lat = 34.085,\n    label = \"label w/ textsize 15px\",\n    labelOptions = labelOptions(noHide = T, textsize = \"15px\")) %&gt;%\n  addMarkers(\n    lng = -118.456554, lat = 34.075,\n    label = \"Label w/ custom CSS style\",\n    labelOptions = labelOptions(noHide = T, direction = \"bottom\",\n      style = list(\n        \"color\" = \"red\",\n        \"font-family\" = \"serif\",\n        \"font-style\" = \"italic\",\n        \"box-shadow\" = \"3px 3px rgba(0,0,0,0.25)\",\n        \"font-size\" = \"12px\",\n        \"border-color\" = \"rgba(0,0,0,0.5)\"\n      )))",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Popups and Shapes"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html#labels-without-markers",
    "href": "contents/website/leaflet4r/chapters/ch04_popups_and_shapes.html#labels-without-markers",
    "title": "Popups and Shapes",
    "section": "2.2 Labels without markers",
    "text": "2.2 Labels without markers\n\n\nCode\nleaflet() %&gt;%\n  addTiles() %&gt;%\n  addLabelOnlyMarkers(\n    data = breweries91,\n    label = as.character(breweries91$brewery),\n    group = \"brew\",\n    labelOptions = leaflet::labelOptions(\n      noHide = TRUE,\n      direction = \"bottom\",\n      textOnly = TRUE,\n      offset = c(0, -10),\n      opacity = 1\n    )\n  )",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Popups and Shapes"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch02_basemaps.html",
    "href": "contents/website/leaflet4r/chapters/ch02_basemaps.html",
    "title": "Basemap",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)\n\n\n\n1 Default\n\nOpenstreet Mapが使われる.\n\n\n\nCode\nm &lt;- leaflet() %&gt;% setView(lng = -71.0589, lat = 42.3601, zoom = 12)\nm %&gt;% addTiles()\n\n\n\n\n\n\n\n\n2 Third-Party Tiles\n\n\nCode\nm %&gt;% addProviderTiles(providers$CartoDB.Positron)\n\n\n\n\n\n\n\n\n3 Custom Tile URL Template\n\n\nCode\nleaflet() %&gt;% addTiles() %&gt;% setView(-93.65, 42.0285, zoom = 4) %&gt;%\n  addWMSTiles(\n    \"http://mesonet.agron.iastate.edu/cgi-bin/wms/nexrad/n0r.cgi\",\n    layers = \"nexrad-n0r-900913\",\n    options = WMSTileOptions(format = \"image/png\", transparent = TRUE),\n    attribution = \"Weather data © 2012 IEM Nexrad\"\n  )\n\n\n\n\n\n\n\n\n4 Combining Tile Layers\nbasemapは重ねることが可能である. ちなみにvector tileをどのように重ねたらよいのかはまだわかっていない。\n\n\nCode\nm |&gt; \n  addProviderTiles(providers$MtbMap) |&gt;\n  addProviderTiles(providers$Stamen.TonerLines,\n    options = providerTileOptions(opacity = 0.35)) |&gt;\n  addProviderTiles(providers$Stamen.TonerLabels)\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Basemap"
    ]
  },
  {
    "objectID": "contents/website/udemy_docker_from_zero/index.html",
    "href": "contents/website/udemy_docker_from_zero/index.html",
    "title": "米国AI開発者がゼロから教えるDocker講座",
    "section": "",
    "text": "1 はじめに\n\nUdemyの勉強ノートです\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/website/udemy_dbt/index.html",
    "href": "contents/website/udemy_dbt/index.html",
    "title": "dbt × SQL × DuckDBで実践！データ変換の自動化・テスト駆動開発・ドキュメント生成まで、モダンデータスタックの基礎から応用まで完全マスター",
    "section": "",
    "text": "1 はじめに\n\nUdemyの勉強ノートです\n\n\n\n2 Python環境\n\ndbtはpythonのモジュールとしてインストールしています\n今回はpyenv+poetryを使用して環境構築を行います\npoetryでrequirements.txtを使ってインストールするには次を使います\n\ncat requirements.txt | xargs -n1 poetry add\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/website/r4ds2e/index.html",
    "href": "contents/website/r4ds2e/index.html",
    "title": "R for Data Science 2e",
    "section": "",
    "text": "1 はじめに\n\nR for Data Science 2eの学習ノートです\nすべてではなく気になった部分だけを記録しています\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "R for Data Science 2e",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/index.html",
    "href": "contents/website/graphic_design_with_ggplot2/index.html",
    "title": "Graphic Design with ggplot2",
    "section": "",
    "text": "1 はじめに\n\nrstudioによるggplotのワークショップである\nかなり細かいところまでやっており非常に情報量が多い\nChatGPTの世界であるがやはりハンズオン形式で勉強したい\nサイト\nパッケージだけでなくフォントのインストールも忘れずに\n\n\n\nCode\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\n\n\n\n\nCode\npackages &lt;- c(\n  \"ggplot2\", \"readr\", \"tibble\", \"tidyr\", \"forcats\", \"stringr\",\n  \"lubridate\", \"here\", \"systemfonts\", \"magick\", \"scales\", \"grid\",\n  \"grDevices\", \"colorspace\", \"viridis\", \"RColorBrewer\", \"rcartocolor\",\n  \"scico\", \"ggsci\", \"ggthemes\", \"nord\", \"MetBrewer\", \"ggrepel\",\n  \"ggforce\", \"ggtext\", \"ggdist\", \"ggbeeswarm\", \"gghalves\", \"patchwork\", \n  \"palmerpenguins\", \"rnaturalearth\", \"sf\", \"rmapshaper\", \"devtools\"\n)\n\ninstall.packages(setdiff(packages, rownames(installed.packages())))  \n\n## install {colorblindr} and requirements2\nremotes::install_github(\"wilkelab/cowplot\")\nremotes::install_github(\"clauswilke/colorblindr\")\nremotes::install_git(\"https://git.sr.ht/~hrbrmstr/albersusa\")\n\n\n\n\n2 ggplot2 examples\n\n\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"ggplot2\", \"readr\", \"tibble\", \"tidyr\", \"forcats\", \n  \"stringr\",\n  \"lubridate\", \"here\", \"systemfonts\", \"magick\", \n  \"scales\", \"grid\",\n  \"grDevices\", \"colorspace\", \"viridis\", \n  \"RColorBrewer\", \"rcartocolor\",\n  \"scico\", \"ggsci\", \"ggthemes\", \"nord\", \n  \"MetBrewer\", \"ggrepel\",\n  \"ggforce\", \"ggtext\", \"ggdist\", \"ggbeeswarm\", \n  \"gghalves\", \"patchwork\", \n  \"palmerpenguins\", \"rnaturalearth\", \"sf\", \"rmapshaper\", \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\n\n\nCode\n# フォントのデータベースを作成\nfont_import(prompt = FALSE)\n\n\n\n\nCode\n\n# フォントのデータベースをロード\nloadfonts(device = \"win\")\n\n\n\n\nCode\nbikes &lt;- read_csv(here::here(cur_dir,\"ggplot2-course-data/london-bikes-custom.csv\"))\n#&gt; Rows: 1454 Columns: 14\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr   (3): day_night, season, weather_type\n#&gt; dbl  (10): year, month, count, is_workday, is_weekend, is_holiday, temp, tem...\n#&gt; date  (1): date\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nbikes\n#&gt; # A tibble: 1,454 × 14\n#&gt;    date       day_night  year month season count is_workday is_weekend\n#&gt;    &lt;date&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n#&gt;  1 2015-01-04 day        2015     1 winter  6830          0          1\n#&gt;  2 2015-01-04 night      2015     1 winter  2404          0          1\n#&gt;  3 2015-01-05 day        2015     1 winter 14763          1          0\n#&gt;  4 2015-01-05 night      2015     1 winter  5609          1          0\n#&gt;  5 2015-01-06 day        2015     1 winter 14501          1          0\n#&gt;  6 2015-01-06 night      2015     1 winter  6112          1          0\n#&gt;  7 2015-01-07 day        2015     1 winter 16358          1          0\n#&gt;  8 2015-01-07 night      2015     1 winter  4706          1          0\n#&gt;  9 2015-01-08 day        2015     1 winter  9971          1          0\n#&gt; 10 2015-01-08 night      2015     1 winter  5630          1          0\n#&gt; # ℹ 1,444 more rows\n#&gt; # ℹ 6 more variables: is_holiday &lt;dbl&gt;, temp &lt;dbl&gt;, temp_feel &lt;dbl&gt;,\n#&gt; #   humidity &lt;dbl&gt;, wind_speed &lt;dbl&gt;, weather_type &lt;chr&gt;\n\n\n\n\nCode\ncodes &lt;- c(\n    \"0\" = \"Workday\", \n    \"1\" = \"Weekend or Holiday\"\n)\n\nggplot(bikes, aes(temp_feel, count)) +\n    geom_point(\n        color = \"black\", \n        fill = \"white\", \n        shape = 21, \n        size = 2.8\n    ) + \n    geom_point(\n        color = \"white\", \n        size = 2.2\n    ) + \n    geom_point(\n        aes(color = forcats::fct_relabel(season, str_to_title)), \n        size = 2.2, \n        alpha = .55\n    ) + \n    facet_grid(\n        day_night ~ is_workday, \n        scales = \"free_y\", \n        space = \"free_y\", \n        labeller = labeller(\n            day_night = stringr::str_to_title, \n            is_workday = codes\n        )\n    ) + \n    scale_x_continuous(\n        expand = c(.02, .02), \n        breaks = 0:6 * 5, \n        labels = \\(x) paste0(x, \"°C\")\n    ) + \n    scale_y_continuous(\n        expand = c(.1, .1), \n        limits = c(0, NA), \n        breaks = 0:5 * 10000, \n        labels = scales::comma_format()\n    ) + \n    scale_color_manual(\n        values = c(\"#3c89d9\", \"#1ec99b\", \"#F7B01B\", \"#a26e7c\"), \n        name = NULL,\n        guide = guide_legend(override.aes = list(size = 5))\n\n    ) +\n    labs(\n      x = \"Feels-Like Temperature\", y = NULL,\n      caption = \"Data: TfL (Transport for London), Jan 2015 — Dec 2016\",\n      title = \"Reported bike rents versus feels-like temperature in London per time of day, period, and season.\"\n    ) +\n    theme_light(\n        base_size = 14, \n        base_family = \"Calibri\"\n    ) + \n    theme(\n        plot.title.position = \"plot\", \n        plot.caption.position = \"plot\", \n        plot.title = element_text(face = \"bold\", size = rel(.9)) , \n        # axis.text = element_text(family = \"Tabular\"), \n        axis.title.x = element_text(hjust = 0, color = \"grey30\", margin = margin(t = 12)), \n        strip.text = element_text(face = \"bold\", size = rel(1)), \n        strip.text.y.right = element_text(face = \"bold\", size = rel(.7)), \n        panel.grid.major.x = element_blank(), \n        panel.grid.minor = element_blank(), \n        panel.spacing = unit(1.2, \"lines\"), \n        legend.position = \"top\", \n        legend.text = element_text(size = rel(1)), \n        panel.spacing.x = unit(.1, \"lines\"), \n        panel.spacing.y = unit(.1, \"lines\"),\n        \n        legend.key = element_rect(color = \"#f8f8f8\", fill = \"#f8f8f8\"), \n        legend.background = element_rect(color = \"#f8f8f8f8\", fill = \"#f8f8f8f8\"), \n        plot.background = element_rect(color = \"#f8f8f8\", fill = \"#f8f8f8\")\n        \n    )\n#&gt; Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n#&gt; not found in Windows font database\n\n#&gt; Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n#&gt; not found in Windows font database\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n#&gt; Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n#&gt; not found in Windows font database\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n#&gt; Warning in grid.Call(C_stringMetric, as.graphicsAnnot(x$label)): font family\n#&gt; not found in Windows font database\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n#&gt; Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n#&gt; font family not found in Windows font database\n\n#&gt; Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n#&gt; font family not found in Windows font database\n\n#&gt; Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n#&gt; font family not found in Windows font database\n\n#&gt; Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n#&gt; font family not found in Windows font database\n\n#&gt; Warning in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y, :\n#&gt; font family not found in Windows font database\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n#&gt; Warning in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y, : font\n#&gt; family not found in Windows font database\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html",
    "title": "Working with Colors",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggfittext\",\n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Working with Colors"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#rcartocolor",
    "href": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#rcartocolor",
    "title": "Working with Colors",
    "section": "2.1 rcartocolor",
    "text": "2.1 rcartocolor\n\n\nCode\n\n# install.packages(\"rcartocolor\")\n\nggplot(\n    bikes, \n    aes(x = day_night, y = count, \n        fill = season)\n  ) +\n  geom_boxplot() +\n  rcartocolor::scale_fill_carto_d(\n    palette = \"Vivid\" \n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nrcartocolor::display_carto_all(colorblind_friendly = TRUE)",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Working with Colors"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#scico-and-else",
    "href": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#scico-and-else",
    "title": "Working with Colors",
    "section": "2.2 scico and else",
    "text": "2.2 scico and else\n\n\nCode\n# install.packages(\"scico\")\n\nggplot(\n    bikes, \n    aes(x = day_night, y = count, \n        fill = season)\n  ) +\n  geom_boxplot() +\n  scico::scale_fill_scico_d(\n    palette = \"hawaii\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggsci\")\nggplot(\n    bikes, \n    aes(x = day_night, y = count, \n        fill = season)\n  ) +\n  geom_boxplot() +\n  ggsci::scale_fill_npg()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggthemes\")\nggplot(\n    bikes, \n    aes(x = day_night, y = count, \n        fill = season)\n  ) +\n  geom_boxplot() +\n  ggthemes::scale_fill_gdocs()",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Working with Colors"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#emulate-cvd",
    "href": "contents/website/graphic_design_with_ggplot2/ch06_working_with_colors.html#emulate-cvd",
    "title": "Working with Colors",
    "section": "5.1 Emulate CVD",
    "text": "5.1 Emulate CVD\n色盲タイプ別の色チェックということだと思う。\n\n\nCode\ndeut &lt;- \n  colorspace::deutan(\n    viridis::turbo(\n      n = 100, direction = -1\n    )\n  )\n\nggplot(\n    bikes, \n    aes(x = temp_feel, y = count,\n        color = temp_feel)\n  ) +\n  geom_point() +\n  scale_color_gradientn(\n    colors = deut\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ng &lt;- \n  ggplot(\n    bikes, \n    aes(x = day_night, y = count, \n        fill = season)\n  ) +\n  geom_boxplot() +\n  scale_fill_manual(\n    values = carto_custom\n  )\n\n# devtools::install_github(\n#   \"clauswilke/colorblindr\"\n# )\n\ncolorblindr::cvd_grid(g)",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Working with Colors"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch03_data_communication.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch03_data_communication.html",
    "title": "Data Communication",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\n\n\n\n\nCode\n\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\n\npackages &lt;- c(\n  \"tidyverse\", \n  \"ggplot2\", \"readr\", \"tibble\", \"tidyr\", \"forcats\", \n  \"stringr\",\n  \"lubridate\", \"here\", \"systemfonts\", \"magick\", \n  \"scales\", \"grid\",\n  \"grDevices\", \"colorspace\", \"viridis\", \n  \"RColorBrewer\", \"rcartocolor\",\n  \"scico\", \"ggsci\", \"ggthemes\", \"nord\", \n  \"MetBrewer\", \"ggrepel\",\n  \"ggforce\", \"ggtext\", \"ggdist\", \"ggbeeswarm\", \n  \"gghalves\", \"patchwork\", \n  \"palmerpenguins\", \"rnaturalearth\", \"sf\", \"rmapshaper\", \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 note\n\nOur data is never a perfect reflectionof the real world.\n\n\nonly a subset\ncollected by humans\ncollected by machines\n\n\nThe best use of data is to teach us what isn’t true\n\n\ndont’ formulate a single statement\nconfront yourself with a falsifiable universal statement\n\nデータヴィジュアライゼーションの参考サイトは次です。実務で１番参考になるのは１番上の、From Data to Vizというサイトです。このサイトでは、R、Python、D3、Reactのコードも記載されており、実務ですぐ使えとなります。\n\nfrom Data to Viz\nData Viz Project\nvisualizationuniverse\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Data Communication"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"ggplot2\", \"readr\", \"tibble\", \"tidyr\", \"forcats\", \n  \"stringr\",\n  \"lubridate\", \"here\", \"systemfonts\", \"magick\", \n  \"scales\", \"grid\",\n  \"grDevices\", \"colorspace\", \"viridis\", \n  \"RColorBrewer\", \"rcartocolor\",\n  \"scico\", \"ggsci\", \"ggthemes\", \"nord\", \n  \"MetBrewer\", \"ggrepel\",\n  \"ggforce\", \"ggtext\", \"ggdist\", \"ggbeeswarm\", \n  \"gghalves\", \"patchwork\", \n  \"palmerpenguins\", \"rnaturalearth\", \"sf\", \"rmapshaper\", \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#the-data",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#the-data",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "1.1 the data",
    "text": "1.1 the data\n\n\nCode\nbikes &lt;- \n    read_csv(\n        here(\n            cur_dir, \n            \"ggplot2-course-data\", \n            \"london-bikes-custom.csv\"),\n        col_types = \"Dcfffilllddddc\"\n    )\n\nbikes$season &lt;- forcats::fct_inorder(bikes$season)\n\n\nbikes |&gt; \n    head() |&gt; \n    rmarkdown::paged_table()",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#the-group",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#the-group",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "1.2 the group",
    "text": "1.2 the group\n\n\nCode\nbikes |&gt; \n    ggplot(aes(x = temp_feel, y = count)) + \n    geom_point(aes(color = season), alpah = .5) +\n    geom_smooth(\n        aes(group = day_night), \n        method = \"lm\"\n    )\n#&gt; Warning in geom_point(aes(color = season), alpah = 0.5): Ignoring unknown\n#&gt; parameters: `alpah`\n#&gt; `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#stat-geom",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#stat-geom",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "1.3 stat, geom",
    "text": "1.3 stat, geom\n\n\n\n\nCode\nggplot(bikes, aes(x = season)) + \n    stat_count(geom = \"bar\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(bikes, aes(x = season)) + \n    geom_bar(stat = \"count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(bikes, aes(x = date, y = temp_feel)) + \n    stat_identity(geom = \"point\")\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(bikes, aes(x = date, y = temp_feel)) + \n    geom_point(stat = \"identity\")",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#statisitical-summarys",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#statisitical-summarys",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "1.4 Statisitical Summarys",
    "text": "1.4 Statisitical Summarys\n\n\nCode\nggplot(\n    bikes, \n    aes(x = season , y = temp_feel)\n) + \n    geom_boxplot() + \n    stat_summary(\n        fun = mean, \n        geom = \"point\", \n        color = \"#28a87d\", \n        size = 3\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes, \n    aes(x = season, y = temp_feel)\n  ) +\n  stat_summary(\n    fun = mean, \n    fun.max = function(y) mean(y) + sd(y), \n    fun.min = function(y) mean(y) - sd(y) \n  )",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#how-to-work-with-aspect-ratios",
    "href": "contents/website/graphic_design_with_ggplot2/ch01_concepts_of_the_ggplot2_packages.html#how-to-work-with-aspect-ratios",
    "title": "Concepts of the ggplot2 Packages pt 1",
    "section": "3.1 How to Work with Aspect Ratios",
    "text": "3.1 How to Work with Aspect Ratios\nRStudioのViewerは、実際にファイルへ出力したときと見た目が異なる。 これに対して１度保存してから確認するということもあるが、chunkのセッティングを調整することもできる. もしくはcamcorderを使うことになる。動かないのでとりあえず無視する.\n\n\nCode\nlibrary(camcorder)\n\n\ngg_record(\n    dir = here(cur_dir, \"output\", \"temp_plots\"), \n    device = \"pdf\", \n    width  =  297, \n    height = 210, \n    units  = \"mm\"\n)\n\ng",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 1"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html",
    "title": "Spatial analysis with US Census data",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\nCode\nlibrary(here)\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(sf)\nlibrary(mapview)\n\ncur_dir &lt;- here()\noptions(tigris_use_cache = TRUE)",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#idenfifying-eometrices-within-a-metropolitan-area",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#idenfifying-eometrices-within-a-metropolitan-area",
    "title": "Spatial analysis with US Census data",
    "section": "1.1 Idenfifying eometrices within a metropolitan area",
    "text": "1.1 Idenfifying eometrices within a metropolitan area\ntigrisから2020年のデータを取得して処理する事例を試す.\n\n\nCode\nks_mo_tracts &lt;- \n    map_dfr(c(\"KS\", \"MO\"), \\(x) {\n        tracts(x, cb = TRUE, year = 2020)\n    }) |&gt; \n    st_transform(8528)\n\nkc_metro &lt;- \n    core_based_statistical_areas(cb = TRUE, year = 2020) |&gt; \n    filter(str_detect(NAME, \"Kansas City\")) |&gt; \n    st_transform(8528)\n\nggplot() + \n    geom_sf(data = ks_mo_tracts, fill = \"white\", color = \"grey\") + \n    geom_sf(data = kc_metro, fill = NA, color = \"red\") + \n    theme_void()",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#spatial-subsets",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#spatial-subsets",
    "title": "Spatial analysis with US Census data",
    "section": "1.2 Spatial Subsets",
    "text": "1.2 Spatial Subsets\nSpatial Subsettinghはデータのextentを使う. Spatial predicateが 定義されていることにより、空間部分集合はインデックス記法で記述することが可能である.\n\n\nCode\nkc_tracts &lt;- ks_mo_tracts[kc_metro, ]\n\nggplot() + \n    geom_sf(data = kc_tracts, fill = \"white\", color = \"grey\") + \n    geom_sf(data = kc_metro, fill =NA, color = \"red\") + \n    theme_void()\n\n\n\n\n\n\n\n\n\n上記で抽出対象となるのはinterects演算によるものである.\n\n\n\n\n\n\nNote\n\n\n\nintersectsは交差判定だけなので高速で演算できる. 交差集合を算出するintersectionはそれなりに時間を要する.\n\n\n一般的にはCensusデータ分析において、 あたえた都市領域からデータを抽出する債にはst_withinを使うことが求められる。\n\n\nCode\n# 前述の記法と同じ内容\nkc_tracts_within &lt;- \n    ks_mo_tracts |&gt; \n    st_filter(kc_metro, .predicate = st_within) \n\n\nggplot() + \n    geom_sf(data = kc_tracts_within, fill = \"white\", color = \"grey\") + \n    geom_sf(data = kc_metro, fill =NA, color = \"red\") + \n    theme_void()",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#point-in-polygon",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#point-in-polygon",
    "title": "Spatial analysis with US Census data",
    "section": "2.1 Point in polygon",
    "text": "2.1 Point in polygon\n\n\nCode\nlibrary(mapview)\n\ngainesville_patients &lt;- tibble(\n  patient_id = 1:10,\n  longitude = c(-82.308131, -82.311972, -82.361748, -82.374377, \n                -82.38177, -82.259461, -82.367436, -82.404031, \n                -82.43289, -82.461844),\n  latitude = c(29.645933, 29.655195, 29.621759, 29.653576, \n               29.677201, 29.674923, 29.71099, 29.711587, \n               29.648227, 29.624037)\n)\n\n\n上記のデータをst_as_sfで地理空間データに変換する. これならポイントのテーブルデータを簡単にGISデータに変換することが可能ですね.\n\n\nCode\ngainesville_sf &lt;- \n    gainesville_patients |&gt; \n    st_as_sf(\n        coords = c(\"longitude\", \"latitude\"), \n        crs = 4326\n    ) |&gt; \n    st_transform(6440)\n\ngainesville_sf\n#&gt; Simple feature collection with 10 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 797379.2 ymin: 70862.57 xmax: 816865.2 ymax: 80741.87\n#&gt; Projected CRS: NAD83(2011) / Florida North\n#&gt; # A tibble: 10 × 2\n#&gt;    patient_id            geometry\n#&gt;  *      &lt;int&gt;         &lt;POINT [m]&gt;\n#&gt;  1          1 (812216.7 73640.25)\n#&gt;  2          2 (811825.2 74659.57)\n#&gt;  3          3 (807076.4 70862.57)\n#&gt;  4          4 (805787.7 74365.85)\n#&gt;  5          5  (805023.4 76970.8)\n#&gt;  6          6 (816865.2 76944.63)\n#&gt;  7          7 (806340.6 80741.36)\n#&gt;  8          8   (802799 80741.87)\n#&gt;  9          9 (800134.3 73668.88)\n#&gt; 10         10 (797379.2 70937.49)\n\n\nspatial datasetに変換できたので、 地図上に載せることが可能となる.\n\n\nCode\nmapview(\n    gainesville_sf, \n    col.regions = \"red\", \n    legend = FALSE\n)\n\n\n\n\n\n\nポイントが準備できたので、healthデータを準備する.\n\n\nCode\nalachua_insurance &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"DP03_0096P\",\n  state = \"FL\",\n  county = \"Alachua\",\n  year = 2019,\n  geometry = TRUE\n) %&gt;%\n  select(GEOID, pct_insured = estimate, \n         pct_insured_moe = moe) %&gt;%\n  st_transform(6440)\n#&gt; Getting data from the 2015-2019 5-year ACS\n#&gt; Warning: • You have not set a Census API key. Users without a key are limited to 500\n#&gt; queries per day and may experience performance limitations.\n#&gt; ℹ For best results, get a Census API key at\n#&gt; http://api.census.gov/data/key_signup.html and then supply the key to the\n#&gt; `census_api_key()` function to use it throughout your tidycensus session.\n#&gt; This warning is displayed once per session.\n#&gt; Using the ACS Data Profile\n\n\n上記で得られたデータを紐付ける前に、一度地図上で空間関係を確認する.\n\n\nCode\nmapview(\n    alachua_insurance, \n    zcol = \"pct_insured\", \n    layer.name = \"% with health &lt;br/&gt;insurance\"\n) + \n    mapview(\n        gainesville_sf, \n        col.regions = \"red\", \n        legend = FALSE\n    )\n\n\n\n\n\n\nmap上ならば関係性が明らかである. これをデータとして紐付ける.\n\n\nCode\npatients_joined &lt;- \n    st_join(\n        gainesville_sf, \n        alachua_insurance\n    )\n\npatients_joined\n#&gt; Simple feature collection with 10 features and 4 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 797379.2 ymin: 70862.57 xmax: 816865.2 ymax: 80741.87\n#&gt; Projected CRS: NAD83(2011) / Florida North\n#&gt; # A tibble: 10 × 5\n#&gt;    patient_id            geometry GEOID       pct_insured pct_insured_moe\n#&gt;  *      &lt;int&gt;         &lt;POINT [m]&gt; &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt;\n#&gt;  1          1 (812216.7 73640.25) 12001000700        81.6             7  \n#&gt;  2          2 (811825.2 74659.57) 12001000500        91               5.1\n#&gt;  3          3 (807076.4 70862.57) 12001001515        85.2             6.2\n#&gt;  4          4 (805787.7 74365.85) 12001001603        88.3             5.1\n#&gt;  5          5  (805023.4 76970.8) 12001001100        96.2             2.7\n#&gt;  6          6 (816865.2 76944.63) 12001001902        86               5.9\n#&gt;  7          7 (806340.6 80741.36) 12001001803        92.3             4  \n#&gt;  8          8   (802799 80741.87) 12001001813        97.9             1.4\n#&gt;  9          9 (800134.3 73668.88) 12001002207        95.7             2.4\n#&gt; 10         10 (797379.2 70937.49) 12001002205        96.5             1.6",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#spatial-joins-and-group-wise-spatial-analysis",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#spatial-joins-and-group-wise-spatial-analysis",
    "title": "Spatial analysis with US Census data",
    "section": "2.2 Spatial joins and group-wise spatial analysis",
    "text": "2.2 Spatial joins and group-wise spatial analysis\nポリゴン対ポリゴンの空間結合を行う. ポイント対ポリゴンについては明確であるが、新しい概念であるこの結合では 使用する空間述語には十分に注意すること。\n\n\nCode\ntx_cbsa &lt;- get_acs(\n  geography = \"cbsa\",\n  variables = \"B01003_001\",\n  year = 2019,\n  survey = \"acs1\",\n  geometry = TRUE\n) %&gt;%\n  filter(str_detect(NAME, \"TX\")) %&gt;%\n  slice_max(estimate, n = 4) %&gt;%\n  st_transform(6579)\n#&gt; Getting data from the 2019 1-year ACS\n#&gt; The 1-year ACS provides data for geographies with populations of 65,000 and greater.\n\n\n\n\nCode\npct_hispanic &lt;- get_acs(\n  geography = \"tract\",\n  variables = \"DP05_0071P\",\n  state = \"TX\",\n  year = 2019,\n  geometry = TRUE\n) %&gt;%\n  st_transform(6579)\n#&gt; Getting data from the 2015-2019 5-year ACS\n#&gt; Using the ACS Data Profile\n\n\n上記のデータを結合する. left = FALSEにすることで内部欠尾久になる. つまり, 返値は4つの都市圏に含まれる場合のみが返される.\n\n\nCode\nhispanic_by_metro &lt;- st_join(\n    pct_hispanic, \n    tx_cbsa, \n    join = st_within , \n    suffix = c(\"_tracs\", \"_metro\"), \n    left = FALSE\n)\n\nhispanic_by_metro\n#&gt; Simple feature collection with 3189 features and 10 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1538292 ymin: 7167200 xmax: 2045975 ymax: 7705975\n#&gt; Projected CRS: NAD83(2011) / Texas Centric Albers Equal Area\n#&gt; First 10 features:\n#&gt;    GEOID_tracs                                 NAME_tracs variable_tracs\n#&gt; 1  48113019204  Census Tract 192.04, Dallas County, Texas     DP05_0071P\n#&gt; 3  48029190601  Census Tract 1906.01, Bexar County, Texas     DP05_0071P\n#&gt; 8  48201311900    Census Tract 3119, Harris County, Texas     DP05_0071P\n#&gt; 9  48113015500     Census Tract 155, Dallas County, Texas     DP05_0071P\n#&gt; 11 48439102000   Census Tract 1020, Tarrant County, Texas     DP05_0071P\n#&gt; 13 48201450200    Census Tract 4502, Harris County, Texas     DP05_0071P\n#&gt; 14 48201450400    Census Tract 4504, Harris County, Texas     DP05_0071P\n#&gt; 22 48157670300 Census Tract 6703, Fort Bend County, Texas     DP05_0071P\n#&gt; 25 48201554502 Census Tract 5545.02, Harris County, Texas     DP05_0071P\n#&gt; 27 48121020503  Census Tract 205.03, Denton County, Texas     DP05_0071P\n#&gt;    estimate_tracs moe_tracs GEOID_metro\n#&gt; 1            58.1       5.7       19100\n#&gt; 3            86.2       6.7       41700\n#&gt; 8            84.9       5.8       26420\n#&gt; 9            56.6       7.5       19100\n#&gt; 11           15.7       6.4       19100\n#&gt; 13           10.7       3.9       26420\n#&gt; 14           26.8      13.1       26420\n#&gt; 22           27.0       5.8       26420\n#&gt; 25           13.4       3.0       26420\n#&gt; 27           35.3       8.7       19100\n#&gt;                                         NAME_metro variable_metro\n#&gt; 1       Dallas-Fort Worth-Arlington, TX Metro Area     B01003_001\n#&gt; 3         San Antonio-New Braunfels, TX Metro Area     B01003_001\n#&gt; 8  Houston-The Woodlands-Sugar Land, TX Metro Area     B01003_001\n#&gt; 9       Dallas-Fort Worth-Arlington, TX Metro Area     B01003_001\n#&gt; 11      Dallas-Fort Worth-Arlington, TX Metro Area     B01003_001\n#&gt; 13 Houston-The Woodlands-Sugar Land, TX Metro Area     B01003_001\n#&gt; 14 Houston-The Woodlands-Sugar Land, TX Metro Area     B01003_001\n#&gt; 22 Houston-The Woodlands-Sugar Land, TX Metro Area     B01003_001\n#&gt; 25 Houston-The Woodlands-Sugar Land, TX Metro Area     B01003_001\n#&gt; 27      Dallas-Fort Worth-Arlington, TX Metro Area     B01003_001\n#&gt;    estimate_metro moe_metro                       geometry\n#&gt; 1         7573136        NA MULTIPOLYGON (((1801563 765...\n#&gt; 3         2550960        NA MULTIPOLYGON (((1642736 726...\n#&gt; 8         7066140        NA MULTIPOLYGON (((1950592 729...\n#&gt; 9         7573136        NA MULTIPOLYGON (((1778712 763...\n#&gt; 11        7573136        NA MULTIPOLYGON (((1746016 762...\n#&gt; 13        7066140        NA MULTIPOLYGON (((1925521 730...\n#&gt; 14        7066140        NA MULTIPOLYGON (((1922292 730...\n#&gt; 22        7066140        NA MULTIPOLYGON (((1935603 728...\n#&gt; 25        7066140        NA MULTIPOLYGON (((1918552 732...\n#&gt; 27        7573136        NA MULTIPOLYGON (((1766859 768...\n\n\n\n\nCode\nhispanic_by_metro %&gt;%\n  mutate(NAME_metro = str_replace(\n      NAME_metro, \n      \", TX Metro Area\", \n      \"\")) %&gt;%\n  ggplot() + \n  geom_density(\n      aes(x = estimate_tracs), \n      color = \"navy\", \n      fill = \"navy\", \n      alpha = 0.4) + \n  theme_minimal() + \n  facet_wrap(~NAME_metro) + \n  labs(title = \"Distribution of Hispanic/Latino population by Census tract\",\n       subtitle = \"Largest metropolitan areas in Texas\",\n       y = \"Kernel density estimate\",\n       x = \"Percent Hispanic/Latino in Census tract\")\n#&gt; Warning: Removed 9 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\n\n\nグループWiseなデータ分析を通じて、 より大きな地物としてrolled upされることになる。\n\n\nCode\nmedian_by_metro &lt;- \n  hispanic_by_metro %&gt;%\n  group_by(NAME_metro) %&gt;%\n  summarize(median_hispanic = median(estimate_tracs, na.rm = TRUE))\n\nmedian_by_metro\n#&gt; Simple feature collection with 4 features and 2 fields\n#&gt; Geometry type: GEOMETRY\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 1538292 ymin: 7167200 xmax: 2045975 ymax: 7705975\n#&gt; Projected CRS: NAD83(2011) / Texas Centric Albers Equal Area\n#&gt; # A tibble: 4 × 3\n#&gt;   NAME_metro                           median_hispanic                  geometry\n#&gt;   &lt;chr&gt;                                          &lt;dbl&gt;            &lt;GEOMETRY [m]&gt;\n#&gt; 1 Austin-Round Rock-Georgetown, TX Me…            25.9 POLYGON ((1700741 730245…\n#&gt; 2 Dallas-Fort Worth-Arlington, TX Met…            22.6 POLYGON ((1737530 756507…\n#&gt; 3 Houston-The Woodlands-Sugar Land, T…            32.4 MULTIPOLYGON (((1901162 …\n#&gt; 4 San Antonio-New Braunfels, TX Metro…            53.5 POLYGON ((1619499 717051…\n\n\n\n\nCode\nplot(median_by_metro |&gt; slice(1) |&gt; pull(geometry))",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#area-weighted-areal-interpolation",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#area-weighted-areal-interpolation",
    "title": "Spatial analysis with US Census data",
    "section": "3.1 Area-weighted areal interpolation",
    "text": "3.1 Area-weighted areal interpolation\nst_interploate_awを使うことで, Area-weightd areal interpolationを 実施することが可能となる. つまり、「面積加重の面内補間」である。\nwfh15の値をwfh_20の大きさに補間する処理をしている.\n\n\nCode\nwfh_interpolate_aw &lt;- st_interpolate_aw(\n  wfh_15,\n  wfh_20,\n  extensive = TRUE\n) %&gt;%\n  mutate(GEOID = wfh_20$GEOID)\n\n\n\n\nCode\nmapview(wfh_interpolate_aw)\n\n\n\n\n\n\n上記の処理方法に対して、Census blocksを利用した例を考える. これにはst_interploate_pwを使う。これにより面積以外の情報を考慮した 回帰をおこなうことが可能となる. ここれでは人口を表す国勢調査ブロックを 重みとした例を計算する\ntidycensusのなかにもinterpolate_pwが実装されている。\n\n\nCode\nmaricopa_blocks &lt;- blocks(\n  state = \"AZ\",\n  county = \"Maricopa\",\n  year = 2020\n)\n\nwfh_interpolate_pw &lt;- interpolate_pw(\n  wfh_15,\n  wfh_20,\n  to_id = \"GEOID\",\n  extensive = TRUE, \n  weights = maricopa_blocks,\n  weight_column = \"POP20\",\n  crs = 26949\n)\n\n\nデータの形状が2020に統一されたので、left_joinにより データの結合が可能となる.\n\n\nCode\nlibrary(mapboxapi)\n#&gt; Usage of the Mapbox APIs is governed by the Mapbox Terms of Service.\n#&gt; Please visit https://www.mapbox.com/legal/tos/ for more information.\n\nwfh_shift &lt;- \n    wfh_20 %&gt;%\n    left_join(\n        st_drop_geometry(wfh_interpolate_pw), \n        by = \"GEOID\",\n        suffix = c(\"_2020\", \"_2015\")) %&gt;%\n    mutate(wfh_shift = estimate_2020 - estimate_2015)\n\n# maricopa_basemap &lt;- layer_static_mapbox(\n#   location = wfh_shift,\n#   style_id = \"dark-v9\",\n#   username = \"mapbox\"\n# )\n\nggplot() + \n  # maricopa_basemap + \n  geom_sf(\n      data = wfh_shift, \n      aes(fill = wfh_shift), \n      color = NA, \n      alpha = 0.8) + \n  scale_fill_distiller(palette = \"PuOr\", direction = -1) + \n  labs(fill = \"Shift, 2011-2015 to\\n2016-2020 ACS\",\n       title = \"Change in work-from-home population\",\n       subtitle = \"Maricopa County, Arizona\") + \n  theme_void()\n\n\n\n\n\n\n\n\n\n一応、ブロック形状を面的補間したデータについて数値を確認する.\n\n\nCode\nwfh_interpolate_pw$estimate |&gt; sum()\n#&gt; [1] 105836\n\n\n\n\nCode\nwfh_15$estimate |&gt; sum()\n#&gt; [1] 105836",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#catchment-areas",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#catchment-areas",
    "title": "Spatial analysis with US Census data",
    "section": "4.1 Catchment areas",
    "text": "4.1 Catchment areas\n到達圏分析も行うことができる.\n\n\nCode\niowa_methodist &lt;- filter(ia_trauma, ID == \"0009850308\")\n\nbuf5km &lt;- st_buffer(iowa_methodist, dist = 5000) \n\n\n```{r}\niso10min &lt;- mb_isochrone(\n  iowa_methodist, \n  time = 10, \n  profile = \"driving-traffic\"\n)\nwrite_rds(iso10min, here(cur_dir, \"output\", \"ch07_iso10min.rds\"))\n\n```\n\n\nCode\nlibrary(leaflet)\nlibrary(leafsync)\niso10min &lt;- read_rds(here(cur_dir, \"output\", \"ch07_iso10min.rds\"))\nhospital_icon &lt;- makeAwesomeIcon(icon = \"ios-medical\", \n                                 markerColor = \"red\",\n                                 library = \"ion\")\n\n# The Leaflet package requires data be in CRS 4326\nmap1 &lt;- leaflet() %&gt;% \n  addTiles() %&gt;%\n  addPolygons(data = st_transform(buf5km, 4326)) %&gt;% \n  addAwesomeMarkers(data = st_transform(iowa_methodist, 4326),\n                    icon = hospital_icon)\n\nmap2 &lt;- leaflet() %&gt;% \n  addTiles() %&gt;%\n  addPolygons(data = iso10min) %&gt;% \n  addAwesomeMarkers(data = st_transform(iowa_methodist, 4326),\n                    icon = hospital_icon)\n\nsync(map1, map2)",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#computing-demographic-estimates-for-zones-with-areal-interpolation",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#computing-demographic-estimates-for-zones-with-areal-interpolation",
    "title": "Spatial analysis with US Census data",
    "section": "4.2 Computing demographic estimates for zones with areal interpolation",
    "text": "4.2 Computing demographic estimates for zones with areal interpolation\n\n\nCode\npolk_poverty &lt;- get_acs(\n  geography = \"block group\",\n  variables = c(poverty_denom = \"B17010_001\",\n                poverty_num = \"B17010_002\"),\n  state = \"IA\",\n  county = \"Polk\",\n  geometry = TRUE,\n  output = \"wide\",\n  year = 2020\n) %&gt;%\n  select(poverty_denomE, poverty_numE) %&gt;%\n  st_transform(26975)\n#&gt; Getting data from the 2016-2020 5-year ACS\n\n\n\n\nCode\nlibrary(glue)\n\npolk_blocks &lt;- blocks(\n  state = \"IA\",\n  county = \"Polk\",\n  year = 2020\n)\n\nbuffer_pov &lt;- interpolate_pw(\n  from = polk_poverty, \n  to = buf5km,\n  extensive = TRUE,\n  weights = polk_blocks,\n  weight_column = \"POP20\",\n  crs = 26975\n) %&gt;%\n  mutate(pct_poverty = 100 * (poverty_numE / poverty_denomE))\n\niso_pov &lt;- interpolate_pw(\n  from = polk_poverty, \n  to = iso10min,\n  extensive = TRUE,\n  weights = polk_blocks,\n  weight_column = \"POP20\",\n  crs = 26975\n) %&gt;%\n  mutate(pct_poverty = 100 * (poverty_numE / poverty_denomE))",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#understanding-spatial-neighborhoods",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#understanding-spatial-neighborhoods",
    "title": "Spatial analysis with US Census data",
    "section": "6.1 Understanding spatial neighborhoods",
    "text": "6.1 Understanding spatial neighborhoods\n隣接は空間データ分析において非常に重要な概念である. 探索空間データ分析に向いているのはspdepである.\n隣接の概念には次の３つがある.\n\n6.1.1 距離に基づく隣接性（Proximity-based neighbors）：\n隣接するフィーチャーは、何らかの距離の基準に基づいて識別されます。隣接するものとして定義されるのは、与えられた距離のしきい値内にあるもの（例：与えられたフィーチャーから2km以内のすべてのフィーチャー）や、k-最近傍法（例：与えられたフィーチャーに最も近い8つのフィーチャー）です。\n\n\n6.1.2 グラフに基づく隣接性（Graph-based neighbors）：\n隣接関係はネットワーク関係（例：通りのネットワーク沿い）を通じて定義されます。\n\n\n6.1.3 接触に基づく隣接性（Contiguity-based neighbors）：\n地理的なフィーチャーが多角形の場合に使用されます。接触に基づく空間関係のオプションには、クイーンケースの隣接性が含まれ、少なくとも1つの頂点を共有するすべての多角形が隣接とみなされます。また、ルークケースの隣接性もあり、多角形が少なくとも1つの線分を共有する必要があります。",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#example",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#example",
    "title": "Spatial analysis with US Census data",
    "section": "6.2 Example",
    "text": "6.2 Example\n「queen’s case」」とは２つの多角形が少なくとも１つの頂点を共有している場合に、それらを隣接と見なす方法である。つまり、クイーンケースでは多角形が辺を共有しちえるだけでなく、各（頂点）のみで触れている場合でも隣接しているとみなす。\n\n\nCode\nneighbors &lt;- poly2nb(dfw_tracts, queen = TRUE)\n\nsummary(neighbors)\n#&gt; Neighbour list object:\n#&gt; Number of regions: 1699 \n#&gt; Number of nonzero links: 10930 \n#&gt; Percentage nonzero weights: 0.378646 \n#&gt; Average number of links: 6.433196 \n#&gt; Link number distribution:\n#&gt; \n#&gt;   2   3   4   5   6   7   8   9  10  11  12  13  14  15  17 \n#&gt;   8  52 172 305 396 343 218 112  44  29  11   5   2   1   1 \n#&gt; 8 least connected regions:\n#&gt; 59 438 470 763 1163 1305 1365 1524 with 2 links\n#&gt; 1 most connected region:\n#&gt; 1101 with 17 links\n\n\n\n\nCode\ndfw_coords &lt;- dfw_tracts %&gt;%\n  st_centroid() %&gt;%\n  st_coordinates()\n#&gt; Warning: st_centroid assumes attributes are constant over geometries\n\nplot(dfw_tracts$geometry)\nplot(neighbors, \n     coords = dfw_coords, \n     add = TRUE, \n     col = \"blue\", \n     points = FALSE)",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#generating-the-spatial-weights-matrix",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#generating-the-spatial-weights-matrix",
    "title": "Spatial analysis with US Census data",
    "section": "6.3 Generating the spatial weights matrix",
    "text": "6.3 Generating the spatial weights matrix\nneighbors listをspatial weightsにすることで、空間データ解析が行えるようになる。 nb2listwを使うｔこで対応することができる. style = wを指定すると行に正則となる.\n\n\nCode\nweights &lt;- nb2listw(neighbors, style = \"W\")\n\nweights$weights[[1]]\n#&gt; [1] 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#moralns-i",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#moralns-i",
    "title": "Spatial analysis with US Census data",
    "section": "7.1 Moralns I",
    "text": "7.1 Moralns I\n\n\nCode\ndfw_tracts$lag_estimate &lt;- lag.listw(weights, dfw_tracts$estimate)\n\n\n\n\nCode\nggplot(dfw_tracts, aes(x = estimate, y = lag_estimate)) + \n  geom_point(alpha = 0.3) + \n  geom_abline(color = \"red\") + \n  theme_minimal() + \n  labs(title = \"Median age by Census tract, Dallas-Fort Worth TX\",\n       x = \"Median age\",\n       y = \"Spatial lag, median age\", \n       caption = \"Data source: 2016-2020 ACS via the tidycensus R package.\\nSpatial relationships based on queens-case polygon contiguity.\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nmoran.test(dfw_tracts$estimate, weights)\n#&gt; \n#&gt;  Moran I test under randomisation\n#&gt; \n#&gt; data:  dfw_tracts$estimate  \n#&gt; weights: weights    \n#&gt; \n#&gt; Moran I statistic standard deviate = 21.275, p-value &lt; 2.2e-16\n#&gt; alternative hypothesis: greater\n#&gt; sample estimates:\n#&gt; Moran I statistic       Expectation          Variance \n#&gt;      0.2926713016     -0.0005889282      0.0001900099\n\n\nモーランテストの帰無仮説は空間に対してランダムな状態にあるということである。ポジティブのときには空間的なクラスターがあるということになる。\n正規化した分散共分散行列と同じであるので平均に対して正の値が集合しているや、負の値が集合しているということが判定できる。\n\n\n\n\n\n\nNote\n\n\n\nThe Moran’s I statistic of 0.292 is positive, and the small p-value suggests that we reject the null hypothesis of spatial randomness in our dataset. (See Section 8.2.4 for additional discussion of p-values). As the statistic is positive, it suggests that our data are spatially clustered; a negative statistic would suggest spatial uniformity. In a practical sense, this means that Census tracts with older populations tend to be located near one another, and Census tracts with younger populations also tend to be found in the same areas.\n\n\n\n\nCode\n# For Gi*, re-compute the weights with `include.self()`\nlocalg_weights &lt;- nb2listw(include.self(neighbors))\n\ndfw_tracts$localG &lt;- spdep::localG(dfw_tracts$estimate, localg_weights)\n\nggplot(dfw_tracts) + \n  geom_sf(aes(fill = as.numeric(localG)), color = NA) + \n  scale_fill_distiller(palette = \"RdYlBu\") + \n  theme_void() + \n  labs(fill = \"Local Gi* statistic\")\n\n\n\n\n\n\n\n\n\n\n\nCode\ndfw_tracts &lt;- dfw_tracts %&gt;%\n  mutate(hotspot = case_when(\n    localG &gt;= 2.576 ~ \"High cluster\",\n    localG &lt;= -2.576 ~ \"Low cluster\",\n    TRUE ~ \"Not significant\"\n  ))\n\nggplot(dfw_tracts) + \n  geom_sf(aes(fill = hotspot), color = \"grey90\", size = 0.1) + \n  scale_fill_manual(values = c(\"red\", \"blue\", \"grey\")) + \n  theme_void()",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#lisa",
    "href": "contents/website/analizeUSCensus/ch07_spatial_analysis_with_us_census_data.html#lisa",
    "title": "Spatial analysis with US Census data",
    "section": "7.2 LISA",
    "text": "7.2 LISA\n\\[\nI_i = z_i \\sum_{j} w_{ij}z_j\n\\]\n\n\nCode\nset.seed(1983)\n\ndfw_tracts$scaled_estimate &lt;- as.numeric(scale(dfw_tracts$estimate))\n\ndfw_lisa &lt;- localmoran_perm(\n  dfw_tracts$scaled_estimate, \n  weights, \n  nsim = 999L, \n  alternative = \"two.sided\"\n) %&gt;%\n  as_tibble() %&gt;%\n  set_names(c(\"local_i\", \"exp_i\", \"var_i\", \"z_i\", \"p_i\",\n              \"p_i_sim\", \"pi_sim_folded\", \"skewness\", \"kurtosis\"))\n\ndfw_lisa_df &lt;- dfw_tracts %&gt;%\n  select(GEOID, scaled_estimate) %&gt;%\n  mutate(lagged_estimate = lag.listw(weights, scaled_estimate)) %&gt;%\n  bind_cols(dfw_lisa)\n\n\n\n\nCode\ndfw_lisa_clusters &lt;- dfw_lisa_df %&gt;%\n  mutate(lisa_cluster = case_when(\n    p_i &gt;= 0.05 ~ \"Not significant\",\n    scaled_estimate &gt; 0 & local_i &gt; 0 ~ \"High-high\",\n    scaled_estimate &gt; 0 & local_i &lt; 0 ~ \"High-low\",\n    scaled_estimate &lt; 0 & local_i &gt; 0 ~ \"Low-low\",\n    scaled_estimate &lt; 0 & local_i &lt; 0 ~ \"Low-high\"\n  ))\n\n\n\n\nCode\ncolor_values &lt;- c(`High-high` = \"red\", \n                  `High-low` = \"pink\", \n                  `Low-low` = \"blue\", \n                  `Low-high` = \"lightblue\", \n                  `Not significant` = \"white\")\n\nggplot(dfw_lisa_clusters, aes(x = scaled_estimate, \n                              y = lagged_estimate,\n                              fill = lisa_cluster)) + \n  geom_point(color = \"black\", shape = 21, size = 2) + \n  theme_minimal() + \n  geom_hline(yintercept = 0, linetype = \"dashed\") + \n  geom_vline(xintercept = 0, linetype = \"dashed\") + \n  scale_fill_manual(values = color_values) + \n  labs(x = \"Median age (z-score)\",\n       y = \"Spatial lag of median age (z-score)\",\n       fill = \"Cluster type\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(dfw_lisa_clusters, aes(fill = lisa_cluster)) + \n  geom_sf(size = 0.1) + \n  theme_void() + \n  scale_fill_manual(values = color_values) + \n  labs(fill = \"Cluster type\")",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Spatial analysis with US Census data"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Tips Site",
    "section": "",
    "text": "1 はじめに\n\nRやPythonなどに関するTips, 勉強ノートです\nデータサイエンスで社会がより豊かになることに貢献したい\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/ch06_mapping_census_data_with_r.html",
    "href": "contents/website/analizeUSCensus/ch06_mapping_census_data_with_r.html",
    "title": "Mapping Census data with R",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\n\n\n\n\nCode\nlibrary(sf)\nlibrary(here)\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(tigris)\nlibrary(spdep)\n\ncur_dir &lt;- here()\noptions(tigris_use_cache = TRUE)\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Mapping Census data with R"
    ]
  },
  {
    "objectID": "contents/website/analizeUSCensus/index.html",
    "href": "contents/website/analizeUSCensus/index.html",
    "title": "Analyzing US Census Data",
    "section": "",
    "text": "1 はじめに\n\nアメリカのセンサスデータのデータ分析\n空間データ分析のトレーニングとして有用\nリンクス\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Analyzing US Census Data",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch02_concepts_of_the_ggplot2_part2.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch02_concepts_of_the_ggplot2_part2.html",
    "title": "Concepts of the ggplot2 Packages pt 2",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\n\n\n\n\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"ggplot2\", \"readr\", \"tibble\", \"tidyr\", \"forcats\", \n  \"stringr\",\n  \"lubridate\", \"here\", \"systemfonts\", \"magick\", \n  \"scales\", \"grid\",\n  \"grDevices\", \"colorspace\", \"viridis\", \n  \"RColorBrewer\", \"rcartocolor\",\n  \"scico\", \"ggsci\", \"ggthemes\", \"nord\", \n  \"MetBrewer\", \"ggrepel\",\n  \"ggforce\", \"ggtext\", \"ggdist\", \"ggbeeswarm\", \n  \"gghalves\", \"patchwork\", \n  \"palmerpenguins\", \"rnaturalearth\", \"sf\", \"rmapshaper\", \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 Setup\n\n\nCode\nlibrary(tidyverse)\n\nbikes &lt;- readr::read_csv(\n  here::here(cur_dir, \"ggplot2-course-data\", \"london-bikes-custom.csv\"),\n  col_types = \"Dcfffilllddddc\"\n)\n\nbikes$season &lt;- forcats::fct_inorder(bikes$season)\n\ntheme_set(theme_light(base_size = 14, base_family = \"Noto Sans\"))\n\ntheme_update(\n  panel.grid.minor = element_blank(),\n  plot.title = element_text(face = \"bold\"),\n  legend.position = \"top\",\n  plot.title.position = \"plot\"\n)\n\ninvisible(Sys.setlocale(\"LC_TIME\", \"C\"))\n\n\n\n\nCode\ng &lt;-\n  ggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = season,\n        group = day_night)\n  ) +\n  geom_point(\n    alpha = .5\n  ) +\n  geom_smooth(\n    method = \"lm\",\n    color = \"black\"\n  )\n\n\n\n\n2 Facets\n\n\nCode\ng +\n  facet_wrap(\n    ~ is_workday + day_night\n  )\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCode\ng +\n  facet_wrap(\n    ~ day_night,\n    ncol = 1,\n    strip.position = \"bottom\"\n  )\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCode\ng +\n  facet_grid(\n    day_night ~ is_workday,\n    scales = \"free\",\n    switch = \"y\"\n  ) + \n  scale_y_continuous(position = \"right\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCode\ng +\n  facet_grid(\n    rows = vars(day_night),\n    cols = vars(is_workday)\n  )\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCode\ng +\n  facet_grid(\n    day_night ~ is_workday, \n    scales = c(\"free_y\"), \n    space = c(\"free_y\")\n  )\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    diamonds,\n    aes(x = carat, y = price)\n  ) +\n  geom_point(\n    alpha = .3,\n    color = \"white\"\n  ) +\n  geom_smooth(\n    method = \"lm\",\n    se = FALSE,\n    color = \"dodgerblue\"\n  ) +\n  facet_grid(\n    cut ~ clarity,\n    space = \"free_x\",\n    scales = \"free_x\"\n  ) + \n  theme_dark()\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n3 Scales\n\n\nCode\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = season)\n  ) +\n  geom_point() +\n  scale_x_date(\n        name = NULL,\n        date_breaks = \"6 months\",\n        date_labels = \"%b '%y\"\n  ) +\n  scale_y_continuous(\n      trans = \"log10\", \n      name = \"Reported bike shares\", \n      breaks = seq(0, 60000, by = 15000), \n      labels = \\(x) paste0(x, \" bikes\"), \n      limits = c(NA, 60000), \n      expand = expansion(0, 0), # add, mult\n      # guide = \"none\"\n  ) +\n  scale_color_discrete()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = season, y = count)\n  ) +\n  geom_boxplot() +\n  scale_x_discrete(\n    name = \"Period\",\n    labels = c(\"Dec-Feb\", \"Mar-May\", \"Jun-Aug\", \"Sep-Nov\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = season)\n  ) +\n  geom_point() +\n  scale_color_discrete(\n    name = \"Season:\",\n    type = c(\"#69b0d4\", \"#00CB79\", \"#F7B01B\", \"#a78f5f\")\n  )\n\n\n\n\n\n\n\n\n\n色についてもscale_colorの中でマニュアルで指定することが可能である.\n\n\nCode\nmy_colors &lt;- c(\n  `winter` = \"#3c89d9\",\n  `spring` = \"#1ec99b\",\n  `summer` = \"#F7B01B\",\n  `autumn` = \"#a26e7c\"\n)\n\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = season)\n  ) +\n  geom_point() +\n  scale_color_discrete(\n    name = \"Season:\",\n    type = my_colors\n  )\n\n\n\n\n\n\n\n\n\nNAは別の値で決めることができる. パステルカラーにブラックはよくわかるので良さそうである.\n\n\nCode\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = weather_type)\n  ) +\n  geom_point() +\n  scale_color_manual(\n    name = \"Season:\",\n    values = brewer.pal(n = 6, name = \"Pastel1\"),\n    na.value = \"black\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = date, y = count,\n        color = weather_type)\n  ) +\n  geom_point() +\n  rcartocolor::scale_color_carto_d(\n    name = \"Season:\",\n    palette = \"Pastel\",\n    na.value = \"black\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nfacet &lt;-\n  ggplot(\n    diamonds,\n    aes(x = carat, y = price)\n  ) +\n  geom_point(\n    alpha = .3\n  ) +\n  geom_smooth(\n    aes(color = cut),\n    method = \"lm\",\n    se = FALSE\n  ) +\n  facet_grid(\n    cut ~ clarity,\n    space = \"free_x\",\n    scales = \"free_x\"\n  )\n\nfacet + \n    scale_x_continuous(breaks = 0:5) + \n    scale_y_continuous(\n        limits = c(0, 30000), \n        breaks = 0:3 * 10000, \n        labels = \\(x) paste0(\"$\", format(x, big.mark = \",\", trim = TRUE))\n    ) + \n    scale_color_brewer(palette = \"Set2\") + \n    theme(legend.position = \"none\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n#&gt; Warning: Removed 131 rows containing missing values (`geom_smooth()`).\n\n\n\n\n\n\n\n\n\n\n\n4 Coordinates Systems\n線形の座標系は次である.\n\ncoord_cartesian()\ncoord_fixed()\ncoord_flip()\n\n非線形の座標系として次がある.\n\ncoord_polar()\ncoord_map()\ncoord_sf()\ncoord_trans()\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = season, y = count)\n  ) +\n  geom_boxplot() +\n  coord_cartesian(\n      ylim = c(NA, 15000)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = season, y = count)\n  ) +\n  geom_boxplot() +\n  scale_y_continuous(\n      limits = c(NA, 15000)\n  )\n#&gt; Warning: Removed 575 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\n\n\n\n\n\n\nclip = \"off\"を使うとpanel領域をはみ出してグラフを記述することが出来る.\n\n\nCode\nggplot(\n    bikes,\n    aes(x = season, y = count)\n  ) +\n  geom_boxplot() +\n  coord_cartesian(\n    ylim = c(NA, 15000),\n    clip = \"off\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, is_holiday == TRUE),\n    aes(x = temp_feel, y = count)\n  ) +\n  geom_point() +\n  geom_text(\n    aes(label = season),\n    nudge_x = .3,\n    hjust = 0, \n    vjust = -1\n  ) +\n  coord_cartesian(\n    clip = \"off\"\n  )\n\n\n\n\n\n\n\n\n\nパディングをすべて消すことも可能である.\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count)\n  ) +\n  geom_point() +\n  coord_cartesian(\n    expand = FALSE,\n    clip = \"off\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = temp)\n  ) +\n  geom_point() +\n  coord_fixed()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = temp)\n  ) +\n  geom_point() +\n  coord_fixed(ratio = 4) # y / x\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(y = weather_type)\n  ) +\n  geom_bar() +\n  coord_cartesian()\n\n\n\n\n\n\n\n\n\nreorderすることも出来る.\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(y = fct_rev(fct_infreq(weather_type)))\n  ) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(x = weather_type,\n        fill = weather_type)\n  ) +\n  geom_bar() +\n  coord_polar(theta = \"y\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(x = fct_infreq(weather_type),\n        fill = weather_type)\n  ) +\n  geom_bar(width = 1) +\n  coord_polar()\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(x = fct_infreq(weather_type),\n        fill = weather_type)\n  ) +\n  geom_bar(width = 1) +\n  coord_cartesian()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(x = 1, fill = weather_type)\n  ) +\n  geom_bar(position = \"stack\") +\n  coord_polar(theta = \"y\") \n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, !is.na(weather_type)),\n    aes(x = 1, fill = weather_type)\n  ) +\n  geom_bar(position = \"stack\") +\n  coord_cartesian() \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp, y = count,\n        group = day_night)\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  coord_trans(y = \"log10\")\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp, y = count,\n        group = day_night)\n  ) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_y_log10()\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Spaital Coordiate\n\n\nCode\ncountries &lt;- rnaturalearth::ne_countries(\n  returnclass = \"sf\"\n)\n\n\n\nggplot() +\n  geom_sf(\n    data = countries,\n    color = \"#79dfbd\",\n    fill = \"#28a87d\",\n    size = .3\n  ) +\n  coord_sf(\n    crs = \"+proj=moll\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\noceans &lt;- rnaturalearth::ne_download(\n  category = \"physical\", type = \"ocean\", returnclass = \"sf\"\n)\nggplot() +\n  geom_sf(\n    data = oceans,\n    fill = \"#d8f1f6\",\n    color = \"white\"\n  ) +\n  geom_sf(\n    data = countries,\n    aes(fill = economy),\n    color = \"white\",\n    size = .3\n  ) +\n  coord_sf(\n    crs = \"+proj=bonne +lat_1=10\"\n  ) +\n  scale_fill_viridis_d(option = \"magma\") +\n  theme_void() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Concepts of the ggplot2 Packages pt 2"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch04_working_with_labels_and_annotations.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch04_working_with_labels_and_annotations.html",
    "title": "Working with Labels and Annotations",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\n\n\n\n\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 Setup\n\n\nCode\nbikes &lt;- read_csv(\n    here(cur_dir, \"ggplot2-course-data/london-bikes-custom.csv\"), \n    col_types = \"Dcfffilllddddc\"\n)\n\nbikes$season &lt;- forcats::fct_inorder(bikes$season)\n\ntheme_set(\n    theme_light(\n        base_size = 14, \n        base_family = \"Noto Sans\"\n    )\n)\n\nglimpse(bikes)\n#&gt; Rows: 1,454\n#&gt; Columns: 14\n#&gt; $ date         &lt;date&gt; 2015-01-04, 2015-01-04, 2015-01-05, 2015-01-05, 2015-01-…\n#&gt; $ day_night    &lt;chr&gt; \"day\", \"night\", \"day\", \"night\", \"day\", \"night\", \"day\", \"n…\n#&gt; $ year         &lt;fct&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 201…\n#&gt; $ month        &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n#&gt; $ season       &lt;fct&gt; winter, winter, winter, winter, winter, winter, winter, w…\n#&gt; $ count        &lt;int&gt; 6830, 2404, 14763, 5609, 14501, 6112, 16358, 4706, 9971, …\n#&gt; $ is_workday   &lt;lgl&gt; FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, TRUE, T…\n#&gt; $ is_weekend   &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FAL…\n#&gt; $ is_holiday   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, F…\n#&gt; $ temp         &lt;dbl&gt; 2.166667, 2.791667, 8.958333, 7.125000, 9.000000, 6.70833…\n#&gt; $ temp_feel    &lt;dbl&gt; -0.750000, 2.041667, 7.708333, 5.708333, 6.458333, 4.2083…\n#&gt; $ humidity     &lt;dbl&gt; 95.16667, 93.37500, 81.08333, 79.54167, 80.20833, 77.5833…\n#&gt; $ wind_speed   &lt;dbl&gt; 10.416667, 4.583333, 8.666667, 9.041667, 19.208333, 12.79…\n#&gt; $ weather_type &lt;chr&gt; \"broken clouds\", \"clear\", \"broken clouds\", \"cloudy\", \"bro…\n\n\n\n\n2 Labels + theme\n\n\nCode\n\ng &lt;- ggplot(\n    bikes, \n    aes(x = temp_feel, y = count, color = season)\n) +\n    geom_point(\n        alpha = .5\n    ) + \n    labs(\n        x = \"Feels Like temperature (℉)\", \n        y = \"Reported bike shares\", \n        title = \"TfL bike shareing trends\", \n        subtitle = \"Reporeted bike rents versus Feels Like temperature in London\", \n        caption= \"Data: TfL\", \n        color = \"Season: \", \n        tag = \"1.\"\n    )\n\nplot(g)\n\n\n\n\n\n\n\n\n\n上記のグラフをカスタマイズする.\n\n\nCode\ng + \n    theme(\n        plot.title = element_text(face = \"bold\"), \n        plot.title.position = \"plot\", \n        axis.text = element_text(\n            color = \"#28a87d\", \n            family = \"Noto Sans\", \n            face = \"italic\", \n            hjust = 1, \n            vjust = 0, \n            angle = 45, \n            lineheight = 1.3, \n            margin = margin(10, 0, 20, 0), \n            debug = TRUE\n        ), \n        axis.text.x = element_text(\n            margin = margin(0, 12, -8, 0)\n        ), \n        plot.tag = element_text(\n            margin = margin(0, 12, -8, 0), \n            debug = TRUE\n        )\n    )\n\n\n\n\n\n\n\n\n\n\n\n3 Labels + scale\n\n\nCode\ng &lt;- \n    g + \n    labs(\n        tag = NULL, \n        title = NULL, \n        subtitle = NULL\n    )\n\ng + \n    scale_y_continuous(\n        breaks = 0:4 * 15000, \n        labels = scales::comma_format(\n            suffix = \"\\n bikes shared\"\n        ), \n        name = NULL\n    ) + \n    theme(\n        axis.text.y = element_text(\n            hjust = .5\n        )\n    )\n\n\n\n\n\n\n\n\n\n\n\nCode\ng +\n  scale_y_continuous(\n    breaks = 0:4*15000,\n    labels = function(y) y / 1000,\n    name = \"Reported bike shares in thousands\",\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = season, y = count)\n  ) +\n  geom_boxplot() +\n  scale_x_discrete(\n    name = NULL,\n    labels = stringr::str_to_title\n  )\n\n\n\n\n\n\n\n\n\n\n\n4 Labels + element_markdown\nggtextパッケージを使うことで対応することができる.\n\n\nCode\n# install.packages(\"ggtext\")\n\ng +\n  ggtitle(\"**TfL bike sharing trends by _season_**\") +\n  theme(\n    plot.title = ggtext::element_markdown()\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# install.packages(\"ggtext\")\n\ng +\n  ggtitle(\"&lt;b style='font-size:25pt'&gt;TfL&lt;/b&gt; bike sharing trends by &lt;i style='color:#28a87d;'&gt;season&lt;/i&gt;\") +\n  theme(\n    plot.title = ggtext::element_markdown()\n  )\n\n\n\n\n\n\n\n\n\n\n\n5 Labels + facet\n\n\nCode\ncodes &lt;- c(\n  `TRUE` = \"Workday\",\n  `FALSE` = \"Weekend or Holiday\"\n)\n\ng +\n  facet_grid(\n    day_night ~ is_workday,\n    scales = \"free\",\n    space = \"free\",\n    labeller = labeller(\n      day_night = stringr::str_to_title,\n      is_workday = codes\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n6 Handling Long Labels\n\n\nCode\nggplot(\n    bikes,\n    aes(x = stringr::str_wrap(weather_type, 6),\n        y = count)\n  ) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nタイトルを調整することも可能です.\n\n\nCode\ng +\n  ggtitle(\"TfL bike sharing trends in 2015 and 2016 by season for day and night periods\") +\n  theme(\n    plot.title =\n      ggtext::element_textbox_simple(\n          margin = margin(t = 12, b = 12), \n          lineheight = .9, \n          fill = \"grey90\", \n          padding = margin(rep(12, 4)), \n          r = unit(9, \"pt\"),\n          box.color = \"grey40\",  \n          halign = .5, \n          size = 20),\n    plot.title.position = \"plot\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n7 Annotations with annotate\n\n\nCode\nggplot(bikes, aes(humidity, temp)) +\n  geom_point(size = 2, color = \"grey\") +\n  annotate(\n    geom = \"text\",\n    x = c(90, 50),\n    y = c(27.5, 3.5),\n    label = c(\"Text A\", \"Text B\"),\n    color = c(\"black\", \"firebrick\"),\n    size = c(5, 10),\n    fontface = c(\"plain\", \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n\nggplot(bikes, aes(humidity, temp)) +\n  annotate(\n    geom = \"rect\",\n    xmin = -Inf,\n    xmax = 60,\n    ymin = 20,\n    ymax = Inf,\n    fill = \"#663399\"\n  ) +\n  geom_point(size = 2, color = \"grey\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n\nggplot(bikes, aes(humidity, temp)) +\n  geom_point(size = 2, color = \"grey\") +\n  annotate(\n    geom = \"text\",\n    x = 90,\n    y = 27.5,\n    label = \"Some\\nadditional\\ntext\",\n    size = 6,\n    lineheight = .9\n  ) +\n  annotate(\n    geom = \"curve\",\n    x = 90, xend = 82,\n    y = 25, yend = 18.5, \n    curvature = -.5, \n    lwd = 5, \n    arrow = arrow(\n        length = unit(20, \"pt\"), \n        type = \"closed\", \n        ends = \"both\", \n        \n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\n8 Annotations with geom\n\n\nCode\nggplot(\n    filter(bikes, temp &gt;= 27),\n    aes(x = humidity, y = temp)\n  ) +\n  geom_point(\n    data = bikes,\n    color = \"grey65\", alpha = .3\n  ) +\n  geom_point(size = 2.5) +\n  geom_text(\n    aes(label = season),\n    nudge_x = .3,\n    hjust = 0\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n\nggplot(\n    filter(bikes, temp &gt;= 27),\n    aes(x = humidity, y = temp,\n        color = season == \"summer\")\n  ) +\n  geom_point(\n    data = bikes,\n    color = \"grey65\", alpha = .3\n  ) +\n  geom_point(size = 2.5) +\n  ggrepel::geom_text_repel(\n    aes(label = str_to_title(season))\n  ) +\n  scale_color_manual(\n    values = c(\"firebrick\", \"black\"),\n    guide = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, temp &gt;= 27),\n    aes(\n        x = humidity, \n        y = temp,\n        color = season == \"summer\")\n  ) +\n  geom_point(\n    data  = bikes,\n    color = \"grey65\", \n    alpha = .3\n  ) +\n  geom_point(size = 2.5) +\n  ggrepel::geom_text_repel(\n    aes(label = str_to_title(season)),\n    ## force to the right\n    xlim = c(NA, 35), \n    hjust = 1\n  ) +\n  scale_color_manual(\n    values = c(\"firebrick\", \"black\"),\n    guide  = \"none\"\n  ) +\n  xlim(25, NA)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, temp &gt; 20 & season != \"summer\"),\n    aes(x = humidity, y = temp,\n        color = season)\n  ) +\n  geom_point(\n    data = bikes,\n    color = \"grey65\", alpha = .3\n  ) +\n  geom_point() +\n  ggforce::geom_mark_rect(\n    aes(label = str_to_title(season))\n  ) +\n  scale_color_brewer(\n    palette = \"Dark2\",\n    guide = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    filter(bikes, temp &gt; 20 & season != \"summer\"),\n    aes(x = humidity, y = temp,\n        color = season)\n  ) +\n  geom_point(\n    data = bikes,\n    color = \"grey65\", alpha = .3\n  ) +\n  geom_point() +\n  ggforce::geom_mark_rect(\n    aes(label = str_to_title(season)),\n    expand = unit(5, \"pt\"),\n    radius = unit(0, \"pt\"),\n    con.cap = unit(0, \"pt\"),\n    label.buffer = unit(15, \"pt\"),\n    con.type = \"straight\",\n    label.fill = \"transparent\"\n  ) +\n  scale_color_brewer(\n    palette = \"Dark2\",\n    guide = \"none\"\n  ) +\n  ylim(NA, 35)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = humidity, y = temp,\n        color = season == \"summer\")\n  ) +\n  geom_point(alpha = .4) +\n  ggforce::geom_mark_hull(\n    aes(label = str_to_title(season),\n        filter = season == \"summer\",\n        description = \"June to August\"),\n    expand = unit(10, \"pt\")\n  ) +\n  scale_color_manual(\n    values = c(\"grey65\", \"firebrick\"),\n    guide = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n9 Adding Images\n\n\nCode\nurl &lt;- \"https://d33wubrfki0l68.cloudfront.net/dbb07b06a7b3fe056db386fef0b158cc2fd33cb9/8b491/assets/img/2022conf/logo-rstudio-conf.png\"\nimg &lt;- magick::image_read(url)\nimg &lt;- magick::image_negate(img)\n\nimg\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(bikes, aes(date, temp_feel)) +\n  annotation_custom(\n    grid::rasterGrob(\n      image = img,\n      x = .47,\n      y = 1.15,\n      width = .9\n    )\n  ) +\n  geom_point(color = \"#71a5d4\") +\n  coord_cartesian(clip = \"off\") +\n  theme(\n    plot.margin = margin(90, 10, 10, 10)\n  )\n\n\n\n\n\n\n\n\n\n\n\n10 Exercise 1\nggtextを調べろ\n\n\n11 Exercise 2\n\n\nCode\n\ng &lt;- \n    palmerpenguins::penguins |&gt; \n    filter(complete.cases(pick(everything()))) |&gt; \n    # mutate(body_mass_kg_fct = body_mass_g |&gt; \n    #            set_units(\"g\") |&gt; \n    #            set_units(\"kg\") |&gt; \n    #            round(0) |&gt; \n    #            as.character() |&gt; \n    #            fct_inseq(ordered = TRUE)) |&gt; \n    ggplot() + \n    geom_point(\n        aes(\n            x = bill_length_mm, \n            y = bill_depth_mm, \n            color = species, \n            size = body_mass_g / 1000\n        ), \n        alpha = .1\n    ) + \n    labs(\n        title = \"Bill dimensions fo brush-tailed penguins Pygoscelis spec\", \n        x = \"Bill length(&lt;i&gt;mm&lt;/i&gt;)\", \n        y = \"Bill depth(&lt;i&gt;mm&lt;/i&gt;)\", \n        caption = \"Horst AM, Hill AP, Gorman KB(2020). palmerpenguins R package version 0.1.0\", \n        size = \"Body mass\"\n    ) + \n    scale_y_continuous(\n        breaks = seq(12.5, 22.5, 2.5), \n        labels = \\(x) format(x, digits = 3), \n        limits = c(12.5, 22.5), \n        expand = expansion(0, 0),\n    ) + \n    scale_x_continuous(\n        breaks = seq(30, 60, 5), \n        limits = c(30, 60), \n        expand = expansion(0, 0),\n    ) +\n    scale_color_manual(\n        values =  c(\"Adelie\" = \"red\", \"Chinstrap\" = \"blue\", \"Gentoo\" = \"green\"), \n        guide = \"none\"\n    ) + \n    scale_size_continuous(\n        breaks = 3:6,\n        labels = \\(x) paste(x, \"kg\")\n    ) + \n    theme(\n        panel.grid.minor = element_blank(), \n        plot.title.position = \"plot\", \n        plot.title = element_text(size = rel(1.3), margin = margin(b = 1, unit = \"line\")), \n        plot.caption = element_text(hjust = 1, color = \"grey80\"), \n        plot.caption.position = \"plot\", \n        axis.title.y = element_markdown(\n            size = rel(1.1), margin = margin(10, 10, 10, 10, \"pt\")), \n        axis.title.x = element_markdown(\n            size = rel(1.1), margin = margin(10, 10, 10, 10, \"pt\")),\n        axis.ticks = element_blank()\n    ) + \n    guides(\n        size = guide_legend(\n            override.aes = list(color = \"grey80\")\n        )\n    )\n\n\ng\n\n\n\n\n\n\n\n\n\n上記に、テキストボックスを追加する。と思ったけどどうすればいいのかがよくわからない・・・・\n\n\nCode\ncodes &lt;- c(\n    \"Adelie\" = \"&lt;span style='font-size:20px'&gt;&lt;b&gt;P.adelie&lt;/b&gt;&lt;/span&gt;&lt;br&gt;(Adelie penguin)\", \n    \"Chinstrap\" = \"&lt;span style='font-size:20px'&gt;&lt;b&gt;P.antarctica&lt;/b&gt;&lt;/span&gt;&lt;br&gt;(Chinstrap pengui)\", \n    \"Gentoo\" = \"&lt;span style='font-size:20px'&gt;&lt;b&gt;P.papua&lt;/b&gt;&lt;/span&gt;&lt;br&gt;(Gentoo pengui)\"\n)\n\nd &lt;- \n    palmerpenguins::penguins |&gt; \n    filter(complete.cases(pick(everything()))) |&gt; \n    group_by(species) |&gt; \n    summarise(across(c(bill_length_mm, bill_depth_mm), mean)) |&gt; \n    ungroup() |&gt; \n    mutate(label = codes[species]) \n\ng + \n    geom_richtext(\n        data = d, \n        aes(x = bill_length_mm, y = bill_depth_mm, label = label), \n        fill = \"white\", \n        color = \"transparent\", \n        alpha = .1, \n        label.padding = unit(.7, units = \"lines\"), \n        label.r = unit(10, \"pt\"), \n    ) +\n    geom_richtext(\n        data = d, \n        aes(x = bill_length_mm, y = bill_depth_mm, label = label), \n        fill = \"transparent\", \n        color = c(\"red\", \"blue\", \"green\"),\n        label.padding = unit(.7, units = \"lines\"), \n        label.r = unit(10, \"pt\"), \n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Working with Labels and Annotations"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html",
    "href": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html",
    "title": "Working with Layouts and Composition",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/website/graphic_design_with_ggplot2\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggfittext\",\n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Working with Layouts and Composition"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#discrete",
    "href": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#discrete",
    "title": "Working with Layouts and Composition",
    "section": "2.1 discrete",
    "text": "2.1 discrete\n\n\nCode\npal &lt;- c(\"#3c89d9\", \"#1ec99b\", \"#F7B01B\", \"#a26e7c\")\n\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = season)\n  ) +\n  geom_point() +\n  scale_color_manual(values = pal)",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Working with Layouts and Composition"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#continuous",
    "href": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#continuous",
    "title": "Working with Layouts and Composition",
    "section": "2.2 Continuous",
    "text": "2.2 Continuous\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = humidity)\n  ) +\n  geom_point() +\n  scale_color_viridis_c() + \n  theme(\n      legend.position = \"bottom\", \n      legend.justification = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = humidity)\n  ) +\n  geom_point() +\n  scale_color_viridis_c(\n      guide = \"colorbar\"\n  ) + \n  theme(\n      legend.position = c(.25, .85), \n      legend.direction = \"horizontal\"\n  )\n#&gt; Warning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n#&gt; 3.5.0.\n#&gt; ℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\n\n次は凡例を離散的に表現しているが、 プロットの色自体は色分けされていないことに注意する。\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = humidity)\n  ) +\n  geom_point() +\n  scale_color_viridis_c(\n      guide = \"colorsteps\",\n  ) + \n  theme(\n      legend.position = c(.25, .85), \n      legend.direction = \"horizontal\"\n  )\n\n\n\n\n\n\n\n\n\n次はプロットの色自体も離散的にしている。つまり、viridis_bを使っている。\n\n\nCode\nbikes |&gt; \n    ggplot(\n        aes(x = temp_feel, y = count, color = humidity)\n    ) + \n    geom_point() + \n    scale_color_viridis_b(\n        guide = \"colorsteps\"\n    ) + \n    theme(\n        legend.position = c(.25, .85), \n        legend.direction = \"horizontal\"\n    )\n\n\n\n\n\n\n\n\n\n細かく凡例を調整したいときには次のようにguide_colorstepsを使う。\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = humidity)\n  ) +\n  geom_point() +\n  scale_color_viridis_b(\n    breaks = seq(30, 90, 10),\n    limits = c(30, 100),\n    guide = guide_colorsteps(\n      title.position = \"top\",\n      title.hjust = .5,\n      show.limits = TRUE,\n      frame.colour = \"black\",\n      frame.linewidth = 3,\n      barwidth = unit(8, \"lines\"), \n      ticks.linewidth = 1\n    )\n  ) +\n  theme(\n    legend.position = c(.25, .85),\n    legend.direction = \"horizontal\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = temp_feel, y = count,\n        color = humidity)\n  ) +\n  geom_point() +\n  scale_color_viridis_c(\n    breaks = 3:10*10,\n    limits = c(30, 100),\n    guide = guide_colorbar(\n      title.position = \"top\",\n      title.hjust = .5,\n      ticks = FALSE,\n      barwidth = unit(20, \"lines\"),\n      barheight = unit(.6, \"lines\")\n    )\n  ) +\n  theme(\n    legend.position = \"top\"\n  )",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Working with Layouts and Composition"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#key-glyphs",
    "href": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#key-glyphs",
    "title": "Working with Layouts and Composition",
    "section": "2.3 Key Glyphs",
    "text": "2.3 Key Glyphs\n\n\nCode\nggplot(\n    bikes,\n    aes(x = lubridate::week(date),\n        y = count,\n        color = day_night)\n  ) +\n  stat_summary(\n    geom = \"line\",\n    fun = sum,\n    size = 1\n  ) +\n  scale_color_manual(\n    values = c(\"#28A87D\", \"#663399\"),\n    name = NULL\n  ) +\n  theme(\n    legend.text = element_text(size = 16)\n  )\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(\n    bikes,\n    aes(x = lubridate::week(date),\n        y = count,\n        color = day_night)\n  ) +\n  stat_summary(\n    geom = \"line\",\n    fun = sum,\n    key_glyph = \"timeseries\",\n    size = 1\n  ) +\n  scale_color_manual(\n    values = c(\"#28A87D\", \"#663399\"),\n    name = NULL\n  ) +\n  theme(\n    legend.text = element_text(size = 16)\n  )",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Working with Layouts and Composition"
    ]
  },
  {
    "objectID": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#patchwork",
    "href": "contents/website/graphic_design_with_ggplot2/ch07_working_with_layouts_and_composition.html#patchwork",
    "title": "Working with Layouts and Composition",
    "section": "3.1 patchwork",
    "text": "3.1 patchwork\n\n\nCode\ntheme_std &lt;- theme_set(theme_minimal(base_size = 18, base_family = \"Noto Sans\"))\ntheme_update(\n  panel.grid = element_blank(),\n  axis.text = element_text(color = \"grey50\", size = 12),\n  axis.title = element_text(color = \"grey40\", face = \"bold\"),\n  axis.title.x = element_text(margin = margin(t = 12)),\n  axis.title.y = element_text(margin = margin(r = 12)),\n  axis.line = element_line(color = \"grey80\", size = .4),\n  legend.text = element_text(color = \"grey50\", size = 12),\n  plot.tag = element_text(size = 40, margin = margin(b = 15)),\n  plot.background = element_rect(fill = \"white\", color = \"white\")\n)\n#&gt; Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n#&gt; ℹ Please use the `linewidth` argument instead.\n\nbikes_sorted &lt;-\n  bikes %&gt;%\n  filter(!is.na(weather_type)) %&gt;%\n  group_by(weather_type) %&gt;%\n  mutate(sum = sum(count)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    weather_type = forcats::fct_reorder(\n      str_to_title(str_wrap(weather_type, 5)), sum\n    )\n  )\n\np1 &lt;- ggplot(\n    bikes_sorted,\n    aes(x = weather_type, y = count, color = weather_type)\n  ) +\n  geom_hline(yintercept = 0, color = \"grey80\", size = .4) +\n  stat_summary(\n    geom = \"point\", fun = \"sum\", size = 12\n  ) +\n  stat_summary(\n    geom = \"linerange\", ymin = 0, fun.max = function(y) sum(y),\n    size = 2, show.legend = FALSE\n  ) +\n  coord_flip(ylim = c(0, NA), clip = \"off\") +\n  scale_y_continuous(\n    expand = c(0, 0), limits = c(0, 8500000),\n    labels = scales::comma_format(scale = .0001, suffix = \"K\")\n  ) +\n  scale_color_viridis_d(\n    option = \"magma\", direction = -1, begin = .1, end = .9, name = NULL,\n    guide = guide_legend(override.aes = list(size = 7))\n  ) +\n  labs(\n    x = NULL, y = \"Sum of reported bike shares\", tag = \"P1\",\n  ) +\n  theme(\n    axis.line.y = element_blank(),\n    axis.text.y = element_text(color = \"grey50\", face = \"bold\",\n                               margin = margin(r = 15), lineheight = .9)\n  )\n\n\np2 &lt;- bikes_sorted %&gt;%\n  filter(season == \"winter\", is_weekend == TRUE, day_night == \"night\") %&gt;%\n  group_by(weather_type, .drop = FALSE) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ggplot(\n      aes(x = weather_type, y = id, color = weather_type)\n    ) +\n    geom_point(size = 4.5) +\n    scale_color_viridis_d(\n      option = \"magma\", direction = -1, begin = .1, end = .9, name = NULL,\n      guide = guide_legend(override.aes = list(size = 7))\n    ) +\n    labs(\n      x = NULL, y = \"Reported bike shares on\\nweekend winter nights\", tag = \"P2\",\n    ) +\n    coord_cartesian(ylim = c(.5, NA), clip = \"off\")\n\n\nmy_colors &lt;- c(\"#cc0000\", \"#000080\")\n\np3 &lt;- bikes %&gt;%\n  group_by(week = lubridate::week(date), day_night, year) %&gt;%\n  summarize(count = sum(count)) %&gt;%\n  group_by(week, day_night) %&gt;%\n  mutate(avg = mean(count)) %&gt;%\n  ggplot(aes(x = week, y = count, group = interaction(day_night, year))) +\n    geom_line(color = \"grey65\", size = 1) +\n    geom_line(aes(y = avg, color = day_night), stat = \"unique\", size = 1.7) +\n    annotate(\n      geom = \"text\", label = c(\"Day\", \"Night\"), color = my_colors,\n      x = c(5, 18), y = c(125000, 29000), size = 8, fontface = \"bold\"\n    ) +\n    scale_x_continuous(breaks = c(1, 1:10*5)) +\n    scale_y_continuous(labels = scales::comma_format()) +\n    scale_color_manual(values = my_colors, guide = \"none\") +\n    labs(\n      x = \"Week of the Year\", y = \"Reported bike shares\\n(cumulative # per week)\", tag = \"P3\",\n    )\n#&gt; `summarise()` has grouped output by 'week', 'day_night'. You can override using\n#&gt; the `.groups` argument.\n\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n(p1 + p2) / p3\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + p2) / p3 + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n((p1 + p2) / p3 & theme(legend.justification = \"top\")) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + p2) / p3 & theme(legend.position = \"none\", plot.background = element_rect(color = \"black\", size = 3))\n#&gt; Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n#&gt; ℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n((p1 + p2) / p3 & theme(legend.position = \"none\")) +\n  plot_layout(heights = c(.2, .1), widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\npicasso &lt;- \"\nAAAAAA#BBBB\nCCCCCCCCC##\nCCCCCCCCC##\"\n(p1 + p2 + p3 & theme(legend.position = \"none\")) + plot_layout(design = picasso)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntheme_std &lt;- theme_set(theme_minimal(base_size = 18, base_family = \"Noto Sans\"))\ntheme_update(\n  panel.grid = element_blank(),\n  axis.text = element_text(color = \"grey50\", size = 12),\n  axis.title = element_text(color = \"grey40\", face = \"bold\"),\n  axis.title.x = element_text(margin = margin(t = 12)),\n  axis.title.y = element_text(margin = margin(r = 12)),\n  axis.line = element_line(color = \"grey80\", size = .4),\n  legend.text = element_text(color = \"grey50\", size = 12),\n  plot.tag = element_text(size = 40, margin = margin(b = 15)),\n  plot.background = element_rect(fill = \"white\", color = \"white\")\n)\n\nbikes_sorted &lt;-\n  bikes %&gt;%\n  filter(!is.na(weather_type)) %&gt;%\n  group_by(weather_type) %&gt;%\n  mutate(sum = sum(count)) %&gt;%\n  ungroup() %&gt;%\n  mutate(\n    weather_type = forcats::fct_reorder(\n      str_to_title(str_wrap(weather_type, 5)), sum\n    )\n  )\n\np1 &lt;- ggplot(\n    bikes_sorted,\n    aes(x = weather_type, y = count, color = weather_type)\n  ) +\n  geom_hline(yintercept = 0, color = \"grey80\", size = .4) +\n  stat_summary(\n    geom = \"point\", fun = \"sum\", size = 12\n  ) +\n  stat_summary(\n    geom = \"linerange\", ymin = 0, fun.max = function(y) sum(y),\n    size = 2, show.legend = FALSE\n  ) +\n  coord_flip(ylim = c(0, NA), clip = \"off\") +\n  scale_y_continuous(\n    expand = c(0, 0), limits = c(0, 8500000),\n    labels = scales::comma_format(scale = .0001, suffix = \"K\")\n  ) +\n  scale_color_viridis_d(\n    option = \"magma\", direction = -1, begin = .1, end = .9, name = NULL,\n    guide = guide_legend(override.aes = list(size = 7))\n  ) +\n  labs(\n    x = NULL, y = \"Sum of reported bike shares\", tag = \"P1\",\n  ) +\n  theme(\n    axis.line.y = element_blank(),\n    axis.text.y = element_text(color = \"grey50\", face = \"bold\",\n                               margin = margin(r = 15), lineheight = .9)\n  )\n\n\np2 &lt;- bikes_sorted %&gt;%\n  filter(season == \"winter\", is_weekend == TRUE, day_night == \"night\") %&gt;%\n  group_by(weather_type, .drop = FALSE) %&gt;%\n  mutate(id = row_number()) %&gt;%\n  ggplot(\n      aes(x = weather_type, y = id, color = weather_type)\n    ) +\n    geom_point(size = 4.5) +\n    scale_color_viridis_d(\n      option = \"magma\", direction = -1, begin = .1, end = .9, name = NULL,\n      guide = guide_legend(override.aes = list(size = 7))\n    ) +\n    labs(\n      x = NULL, y = \"Reported bike shares on\\nweekend winter nights\", tag = \"P2\",\n    ) +\n    coord_cartesian(ylim = c(.5, NA), clip = \"off\")\n\n\nmy_colors &lt;- c(\"#cc0000\", \"#000080\")\n\np3 &lt;- bikes %&gt;%\n  group_by(week = lubridate::week(date), day_night, year) %&gt;%\n  summarize(count = sum(count)) %&gt;%\n  group_by(week, day_night) %&gt;%\n  mutate(avg = mean(count)) %&gt;%\n  ggplot(aes(x = week, y = count, group = interaction(day_night, year))) +\n    geom_line(color = \"grey65\", size = 1) +\n    geom_line(aes(y = avg, color = day_night), stat = \"unique\", size = 1.7) +\n    annotate(\n      geom = \"text\", label = c(\"Day\", \"Night\"), color = my_colors,\n      x = c(5, 18), y = c(125000, 29000), size = 8, fontface = \"bold\"\n    ) +\n    scale_x_continuous(breaks = c(1, 1:10*5)) +\n    scale_y_continuous(labels = scales::comma_format()) +\n    scale_color_manual(values = my_colors, guide = \"none\") +\n    labs(\n      x = \"Week of the Year\", y = \"Reported bike shares\\n(cumulative # per week)\", tag = \"P3\",\n    )\n#&gt; `summarise()` has grouped output by 'week', 'day_night'. You can override using\n#&gt; the `.groups` argument.\n\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n(p1 + p2) / p3\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + p2) / p3 + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n((p1 + p2) / p3 & theme(legend.justification = \"top\")) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + p2) / p3 & theme(legend.position = \"none\", plot.background = element_rect(color = \"black\", size = 3))\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n((p1 + p2) / p3 & theme(legend.position = \"none\")) +\n  plot_layout(heights = c(.2, .1), widths = c(2, 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\npicasso &lt;- \"\nAAAAAA#BBBB\nCCCCCCCCC##\nCCCCCCCCC##\"\n(p1 + p2 + p3 & theme(legend.position = \"none\")) + plot_layout(design = picasso)\n\n\n\n\n\n\n\n\n\n\n\nCode\n\ntext &lt;- tibble(\n  x = 0, y = 0, label = \"Lorem ipsum dolor sit amet, **consectetur adipiscing elit**, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation &lt;b style='color:#000080;'&gt;ullamco laboris nisi&lt;/b&gt; ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat &lt;b style='color:#cc0000;'&gt;cupidatat non proident&lt;/b&gt;, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n)\n\npt &lt;- ggplot(text, aes(x = x, y = y)) +\n  ggtext::geom_textbox(\n    aes(label = label),\n    box.color = NA, width = unit(23, \"lines\"),\n    color = \"grey40\", size = 6.5, lineheight = 1.4\n  ) +\n  coord_cartesian(expand = FALSE, clip = \"off\") +\n  theme_void()\n\npt\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + pt) / p3\n\n\n\n\n\n\n\n\n\n\n\nCode\np1 + inset_element(p2, l = .6, b = .1, r = 1, t = .6)\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + inset_element(p2, l = .6, b = .1, r = 1, t = .6) + pt) / p3",
    "crumbs": [
      "ALL",
      "Graphic Design with ggplot2",
      "Working with Layouts and Composition"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html",
    "href": "contents/website/leaflet4r/index.html",
    "title": "Leaflet for R",
    "section": "",
    "text": "Leaflet for Rの学習ノートです\nAPI reference\nすべてではなく気になった部分だけを記録しています\nalbersusaパッケージはここインストールしています",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#文字列へのリンク",
    "href": "contents/website/leaflet4r/index.html#文字列へのリンク",
    "title": "Leaflet for R",
    "section": "3.1 文字列へのリンク",
    "text": "3.1 文字列へのリンク\nleaflet map",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#画像へのリンク",
    "href": "contents/website/leaflet4r/index.html#画像へのリンク",
    "title": "Leaflet for R",
    "section": "3.2 画像へのリンク",
    "text": "3.2 画像へのリンク\nlightboxの効果はないのに注意すること.\n\n\n\n画像へのリンク作成",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#geojson",
    "href": "contents/website/leaflet4r/index.html#geojson",
    "title": "Leaflet for R",
    "section": "3.3 GeoJSON",
    "text": "3.3 GeoJSON\n\n\nCode\ngeojson &lt;- \"https://www.geospatial.jp/ckan/dataset/669fc4ff-317b-48cf-a43d-9340ff04ff18/resource/ccff82d4-2353-49e6-9845-371ee6d622de/download/biomasspowerstation42.geojson\"\ngeojson &lt;- \"https://www.geospatial.jp/ckan/dataset/a4f2f167-1433-47ea-b7be-ce88660a30a2/resource/72f6ed4e-a68c-4082-a90f-dd7e5423d87a/download/p35_18_03.geojson\"\nx &lt;- fromJSON(file = geojson)\ny &lt;- read_sf(geojson)\nytext &lt;- \n    y %&gt;%\n    \"st_geometry&lt;-\"(NULL) %&gt;%\n    transpose() %&gt;%\n    map( ~ glue(\"{names(.x)} = {.x}\")) %&gt;%\n    map_chr(~ str_c(.x, collapse = \"&lt;br/&gt;\"))\nas_longstyle_htmltable &lt;- function(data) {\n    data %&gt;%\n        mutate(across(everything(), as.character)) %&gt;%\n        rowwise() %&gt;%\n        summarise(htmltbl = list(tibble(`属性` = names(cur_data()), `値` = flatten_chr(cur_data())))) %&gt;%\n        mutate(htmltbl = map(htmltbl, ~ kable(.x, format = \"html\"))) %&gt;%\n        mutate(htmltbl = map(htmltbl, ~ str_replace(.x, \"&lt;table&gt;\", \"&lt;table style='border: solid 1px #f8f8f8;border-collapse:collapse;height:auto;max-height:35vh;overflow-y:scroll'&gt;\"))) %&gt;%\n        mutate(htmltbl = map(htmltbl, ~ str_replace_all(.x, \"&lt;th style=\\\"text-align:left;\\\"&gt;\", \"&lt;th style=\\\"text-align:center;background-color:black;color:white\\\"&gt;\"))) %&gt;%\n        mutate(htmltbl = map(htmltbl, ~ str_replace_all(.x, \"&lt;tr&gt;\", \"&lt;tr style='border: solid 1px #f8f8f8;'&gt;\"))) %&gt;%\n        mutate(htmltbl = map(htmltbl, ~ str_replace_all(.x, \"&lt;td style=\\\"text-align:left;\\\"&gt;\", \"&lt;td style=\\\"text-align:left;padding:0.5em\\\"&gt;\")))\n}\n\nyattr      &lt;- \"st_geometry&lt;-\"(y, NULL)\nytabletext &lt;- as_longstyle_htmltable(yattr)\n#&gt; Warning: There was 1 warning in `summarise()`.\n#&gt; ℹ In argument: `htmltbl = list(tibble(属性 = names(cur_data()), 値 =\n#&gt;   flatten_chr(cur_data())))`.\n#&gt; ℹ In row 1.\n#&gt; Caused by warning:\n#&gt; ! `cur_data()` was deprecated in dplyr 1.1.0.\n#&gt; ℹ Please use `pick()` instead.\n\n\nrm(m)\nm &lt;- \n    leaflet(\n        width = \"100%\",\n        height = \"800\"\n    ) %&gt;%\n    addTiles(urlTemplate = \"https://cyberjapandata.gsi.go.jp/xyz/seamlessphoto/{z}/{x}/{y}.jpg\", group = \"空中写真\", layerId = \"photo\") %&gt;%\n    addTiles(urlTemplate = \"http://cyberjapandata.gsi.go.jp/xyz/std/{z}/{x}/{y}.png\", group = \"標準地図\", layerId = \"gsi\") %&gt;%\n    addTiles(urlTemplate = \"https://tile.geospatial.jp/ks-shinsuisoutei/A31-19_83_SHP_tile/{z}/{x}/{y}.png\", group = \"MLIT\") %&gt;%\n    addFeatures(data = st_geometry(y), popup = ytabletext$htmltbl, group = \"new\") %&gt;%\n    addLayersControl(\n        baseGroup     = c(\"標準地図\", \"空中写真\"), \n        overlayGroups = c(\"new\", \"MLIT\")\n    )\n    \n\nm %&gt;%\n    addFeatures(data = st_geometry(y), popup = ytabletext$htmltbl, group = \"new2\") %&gt;%\n    addLayersControl(\n        baseGroup     = c(\"標準地図\", \"空中写真\"), \n        overlayGroups = c(\"new\", \"new2\")\n    )",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#geotiff",
    "href": "contents/website/leaflet4r/index.html#geotiff",
    "title": "Leaflet for R",
    "section": "3.4 GeoTiff",
    "text": "3.4 GeoTiff\n\n\nCode\nlibrary(stars)\nlibrary(leafem)\n\nr &lt;- read_stars(here(cur_dir, \"data/DCWD_map.tif\"))\nleaflet() %&gt;%\n    addTiles(group = \"OpenStreetMap\") %&gt;% \n    addStarsRGB(r)",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#gps",
    "href": "contents/website/leaflet4r/index.html#gps",
    "title": "Leaflet for R",
    "section": "3.5 GPS",
    "text": "3.5 GPS\n\n\nCode\nlibrary(leaflet)\nlibrary(leaflet.extras)\n\nmap &lt;- leaflet() %&gt;% addTiles()\n\nmap &lt;- addControlGPS(\n    map, \n    options = gpsOptions(\n        position = \"topleft\", \n        activate = TRUE, \n        autoCenter = TRUE, \n        maxZoom = 10, \n        setView = TRUE\n    )\n)\n\nactivateGPS(map)",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/index.html#vector-tile",
    "href": "contents/website/leaflet4r/index.html#vector-tile",
    "title": "Leaflet for R",
    "section": "3.6 Vector tile",
    "text": "3.6 Vector tile\n\n\nCode\nlibrary(leaflet)\nlibrary(htmltools)\n\nm &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%\n  setView(lng = 174.768, lat = -36.852, zoom = 10)\n\nm_html &lt;- as.character(m)\n\nvector_tile_js &lt;- \"\nL.vectorGrid.protobuf('https://cyberjapandata.gsi.go.jp/xyz/experimental_bvmap/{z}/{x}/{y}.pbf').addTo(map);\n\"\n\nbrowsable(\n  tagList(\n    tags$head(tags$script(src = \"https://unpkg.com/leaflet.vectorgrid/dist/Leaflet.VectorGrid.js\")),\n    HTML(m_html),\n    tags$script(HTML(vector_tile_js))\n  )\n)",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html",
    "href": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html",
    "title": "03 DBTの開発環境を構築しよう",
    "section": "",
    "text": "Jinjaテンプレートエンジンを使用してSQLを拡張\nYAMLファイルでモデルのメタデータを管理\nソフトウェア開発の手法をデータ分析に適用\n\n\n\n\n\n\njaffle shop\n\n\n\n\n\nseeds\n\nraw\n\nstaging\n\nstg\n\nmarts\n\nfact/dim"
  },
  {
    "objectID": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html#dbtの利点",
    "href": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html#dbtの利点",
    "title": "03 DBTの開発環境を構築しよう",
    "section": "",
    "text": "Jinjaテンプレートエンジンを使用してSQLを拡張\nYAMLファイルでモデルのメタデータを管理\nソフトウェア開発の手法をデータ分析に適用"
  },
  {
    "objectID": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html#公式サンプルデータセット",
    "href": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html#公式サンプルデータセット",
    "title": "03 DBTの開発環境を構築しよう",
    "section": "",
    "text": "jaffle shop"
  },
  {
    "objectID": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html#層構成",
    "href": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html#層構成",
    "title": "03 DBTの開発環境を構築しよう",
    "section": "",
    "text": "seeds\n\nraw\n\nstaging\n\nstg\n\nmarts\n\nfact/dim"
  },
  {
    "objectID": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html#dbt",
    "href": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html#dbt",
    "title": "03 DBTの開発環境を構築しよう",
    "section": "2.1 dbt",
    "text": "2.1 dbt\n\n2.1.1 ビルド\njaffle_shopにカレントディレクトリを移動して、dbtを実行します\npoetry run dbt run --profiles-dir . \n特定のテーブルだけを更新するとき\npoetry run dbt run --select my_model\n\n\n2.1.2 テスト\nテストを実行したい。modelsの下に、schema.yamlを作成して、テストを定義します。\n# すべてのテストを実行\npoetry run dbt test --profiles-dir .\n\n# 特定のモデルのテストのみ実行\npoetry run dbt test --select stg_customers --profiles-dir .\nカスタムのテストをおこないたいときは、testsディレクトリにSQLファイルを作成します。\npoetry run dbt test --select assert_order_has_items --profiles-dir .\ndbtのカスタムテストでは「例外」を返す形で定義する。 つまり、期待値は0であるようなクエリを定義して、そのクエリがレコードを返すようだとテストが失敗ということになります。\n\n\n2.1.3 ドキュメント\nドキュメントを生成するときには次のコマンドを実行する。\npoetry run dbt docs generate --profiles-dir .\nサーバーの立ち上げ\npoetry run dbt docs serve --profiles-dir .\n\n\n2.1.4 ソース\ndbtで追加したわけでないテーブルについては、ソースとして、schema.yamlに定義します。 そのときに, databaseやschemaを指定する必要があり, それに対して, 名前が付くという形になります。\nカラム定義はテスト用なので必須ではない。\n# ソースが正しく定義されているか確認\npoetry run dbt list --resource-type source --profiles-dir .\n\n# ソースデータの鮮度を確認（freshness設定がある場合）\npoetry run dbt source freshness --profiles-dir .\n\n# ソース定義に含まれるテストを実行\npoetry run dbt test --select \"source:*\" --profiles-dir .\n\n\n2.1.5 インクリメンタル\n# 初回実行（フルリフレッシュ）\npoetry run dbt run --select fact_orders_incremental --profiles-dir .\n\n# 2回目以降の実行（増分のみ）\npoetry run dbt run --select fact_orders_incremental --profiles-dir .\n\n# 強制的にフルリフレッシュ\npoetry run dbt run --select fact_orders_incremental --full-refresh --profiles-dir .\n\n\n2.1.6 マクロとコンパイル\nマクロを定義して、SQLの中で使うことができます。\n\n{{\n    config(\n        materialized='table'\n    )\n}}\n\nselect\n    store_id,\n    {% for product_type in get_product_types() %}\n    count(distinct case when p.type = '{{ product_type }}' then i.sku end) as {{ product_type }}_product_count\n    {% if not loop.last %},{% endif %}\n    {% endfor %}\nfrom {{ ref('stg_orders') }} o\njoin {{ ref('stg_items') }} i on o.order_id = i.order_id\njoin {{ ref('stg_products') }} p on i.product_sku = p.product_sku\ngroup by store_id\nコンパイルをおこなう\npoetry run dbt compile --select product_count_by_store --profiles-dir .\n\n\n2.1.7 dbt-Utils\ndbt-utilsを使うと、dbt側で作成された便利なマクロが使えます。\npackages.ymlを作成する. 次を実行するとUtilsがインストールされます。\ndbt deps\n\n\n2.1.8 スナップショット\nスナップショットを作成することで, マスタの変更履歴などを管理することができる\n{% snapshot customers_snapshot %}\n\n{{\n    config(\n      target_schema='snapshots',\n      unique_key='id',\n      strategy='check',\n      check_cols=['name'],\n    )\n}}\n\nselect\n    *\nfrom {{ source('jaffle_shop_raw', 'customers') }}\n\n{% endsnapshot %}\npoetry run dbt snapshot --profiles-dir ."
  },
  {
    "objectID": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html#poetry",
    "href": "contents/website/udemy_dbt/03_dbtの開発環境を構築.html#poetry",
    "title": "03 DBTの開発環境を構築しよう",
    "section": "2.2 poetry",
    "text": "2.2 poetry\npoetryのバージョンがおかしいとき\n\npoetryの依存関係がおかしいとき, pyprojectを変数して, 一旦ロックして, installし直す\npoetry lock\npoetry install"
  },
  {
    "objectID": "contents/website/udemy_docker_from_zero/04_Dockerを使ってみる.html",
    "href": "contents/website/udemy_docker_from_zero/04_Dockerを使ってみる.html",
    "title": "04 Dockerを使ってみる",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch01_map_object.html",
    "href": "contents/website/leaflet4r/chapters/ch01_map_object.html",
    "title": "The Map Widget",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)\n\n\n\n1 ランダムメモ\n\n基本的にはleafletのAPIドックと同じものが動こう\n\n\n\n2 プリミティブな図形\n\ndata frame作成してしまうのが１番簡単である\n\n\n\nCode\n# add some circles to a map\ndf = data.frame(Lat = 1:10, Long = rnorm(10))\nleaflet(df) |&gt; addCircles()\n\n\n\n\n\n\n\n\nCode\nlibrary(maps)\n#&gt; \n#&gt; Attaching package: 'maps'\n#&gt; The following object is masked from 'package:purrr':\n#&gt; \n#&gt;     map\nmapStates = map(\"state\", fill = TRUE, plot = FALSE)\nleaflet(data = mapStates) %&gt;% addTiles() %&gt;%\n  addPolygons(fillColor = topo.colors(10, alpha = NULL), stroke = FALSE)\n\n\n\n\n\n\n上記の場合にはデータはリスト形になっている. NAがあるとペンが外れるGISではよくあるプリミティブな形状で、 記述することができる。\n\n\nCode\nmapStates |&gt; str(1)\n#&gt; List of 4\n#&gt;  $ x    : num [1:15599] -87.5 -87.5 -87.5 -87.5 -87.6 ...\n#&gt;  $ y    : num [1:15599] 30.4 30.4 30.4 30.3 30.3 ...\n#&gt;  $ range: num [1:4] -124.7 -67 25.1 49.4\n#&gt;  $ names: chr [1:63] \"alabama\" \"arizona\" \"arkansas\" \"california\" ...\n#&gt;  - attr(*, \"class\")= chr \"map\"\n\n\n\n\n3 Formula interface\nRらしい記述を使うことが可能である.\n\n\nCode\nm = leaflet() %&gt;% addTiles()\ndf = data.frame(\n  lat = rnorm(100),\n  lng = rnorm(100),\n  size = runif(100, 5, 20),\n  color = sample(colors(), 100)\n)\nm = leaflet(df) %&gt;% addTiles()\nm %&gt;% addCircleMarkers(radius = ~size, color = ~color, fill = FALSE)\n#&gt; Assuming \"lng\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\n\n\nCode\nm %&gt;% addCircleMarkers(radius = runif(100, 4, 10), color = c('red'))\n#&gt; Assuming \"lng\" and \"lat\" are longitude and latitude, respectively\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "The Map Widget"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch03_markers.html",
    "href": "contents/website/leaflet4r/chapters/ch03_markers.html",
    "title": "Markers",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Markers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch03_markers.html#customizing-marker-icons",
    "href": "contents/website/leaflet4r/chapters/ch03_markers.html#customizing-marker-icons",
    "title": "Markers",
    "section": "1.1 Customizing Marker Icons",
    "text": "1.1 Customizing Marker Icons\n\n\nCode\ngreenLeafIcon &lt;- makeIcon(\n  iconUrl = \"https://leafletjs.com/examples/custom-icons/leaf-green.png\",\n  iconWidth = 38, iconHeight = 95,\n  iconAnchorX = 22, iconAnchorY = 94,\n  shadowUrl = \"https://leafletjs.com/examples/custom-icons/leaf-shadow.png\",\n  shadowWidth = 50, shadowHeight = 64,\n  shadowAnchorX = 4, shadowAnchorY = 62\n)\n\nleaflet(data = quakes[1:4,]) %&gt;% addTiles() %&gt;%\n  addMarkers(~long, ~lat, icon = greenLeafIcon)\n\n\n\n\n\n\n\n\nCode\noceanIcons &lt;- iconList(\n  ship = makeIcon(\"ferry-18.png\", \"ferry-18@2x.png\", 18, 18),\n  pirate = makeIcon(\"danger-24.png\", \"danger-24@2x.png\", 24, 24)\n)\n\n# Some fake data\ndf &lt;- sp::SpatialPointsDataFrame(\n  cbind(\n    (runif(20) - .5) * 10 - 90.620130,  # lng\n    (runif(20) - .5) * 3.8 + 25.638077  # lat\n  ),\n  data.frame(type = factor(\n    ifelse(runif(20) &gt; 0.75, \"pirate\", \"ship\"),\n    c(\"ship\", \"pirate\")\n  ))\n)\n\nleaflet (df) |&gt; \n    addTiles() |&gt; \n    addMarkers(icon = ~ oceanIcons[type])",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Markers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch03_markers.html#marker-clusters",
    "href": "contents/website/leaflet4r/chapters/ch03_markers.html#marker-clusters",
    "title": "Markers",
    "section": "1.2 Marker Clusters",
    "text": "1.2 Marker Clusters\n\n\nCode\nleaflet(quakes) %&gt;% addTiles() %&gt;% addMarkers(\n  clusterOptions = markerClusterOptions()\n)\n#&gt; Assuming \"long\" and \"lat\" are longitude and latitude, respectively",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Markers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch05_lines_and_shapes.html",
    "href": "contents/website/leaflet4r/chapters/ch05_lines_and_shapes.html",
    "title": "Lines and Shapes",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Lines and Shapes"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch05_lines_and_shapes.html#simplifying-complex-polygons",
    "href": "contents/website/leaflet4r/chapters/ch05_lines_and_shapes.html#simplifying-complex-polygons",
    "title": "Lines and Shapes",
    "section": "1.1 Simplifying complex polygons",
    "text": "1.1 Simplifying complex polygons\nデータが複雑が重たいときにはシンプリファイする必要がある.\n```{r}\nsimplified &lt;- rmapshaper::ms_simplify(fullsize)\nobject.size(simplified)\n```",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Lines and Shapes"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch07_raster_images.html",
    "href": "contents/website/leaflet4r/chapters/ch07_raster_images.html",
    "title": "Raster Images",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")\n\n\n2次元のSpatRasterオブジェクトについては,addRasterImageにより表示することが可能である. 読み込んだラスターデータは,EPSGは3857に変換されるとともに, 各セルの値はRGBAに変換される.\n\n1 Large Raster\nterra::resampleを使って解像度を調整すること.\n\n\n2 Projection Performance\n大量のデータがある場合にはleaflet::projectRasterFroLeaflet を使って事前にデータを変換しておくのがよい.\nその上でaddRasterImageでproject=FALSEにしておく.\n\n\n3 Coloring\n色々できるという感じです.\n\n\n4 Example\n\n\nCode\nr &lt;- rast(here(data_dir, \"FG-GML-5339-05-DEM5A.tif\")) \n\npal &lt;- \n    colorNumeric(\n        c(\"#0C2C84\", \"#41B6C4\", \"#FFFFCC\"), \n        values(r),\n        na.color = \"transparent\")\n\nleaflet() |&gt; \n    addTiles() |&gt;\n    addRasterImage(r, colors = pal, opacity = .5) |&gt; \n    addLegend(\n        pal = pal, \n        values = values(r), \n        title = \"Surface terrain\"\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Raster Images"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch09_Colors.html",
    "href": "contents/website/leaflet4r/chapters/ch09_Colors.html",
    "title": "Colors",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Colors"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch09_Colors.html#coloring-continuous-data",
    "href": "contents/website/leaflet4r/chapters/ch09_Colors.html#coloring-continuous-data",
    "title": "Colors",
    "section": "2.1 Coloring continuous data",
    "text": "2.1 Coloring continuous data\n\n\nCode\ncountries &lt;- sf::st_read(\"https://rstudio.github.io/leaflet/json/countries.geojson\")\n#&gt; Reading layer `countries' from data source \n#&gt;   `https://rstudio.github.io/leaflet/json/countries.geojson' \n#&gt;   using driver `GeoJSON'\n#&gt; Simple feature collection with 177 features and 2 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -180 ymin: -90 xmax: 180 ymax: 83.64513\n#&gt; Geodetic CRS:  WGS 84\nmap &lt;- leaflet(countries)\n\n\n\n\nCode\npar(mar = c(5,5,0,0), cex = 0.8)\nhist(countries$gdp_md_est, breaks = 20, main = \"\")\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create a continuous palette function\npal &lt;- colorNumeric(\n  palette = \"Blues\",\n  domain = countries$gdp_md_est)\n\n# Apply the function to provide RGB colors to addPolygons\nmap %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~pal(gdp_md_est))\n\n\n\n\n\n\n\n\nCode\nbinpal &lt;- colorBin(\"Blues\", countries$gdp_md_est, 6, pretty = FALSE)\n\nmap %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~binpal(gdp_md_est))\n\n\n\n\n\n\n\n\nCode\nqpal &lt;- colorQuantile(\"Blues\", countries$gdp_md_est, n = 7)\nmap %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~qpal(gdp_md_est))",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Colors"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch09_Colors.html#coloring-categorical-data",
    "href": "contents/website/leaflet4r/chapters/ch09_Colors.html#coloring-categorical-data",
    "title": "Colors",
    "section": "2.2 Coloring categorical data",
    "text": "2.2 Coloring categorical data\n\n\nCode\n# Make up some random levels. (TODO: Better example)\ncountries$category &lt;- factor(sample.int(5L, nrow(countries), TRUE))\n\nfactpal &lt;- colorFactor(topo.colors(5), countries$category)\n\nleaflet(countries) %&gt;%\n  addPolygons(stroke = FALSE, smoothFactor = 0.2, fillOpacity = 1,\n    color = ~factpal(category))",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Colors"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html",
    "href": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html",
    "title": "Show/Hide Layers",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Show/Hide Layers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html#interactive-layer-display",
    "href": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html#interactive-layer-display",
    "title": "Show/Hide Layers",
    "section": "1.1 Interactive Layer Display",
    "text": "1.1 Interactive Layer Display\n\n\nCode\noutline &lt;- quakes[chull(quakes$long, quakes$lat),]\n\nmap &lt;- leaflet(quakes) %&gt;%\n  # Base groups\n  addTiles(group = \"OSM (default)\") %&gt;%\n  addProviderTiles(providers$Stamen.Toner, group = \"Toner\") %&gt;%\n  addProviderTiles(providers$Stamen.TonerLite, group = \"Toner Lite\") %&gt;%\n  # Overlay groups\n  addCircles(~long, ~lat, ~10^mag/5, stroke = F, group = \"Quakes\") %&gt;%\n  addPolygons(data = outline, lng = ~long, lat = ~lat,\n    fill = F, weight = 2, color = \"#FFFFCC\", group = \"Outline\") %&gt;%\n  # Layers control\n  addLayersControl(\n    baseGroups = c(\"OSM (default)\", \"Toner\", \"Toner Lite\"),\n    overlayGroups = c(\"Quakes\", \"Outline\"),\n    options = layersControlOptions(collapsed = FALSE)\n  )\nmap",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Show/Hide Layers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html#programmatic-layer-display",
    "href": "contents/website/leaflet4r/chapters/ch11_show_hide_layers.html#programmatic-layer-display",
    "title": "Show/Hide Layers",
    "section": "1.2 Programmatic Layer Display",
    "text": "1.2 Programmatic Layer Display\nshowGroupやhideGroupを使うことでレイヤーの表示、 非表示をプログラムから管理することが可能となる.\nこれは特にShinyから操作するときに意味を持つ.\n\n\nCode\nmap %&gt;% hideGroup(\"Outline\")",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Show/Hide Layers"
    ]
  },
  {
    "objectID": "contents/website/leaflet4r/chapters/ch13_projection.html",
    "href": "contents/website/leaflet4r/chapters/ch13_projection.html",
    "title": "Projections",
    "section": "",
    "text": "Code\nlibrary(here)\nlibrary(shiny)\nlibrary(leaflet)\nlibrary(leafem)\nlibrary(htmltools)\nlibrary(htmlwidgets)\nlibrary(rjson)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(stars)\nlibrary(terra)\nlibrary(glue)\nlibrary(knitr)\n\ndata_dir &lt;- here(\"contents/website/leaflet4r/data\")\n\n\nleafletではWGS84で特定できる座標値であることが期待されている. 標準では, 3857で表示される. その他の座標系で表示させたいときには Proj4Leafletプラグインを組み込む必要がある.\n\n1 Defining a custom CRS\n\n\nCode\ncrs &lt;- leafletCRS(crsClass = \"L.Proj.CRS\", code = \"ESRI:102003\",\n  proj4def = \"+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=37.5 +lon_0=-96 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs\",\n  resolutions = 1.5^(25:15))\n\n\n\n\n2 Dsiplaying basemap tiles with cusotm projections\nbasemapが対応していないのでレンダリングが出来ない.\n\n\nCode\n\nepsg3006 &lt;- leafletCRS(crsClass = \"L.Proj.CRS\", code = \"EPSG:3006\",\n  proj4def = \"+proj=utm +zone=33 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs\",\n  resolutions = 2^(13:-1), # 8192 down to 0.5\n  origin = c(0, 0)\n)\n\ntile_url &lt;- \"http://api.geosition.com/tile/osm-bright-3006/{z}/{x}/{y}.png\"\ntile_attrib &lt;- \"Map data &copy; &lt;a href='http://www.openstreetmap.org/copyright'&gt;OpenStreetMap contributors&lt;/a&gt;, Imagery &copy; 2013 &lt;a href='http://www.kartena.se/'&gt;Kartena&lt;/a&gt;\"\n\nleaflet(options = leafletOptions(worldCopyJump = F, crs = epsg3006)) %&gt;%\n  setView(11.965053, 57.70451, 13) %&gt;%\n  addTiles(urlTemplate = tile_url,\n    attribution = tile_attrib,\n    options = tileOptions(minZoom = 0, maxZoom = 14, continuousWorld = T)) %&gt;%\n  addMarkers(11.965053, 57.70451)\n\n\n\n\n\n\n\n\nCode\nleaflet() %&gt;%\n  setView(11.965053, 57.70451, 16) %&gt;%\n  addTiles() %&gt;%\n  addMarkers(11.965053, 57.70451)\n\n\n\n\n\n\n\n\n3 Displaying shapes with cusotm projections\n\n\nCode\nlibrary(sp)\nlibrary(albersusa)\n#&gt; Please note that 'maptools' will be retired during October 2023,\n#&gt; plan transition at your earliest convenience (see\n#&gt; https://r-spatial.org/r/2023/05/15/evolution4.html and earlier blogs\n#&gt; for guidance);some functionality will be moved to 'sp'.\n#&gt;  Checking rgeos availability: TRUE\n#&gt; Please note that rgdal will be retired during October 2023,\n#&gt; plan transition to sf/stars/terra functions using GDAL and PROJ\n#&gt; at your earliest convenience.\n#&gt; See https://r-spatial.org/r/2023/05/15/evolution4.html and https://github.com/r-spatial/evolution\n#&gt; rgdal: version: 1.6-7, (SVN revision 1203)\n#&gt; Geospatial Data Abstraction Library extensions to R successfully loaded\n#&gt; Loaded GDAL runtime: GDAL 3.6.2, released 2023/01/02\n#&gt; Path to GDAL shared files: H:/R_USER/R/win-library/4.3/rgdal/gdal\n#&gt;  GDAL does not use iconv for recoding strings.\n#&gt; GDAL binary built with GEOS: TRUE \n#&gt; Loaded PROJ runtime: Rel. 9.2.0, March 1st, 2023, [PJ_VERSION: 920]\n#&gt; Path to PROJ shared files: C:\\Program Files\\PostgreSQL\\12\\share\\contrib\\postgis-3.1\\proj\n#&gt; PROJ CDN enabled: FALSE\n#&gt; Linking to sp version:1.6-1\n#&gt; To mute warnings of possible GDAL/OSR exportToProj4() degradation,\n#&gt; use options(\"rgdal_show_exportToProj4_warnings\"=\"none\") before loading sp or rgdal.\n#&gt; rgeos version: 0.6-3, (SVN revision 696)\n#&gt;  GEOS runtime version: 3.11.2-CAPI-1.17.2 \n#&gt;  Please note that rgeos will be retired during October 2023,\n#&gt; plan transition to sf or terra functions using GEOS at your earliest convenience.\n#&gt; See https://r-spatial.org/r/2023/05/15/evolution4.html for details.\n#&gt;  GEOS using OverlayNG\n#&gt;  Linking to sp version: 2.0-0 \n#&gt;  Polygon checking: TRUE\n\nspdf &lt;- rmapshaper::ms_simplify(usa_sf(), keep = 0.1)\n#&gt; old-style crs object detected; please recreate object with a recent sf::st_crs()\npal &lt;- colorNumeric(\"Blues\", domain = spdf$pop_2014)\nepsg2163 &lt;- leafletCRS(\n  crsClass = \"L.Proj.CRS\",\n  code = \"EPSG:2163\",\n  proj4def = \"+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs\",\n  resolutions = 2^(16:7))\n\nleaflet(spdf, options = leafletOptions(crs = epsg2163)) %&gt;%\n  addPolygons(weight = 1, color = \"#444444\", opacity = 1,\n    fillColor = ~pal(pop_2014), fillOpacity = 0.7, smoothFactor = 0.5,\n    label = ~paste(name, pop_2014),\n    labelOptions = labelOptions(direction = \"auto\"))\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "Leaflet for R",
      "Projections"
    ]
  },
  {
    "objectID": "contents/website/quarto/guide/index.html",
    "href": "contents/website/quarto/guide/index.html",
    "title": "Quarto Guide",
    "section": "",
    "text": "1 はじめに\nQuartoに関する学習ノートです。\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "ALL",
      "QuartoDashboard",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/website/udemy_database-specialist/am-2-note.html",
    "href": "contents/website/udemy_database-specialist/am-2-note.html",
    "title": "Udemy 午前2",
    "section": "",
    "text": "Udemyにおけるデータベーススペシャリスト講座 午前2のノート\n動画ではMySQLを使用しているがここではDuckDBを使う\nSQLの資料\nDBMSの資料\n設計の資料",
    "crumbs": [
      "ALL",
      "Udemyデータベーススペシャリスト",
      "Udemy 午前2"
    ]
  },
  {
    "objectID": "contents/website/udemy_database-specialist/am-2-note.html#値論理",
    "href": "contents/website/udemy_database-specialist/am-2-note.html#値論理",
    "title": "Udemy 午前2",
    "section": "3.1 3値論理",
    "text": "3.1 3値論理\nNULLはナルと読む。 NULLは状態であり、演算が定義されていない。NULLを含む演算はNULLになる。\n\n\nCode\n%%sql \n\nselect (4 &gt; NULL) or (4 &gt; 6) or (NULL &gt; 6);\n\n\nRunning query in 'duckdb'\n\n\n\n\n\n((4 &gt; NULL) OR (4 &gt; 6) OR (NULL &gt; 6))\n\n\n\n\nNone\n\n\n\n\n\n\n\nCode\nconn.close()",
    "crumbs": [
      "ALL",
      "Udemyデータベーススペシャリスト",
      "Udemy 午前2"
    ]
  },
  {
    "objectID": "contents/libs/Python/duckdb/index.html",
    "href": "contents/libs/Python/duckdb/index.html",
    "title": "duckdb",
    "section": "",
    "text": "1 はじめに\n\nduckdbの使い方を整理する\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "duckdb",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/plotnine/index.html",
    "href": "contents/libs/Python/plotnine/index.html",
    "title": "plotnine",
    "section": "",
    "text": "1 はじめに\nplotnine\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "plotnine",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/pydantic/index.html",
    "href": "contents/libs/Python/pydantic/index.html",
    "title": "Pydantic 2.4",
    "section": "",
    "text": "#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.0\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function\n\n\n1 はじめに\n\npydantic\ndataclassの高機能バージョン\nrustで記述されているので高速\nPythonを使う上で欠かすことができない\n\n\n\n2 Quick Example\n\n\nCode\nfrom datetime import datetime\nfrom typing import Tuple\n\nfrom pydantic import BaseModel\n\n\nclass Delivery(BaseModel):\n    timestamp: datetime\n    dimensions: Tuple[int, int]\n\n\nm = Delivery(timestamp='2020-01-02T03:04:05Z', dimensions=['10', '20'])\nprint(repr(m.timestamp))\n#&gt; datetime.datetime(2020, 1, 2, 3, 4, 5, tzinfo=TzInfo(UTC))\n\n\n\n\nCode\n#&gt; datetime.datetime(2020, 1, 2, 3, 4, 5, tzinfo=TzInfo(UTC))\nprint(m.dimensions)\n#&gt; (10, 20)\n#&gt; (10, 20)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "pydantic",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/Python/sqlite3/02_io.html",
    "href": "contents/libs/Python/sqlite3/02_io.html",
    "title": "θ結合",
    "section": "",
    "text": "#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.4\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function\n\n\n1 はじめに\n\npalmerpenguinsが入ったテーブルを作成する。\n\n\n\n2 Quick Example\n\n\nCode\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns\nimport os\n\n# palmerpenguinsデータセットを読み込む\npenguins = sns.load_dataset('penguins')\n\n# 欠損値を削除する\npenguins = penguins.dropna()\n\n# SQLiteデータベースに接続（データベースが存在しない場合は新規作成）\nconn = sqlite3.connect(os.path.join(r.cur_dir, 'penguins.db'))\ncursor = conn.cursor()\n\n# テーブルを作成\ncursor.execute('''\n    CREATE TABLE IF NOT EXISTS penguins (\n        species TEXT,\n        island TEXT,\n        bill_length_mm REAL,\n        bill_depth_mm REAL,\n        flipper_length_mm REAL,\n        body_mass_g REAL,\n        sex TEXT\n    )\n''')\n#&gt; &lt;sqlite3.Cursor object at 0x0000024DC39990C0&gt;\n\ncursor.execute(\"DELETE FROM penguins\")\n#&gt; &lt;sqlite3.Cursor object at 0x0000024DC39990C0&gt;\n\n# pandas DataFrameをSQLiteテーブルに書き込む\npenguins.to_sql('penguins', conn, if_exists='replace', index=False)\n#&gt; 333\n\n# 接続をコミットして閉じる\nconn.commit()\nconn.close()\n\n\nif os.path.exists(os.path.join(r.cur_dir, 'penguins.db')):\n    print(\"SQLiteデータベース 'penguins.db' が既に存在します。\")\nelse:\n  print(\"penguins.dbの作成に失敗しました\")\n#&gt; SQLiteデータベース 'penguins.db' が既に存在します。\n\n\n\n\nCode\n(\n  penguins\n  .groupby('sex', as_index=False)\n  .agg(\n    bill_length_mm=('bill_length_mm', lambda var: var.mean().round(2))\n  )\n)\n#&gt;       sex  bill_length_mm\n#&gt; 0  Female           42.10\n#&gt; 1    Male           45.85\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "sqlite3",
      "θ結合"
    ]
  },
  {
    "objectID": "contents/libs/Python/sqlite3/index.html",
    "href": "contents/libs/Python/sqlite3/index.html",
    "title": "sqlite3",
    "section": "",
    "text": "1 はじめに\n\nsqlite3の使い方を整理する\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "sqlite3",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/DBI/spatialite.html",
    "href": "contents/libs/R/DBI/spatialite.html",
    "title": "Spatialite",
    "section": "",
    "text": "Code\n\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(showtext)\nlibrary(arrow)\nlibrary(nanoarrow)\nlibrary(odbc)\nlibrary(DBI)\nlibrary(RSQLite)\nlibrary(sf)\n\n\ncur_dir &lt;- here::here(\"contents/libs/R/DBI\")\nshowtext::showtext_auto()\nsysfonts::font_add_google(\"Noto Sans\")"
  },
  {
    "objectID": "contents/libs/R/DBI/spatialite.html#基本",
    "href": "contents/libs/R/DBI/spatialite.html#基本",
    "title": "Spatialite",
    "section": "2.1 基本",
    "text": "2.1 基本\n\nコネクションの設定\n事前に、シェルで.load mod_spatialiteなどを実行しておく\nこれらはRSQLiteからは実行することができない\nsfを使うにしても先にデータベースのファイルは作成しておく必要がある\n\n.load mod_spatialite\nselect InitSpatialMetadata();\nselect spatialite_version();"
  },
  {
    "objectID": "contents/libs/R/DBI/spatialite.html#sfから使う",
    "href": "contents/libs/R/DBI/spatialite.html#sfから使う",
    "title": "Spatialite",
    "section": "2.2 sfから使う",
    "text": "2.2 sfから使う\nここで作成したＤＢにはInitSpatialMetadataが実行されていないので、シェルで実行する必要がある。\n\n\nCode\ninputfile &lt;- file.path(cur_dir, \"dataset\", \"N03-20250101_prefecture.shp\")\npref &lt;- read_sf(inputfile)\n\npref\n#&gt; Simple feature collection with 121990 features and 6 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 122.9326 ymin: 20.42275 xmax: 153.9867 ymax: 45.55724\n#&gt; Geodetic CRS:  JGD2011\n#&gt; # A tibble: 121,990 × 7\n#&gt;    N03_001 N03_002 N03_003 N03_004 N03_005 N03_007                      geometry\n#&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;                   &lt;POLYGON [°]&gt;\n#&gt;  1 北海道  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    01000   ((141.9412 45.52146, 141.941…\n#&gt;  2 北海道  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    01000   ((148.7526 45.55693, 148.752…\n#&gt;  3 北海道  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    01000   ((146.1667 44.5175, 146.1667…\n#&gt;  4 北海道  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    01000   ((146.832 43.88693, 146.8325…\n#&gt;  5 北海道  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    01000   ((141.1992 45.25882, 141.199…\n#&gt;  6 北海道  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    01000   ((139.5597 42.24942, 139.559…\n#&gt;  7 北海道  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    01000   ((140.9673 45.46422, 140.967…\n#&gt;  8 北海道  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    01000   ((146.135 43.5489, 146.135 4…\n#&gt;  9 北海道  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    01000   ((145.8877 43.46222, 145.887…\n#&gt; 10 北海道  &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    01000   ((146.3122 43.65147, 146.312…\n#&gt; # ℹ 121,980 more rows\n\n\n上記のデータからSpatialiteを作成する。\n\n\nCode\ntarget_db &lt;- file.path(cur_dir, \"spatial.sqlite\")\nst_write(\n  pref, \n  target_db, \n  layer = \"pref\", \n  delete_layer = TRUE, \n  driver = \"SQLite\"\n)\n#&gt; Deleting layer `pref' using driver `SQLite'\n#&gt; Writing layer `pref' to data source \n#&gt;   `C:/Users/suzuk/Dropbox/R/Workspace/RTipsSite/contents/libs/R/DBI/spatial.sqlite' using driver `SQLite'\n#&gt; Writing 121990 features with 6 fields and geometry type Polygon.\n\n\nメタデータの初期化をおこなう。少し時間がかかることに注意する。これをしないとQGISなどでメタデータエラーが出る。 SQLiteで直接開いてやらないと終わらない・・・？\n\"SELECT InitSpatialMetadata();\"\n作成したデータベースからレイヤーを読み出す\n\n\nCode\nst_layers(target_db)\n#&gt; Driver: SQLite \n#&gt; Available layers:\n#&gt;      layer_name geometry_type features fields crs_name\n#&gt; 1 pref__saitama       Polygon        2      1  JGD2011\n#&gt; 2          pref       Polygon   121990      6  JGD2011\n\n\n\nレイヤー名を指定してデータを抽出\n\n\n\nCode\npref_from_db &lt;- \n  st_read(\n    target_db, \n    layer = \"pref\"\n  )\n#&gt; Reading layer `pref' from data source \n#&gt;   `C:\\Users\\suzuk\\Dropbox\\R\\Workspace\\RTipsSite\\contents\\libs\\R\\DBI\\spatial.sqlite' \n#&gt;   using driver `SQLite'\n#&gt; Simple feature collection with 121990 features and 6 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 122.9326 ymin: 20.42275 xmax: 153.9867 ymax: 45.55724\n#&gt; Geodetic CRS:  JGD2011\n\npref_from_db\n#&gt; Simple feature collection with 121990 features and 6 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 122.9326 ymin: 20.42275 xmax: 153.9867 ymax: 45.55724\n#&gt; Geodetic CRS:  JGD2011\n#&gt; First 10 features:\n#&gt;    n03_001 n03_002 n03_003 n03_004 n03_005 n03_007\n#&gt; 1   北海道    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   01000\n#&gt; 2   北海道    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   01000\n#&gt; 3   北海道    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   01000\n#&gt; 4   北海道    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   01000\n#&gt; 5   北海道    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   01000\n#&gt; 6   北海道    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   01000\n#&gt; 7   北海道    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   01000\n#&gt; 8   北海道    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   01000\n#&gt; 9   北海道    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   01000\n#&gt; 10  北海道    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;    &lt;NA&gt;   01000\n#&gt;                          GEOMETRY\n#&gt; 1  POLYGON ((141.9412 45.52146...\n#&gt; 2  POLYGON ((148.7526 45.55693...\n#&gt; 3  POLYGON ((146.1667 44.5175,...\n#&gt; 4  POLYGON ((146.832 43.88693,...\n#&gt; 5  POLYGON ((141.1992 45.25882...\n#&gt; 6  POLYGON ((139.5597 42.24942...\n#&gt; 7  POLYGON ((140.9673 45.46422...\n#&gt; 8  POLYGON ((146.135 43.5489, ...\n#&gt; 9  POLYGON ((145.8877 43.46222...\n#&gt; 10 POLYGON ((146.3122 43.65147...\n\n\n\nsqlを使って抽出\n埼玉県を抽出する\n\n\n\nCode\nsaitama &lt;- \n  st_read(\n    target_db, \n    query = \"\n      SELECT n03_001, geometry\n      FROM pref a\n      WHERE a.n03_001 = '埼玉県'\n    \"\n  )\n#&gt; Reading query `\n#&gt;       SELECT n03_001, geometry\n#&gt;       FROM pref a\n#&gt;       WHERE a.n03_001 = '埼玉県'\n#&gt;     '\n#&gt; from data source `C:\\Users\\suzuk\\Dropbox\\R\\Workspace\\RTipsSite\\contents\\libs\\R\\DBI\\spatial.sqlite' \n#&gt;   using driver `SQLite'\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 138.7114 ymin: 35.75338 xmax: 139.9003 ymax: 36.28341\n#&gt; Geodetic CRS:  JGD2011\n\nsaitama\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 138.7114 ymin: 35.75338 xmax: 139.9003 ymax: 36.28341\n#&gt; Geodetic CRS:  JGD2011\n#&gt;   n03_001                       GEOMETRY\n#&gt; 1  埼玉県 POLYGON ((139.1357 36.27844...\n#&gt; 2  埼玉県 POLYGON ((139.2627 36.24473...\n\n\n\n\nCode\nsaitama |&gt; \n  ggplot() + \n  geom_sf() + \n  theme_dark()\n\n\n\n\n\n\n\n\n\n\n切り出したsaitmaのレイヤーを作成する\n\n\n\nCode\ntarget_db &lt;- file.path(cur_dir, \"spatial.sqlite\")\nst_write(\n  saitama, \n  target_db, \n  layer = \"pref__saitama\", \n  delete_layer = TRUE, \n  driver = \"SQLite\", \n  layer_options = c(\"SPATIALITE=YES\")\n)\n#&gt; Deleting layer `pref__saitama' using driver `SQLite'\n#&gt; Writing layer `pref__saitama' to data source \n#&gt;   `C:/Users/suzuk/Dropbox/R/Workspace/RTipsSite/contents/libs/R/DBI/spatial.sqlite' using driver `SQLite'\n#&gt; options:        SPATIALITE=YES\n#&gt; Warning in CPL_write_ogr(obj, dsn, layer, driver,\n#&gt; as.character(dataset_options), : GDAL Message 6: dataset\n#&gt; C:/Users/suzuk/Dropbox/R/Workspace/RTipsSite/contents/libs/R/DBI/spatial.sqlite\n#&gt; does not support layer creation option SPATIALITE\n#&gt; Writing 2 features with 1 fields and geometry type Polygon.\n\n\n\n\nCode\nst_layers(target_db)\n#&gt; Driver: SQLite \n#&gt; Available layers:\n#&gt;      layer_name geometry_type features fields crs_name\n#&gt; 1          pref       Polygon   121990      6  JGD2011\n#&gt; 2 pref__saitama       Polygon        2      1  JGD2011\n\n\n\n埼玉を呼び出す\n\n\n\nCode\nst_read(\n    target_db, \n    query = '\n      SELECT * \n      FROM \"pref__saitama\"\n    '\n) |&gt; \n  ggplot() + \n  geom_sf() + \n  theme_dark()\n#&gt; Reading query `\n#&gt;       SELECT * \n#&gt;       FROM \"pref__saitama\"\n#&gt;     '\n#&gt; from data source `C:\\Users\\suzuk\\Dropbox\\R\\Workspace\\RTipsSite\\contents\\libs\\R\\DBI\\spatial.sqlite' \n#&gt;   using driver `SQLite'\n#&gt; Simple feature collection with 2 features and 1 field\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 138.7114 ymin: 35.75338 xmax: 139.9003 ymax: 36.28341\n#&gt; Geodetic CRS:  JGD2011"
  },
  {
    "objectID": "contents/libs/R/GGally/working.html",
    "href": "contents/libs/R/GGally/working.html",
    "title": "GGally",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/GGally\")\nCode\n library(ComplexHeatmap)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "GGally",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/GGally/working.html#ggparcoord",
    "href": "contents/libs/R/GGally/working.html#ggparcoord",
    "title": "GGally",
    "section": "2.1 ggparcoord",
    "text": "2.1 ggparcoord\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           showPoints = TRUE) +\n           scale_color_brewer(palette = \"Set2\") \n\n\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           splineFactor = TRUE) +\n           scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\nスケールを色々と変更することが可能である。\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           scale = \"robust\") +\n           scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           scale = \"uniminmax\") +\n           scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           scale = \"globalminmax\") +\n           scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           scale = \"center\") +\n           scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggparcoord(data = iris,\n           columns = 1:4,\n           groupColumn = \"Species\",\n           scale = \"centerObs\") +\n           scale_color_brewer(palette = \"Set2\")",
    "crumbs": [
      "R",
      "GGally",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/arrow/index.html",
    "href": "contents/libs/R/arrow/index.html",
    "title": "arrow",
    "section": "",
    "text": "parquetのIOライブラリ\ndplyrの構文サポート",
    "crumbs": [
      "R",
      "arrow",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/arrow/index.html#read-a-parquet-file",
    "href": "contents/libs/R/arrow/index.html#read-a-parquet-file",
    "title": "arrow",
    "section": "2.1 Read a Parquet file",
    "text": "2.1 Read a Parquet file\n参考文献\n\n\nCode\ntf &lt;- tempfile()\non.exit(unlink(tf))\nwrite_parquet(mtcars, tf)\ndf &lt;- read_parquet(tf, col_select = starts_with(\"d\"))\nhead(df)\n#&gt;   disp drat\n#&gt; 1  160 3.90\n#&gt; 2  160 3.90\n#&gt; 3  108 3.85\n#&gt; 4  258 3.08\n#&gt; 5  360 3.15\n#&gt; 6  225 2.76",
    "crumbs": [
      "R",
      "arrow",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/arrow/index.html#reading-and-writing-data-files",
    "href": "contents/libs/R/arrow/index.html#reading-and-writing-data-files",
    "title": "arrow",
    "section": "2.2 Reading and writing data files",
    "text": "2.2 Reading and writing data files\nas_data_frame=FALSEとすることでArro Tableとして読み込むことが出来る。",
    "crumbs": [
      "R",
      "arrow",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/arrow/index.html#data-analysis-with-dplyr-syntax",
    "href": "contents/libs/R/arrow/index.html#data-analysis-with-dplyr-syntax",
    "title": "arrow",
    "section": "2.3 Data analysis with dplyr syntax",
    "text": "2.3 Data analysis with dplyr syntax\n\n2.3.1 Single\nArrow data(Table data Dataset) はdplyrの構文と同じように使える。\n\n\nCode\nlibrary(dplyr, warn.conflicts = FALSE)\n#&gt; Warning: package 'dplyr' was built under R version 4.3.3\nlibrary(arrow, warn.conflicts = FALSE)\nsw &lt;- arrow_table(starwars, as_data_frame = FALSE)\n\n\n\n\nCode\nsw\n#&gt; Table\n#&gt; 87 rows x 15 columns\n#&gt; $name &lt;string&gt;\n#&gt; $height &lt;int32&gt;\n#&gt; $mass &lt;double&gt;\n#&gt; $hair_color &lt;string&gt;\n#&gt; $skin_color &lt;string&gt;\n#&gt; $eye_color &lt;string&gt;\n#&gt; $birth_year &lt;double&gt;\n#&gt; $sex &lt;string&gt;\n#&gt; $gender &lt;string&gt;\n#&gt; $homeworld &lt;string&gt;\n#&gt; $species &lt;string&gt;\n#&gt; $films: list&lt;item &lt;string&gt;&gt;\n#&gt; $vehicles: list&lt;item &lt;string&gt;&gt;\n#&gt; $starships: list&lt;item &lt;string&gt;&gt;\n#&gt; $as_data_frame &lt;bool&gt;\n\n\ndplyrの構文を使う。\n\n\nCode\nresult &lt;- sw %&gt;%\n  filter(homeworld == \"Tatooine\") %&gt;%\n  rename(height_cm = height, mass_kg = mass) %&gt;%\n  mutate(height_in = height_cm / 2.54, mass_lbs = mass_kg * 2.2046) %&gt;%\n  arrange(desc(birth_year)) %&gt;%\n  select(name, height_in, mass_lbs)\n\nresult\n#&gt; Table (query)\n#&gt; name: string\n#&gt; height_in: double (divide(cast(height, {to_type=double, allow_int_overflow=false, allow_time_truncate=false, allow_time_overflow=false, allow_decimal_truncate=false, allow_float_truncate=false, allow_invalid_utf8=false}), cast(2.54, {to_type=double, allow_int_overflow=false, allow_time_truncate=false, allow_time_overflow=false, allow_decimal_truncate=false, allow_float_truncate=false, allow_invalid_utf8=false})))\n#&gt; mass_lbs: double (multiply_checked(mass, 2.2046))\n#&gt; \n#&gt; * Filter: (homeworld == \"Tatooine\")\n#&gt; * Sorted by birth_year [desc]\n#&gt; See $.data for the source Arrow object\n\n\n遅延評価がおこなわれている。computeかcollectを使うことで実体が評価される。 computeを使うとArrow Tableが返り、collectを使うとデータフレームが返る。\n\n\nCode\ncompute(result)\n#&gt; Table\n#&gt; 10 rows x 3 columns\n#&gt; $name &lt;string&gt;\n#&gt; $height_in &lt;double&gt;\n#&gt; $mass_lbs &lt;double&gt;\n\n\n\n\nCode\ncollect(result)\n#&gt; # A tibble: 10 × 3\n#&gt;    name               height_in mass_lbs\n#&gt;    &lt;chr&gt;                  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 C-3PO                   65.7    165. \n#&gt;  2 Cliegg Lars             72.0     NA  \n#&gt;  3 Shmi Skywalker          64.2     NA  \n#&gt;  4 Owen Lars               70.1    265. \n#&gt;  5 Beru Whitesun Lars      65.0    165. \n#&gt;  6 Darth Vader             79.5    300. \n#&gt;  7 Anakin Skywalker        74.0    185. \n#&gt;  8 Biggs Darklighter       72.0    185. \n#&gt;  9 Luke Skywalker          67.7    170. \n#&gt; 10 R5-D4                   38.2     70.5\n\n\nシングルテーブルの処理をサポートしているので、次のような集約を行うことも可能である。\n\n\nCode\nsw %&gt;%\n  group_by(species) %&gt;%\n  summarize(mean_height = mean(height, na.rm = TRUE)) %&gt;%\n  collect()\n#&gt; # A tibble: 38 × 2\n#&gt;    species        mean_height\n#&gt;    &lt;chr&gt;                &lt;dbl&gt;\n#&gt;  1 Human                 178 \n#&gt;  2 Droid                 131.\n#&gt;  3 Wookiee               231 \n#&gt;  4 Rodian                173 \n#&gt;  5 Hutt                  175 \n#&gt;  6 &lt;NA&gt;                  175 \n#&gt;  7 Yoda's species         66 \n#&gt;  8 Trandoshan            190 \n#&gt;  9 Mon Calamari          180 \n#&gt; 10 Ewok                   88 \n#&gt; # ℹ 28 more rows\n\n\n\n\n2.3.2 Two-Table\n等値結合はサポートされている。\n\n\nCode\njedi &lt;- data.frame(\n  name = c(\"C-3PO\", \"Luke Skywalker\", \"Obi-Wan Kenobi\"),\n  jedi = c(FALSE, TRUE, TRUE)\n)\n\nsw %&gt;%\n  select(1:3) %&gt;%\n  right_join(jedi) %&gt;%\n  collect()\n#&gt; # A tibble: 3 × 4\n#&gt;   name           height  mass jedi \n#&gt;   &lt;chr&gt;           &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt;\n#&gt; 1 Luke Skywalker    172    77 TRUE \n#&gt; 2 C-3PO             167    75 FALSE\n#&gt; 3 Obi-Wan Kenobi    182    77 TRUE\n\n\n\n\n2.3.3 Registering custom bindgs\nカスタム関数の作成をサポートしている。contextが第一引数にくるのが特徴である。\n\n\nCode\nto_snake_name &lt;- function(context, string) {\n  replace &lt;- c(`'` = \"\", `\"` = \"\", `-` = \"\", `\\\\.` = \"_\", ` ` = \"_\")\n  string %&gt;%\n    stringr::str_replace_all(replace) %&gt;%\n    stringr::str_to_lower() %&gt;%\n    stringi::stri_trans_general(id = \"Latin-ASCII\")\n}\n\n\n作成したら登録が必要である。\n\n\nCode\nregister_scalar_function(\n  name = \"to_snake_name\",\n  fun = to_snake_name,\n  in_type = utf8(),\n  out_type = utf8(),\n  auto_convert = TRUE\n)\n\n\n\n\nCode\nsw %&gt;%\n  mutate(name, snake_name = to_snake_name(name), .keep = \"none\") %&gt;%\n  collect()\n#&gt; # A tibble: 87 × 2\n#&gt;    name               snake_name        \n#&gt;    &lt;chr&gt;              &lt;chr&gt;             \n#&gt;  1 Luke Skywalker     luke_skywalker    \n#&gt;  2 C-3PO              c3po              \n#&gt;  3 R2-D2              r2d2              \n#&gt;  4 Darth Vader        darth_vader       \n#&gt;  5 Leia Organa        leia_organa       \n#&gt;  6 Owen Lars          owen_lars         \n#&gt;  7 Beru Whitesun Lars beru_whitesun_lars\n#&gt;  8 R5-D4              r5d4              \n#&gt;  9 Biggs Darklighter  biggs_darklighter \n#&gt; 10 Obi-Wan Kenobi     obiwan_kenobi     \n#&gt; # ℹ 77 more rows\n\n\n\n\n2.3.4 Handling unsupported expressions\n処理途中で未登録の関数があったら自動でcollectが走る。\n\n\nCode\nsw %&gt;%\n  filter(!is.na(height), !is.na(mass)) %&gt;%\n  transmute(name, height, mass, res = residuals(lm(mass ~ height)))\n#&gt; Warning: In residuals(lm(mass ~ height)): \n#&gt; ℹ Expression not supported in Arrow\n#&gt; → Pulling data into R\n#&gt; # A tibble: 59 × 4\n#&gt;    name               height  mass   res\n#&gt;    &lt;chr&gt;               &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 Luke Skywalker        172    77 -18.8\n#&gt;  2 C-3PO                 167    75 -17.7\n#&gt;  3 R2-D2                  96    32 -16.4\n#&gt;  4 Darth Vader           202   136  21.4\n#&gt;  5 Leia Organa           150    49 -33.1\n#&gt;  6 Owen Lars             178   120  20.4\n#&gt;  7 Beru Whitesun Lars    165    75 -16.5\n#&gt;  8 R5-D4                  97    32 -17.0\n#&gt;  9 Biggs Darklighter     183    84 -18.7\n#&gt; 10 Obi-Wan Kenobi        182    77 -25.1\n#&gt; # ℹ 49 more rows",
    "crumbs": [
      "R",
      "arrow",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/arrow/index.html#working-with-multi-file-data-sets",
    "href": "contents/libs/R/arrow/index.html#working-with-multi-file-data-sets",
    "title": "arrow",
    "section": "2.4 Working with multi-file data sets",
    "text": "2.4 Working with multi-file data sets\nApache Arrowを使うことでメモリ以上のデータをdplyrの構文で扱うことが可能となる。\n巨大なデータとしてNYCを扱う。\n500MB程度のシングルファイルが160程度あり、全体で70GBある。\nここではtinyバージョンを使う\n\n\nCode\nbucket &lt;- s3_bucket(\"voltrondata-labs-datasets/nyc-taxi-tiny\")\n\n\n\n\nCode\ncopy_files(from = bucket, to = file.path(cur_dir, \"nyc-taxi\"))\n\n\n\n2.4.1 Opening datasets\n\n\nCode\nds &lt;- open_dataset(\"nyc-taxi\", partitioning = c(\"year\", \"month\"))\nds\n#&gt; FileSystemDataset with 158 Parquet files\n#&gt; 24 columns\n#&gt; vendor_name: string\n#&gt; pickup_datetime: timestamp[ms]\n#&gt; dropoff_datetime: timestamp[ms]\n#&gt; passenger_count: int64\n#&gt; trip_distance: double\n#&gt; pickup_longitude: double\n#&gt; pickup_latitude: double\n#&gt; rate_code: string\n#&gt; store_and_fwd: string\n#&gt; dropoff_longitude: double\n#&gt; dropoff_latitude: double\n#&gt; payment_type: string\n#&gt; fare_amount: double\n#&gt; extra: double\n#&gt; mta_tax: double\n#&gt; tip_amount: double\n#&gt; tolls_amount: double\n#&gt; total_amount: double\n#&gt; improvement_surcharge: double\n#&gt; congestion_surcharge: double\n#&gt; ...\n#&gt; 4 more columns\n#&gt; Use `schema()` to see entire schema\n\n\n\n\n2.4.2 Querying Datasets\n\n\nCode\nsystem.time(ds %&gt;%\n  count() |&gt; \n    collect() |&gt; \n    print()\n)\n#&gt; # A tibble: 1 × 1\n#&gt;         n\n#&gt;     &lt;int&gt;\n#&gt; 1 1672513\n#&gt;    user  system elapsed \n#&gt;    0.03    0.01    0.34\n\n\n\n\nCode\nsystem.time(ds %&gt;%\n  filter(total_amount &gt; 100, year == 2015) %&gt;%\n  select(tip_amount, total_amount, passenger_count) %&gt;%\n  mutate(tip_pct = 100 * tip_amount / total_amount) %&gt;%\n  group_by(passenger_count) %&gt;%\n  summarise(\n    median_tip_pct = median(tip_pct),\n    n = n()\n  ) %&gt;%\n  collect() %&gt;%\n  print())\n#&gt; Warning: median() currently returns an approximate median in Arrow\n#&gt; This warning is displayed once per session.\n#&gt; # A tibble: 6 × 3\n#&gt;   passenger_count median_tip_pct     n\n#&gt;             &lt;int&gt;          &lt;dbl&gt; &lt;int&gt;\n#&gt; 1               1          16.7    153\n#&gt; 2               3           5.76     8\n#&gt; 3               2          16.7     44\n#&gt; 4               5          15.0      4\n#&gt; 5               0          14.3      1\n#&gt; 6               6           0        1\n#&gt;    user  system elapsed \n#&gt;    0.00    0.00    0.25\n\n\n\n\n2.4.3 Batch processing\n\n\nCode\nsampled_data &lt;- ds %&gt;%\n  filter(year == 2015) %&gt;%\n  select(tip_amount, total_amount, passenger_count) %&gt;%\n  map_batches(~ as_record_batch(sample_frac(as.data.frame(.), 1e-4))) %&gt;%\n  mutate(tip_pct = tip_amount / total_amount) %&gt;%\n  collect()\n\nstr(sampled_data)\n#&gt; tibble [12 × 4] (S3: tbl_df/tbl/data.frame)\n#&gt;  $ tip_amount     : num [1:12] 1 1.32 1.15 0 1 0 2.75 2 4.8 1.16 ...\n#&gt;  $ total_amount   : num [1:12] 9.3 6.62 6.95 9.8 13.8 ...\n#&gt;  $ passenger_count: int [1:12] 1 1 2 1 1 1 1 1 1 1 ...\n#&gt;  $ tip_pct        : num [1:12] 0.1075 0.1994 0.1655 0 0.0725 ...",
    "crumbs": [
      "R",
      "arrow",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/arrow/index.html#connecting-to-a-flight-server",
    "href": "contents/libs/R/arrow/index.html#connecting-to-a-flight-server",
    "title": "arrow",
    "section": "2.5 Connecting to a Flight server",
    "text": "2.5 Connecting to a Flight server\nFlight機能というネットワーク越しのデータハンドリングを使う。よくわからないが基本動かない\n\n\nCode\ninstall.packages(\"reticulate\")\narrow::install_pyarrow()\n\n\n\n\nCode\nlibrary(arrow)\ndemo_server &lt;- load_flight_server(\"demo_flight_server\")\nserver &lt;- demo_server$DemoFlightServer(port = 8089)\nserver$serve()\n\n\n\n\nCode\nlibrary(arrow)\nclient &lt;- flight_connect(port = 8089)\nflight_put(client, iris, path = \"test_data/iris\")\n\n\n\n\nCode\nlibrary(arrow)\nlibrary(dplyr)\nclient &lt;- flight_connect(port = 8089)\nclient %&gt;%\n  flight_get(\"test_data/iris\") %&gt;%\n  group_by(Species) %&gt;%\n  summarize(max_petal = max(Petal.Length))\n\n## # A tibble: 3 x 2\n##   Species    max_petal\n##   &lt;fct&gt;          &lt;dbl&gt;\n## 1 setosa           1.9\n## 2 versicolor       5.1\n## 3 virginica        6.9",
    "crumbs": [
      "R",
      "arrow",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/base/working.html",
    "href": "contents/libs/R/base/working.html",
    "title": "base",
    "section": "",
    "text": "baseパッケージ関連のノートです。",
    "crumbs": [
      "R",
      "base",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/base/working.html#approxapproxxfun",
    "href": "contents/libs/R/base/working.html#approxapproxxfun",
    "title": "base",
    "section": "2.1 approx/approxxfun",
    "text": "2.1 approx/approxxfun\n\nデータ点から近似関数を作成する\n1次元の他に使うことができる\napproxは直接近似値を返し, approxfunは近似関数を返す\n\n\n\nCode\nx &lt;- seq(0, 2 * pi, length.out = 10)\ny &lt;- sin(x)\nay &lt;- approxfun(x, y)(x)\nplot(x, y, pch = 19, cex = 3)\npoints(x, ay, col = \"blue\", type = \"l\")",
    "crumbs": [
      "R",
      "base",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/dbplyr/working.html",
    "href": "contents/libs/R/dbplyr/working.html",
    "title": "dbplyr",
    "section": "",
    "text": "Code\n\ncur_dir &lt;- here::here(\"contents/libs/R/dbplyr\")\nprint(here::here())\n#&gt; [1] \"C:/Users/114012/Dropbox/R/Workspace/RTipsSite\"\nprint(cur_dir)\n#&gt; [1] \"C:/Users/114012/Dropbox/R/Workspace/RTipsSite/contents/libs/R/dbplyr\"\n\n\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(showtext)\nlibrary(arrow)\nlibrary(nanoarrow)\nlibrary(odbc)\nlibrary(DBI)\nlibrary(RSQLite)\ndotenv::load_dot_env(file.path(here::here(), \".env\"))\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "dbplyr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/dbplyr/working.html#コネクション",
    "href": "contents/libs/R/dbplyr/working.html#コネクション",
    "title": "dbplyr",
    "section": "2.1 コネクション",
    "text": "2.1 コネクション\n\n\nCode\n# sqlite\ncon &lt;- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")",
    "crumbs": [
      "R",
      "dbplyr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/dbplyr/working.html#データ転送",
    "href": "contents/libs/R/dbplyr/working.html#データ転送",
    "title": "dbplyr",
    "section": "2.2 データ転送",
    "text": "2.2 データ転送\n\n\nCode\n# 新規作成される?\ncopy_to(con, mtcars, \"mtcars\", temporary = TRUE, overwrite = TRUE)",
    "crumbs": [
      "R",
      "dbplyr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/dbplyr/working.html#dbplyrでの利用",
    "href": "contents/libs/R/dbplyr/working.html#dbplyrでの利用",
    "title": "dbplyr",
    "section": "2.3 dbplyrでの利用",
    "text": "2.3 dbplyrでの利用\n\n2.3.1 compute/collect/colapse\n\n\nCode\nremote &lt;- \n  tbl(con, \"mtcars\") |&gt; \n    filter(mpg &gt; 20) |&gt; \n    select(mpg, cyl, disp)\n\n\nremote |&gt; show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT `mpg`, `cyl`, `disp`\n#&gt; FROM `mtcars`\n#&gt; WHERE (`mpg` &gt; 20.0)\n\n\nデータをメモリ上に展開するには次のようにcollectを活用する\n\n\nCode\n# compute query and save in remote table\ncollect(\n  remote\n)\n#&gt; # A tibble: 14 × 3\n#&gt;      mpg   cyl  disp\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21       6 160  \n#&gt;  2  21       6 160  \n#&gt;  3  22.8     4 108  \n#&gt;  4  21.4     6 258  \n#&gt;  5  24.4     4 147. \n#&gt;  6  22.8     4 141. \n#&gt;  7  32.4     4  78.7\n#&gt;  8  30.4     4  75.7\n#&gt;  9  33.9     4  71.1\n#&gt; 10  21.5     4 120. \n#&gt; 11  27.3     4  79  \n#&gt; 12  26       4 120. \n#&gt; 13  30.4     4  95.1\n#&gt; 14  21.4     4 121\n\n\nデータをDB上に展開するときには次のように, computeを活用する\n\n\nCode\n# compute query and save in remote table\ncompute(\n  remote, \n  name = \"remote_table\"\n)\n#&gt; # Source:   table&lt;`remote_table`&gt; [?? x 3]\n#&gt; # Database: sqlite 3.49.1 [:memory:]\n#&gt;      mpg   cyl  disp\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21       6 160  \n#&gt;  2  21       6 160  \n#&gt;  3  22.8     4 108  \n#&gt;  4  21.4     6 258  \n#&gt;  5  24.4     4 147. \n#&gt;  6  22.8     4 141. \n#&gt;  7  32.4     4  78.7\n#&gt;  8  30.4     4  75.7\n#&gt;  9  33.9     4  71.1\n#&gt; 10  21.5     4 120. \n#&gt; 11  27.3     4  79  \n#&gt; 12  26       4 120. \n#&gt; 13  30.4     4  95.1\n#&gt; 14  21.4     4 121\n\n\n\n\nCode\ntbl(con, \"remote_table\") |&gt; count(cyl, name = \"size\") |&gt; collect()\n#&gt; # A tibble: 2 × 2\n#&gt;     cyl  size\n#&gt;   &lt;dbl&gt; &lt;int&gt;\n#&gt; 1     4    11\n#&gt; 2     6     3\n\n\n\n\nCode\ndbReadTableArrow(con, \"remote_table\")\n#&gt; &lt;nanoarrow_array_stream struct&lt;mpg: double, cyl: double, disp: double&gt;&gt;\n#&gt;  $ get_schema:function ()  \n#&gt;  $ get_next  :function (schema = x$get_schema(), validate = TRUE)  \n#&gt;  $ release   :function ()\n\n\n\n\nCode\ndbDisconnect(con)",
    "crumbs": [
      "R",
      "dbplyr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/dbplyr/working.html#コネクション-1",
    "href": "contents/libs/R/dbplyr/working.html#コネクション-1",
    "title": "dbplyr",
    "section": "3.1 コネクション",
    "text": "3.1 コネクション\n\n\nCode\ncon &lt;- dbConnect(\n  odbc::odbc(), \n  Driver = \"SnowflakeDSIIDriver\",\n  Server = paste0(Sys.getenv(\"SNOW_ACCOUNT\"), \".snowflakecomputing.com\"), \n  UID = Sys.getenv(\"USER\"), \n  PWD = Sys.getenv(\"PASSWORD\"), \n  warehouse = Sys.getenv(\"SNOW_WAREHOUSE\"), \n  role = Sys.getenv(\"ROLE\")\n)",
    "crumbs": [
      "R",
      "dbplyr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/dbplyr/working.html#データ転送-1",
    "href": "contents/libs/R/dbplyr/working.html#データ転送-1",
    "title": "dbplyr",
    "section": "3.2 データ転送",
    "text": "3.2 データ転送\ncopy_toを使うとエラーが型変換のところでエラーが発生する。dbWriteTableを使うとことで簡単に処理が可能であった。 ・・・これはDBIなので違うような気もするが避けることはできない\n\n\nCode\ndbWriteTable(\n  con, \n  \"mtcars\", \n  mtcars, \n  overwrite = TRUE\n)\n\n\n\n\nCode\ntbl(con, \"mtcars\") |&gt; \n  mutate(\n    max_mpg = first(mpg, order_by = c(\"vs\", \"mpg\")), \n    .by = c(\"cyl\")\n  ) |&gt; \n  select(\n    cyl, \n    vs, \n    mpg, \n    max_mpg\n  ) |&gt; \n  arrange(cyl, vs, mpg) |&gt; \n  show_query()\n#&gt; &lt;SQL&gt;\n#&gt; SELECT\n#&gt;   \"cyl\",\n#&gt;   \"vs\",\n#&gt;   \"mpg\",\n#&gt;   FIRST_VALUE(\"mpg\") OVER (PARTITION BY \"cyl\" ORDER BY \"vs\", \"mpg\") AS \"max_mpg\"\n#&gt; FROM \"mtcars\"\n#&gt; ORDER BY \"cyl\", \"vs\", \"mpg\"\n\n\n\n\nCode\ndbReadTable(\n  con, \n  \"mtcars\"\n)\n#&gt;     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\n#&gt; 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\n#&gt; 2  21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n#&gt; 3  15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\n#&gt; 4  19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n#&gt; 5  15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n#&gt; 6  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n#&gt; 7  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\n#&gt; 8  27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\n#&gt; 9  19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\n#&gt; 10 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\n#&gt; 11 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\n#&gt; 12 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\n#&gt; 13 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\n#&gt; 14 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\n#&gt; 15 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\n#&gt; 16 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\n#&gt; 17 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\n#&gt; 18 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\n#&gt; 19 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\n#&gt; 20 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\n#&gt; 21 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\n#&gt; 22 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\n#&gt; 23 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\n#&gt; 24 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\n#&gt; 25 22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\n#&gt; 26 24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\n#&gt; 27 14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\n#&gt; 28 18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\n#&gt; 29 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\n#&gt; 30 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n#&gt; 31 22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\n#&gt; 32 21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\n\n\nクエリ自体をパイプリアンとして使うことも可能である\n\n\nCode\ntbl(\n  con, \n  sql('\n    SELECT * \n    FROM \"mtcars\"\n  ')\n) |&gt; \n  collect()\n#&gt; # A tibble: 32 × 11\n#&gt;      mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1  21       6 160     110  3.9   2.62  16.5     0     1     4     4\n#&gt;  2  21.4     4 121     109  4.11  2.78  18.6     1     1     4     2\n#&gt;  3  15       8 301     335  3.54  3.57  14.6     0     1     5     8\n#&gt;  4  19.7     6 145     175  3.62  2.77  15.5     0     1     5     6\n#&gt;  5  15.8     8 351     264  4.22  3.17  14.5     0     1     5     4\n#&gt;  6  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2\n#&gt;  7  26       4 120.     91  4.43  2.14  16.7     0     1     5     2\n#&gt;  8  27.3     4  79      66  4.08  1.94  18.9     1     1     4     1\n#&gt;  9  19.2     8 400     175  3.08  3.84  17.0     0     0     3     2\n#&gt; 10  13.3     8 350     245  3.73  3.84  15.4     0     0     3     4\n#&gt; # ℹ 22 more rows",
    "crumbs": [
      "R",
      "dbplyr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/dbplyr/working.html#disconnect",
    "href": "contents/libs/R/dbplyr/working.html#disconnect",
    "title": "dbplyr",
    "section": "3.3 disconnect",
    "text": "3.3 disconnect\n\n\nCode\ndbDisconnect(con)",
    "crumbs": [
      "R",
      "dbplyr",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/geoarrow/cheetsheet.html",
    "href": "contents/libs/R/geoarrow/cheetsheet.html",
    "title": "チートシート",
    "section": "",
    "text": "1 CA Vector Buildings\nCA Vector Buildingsのスライドを参考にして, Microsoftによるビルディングフットプリントのデータを使って, geoparquetの性能を体感する。\n\nUSBuildingFootprints\n上記のデータは州別であるので, ここから, カリフォルニアのデータを使う\n\n\n\nCode\ntic()\ndf &lt;- geojsonsf::geojson_sf(file.path(cur_dir, \"dataset\", \"California.geojson\"))\ntoc()\n# 685.85 sec elapsed\n\n# write it out\ntic()\nst_write_parquet(df, file.path(cur_dir, \"dataset\", \"ca_bing_buildings.parquet\"))\ntoc()\n\n# read it back in\ntic()\ndf_parq &lt;- st_read_parquet(file.path(cur_dir, \"dataset\", \"ca_bing_buildings.parquet\"))\ntoc()\n# read in 14.724 seconds!\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/libs/R/gganimate/working.html",
    "href": "contents/libs/R/gganimate/working.html",
    "title": "gganimate",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/gganimate\")\n\n\n\n1 はじめに\n\nGithub\n\n\n\n2 An Example\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "gganimate",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggmosaic/working.html",
    "href": "contents/libs/R/ggmosaic/working.html",
    "title": "ggmosaic",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/ggmosaic\")\n\n\n\n\nCode\nlibrary(ggmosaic)\nlibrary(showtext)\nlibrary(dplyr)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 はじめに\nggmosaicで美しいモザイクプロットを作成することができる。\nHPが再発サイトである。\n\n\n2 Example\n\n\nCode\nhappy %&gt;% \n  mutate(finrela = forcats::fct_recode(finrela,\n    \"far below     \" = \"far below average\",\n    \"    below\" = \"below average\",\n    \"average\" = \"average\",\n    \"above    \" = \"above average\", \n    \"l\\n   far above\" = \"far above average\")) %&gt;% \n  ggplot() +\n  geom_mosaic(aes(x = product(finrela), fill=health), show.legend = FALSE) +\n  theme_mosaic() +\n  scale_fill_manual(values = c(\"#4575B4\", \"#ABD9E9\", \"#FEE090\", \"#F46D43\"))\n#&gt; Warning: `unite_()` was deprecated in tidyr 1.2.0.\n#&gt; ℹ Please use `unite()` instead.\n#&gt; ℹ The deprecated feature was likely used in the ggmosaic package.\n#&gt;   Please report the issue at &lt;https://github.com/haleyjeppson/ggmosaic&gt;.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "ggmosaic",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggplot2/index.html",
    "href": "contents/libs/R/ggplot2/index.html",
    "title": "ggplot2",
    "section": "",
    "text": "Code\nlibrary(legendry)\nlibrary(tidyverse)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 はじめに\nggplot2を理解しよう。\n\n\n2 Example\n\n\nCode\nggplot(mpg, aes(cty, hwy)) +\n  # to create a scatterplot\n  geom_point() +\n  # to fit and overlay a loess trendline\n  geom_smooth(formula = y ~ x, method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "ggplot2",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/ggspatial/working.html",
    "href": "contents/libs/R/ggspatial/working.html",
    "title": "ggspatial",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/ggspatial\")\n\n\n\n\nCode\nlibrary(ggspatial)\nlibrary(ggplot2)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 はじめに\n地理データのグラフ化をリッチにすることができる。\ngithubが開発サイトである。\n\n\n2 イントロダクション\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggspatial)\nload_longlake_data()\n\nggplot() +\n  # loads background map tiles from a tile source\n  annotation_map_tile(zoomin = -1) +\n  \n  # annotation_spatial() layers don't train the scales, so data stays central\n  annotation_spatial(longlake_roadsdf, size = 2, col = \"black\") +\n  annotation_spatial(longlake_roadsdf, size = 1.6, col = \"white\") +\n\n  # raster layers train scales and get projected automatically\n  layer_spatial(longlake_depth_raster, aes(color = after_stat(band1))) +\n  # make no data values transparent\n  scale_fill_viridis_c(na.value = NA) +\n  \n  # layer_spatial trains the scales\n  layer_spatial(longlake_depthdf, aes(fill = DEPTH_M)) +\n  \n  # spatial-aware automagic scale bar\n  annotation_scale(location = \"tl\") +\n\n  # spatial-aware automagic north arrow\n  annotation_north_arrow(location = \"br\", which_north = \"true\")\n#&gt; Warning in layer_spatial(longlake_depth_raster, aes(color =\n#&gt; after_stat(band1))): Ignoring unknown aesthetics: colour\n#&gt; Zoom: 14\n#&gt; Warning: Removed 9122 rows containing missing values (`geom_raster()`).\n\n\n\n\n\n\n\n\n\nCode\n\n  \n  labs(caption = \"\\U00a9 OpenStreetMap contributors\")\n#&gt; $caption\n#&gt; [1] \"© OpenStreetMap contributors\"\n#&gt; \n#&gt; attr(,\"class\")\n#&gt; [1] \"labels\"\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "ggspatial",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/gt/cheetsheet.html",
    "href": "contents/libs/R/gt/cheetsheet.html",
    "title": "gt",
    "section": "",
    "text": "ページs\n\n\n\n\n\nCode\nislands_tbl &lt;-\n  tibble(\n    name = names(islands),\n    size = islands\n  ) |&gt;\n  slice_max(size, n = 10)\n\n\nislands_tbl\n#&gt; # A tibble: 10 × 2\n#&gt;    name           size\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;\n#&gt;  1 Asia          16988\n#&gt;  2 Africa        11506\n#&gt;  3 North America  9390\n#&gt;  4 South America  6795\n#&gt;  5 Antarctica     5500\n#&gt;  6 Europe         3745\n#&gt;  7 Australia      2968\n#&gt;  8 Greenland       840\n#&gt;  9 New Guinea      306\n#&gt; 10 Borneo          280\n\n\n\n\nCode\n# Create a display table showing ten of\n# the largest islands in the world\ngt_tbl &lt;- gt(islands_tbl)\n\n# Show the gt Table\ngt_tbl\n\n\n\n\n\n\n\n\nname\nsize\n\n\n\n\nAsia\n16988\n\n\nAfrica\n11506\n\n\nNorth America\n9390\n\n\nSouth America\n6795\n\n\nAntarctica\n5500\n\n\nEurope\n3745\n\n\nAustralia\n2968\n\n\nGreenland\n840\n\n\nNew Guinea\n306\n\n\nBorneo\n280\n\n\n\n\n\n\n\n\n\n\n\n\nヘッダーを追加する\n\n\n\nCode\n\ngt_tbl &lt;- \n  gt_tbl |&gt; \n  tab_header(\n    title = \"Large Landmasses of the World\",\n    subtitle = \"The top ten largest are presented\"\n  )\n\ngt_tbl\n\n\n\n\n\n\n\n\nLarge Landmasses of the World\n\n\nThe top ten largest are presented\n\n\nname\nsize\n\n\n\n\nAsia\n16988\n\n\nAfrica\n11506\n\n\nNorth America\n9390\n\n\nSouth America\n6795\n\n\nAntarctica\n5500\n\n\nEurope\n3745\n\n\nAustralia\n2968\n\n\nGreenland\n840\n\n\nNew Guinea\n306\n\n\nBorneo\n280\n\n\n\n\n\n\n\n\nフットノートのように、ソースノートを使える\n\n\n\nCode\n# Display the `islands_tbl` data with a heading and\n# two source notes\ngt_tbl &lt;-\n  gt_tbl |&gt;\n  tab_source_note(\n    source_note = \"Source: The World Almanac and Book of Facts, 1975, page 406.\"\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\"Reference: McNeil, D. R. (1977) *Interactive Data Analysis*. Wiley.\")\n  )\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n\n\n\nLarge Landmasses of the World\n\n\nThe top ten largest are presented\n\n\nname\nsize\n\n\n\n\nAsia\n16988\n\n\nAfrica\n11506\n\n\nNorth America\n9390\n\n\nSouth America\n6795\n\n\nAntarctica\n5500\n\n\nEurope\n3745\n\n\nAustralia\n2968\n\n\nGreenland\n840\n\n\nNew Guinea\n306\n\n\nBorneo\n280\n\n\n\nSource: The World Almanac and Book of Facts, 1975, page 406.\n\n\nReference: McNeil, D. R. (1977) Interactive Data Analysis. Wiley.\n\n\n\n\n\n\n\n\n\nフットノートにするとセルの中に引用を付けることができる\n\n\n\nCode\n# Add footnotes (the same text) to two different\n# cell; data cells are targeted with `data_cells()`\ngt_tbl &lt;-\n  gt_tbl |&gt;\n  tab_footnote(\n    footnote = \"The Americas.\",\n    locations = cells_body(columns = name, rows = 3:4)\n  )\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n  \n    \n      Large Landmasses of the World\n    \n    \n      The top ten largest are presented\n    \n    \n      name\n      size\n    \n  \n  \n    Asia\n16988\n    Africa\n11506\n    North America1\n9390\n    South America1\n6795\n    Antarctica\n5500\n    Europe\n3745\n    Australia\n2968\n    Greenland\n840\n    New Guinea\n306\n    Borneo\n280\n  \n  \n    \n      Source: The World Almanac and Book of Facts, 1975, page 406.\n    \n    \n      Reference: McNeil, D. R. (1977) Interactive Data Analysis. Wiley.\n    \n  \n  \n    \n      1 The Americas.\n    \n  \n\n\n\n\n\n\n\n\n使わないことも多い\n\n\n\nCode\n# Create a gt table showing ten of the\n# largest islands in the world; this\n# time with a stub\ngt_tbl &lt;-\n  islands_tbl |&gt;\n  gt(rowname_col = \"name\")\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n\n\n\n\nsize\n\n\n\n\nAsia\n16988\n\n\nAfrica\n11506\n\n\nNorth America\n9390\n\n\nSouth America\n6795\n\n\nAntarctica\n5500\n\n\nEurope\n3745\n\n\nAustralia\n2968\n\n\nGreenland\n840\n\n\nNew Guinea\n306\n\n\nBorneo\n280\n\n\n\n\n\n\n\n\n\nCode\n# Create three row groups with the\n# `tab_row_group()` function\ngt_tbl &lt;-\n  gt_tbl |&gt;\n  tab_row_group(\n    label = \"continent\",\n    rows = 1:6\n  ) |&gt;\n  tab_row_group(\n    label = \"country\",\n    rows = c(\"Australia\", \"Greenland\")\n  ) |&gt;\n  tab_row_group(\n    label = \"subregion\",\n    rows = c(\"New Guinea\", \"Borneo\")\n  )\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n\n\n\n\nsize\n\n\n\n\nsubregion\n\n\nNew Guinea\n306\n\n\nBorneo\n280\n\n\ncountry\n\n\nAustralia\n2968\n\n\nGreenland\n840\n\n\ncontinent\n\n\nAsia\n16988\n\n\nAfrica\n11506\n\n\nNorth America\n9390\n\n\nSouth America\n6795\n\n\nAntarctica\n5500\n\n\nEurope\n3745\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n# Modify the `airquality` dataset by adding the year\n# of the measurements (1973) and limiting to 10 rows\nairquality_m &lt;-\n  airquality |&gt;\n  mutate(Year = 1973L) |&gt;\n  slice(1:10)\n\n# Create a display table using the `airquality`\n# dataset; arrange columns into groups\ngt_tbl &lt;-\n  gt(airquality_m) |&gt;\n  tab_header(\n    title = \"New York Air Quality Measurements\",\n    subtitle = \"Daily measurements in New York City (May 1-10, 1973)\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Time\",\n    columns = c(Year, Month, Day)\n  ) |&gt;\n  tab_spanner(\n    label = \"Measurement\",\n    columns = c(Ozone, Solar.R, Wind, Temp)\n  )\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew York Air Quality Measurements\n\n\nDaily measurements in New York City (May 1-10, 1973)\n\n\n\nMeasurement\n\n\nTime\n\n\n\nOzone\nSolar.R\nWind\nTemp\nYear\nMonth\nDay\n\n\n\n\n41\n190\n7.4\n67\n1973\n5\n1\n\n\n36\n118\n8.0\n72\n1973\n5\n2\n\n\n12\n149\n12.6\n74\n1973\n5\n3\n\n\n18\n313\n11.5\n62\n1973\n5\n4\n\n\nNA\nNA\n14.3\n56\n1973\n5\n5\n\n\n28\nNA\n14.9\n66\n1973\n5\n6\n\n\n23\n299\n8.6\n65\n1973\n5\n7\n\n\n19\n99\n13.8\n59\n1973\n5\n8\n\n\n8\n19\n20.1\n61\n1973\n5\n9\n\n\nNA\n194\n8.6\n69\n1973\n5\n10\n\n\n\n\n\n\n\n\n\nCode\n# Move the time-based columns to the start of\n# the column series; modify the column labels of\n# the measurement-based columns\ngt_tbl &lt;-\n  gt_tbl |&gt;\n  cols_move_to_start(\n    columns = c(Year, Month, Day)\n  ) |&gt;\n  cols_label(\n    Ozone = html(\"Ozone,&lt;br&gt;ppbV\"),\n    Solar.R = html(\"Solar R.,&lt;br&gt;cal/m&lt;sup&gt;2&lt;/sup&gt;\"),\n    Wind = html(\"Wind,&lt;br&gt;mph\"),\n    Temp = html(\"Temp,&lt;br&gt;&deg;F\")\n  )\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew York Air Quality Measurements\n\n\nDaily measurements in New York City (May 1-10, 1973)\n\n\n\nTime\n\n\nMeasurement\n\n\n\nYear\nMonth\nDay\nOzone,\nppbV\nSolar R.,\ncal/m2\nWind,\nmph\nTemp,\n°F\n\n\n\n\n1973\n5\n1\n41\n190\n7.4\n67\n\n\n1973\n5\n2\n36\n118\n8.0\n72\n\n\n1973\n5\n3\n12\n149\n12.6\n74\n\n\n1973\n5\n4\n18\n313\n11.5\n62\n\n\n1973\n5\n5\nNA\nNA\n14.3\n56\n\n\n1973\n5\n6\n28\nNA\n14.9\n66\n\n\n1973\n5\n7\n23\n299\n8.6\n65\n\n\n1973\n5\n8\n19\n99\n13.8\n59\n\n\n1973\n5\n9\n8\n19\n20.1\n61\n\n\n1973\n5\n10\nNA\n194\n8.6\n69"
  },
  {
    "objectID": "contents/libs/R/gt/cheetsheet.html#walkthrough-of-the-gt-basics-with-a-simple-table",
    "href": "contents/libs/R/gt/cheetsheet.html#walkthrough-of-the-gt-basics-with-a-simple-table",
    "title": "gt",
    "section": "",
    "text": "Code\nislands_tbl &lt;-\n  tibble(\n    name = names(islands),\n    size = islands\n  ) |&gt;\n  slice_max(size, n = 10)\n\n\nislands_tbl\n#&gt; # A tibble: 10 × 2\n#&gt;    name           size\n#&gt;    &lt;chr&gt;         &lt;dbl&gt;\n#&gt;  1 Asia          16988\n#&gt;  2 Africa        11506\n#&gt;  3 North America  9390\n#&gt;  4 South America  6795\n#&gt;  5 Antarctica     5500\n#&gt;  6 Europe         3745\n#&gt;  7 Australia      2968\n#&gt;  8 Greenland       840\n#&gt;  9 New Guinea      306\n#&gt; 10 Borneo          280\n\n\n\n\nCode\n# Create a display table showing ten of\n# the largest islands in the world\ngt_tbl &lt;- gt(islands_tbl)\n\n# Show the gt Table\ngt_tbl\n\n\n\n\n\n\n\n\nname\nsize\n\n\n\n\nAsia\n16988\n\n\nAfrica\n11506\n\n\nNorth America\n9390\n\n\nSouth America\n6795\n\n\nAntarctica\n5500\n\n\nEurope\n3745\n\n\nAustralia\n2968\n\n\nGreenland\n840\n\n\nNew Guinea\n306\n\n\nBorneo\n280"
  },
  {
    "objectID": "contents/libs/R/gt/cheetsheet.html#adding-parts-to-this-simple-table",
    "href": "contents/libs/R/gt/cheetsheet.html#adding-parts-to-this-simple-table",
    "title": "gt",
    "section": "",
    "text": "ヘッダーを追加する\n\n\n\nCode\n\ngt_tbl &lt;- \n  gt_tbl |&gt; \n  tab_header(\n    title = \"Large Landmasses of the World\",\n    subtitle = \"The top ten largest are presented\"\n  )\n\ngt_tbl\n\n\n\n\n\n\n\n\nLarge Landmasses of the World\n\n\nThe top ten largest are presented\n\n\nname\nsize\n\n\n\n\nAsia\n16988\n\n\nAfrica\n11506\n\n\nNorth America\n9390\n\n\nSouth America\n6795\n\n\nAntarctica\n5500\n\n\nEurope\n3745\n\n\nAustralia\n2968\n\n\nGreenland\n840\n\n\nNew Guinea\n306\n\n\nBorneo\n280\n\n\n\n\n\n\n\n\nフットノートのように、ソースノートを使える\n\n\n\nCode\n# Display the `islands_tbl` data with a heading and\n# two source notes\ngt_tbl &lt;-\n  gt_tbl |&gt;\n  tab_source_note(\n    source_note = \"Source: The World Almanac and Book of Facts, 1975, page 406.\"\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\"Reference: McNeil, D. R. (1977) *Interactive Data Analysis*. Wiley.\")\n  )\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n\n\n\nLarge Landmasses of the World\n\n\nThe top ten largest are presented\n\n\nname\nsize\n\n\n\n\nAsia\n16988\n\n\nAfrica\n11506\n\n\nNorth America\n9390\n\n\nSouth America\n6795\n\n\nAntarctica\n5500\n\n\nEurope\n3745\n\n\nAustralia\n2968\n\n\nGreenland\n840\n\n\nNew Guinea\n306\n\n\nBorneo\n280\n\n\n\nSource: The World Almanac and Book of Facts, 1975, page 406.\n\n\nReference: McNeil, D. R. (1977) Interactive Data Analysis. Wiley.\n\n\n\n\n\n\n\n\n\nフットノートにするとセルの中に引用を付けることができる\n\n\n\nCode\n# Add footnotes (the same text) to two different\n# cell; data cells are targeted with `data_cells()`\ngt_tbl &lt;-\n  gt_tbl |&gt;\n  tab_footnote(\n    footnote = \"The Americas.\",\n    locations = cells_body(columns = name, rows = 3:4)\n  )\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n  \n    \n      Large Landmasses of the World\n    \n    \n      The top ten largest are presented\n    \n    \n      name\n      size\n    \n  \n  \n    Asia\n16988\n    Africa\n11506\n    North America1\n9390\n    South America1\n6795\n    Antarctica\n5500\n    Europe\n3745\n    Australia\n2968\n    Greenland\n840\n    New Guinea\n306\n    Borneo\n280\n  \n  \n    \n      Source: The World Almanac and Book of Facts, 1975, page 406.\n    \n    \n      Reference: McNeil, D. R. (1977) Interactive Data Analysis. Wiley.\n    \n  \n  \n    \n      1 The Americas."
  },
  {
    "objectID": "contents/libs/R/gt/cheetsheet.html#stub",
    "href": "contents/libs/R/gt/cheetsheet.html#stub",
    "title": "gt",
    "section": "",
    "text": "使わないことも多い\n\n\n\nCode\n# Create a gt table showing ten of the\n# largest islands in the world; this\n# time with a stub\ngt_tbl &lt;-\n  islands_tbl |&gt;\n  gt(rowname_col = \"name\")\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n\n\n\n\nsize\n\n\n\n\nAsia\n16988\n\n\nAfrica\n11506\n\n\nNorth America\n9390\n\n\nSouth America\n6795\n\n\nAntarctica\n5500\n\n\nEurope\n3745\n\n\nAustralia\n2968\n\n\nGreenland\n840\n\n\nNew Guinea\n306\n\n\nBorneo\n280\n\n\n\n\n\n\n\n\n\nCode\n# Create three row groups with the\n# `tab_row_group()` function\ngt_tbl &lt;-\n  gt_tbl |&gt;\n  tab_row_group(\n    label = \"continent\",\n    rows = 1:6\n  ) |&gt;\n  tab_row_group(\n    label = \"country\",\n    rows = c(\"Australia\", \"Greenland\")\n  ) |&gt;\n  tab_row_group(\n    label = \"subregion\",\n    rows = c(\"New Guinea\", \"Borneo\")\n  )\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n\n\n\n\nsize\n\n\n\n\nsubregion\n\n\nNew Guinea\n306\n\n\nBorneo\n280\n\n\ncountry\n\n\nAustralia\n2968\n\n\nGreenland\n840\n\n\ncontinent\n\n\nAsia\n16988\n\n\nAfrica\n11506\n\n\nNorth America\n9390\n\n\nSouth America\n6795\n\n\nAntarctica\n5500\n\n\nEurope\n3745"
  },
  {
    "objectID": "contents/libs/R/gt/cheetsheet.html#column-labels",
    "href": "contents/libs/R/gt/cheetsheet.html#column-labels",
    "title": "gt",
    "section": "",
    "text": "Code\n\n# Modify the `airquality` dataset by adding the year\n# of the measurements (1973) and limiting to 10 rows\nairquality_m &lt;-\n  airquality |&gt;\n  mutate(Year = 1973L) |&gt;\n  slice(1:10)\n\n# Create a display table using the `airquality`\n# dataset; arrange columns into groups\ngt_tbl &lt;-\n  gt(airquality_m) |&gt;\n  tab_header(\n    title = \"New York Air Quality Measurements\",\n    subtitle = \"Daily measurements in New York City (May 1-10, 1973)\"\n  ) |&gt;\n  tab_spanner(\n    label = \"Time\",\n    columns = c(Year, Month, Day)\n  ) |&gt;\n  tab_spanner(\n    label = \"Measurement\",\n    columns = c(Ozone, Solar.R, Wind, Temp)\n  )\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew York Air Quality Measurements\n\n\nDaily measurements in New York City (May 1-10, 1973)\n\n\n\nMeasurement\n\n\nTime\n\n\n\nOzone\nSolar.R\nWind\nTemp\nYear\nMonth\nDay\n\n\n\n\n41\n190\n7.4\n67\n1973\n5\n1\n\n\n36\n118\n8.0\n72\n1973\n5\n2\n\n\n12\n149\n12.6\n74\n1973\n5\n3\n\n\n18\n313\n11.5\n62\n1973\n5\n4\n\n\nNA\nNA\n14.3\n56\n1973\n5\n5\n\n\n28\nNA\n14.9\n66\n1973\n5\n6\n\n\n23\n299\n8.6\n65\n1973\n5\n7\n\n\n19\n99\n13.8\n59\n1973\n5\n8\n\n\n8\n19\n20.1\n61\n1973\n5\n9\n\n\nNA\n194\n8.6\n69\n1973\n5\n10\n\n\n\n\n\n\n\n\n\nCode\n# Move the time-based columns to the start of\n# the column series; modify the column labels of\n# the measurement-based columns\ngt_tbl &lt;-\n  gt_tbl |&gt;\n  cols_move_to_start(\n    columns = c(Year, Month, Day)\n  ) |&gt;\n  cols_label(\n    Ozone = html(\"Ozone,&lt;br&gt;ppbV\"),\n    Solar.R = html(\"Solar R.,&lt;br&gt;cal/m&lt;sup&gt;2&lt;/sup&gt;\"),\n    Wind = html(\"Wind,&lt;br&gt;mph\"),\n    Temp = html(\"Temp,&lt;br&gt;&deg;F\")\n  )\n\n# Show the gt table\ngt_tbl\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew York Air Quality Measurements\n\n\nDaily measurements in New York City (May 1-10, 1973)\n\n\n\nTime\n\n\nMeasurement\n\n\n\nYear\nMonth\nDay\nOzone,\nppbV\nSolar R.,\ncal/m2\nWind,\nmph\nTemp,\n°F\n\n\n\n\n1973\n5\n1\n41\n190\n7.4\n67\n\n\n1973\n5\n2\n36\n118\n8.0\n72\n\n\n1973\n5\n3\n12\n149\n12.6\n74\n\n\n1973\n5\n4\n18\n313\n11.5\n62\n\n\n1973\n5\n5\nNA\nNA\n14.3\n56\n\n\n1973\n5\n6\n28\nNA\n14.9\n66\n\n\n1973\n5\n7\n23\n299\n8.6\n65\n\n\n1973\n5\n8\n19\n99\n13.8\n59\n\n\n1973\n5\n9\n8\n19\n20.1\n61\n\n\n1973\n5\n10\nNA\n194\n8.6\n69"
  },
  {
    "objectID": "contents/libs/R/gt/cheetsheet.html#row-groups",
    "href": "contents/libs/R/gt/cheetsheet.html#row-groups",
    "title": "gt",
    "section": "2.1 Row Groups",
    "text": "2.1 Row Groups\n\ngroup_byが効く\n\n\n\nCode\n# Use `group_by()` on `gtcars` and pass that to `gt()`\ngtcars_8 |&gt;\n  group_by(ctry_origin) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nmfr\nmodel\nyear\ntrim\nbdy_style\nhp\nhp_rpm\ntrq\ntrq_rpm\nmpg_c\nmpg_h\ndrivetrain\ntrsmn\nmsrp\n\n\n\n\nGermany\n\n\nBMW\n6-Series\n2016\n640 I Coupe\ncoupe\n315\n5800\n330\n1400\n20\n30\nrwd\n8am\n77300\n\n\nBMW\ni8\n2016\nMega World Coupe\ncoupe\n357\n5800\n420\n3700\n28\n29\nawd\n6am\n140700\n\n\nItaly\n\n\nFerrari\n458 Speciale\n2015\nBase Coupe\ncoupe\n597\n9000\n398\n6000\n13\n17\nrwd\n7a\n291744\n\n\nFerrari\n458 Spider\n2015\nBase\nconvertible\n562\n9000\n398\n6000\n13\n17\nrwd\n7a\n263553\n\n\nJapan\n\n\nAcura\nNSX\n2017\nBase Coupe\ncoupe\n573\n6500\n476\n2000\n21\n22\nawd\n9a\n156000\n\n\nNissan\nGT-R\n2016\nPremium Coupe\ncoupe\n545\n6400\n436\n3200\n16\n22\nawd\n6a\n101770\n\n\nUnited States\n\n\nFord\nGT\n2017\nBase Coupe\ncoupe\n647\n6250\n550\n5900\n11\n18\nrwd\n7a\n447000\n\n\nChevrolet\nCorvette\n2016\nZ06 Coupe\ncoupe\n650\n6400\n650\n3600\n15\n22\nrwd\n7m\n88345"
  },
  {
    "objectID": "contents/libs/R/gt/cheetsheet.html#putting-columns-into-gruops",
    "href": "contents/libs/R/gt/cheetsheet.html#putting-columns-into-gruops",
    "title": "gt",
    "section": "2.2 Putting Columns Into Gruops",
    "text": "2.2 Putting Columns Into Gruops\n\n\nCode\n# Define our preferred order for `ctry_origin`\norder_countries &lt;- c(\"Germany\", \"Italy\", \"United States\", \"Japan\")\n\n# Reorder the table rows by our specific ordering of groups\ngtcars_8 |&gt;\n  arrange(\n    factor(ctry_origin, levels = order_countries), mfr, desc(msrp)\n  ) |&gt;\n  group_by(ctry_origin) |&gt;\n  gt()\n\n\n\n\n\n\n\n\nmfr\nmodel\nyear\ntrim\nbdy_style\nhp\nhp_rpm\ntrq\ntrq_rpm\nmpg_c\nmpg_h\ndrivetrain\ntrsmn\nmsrp\n\n\n\n\nGermany\n\n\nBMW\ni8\n2016\nMega World Coupe\ncoupe\n357\n5800\n420\n3700\n28\n29\nawd\n6am\n140700\n\n\nBMW\n6-Series\n2016\n640 I Coupe\ncoupe\n315\n5800\n330\n1400\n20\n30\nrwd\n8am\n77300\n\n\nItaly\n\n\nFerrari\n458 Speciale\n2015\nBase Coupe\ncoupe\n597\n9000\n398\n6000\n13\n17\nrwd\n7a\n291744\n\n\nFerrari\n458 Spider\n2015\nBase\nconvertible\n562\n9000\n398\n6000\n13\n17\nrwd\n7a\n263553\n\n\nUnited States\n\n\nChevrolet\nCorvette\n2016\nZ06 Coupe\ncoupe\n650\n6400\n650\n3600\n15\n22\nrwd\n7m\n88345\n\n\nFord\nGT\n2017\nBase Coupe\ncoupe\n647\n6250\n550\n5900\n11\n18\nrwd\n7a\n447000\n\n\nJapan\n\n\nAcura\nNSX\n2017\nBase Coupe\ncoupe\n573\n6500\n476\n2000\n21\n22\nawd\n9a\n156000\n\n\nNissan\nGT-R\n2016\nPremium Coupe\ncoupe\n545\n6400\n436\n3200\n16\n22\nawd\n6a\n101770\n\n\n\n\n\n\n\n\n\nCode\n\n# Reorder the table rows by our specific ordering of groups\ntab &lt;-\n  gtcars_8 |&gt;\n  arrange(\n    factor(ctry_origin, levels = order_countries),\n    mfr, desc(msrp)\n  ) |&gt;\n  mutate(car = paste(mfr, model)) |&gt;\n  select(-mfr, -model) |&gt;\n  group_by(ctry_origin) |&gt;\n  gt(rowname_col = \"car\")\n\n# Show the table\ntab\n\n\n\n\n\n\n\n\n\nyear\ntrim\nbdy_style\nhp\nhp_rpm\ntrq\ntrq_rpm\nmpg_c\nmpg_h\ndrivetrain\ntrsmn\nmsrp\n\n\n\n\nGermany\n\n\nBMW i8\n2016\nMega World Coupe\ncoupe\n357\n5800\n420\n3700\n28\n29\nawd\n6am\n140700\n\n\nBMW 6-Series\n2016\n640 I Coupe\ncoupe\n315\n5800\n330\n1400\n20\n30\nrwd\n8am\n77300\n\n\nItaly\n\n\nFerrari 458 Speciale\n2015\nBase Coupe\ncoupe\n597\n9000\n398\n6000\n13\n17\nrwd\n7a\n291744\n\n\nFerrari 458 Spider\n2015\nBase\nconvertible\n562\n9000\n398\n6000\n13\n17\nrwd\n7a\n263553\n\n\nUnited States\n\n\nChevrolet Corvette\n2016\nZ06 Coupe\ncoupe\n650\n6400\n650\n3600\n15\n22\nrwd\n7m\n88345\n\n\nFord GT\n2017\nBase Coupe\ncoupe\n647\n6250\n550\n5900\n11\n18\nrwd\n7a\n447000\n\n\nJapan\n\n\nAcura NSX\n2017\nBase Coupe\ncoupe\n573\n6500\n476\n2000\n21\n22\nawd\n9a\n156000\n\n\nNissan GT-R\n2016\nPremium Coupe\ncoupe\n545\n6400\n436\n3200\n16\n22\nawd\n6a\n101770\n\n\n\n\n\n\n\n\n\nCode\n# Put the first three columns under a spanner\n# column with the label 'Performance'\ntab &lt;-\n  tab |&gt;\n  tab_spanner(\n    label = \"Performance\",\n    columns = c(mpg_c, mpg_h, hp, hp_rpm, trq, trq_rpm)\n  )\n\n# Show the table\ntab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\ntrim\nbdy_style\n\nPerformance\n\ndrivetrain\ntrsmn\nmsrp\n\n\nmpg_c\nmpg_h\nhp\nhp_rpm\ntrq\ntrq_rpm\n\n\n\n\nGermany\n\n\nBMW i8\n2016\nMega World Coupe\ncoupe\n28\n29\n357\n5800\n420\n3700\nawd\n6am\n140700\n\n\nBMW 6-Series\n2016\n640 I Coupe\ncoupe\n20\n30\n315\n5800\n330\n1400\nrwd\n8am\n77300\n\n\nItaly\n\n\nFerrari 458 Speciale\n2015\nBase Coupe\ncoupe\n13\n17\n597\n9000\n398\n6000\nrwd\n7a\n291744\n\n\nFerrari 458 Spider\n2015\nBase\nconvertible\n13\n17\n562\n9000\n398\n6000\nrwd\n7a\n263553\n\n\nUnited States\n\n\nChevrolet Corvette\n2016\nZ06 Coupe\ncoupe\n15\n22\n650\n6400\n650\n3600\nrwd\n7m\n88345\n\n\nFord GT\n2017\nBase Coupe\ncoupe\n11\n18\n647\n6250\n550\n5900\nrwd\n7a\n447000\n\n\nJapan\n\n\nAcura NSX\n2017\nBase Coupe\ncoupe\n21\n22\n573\n6500\n476\n2000\nawd\n9a\n156000\n\n\nNissan GT-R\n2016\nPremium Coupe\ncoupe\n16\n22\n545\n6400\n436\n3200\nawd\n6a\n101770"
  },
  {
    "objectID": "contents/libs/R/gt/cheetsheet.html#mergin-columns",
    "href": "contents/libs/R/gt/cheetsheet.html#mergin-columns",
    "title": "gt",
    "section": "2.3 Mergin Columns",
    "text": "2.3 Mergin Columns\n\n\nCode\n# Perform three column merges to better present\n# MPG, HP, and torque; relabel all the remaining\n# columns for a nicer-looking presentation\ntab &lt;-\n  tab |&gt;\n  cols_merge(\n    columns = c(mpg_c, mpg_h),\n    pattern = \"&lt;&lt;{1}c&lt;br&gt;{2}h&gt;&gt;\"\n  ) |&gt;\n  cols_merge(\n    columns = c(hp, hp_rpm),\n    pattern = \"{1}&lt;br&gt;@{2}rpm\"\n  ) |&gt;\n  cols_merge(\n    columns = c(trq, trq_rpm),\n    pattern = \"{1}&lt;br&gt;@{2}rpm\"\n  ) |&gt;\n  cols_label(\n    mpg_c = \"MPG\",\n    hp = \"HP\",\n    trq = \"Torque\",\n    year = \"Year\",\n    trim = \"Trim\",\n    trsmn = \"Transmission\",\n    msrp = \"MSRP\"\n  )\n\n# Show the table\ntab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nTrim\nbdy_style\n\nPerformance\n\ndrivetrain\nTransmission\nMSRP\n\n\nMPG\nHP\nTorque\n\n\n\n\nGermany\n\n\nBMW i8\n2016\nMega World Coupe\ncoupe\n28c\n29h\n357\n@5800rpm\n420\n@3700rpm\nawd\n6am\n140700\n\n\nBMW 6-Series\n2016\n640 I Coupe\ncoupe\n20c\n30h\n315\n@5800rpm\n330\n@1400rpm\nrwd\n8am\n77300\n\n\nItaly\n\n\nFerrari 458 Speciale\n2015\nBase Coupe\ncoupe\n13c\n17h\n597\n@9000rpm\n398\n@6000rpm\nrwd\n7a\n291744\n\n\nFerrari 458 Spider\n2015\nBase\nconvertible\n13c\n17h\n562\n@9000rpm\n398\n@6000rpm\nrwd\n7a\n263553\n\n\nUnited States\n\n\nChevrolet Corvette\n2016\nZ06 Coupe\ncoupe\n15c\n22h\n650\n@6400rpm\n650\n@3600rpm\nrwd\n7m\n88345\n\n\nFord GT\n2017\nBase Coupe\ncoupe\n11c\n18h\n647\n@6250rpm\n550\n@5900rpm\nrwd\n7a\n447000\n\n\nJapan\n\n\nAcura NSX\n2017\nBase Coupe\ncoupe\n21c\n22h\n573\n@6500rpm\n476\n@2000rpm\nawd\n9a\n156000\n\n\nNissan GT-R\n2016\nPremium Coupe\ncoupe\n16c\n22h\n545\n@6400rpm\n436\n@3200rpm\nawd\n6a\n101770"
  },
  {
    "objectID": "contents/libs/R/gt/cheetsheet.html#using-formatter-functions",
    "href": "contents/libs/R/gt/cheetsheet.html#using-formatter-functions",
    "title": "gt",
    "section": "2.4 Using Formatter Functions",
    "text": "2.4 Using Formatter Functions\nfmt*という関数は、数値か文字列のをフォーマッティングするための便利ツールである。\n\n\nCode\n# Format the `msrp` column to USD currency\n# with no display of the currency subunits\ntab &lt;-\n  tab |&gt;\n  fmt_currency(columns = msrp, decimals = 0)\n\n# Show the table\ntab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nTrim\nbdy_style\n\nPerformance\n\ndrivetrain\nTransmission\nMSRP\n\n\nMPG\nHP\nTorque\n\n\n\n\nGermany\n\n\nBMW i8\n2016\nMega World Coupe\ncoupe\n28c\n29h\n357\n@5800rpm\n420\n@3700rpm\nawd\n6am\n$140,700\n\n\nBMW 6-Series\n2016\n640 I Coupe\ncoupe\n20c\n30h\n315\n@5800rpm\n330\n@1400rpm\nrwd\n8am\n$77,300\n\n\nItaly\n\n\nFerrari 458 Speciale\n2015\nBase Coupe\ncoupe\n13c\n17h\n597\n@9000rpm\n398\n@6000rpm\nrwd\n7a\n$291,744\n\n\nFerrari 458 Spider\n2015\nBase\nconvertible\n13c\n17h\n562\n@9000rpm\n398\n@6000rpm\nrwd\n7a\n$263,553\n\n\nUnited States\n\n\nChevrolet Corvette\n2016\nZ06 Coupe\ncoupe\n15c\n22h\n650\n@6400rpm\n650\n@3600rpm\nrwd\n7m\n$88,345\n\n\nFord GT\n2017\nBase Coupe\ncoupe\n11c\n18h\n647\n@6250rpm\n550\n@5900rpm\nrwd\n7a\n$447,000\n\n\nJapan\n\n\nAcura NSX\n2017\nBase Coupe\ncoupe\n21c\n22h\n573\n@6500rpm\n476\n@2000rpm\nawd\n9a\n$156,000\n\n\nNissan GT-R\n2016\nPremium Coupe\ncoupe\n16c\n22h\n545\n@6400rpm\n436\n@3200rpm\nawd\n6a\n$101,770"
  },
  {
    "objectID": "contents/libs/R/gt/cheetsheet.html#column-alignment-and-style-changes",
    "href": "contents/libs/R/gt/cheetsheet.html#column-alignment-and-style-changes",
    "title": "gt",
    "section": "2.5 Column Alignment and Style Changes",
    "text": "2.5 Column Alignment and Style Changes\n\n\nCode\n# Format the `msrp` column to USD currency\n# with no display of the currency subunits\ntab &lt;-\n  tab |&gt;\n  fmt_currency(columns = msrp, decimals = 0)\n\n# Show the table\ntab\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nTrim\nbdy_style\n\nPerformance\n\ndrivetrain\nTransmission\nMSRP\n\n\nMPG\nHP\nTorque\n\n\n\n\nGermany\n\n\nBMW i8\n2016\nMega World Coupe\ncoupe\n28c\n29h\n357\n@5800rpm\n420\n@3700rpm\nawd\n6am\n$140,700\n\n\nBMW 6-Series\n2016\n640 I Coupe\ncoupe\n20c\n30h\n315\n@5800rpm\n330\n@1400rpm\nrwd\n8am\n$77,300\n\n\nItaly\n\n\nFerrari 458 Speciale\n2015\nBase Coupe\ncoupe\n13c\n17h\n597\n@9000rpm\n398\n@6000rpm\nrwd\n7a\n$291,744\n\n\nFerrari 458 Spider\n2015\nBase\nconvertible\n13c\n17h\n562\n@9000rpm\n398\n@6000rpm\nrwd\n7a\n$263,553\n\n\nUnited States\n\n\nChevrolet Corvette\n2016\nZ06 Coupe\ncoupe\n15c\n22h\n650\n@6400rpm\n650\n@3600rpm\nrwd\n7m\n$88,345\n\n\nFord GT\n2017\nBase Coupe\ncoupe\n11c\n18h\n647\n@6250rpm\n550\n@5900rpm\nrwd\n7a\n$447,000\n\n\nJapan\n\n\nAcura NSX\n2017\nBase Coupe\ncoupe\n21c\n22h\n573\n@6500rpm\n476\n@2000rpm\nawd\n9a\n$156,000\n\n\nNissan GT-R\n2016\nPremium Coupe\ncoupe\n16c\n22h\n545\n@6400rpm\n436\n@3200rpm\nawd\n6a\n$101,770\n\n\n\n\n\n\n\n\n\nCode\n# Use dplyr functions to get the car with the best city gas mileage;\n# this will be used to target the correct cell for a footnote\nbest_gas_mileage_city &lt;-\n  gtcars |&gt;\n  slice_max(mpg_c, n = 1) |&gt;\n  mutate(car = paste(mfr, model)) |&gt;\n  pull(car)\n\n# Use dplyr functions to get the car with the highest horsepower\n# this will be used to target the correct cell for a footnote\nhighest_horsepower &lt;-\n  gtcars |&gt;\n  slice_max(hp, n = 1) |&gt;\n  mutate(car = paste(mfr, model)) |&gt;\n  pull(car)\n\n# Define our preferred order for `ctry_origin`\norder_countries &lt;- c(\"Germany\", \"Italy\", \"United States\", \"Japan\")\n\n# Create a display table with `gtcars`, using all of the previous\n# statements piped together + additional `tab_footnote()` stmts\ntab &lt;-\n  gtcars |&gt;\n  arrange(\n    factor(ctry_origin, levels = order_countries),\n    mfr, desc(msrp)\n  ) |&gt;\n  mutate(car = paste(mfr, model)) |&gt;\n  select(-mfr, -model) |&gt;\n  group_by(ctry_origin) |&gt;\n  gt(rowname_col = \"car\") |&gt;\n  cols_hide(columns = c(drivetrain, bdy_style)) |&gt;\n  cols_move(\n    columns = c(trsmn, mpg_c, mpg_h),\n    after = trim\n  ) |&gt;\n  tab_spanner(\n    label = \"Performance\",\n    columns = c(mpg_c, mpg_h, hp, hp_rpm, trq, trq_rpm)\n  ) |&gt;\n  cols_merge(\n    columns = c(mpg_c, mpg_h),\n    pattern = \"&lt;&lt;{1}c&lt;br&gt;{2}h&gt;&gt;\"\n  ) |&gt;\n  cols_merge(\n    columns = c(hp, hp_rpm),\n    pattern = \"{1}&lt;br&gt;@{2}rpm\"\n  ) |&gt;\n  cols_merge(\n    columns = c(trq, trq_rpm),\n    pattern = \"{1}&lt;br&gt;@{2}rpm\"\n  ) |&gt;\n  cols_label(\n    mpg_c = \"MPG\",\n    hp = \"HP\",\n    trq = \"Torque\",\n    year = \"Year\",\n    trim = \"Trim\",\n    trsmn = \"Transmission\",\n    msrp = \"MSRP\"\n  ) |&gt;\n  fmt_currency(columns = msrp, decimals = 0) |&gt;\n  cols_align(\n    align = \"center\",\n    columns = c(mpg_c, hp, trq)\n  ) |&gt;\n  tab_style(\n    style = cell_text(size = px(12)),\n    locations = cells_body(\n      columns = c(trim, trsmn, mpg_c, hp, trq)\n    )\n  ) |&gt;\n  text_transform(\n    locations = cells_body(columns = trsmn),\n    fn = function(x) {\n      speed &lt;- substr(x, 1, 1)\n\n      type &lt;-\n        dplyr::case_when(\n          substr(x, 2, 3) == \"am\" ~ \"Automatic/Manual\",\n          substr(x, 2, 2) == \"m\" ~ \"Manual\",\n          substr(x, 2, 2) == \"a\" ~ \"Automatic\",\n          substr(x, 2, 3) == \"dd\" ~ \"Direct Drive\"\n        )\n\n      paste(speed, \" Speed&lt;br&gt;&lt;em&gt;\", type, \"&lt;/em&gt;\")\n    }\n  ) |&gt;\n  tab_header(\n    title = md(\"The Cars of **gtcars**\"),\n    subtitle = \"These are some fine automobiles\"\n  ) |&gt;\n  tab_source_note(\n    source_note = md(\n      \"Source: Various pages within the Edmonds website.\"\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = md(\"Best gas mileage (city) of all the **gtcars**.\"),\n    locations = cells_body(\n      columns = mpg_c,\n      rows = best_gas_mileage_city\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = md(\"The highest horsepower of all the **gtcars**.\"),\n    locations = cells_body(\n      columns = hp,\n      rows = highest_horsepower\n    )\n  ) |&gt;\n  tab_footnote(\n    footnote = \"All prices in U.S. dollars (USD).\",\n    locations = cells_column_labels(columns = msrp)\n  )\n\n# Show the table\ntab\n\n\n\n\n\n  \n    \n      The Cars of gtcars\n    \n    \n      These are some fine automobiles\n    \n    \n      \n      Year\n      Trim\n      Transmission\n      \n        Performance\n      \n      MSRP1\n    \n    \n      MPG\n      HP\n      Torque\n    \n  \n  \n    \n      Germany\n    \n    Audi R8\n2015\n4.2 (Manual) Coupe\n6  Speed Manual \n11c20h\n430@7900rpm\n317@4500rpm\n$115,900\n    Audi S8\n2016\nBase Sedan\n8  Speed Automatic/Manual \n15c25h\n520@5800rpm\n481@1700rpm\n$114,900\n    Audi RS 7\n2016\nQuattro Hatchback\n8  Speed Automatic/Manual \n15c25h\n560@5700rpm\n516@1750rpm\n$108,900\n    Audi S7\n2016\nPrestige quattro Hatchback\n7  Speed Automatic \n17c27h\n450@5800rpm\n406@1400rpm\n$82,900\n    Audi S6\n2016\nPremium Plus quattro Sedan\n7  Speed Automatic \n18c27h\n450@5800rpm\n406@1400rpm\n$70,900\n    BMW i8\n2016\nMega World Coupe\n6  Speed Automatic/Manual \n28c29h2\n357@5800rpm\n420@3700rpm\n$140,700\n    BMW M6\n2016\nBase Coupe\n7  Speed Automatic \n15c22h\n560@6000rpm\n500@1500rpm\n$113,400\n    BMW M5\n2016\nBase Sedan\n7  Speed Automatic/Manual \n15c22h\n560@6000rpm\n500@1500rpm\n$94,100\n    BMW 6-Series\n2016\n640 I Coupe\n8  Speed Automatic/Manual \n20c30h\n315@5800rpm\n330@1400rpm\n$77,300\n    BMW M4\n2016\nBase Coupe\n6  Speed Manual \n17c24h\n425@5500rpm\n406@1850rpm\n$65,700\n    Mercedes-Benz AMG GT\n2016\nS Coupe\n7  Speed Automatic \n16c22h\n503@6250rpm\n479@1750rpm\n$129,900\n    Mercedes-Benz SL-Class\n2016\nSL400 Convertible\n7  Speed Automatic/Manual \n20c27h\n329@5250rpm\n354@1600rpm\n$85,050\n    Porsche 911\n2016\nCarrera Coupe\n7  Speed Manual \n20c28h\n350@7400rpm\n287@5600rpm\n$84,300\n    Porsche Panamera\n2016\nBase Sedan\n7  Speed Automatic \n18c28h\n310@6200rpm\n295@3750rpm\n$78,100\n    Porsche 718 Boxster\n2017\nBase Convertible\n6  Speed Manual \n21c28h\n300@6500rpm\n280@1950rpm\n$56,000\n    Porsche 718 Cayman\n2017\nBase Coupe\n6  Speed Manual \n20c29h\n300@6500rpm\n280@1950rpm\n$53,900\n    \n      Italy\n    \n    Ferrari LaFerrari\n2015\nBase Coupe\n7  Speed Automatic \n12c16h\n949@9000rpm3\n664@6750rpm\n$1,416,362\n    Ferrari F12Berlinetta\n2015\nBase Coupe\n7  Speed Automatic \n11c16h\n731@8250rpm\n509@6000rpm\n$319,995\n    Ferrari GTC4Lusso\n2017\nBase Coupe\n7  Speed Automatic \n12c17h\n680@8250rpm\n514@5750rpm\n$298,000\n    Ferrari FF\n2015\nBase Coupe\n7  Speed Automatic \n11c16h\n652@8000rpm\n504@6000rpm\n$295,000\n    Ferrari 458 Speciale\n2015\nBase Coupe\n7  Speed Automatic \n13c17h\n597@9000rpm\n398@6000rpm\n$291,744\n    Ferrari 458 Spider\n2015\nBase\n7  Speed Automatic \n13c17h\n562@9000rpm\n398@6000rpm\n$263,553\n    Ferrari 488 GTB\n2016\nBase Coupe\n7  Speed Automatic \n15c22h\n661@8000rpm\n561@3000rpm\n$245,400\n    Ferrari 458 Italia\n2014\nBase Coupe\n7  Speed Automatic \n13c17h\n562@9000rpm\n398@6000rpm\n$233,509\n    Ferrari California\n2015\nBase Convertible\n7  Speed Automatic \n16c23h\n553@7500rpm\n557@4750rpm\n$198,973\n    Lamborghini Aventador\n2015\nLP 700-4 Coupe\n7  Speed Automatic \n11c18h\n700@8250rpm\n507@5500rpm\n$397,500\n    Lamborghini Huracan\n2015\nLP 610-4 Coupe\n7  Speed Automatic \n16c20h\n610@8250rpm\n413@6500rpm\n$237,250\n    Lamborghini Gallardo\n2014\nLP 550-2 Coupe\n6  Speed Automatic \n12c20h\n550@8000rpm\n398@6500rpm\n$191,900\n    Maserati Granturismo\n2016\nSport Coupe\n6  Speed Automatic/Manual \n13c21h\n454@7600rpm\n384@4750rpm\n$132,825\n    Maserati Quattroporte\n2016\nS Sedan\n8  Speed Automatic/Manual \n16c23h\n404@5500rpm\n406@1500rpm\n$99,900\n    Maserati Ghibli\n2016\nBase Sedan\n8  Speed Automatic/Manual \n17c24h\n345@5250rpm\n369@1750rpm\n$70,600\n    \n      United States\n    \n    Chevrolet Corvette\n2016\nZ06 Coupe\n7  Speed Manual \n15c22h\n650@6400rpm\n650@3600rpm\n$88,345\n    Dodge Viper\n2017\nGT Coupe\n6  Speed Manual \n12c19h\n645@5000rpm\n600@5000rpm\n$95,895\n    Ford GT\n2017\nBase Coupe\n7  Speed Automatic \n11c18h\n647@6250rpm\n550@5900rpm\n$447,000\n    Tesla Model S\n2017\n75D\n1  Speed Direct Drive \n\n259@6100rpm\n243@NArpm\n$74,500\n    \n      Japan\n    \n    Acura NSX\n2017\nBase Coupe\n9  Speed Automatic \n21c22h\n573@6500rpm\n476@2000rpm\n$156,000\n    Nissan GT-R\n2016\nPremium Coupe\n6  Speed Automatic \n16c22h\n545@6400rpm\n436@3200rpm\n$101,770\n    \n      United Kingdom\n    \n    Aston Martin Vanquish\n2016\nBase Coupe\n8  Speed Automatic/Manual \n13c21h\n568@6650rpm\n465@5500rpm\n$287,250\n    Aston Martin DB11\n2017\nBase Coupe\n8  Speed Automatic/Manual \n15c21h\n608@6500rpm\n516@1500rpm\n$211,195\n    Aston Martin Rapide S\n2016\nBase Sedan\n8  Speed Automatic/Manual \n14c21h\n552@6650rpm\n465@5500rpm\n$205,300\n    Aston Martin Vantage\n2016\nV8 GT (Manual) Coupe\n6  Speed Manual \n13c19h\n430@7300rpm\n361@5000rpm\n$103,300\n    Bentley Continental GT\n2016\nV8 Coupe\n8  Speed Automatic/Manual \n15c25h\n500@6000rpm\n487@1700rpm\n$198,500\n    Jaguar F-Type\n2016\nBase (Manual) Coupe\n6  Speed Manual \n16c24h\n340@6500rpm\n332@3500rpm\n$65,000\n    Lotus Evora\n2017\n2+2 Coupe\n6  Speed Manual \n16c24h\n400@7000rpm\n302@3500rpm\n$91,900\n    McLaren 570\n2016\nBase Coupe\n7  Speed Automatic \n16c23h\n570@7500rpm\n443@5000rpm\n$184,900\n    Rolls-Royce Dawn\n2016\nBase Convertible\n8  Speed Automatic \n12c19h\n563@5250rpm\n575@1500rpm\n$335,000\n    Rolls-Royce Wraith\n2016\nBase Coupe\n8  Speed Automatic \n13c21h\n624@5600rpm\n590@1500rpm\n$304,350\n  \n  \n    \n      Source: Various pages within the Edmonds website.\n    \n  \n  \n    \n      1 All prices in U.S. dollars (USD).\n    \n    \n      2 Best gas mileage (city) of all the gtcars.\n    \n    \n      3 The highest horsepower of all the gtcars."
  },
  {
    "objectID": "contents/libs/R/gt/cheetsheet.html#export",
    "href": "contents/libs/R/gt/cheetsheet.html#export",
    "title": "gt",
    "section": "2.6 export",
    "text": "2.6 export\n\n\nCode\noutput_file &lt;- file.path(output_dir, \"gtcars.html\")\ngtsave(tab, output_file)"
  },
  {
    "objectID": "contents/libs/R/igraph/index.html",
    "href": "contents/libs/R/igraph/index.html",
    "title": "igraph",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/igraph\")\n\n\n\n\nCode\nbox::use(\n  igraph[...],\n  ggplot2[...],\n  cowplot[...], \n  showtext[showtext_auto], \n  sysfonts[font_add_google],\n)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 はじめに\nigraphはグラフ構造の解析を行うためのオープンソースのパッケージである。 RだけでなくPython, Mathmaticaでも使うことができる。\nここでは、Rでのigraphの使い方を学ぶこととする。\n\nR interface\nR reference\nigraph\n\nRでコマンド：ネットワークの描写は「igraph」パッケージがやっぱり便利ですを見るとラスターデータもプロット可能である。これを使えばツリー表現も簡単になる気がする。\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "igraph",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/legendry/articles.html",
    "href": "contents/libs/R/legendry/articles.html",
    "title": "articles",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\nCode\nlibrary(legendry)\nlibrary(tidyverse)\nlibrary(showtext)\n\nproject_dir &lt;- here::here()\ncur_dir     &lt;- file.path(project_dir, \"contents/libs/R/legendry\")\n\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "legendry",
      "articles"
    ]
  },
  {
    "objectID": "contents/libs/R/legendry/articles.html#axes",
    "href": "contents/libs/R/legendry/articles.html#axes",
    "title": "articles",
    "section": "2.1 Axes",
    "text": "2.1 Axes\n\n\nCode\n# Turn on axis lines\ntheme_update(axis.line = element_line())\n\n# A standard plot\nstandard &lt;- ggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  labs(\n    x = \"Engine Displacement (Litres)\",\n    y = \"Highway Miles per Gallon\"\n  )\n\nstandard + guides(\n  x = \"axis_base\",\n  y = \"axis_base\"\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\np &lt;- standard + \n  scale_x_continuous(guide = guide_axis_base(bidi = TRUE)) +\n  scale_y_continuous(guide = guide_axis_base(bidi = TRUE))\np\n\n\n\n\n\n\n\n\n\n\n\nCode\np + coord_radial(start = 1.25 * pi, end = 2.75 * pi)\n\n\n\n\n\n\n\n\n\n\nguide_axis_baseを使うことで柔軟な軸を割くしすることが可能となる\n\n\n\nCode\nstandard + aes(colour = cty) +\n  guides(colour = \"axis_base\")",
    "crumbs": [
      "R",
      "legendry",
      "articles"
    ]
  },
  {
    "objectID": "contents/libs/R/legendry/articles.html#nested-axis",
    "href": "contents/libs/R/legendry/articles.html#nested-axis",
    "title": "articles",
    "section": "2.2 Nested Axis",
    "text": "2.2 Nested Axis\n\n\nCode\ndf &lt;- data.frame(\n  item = c(\"Coffee\", \"Tea\", \"Apple\", \"Pear\", \"Car\"),\n  type = c(\"Drink\", \"Drink\", \"Fruit\", \"Fruit\", \"Vehicle\"),\n  amount = c(5, 1, 2, 3, 1)\n)\n\nplain &lt;- ggplot(df, aes(interaction(item, type), amount)) +\n  geom_col()\nplain + guides(x = \"axis_nested\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df, aes(amount, interaction(item, type))) +\n  geom_col() + guides(y = \"axis_nested\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nmy_key &lt;- key_range_manual(\n  start = c(\"Coffee\", \"Apple\"),\n  end   = c(\"Tea\", \"Pear\"),\n  name  = c(\"Drinks\", \"Fruits\"), \n  level = 1\n)\n\nggplot(df, aes(item, amount)) +\n  geom_col() +\n  scale_x_discrete(\n    limits = df$item,\n    guide = guide_axis_nested(\n      regular_key = \"auto\",\n      key = my_key\n    )\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf$item\n#&gt; [1] \"Coffee\" \"Tea\"    \"Apple\"  \"Pear\"   \"Car\"\n\n\n\n\nCode\nplain + guides(x = guide_axis_nested(type = \"box\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\npresidents &lt;- key_range_map(presidential, start = start, end = end, name = name)\n\neco &lt;- ggplot(economics, aes(date, unemploy)) +\n  geom_line() +\n  labs(y = \"Unemployment\")\n  \neco + guides(x = guide_axis_nested(key = presidents))\n\n\n\n\n\n\n\n\n\n\n\nCode\npresidential\n#&gt; # A tibble: 12 × 4\n#&gt;    name       start      end        party     \n#&gt;    &lt;chr&gt;      &lt;date&gt;     &lt;date&gt;     &lt;chr&gt;     \n#&gt;  1 Eisenhower 1953-01-20 1961-01-20 Republican\n#&gt;  2 Kennedy    1961-01-20 1963-11-22 Democratic\n#&gt;  3 Johnson    1963-11-22 1969-01-20 Democratic\n#&gt;  4 Nixon      1969-01-20 1974-08-09 Republican\n#&gt;  5 Ford       1974-08-09 1977-01-20 Republican\n#&gt;  6 Carter     1977-01-20 1981-01-20 Democratic\n#&gt;  7 Reagan     1981-01-20 1989-01-20 Republican\n#&gt;  8 Bush       1989-01-20 1993-01-20 Republican\n#&gt;  9 Clinton    1993-01-20 2001-01-20 Democratic\n#&gt; 10 Bush       2001-01-20 2009-01-20 Republican\n#&gt; 11 Obama      2009-01-20 2017-01-20 Democratic\n#&gt; 12 Trump      2017-01-20 2021-01-20 Republican\n\n\n\n\nCode\npresidents$.level &lt;- rep(1:3, length.out = nrow(presidents))\n\neco + guides(x = guide_axis_nested(\n  key = presidents,\n  levels_text = list(\n    element_text(face = \"bold\"),\n    NULL,\n    element_text(face = \"italic\")\n  )\n))",
    "crumbs": [
      "R",
      "legendry",
      "articles"
    ]
  },
  {
    "objectID": "contents/libs/R/legendry/articles.html#dendrograms",
    "href": "contents/libs/R/legendry/articles.html#dendrograms",
    "title": "articles",
    "section": "2.3 Dendrograms",
    "text": "2.3 Dendrograms\n\n\nCode\nclust &lt;- hclust(dist(scale(mtcars)), method = \"ave\")\nplot(clust)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(mtcars, aes(mpg, rownames(mtcars))) +\n  geom_col() +\n  scale_y_dendro(clust)\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(mtcars, aes(mpg, rownames(mtcars))) +\n  geom_col() +\n  scale_y_dendro(clust) +\n  coord_radial(theta = \"y\", inner.radius = 0.5) +\n  guides(\n    theta = guide_axis_base(angle = 90),\n    theta.sec = primitive_segments(\"dendro\", vanish = TRUE),\n    r = \"none\"\n  ) +\n  theme(\n    axis.title = element_blank(),\n    plot.margin = margin(t = 50, b = 50)\n  )",
    "crumbs": [
      "R",
      "legendry",
      "articles"
    ]
  },
  {
    "objectID": "contents/libs/R/legendry/articles.html#stacking",
    "href": "contents/libs/R/legendry/articles.html#stacking",
    "title": "articles",
    "section": "5.1 Stacking",
    "text": "5.1 Stacking\n\n\nCode\nstandard &lt;- ggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  labs(\n    x = \"Engine displacement (litres)\",\n    y = \"Highway Miles per Gallon\"\n  ) +\n  theme(axis.line = element_line())\n\nstandard + guides(x = guide_axis_stack(\"axis\", \"axis\", \"axis\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\nstaxis &lt;- compose_stack(\"axis_base\", \"axis_base\", \"axis_base\")\n\nstandard + \n  aes(colour = cty) +\n  guides(\n    x = staxis,\n    colour = guide_colbar(first_guide = staxis, second_guide = \"none\")\n  ) +\n  theme(legend.axis.line = element_line())\n#&gt; Warning in gizmo_barcap(): Unable to check the capabilities of the png device.\n\n\n\n\n\n\n\n\n\n\n\nCode\nclass(guide_axis_base())\n#&gt; [1] \"ComposeStack\" \"Compose\"      \"Guide\"        \"ggproto\"      \"gg\"",
    "crumbs": [
      "R",
      "legendry",
      "articles"
    ]
  },
  {
    "objectID": "contents/libs/R/legendry/articles.html#primitive",
    "href": "contents/libs/R/legendry/articles.html#primitive",
    "title": "articles",
    "section": "5.2 Primitive",
    "text": "5.2 Primitive\n\n\nCode\nrange_key &lt;- key_range_manual(\n  start = c(2, 4), end = c(5, 6), \n  name = c(\"First\", \"Second\")\n)\n\nstandard + guides(x = compose_stack(\n  \"axis_base\",\n  primitive_bracket(range_key, \"curvy\"),\n  primitive_spacer(unit(0.5, \"cm\")),\n  primitive_box(range_key),\n  primitive_spacer(unit(0.5, \"cm\")),\n  primitive_fence(range_key, rail = \"outer\")\n))",
    "crumbs": [
      "R",
      "legendry",
      "articles"
    ]
  },
  {
    "objectID": "contents/libs/R/legendry/articles.html#sandwiching",
    "href": "contents/libs/R/legendry/articles.html#sandwiching",
    "title": "articles",
    "section": "5.3 Sandwiching",
    "text": "5.3 Sandwiching\n\n\nCode\nstandard +\n  aes(colour = cty) +\n  scale_colour_viridis_c(\n    guide = compose_sandwich(\n      text     = primitive_labels(), \n      opposite = primitive_ticks()\n    )\n  )\n#&gt; Warning in gizmo_barcap(): Unable to check the capabilities of the png device.",
    "crumbs": [
      "R",
      "legendry",
      "articles"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html",
    "href": "contents/libs/R/patchwork/working.html",
    "title": "patchwork",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/patchwork\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggfittext\",\n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\nlibrary(colorblindr)\n\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#controlling-layout",
    "href": "contents/libs/R/patchwork/working.html#controlling-layout",
    "title": "patchwork",
    "section": "3.1 Controlling layout",
    "text": "3.1 Controlling layout\n通常は自動で、カラム数が２のレイアウトになる。\n\n\nCode\np1 + p2 + p3 + p4\n\n\n\n\n\n\n\n\n\n\n\nCode\np1 + p2 + p3 + p4 + plot_layout(nrow = 3, byrow = FALSE)",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#stacking-and-packing-plots",
    "href": "contents/libs/R/patchwork/working.html#stacking-and-packing-plots",
    "title": "patchwork",
    "section": "3.2 Stacking and packing plots",
    "text": "3.2 Stacking and packing plots\n1行や1列のレイアウトを指定することにより、グラフの大きさを他のグラフと変更することも可能である。 もしくはレイアウトのショートカットとして|や/を使うこともできる。\n/の方が先に演算されているように見える。\n\n\nCode\np1 | (p2 | p3) / p4\n\n\n\n\n\n\n\n\n\n\n\nCode\np1 | p2 | p3 / p4",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#annotating-the-composition",
    "href": "contents/libs/R/patchwork/working.html#annotating-the-composition",
    "title": "patchwork",
    "section": "3.3 Annotating the composition",
    "text": "3.3 Annotating the composition\nplot_annotationを使うことで、結合したプロット全体に対するアノテーションを与えることが可能である\n\n\nCode\n(p1 | (p2 / p3)) + \n  plot_annotation(title = \"The surprising story about mtcars\")\n\n\n\n\n\n\n\n\n\nプロットそれぞれにタグを与えることが可能である。 これによりサブプロットの見分けが付きやすくなる。\n\n\nCode\np1 + p2 + p3 + plot_annotation(tag_levels = \"I\")",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#controlling-guides",
    "href": "contents/libs/R/patchwork/working.html#controlling-guides",
    "title": "patchwork",
    "section": "3.4 Controlling guides",
    "text": "3.4 Controlling guides\nplot_layoutのguides引数を使うことで、凡例の制御が可能である。 そのままプロットするとサブプロットごとの位置に凡例が表示されるが、guidesを使うことで、 プロット全体に対することがわかりやすい位置に配置される。\n\n\nCode\np1 + p2 + p3 + p4\n\n\n\n\n\n\n\n\n\n\n\nCode\np1 + p2 + p3 + p4 + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n初期設定の設定はautoである。keepにすると、かならずサブプロットのプロット領域に凡例が配置されることになる。\n次は複数の凡例を有する場合のプロットである。\n\n\nCode\np1a &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp, colour = mpg, size = wt)) + \n  ggtitle(\"PLot 1a\")\n\np1a | (p2 / p3)\n\n\n\n\n\n\n\n\n\n上記の状態でguidesをcollectにすると、凡例がプロット全体に配置される。\n\n\nCode\n(p1a | (p2 / p3)) + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\n\n凡例を凡例だけの場所を準備してそこにプロットすることも可能である。\n\n\nCode\np1 + p2 + p3 + guide_area() + plot_layout(guides = \"collect\")",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#controlling-scales",
    "href": "contents/libs/R/patchwork/working.html#controlling-scales",
    "title": "patchwork",
    "section": "3.5 Controlling scales",
    "text": "3.5 Controlling scales\nplot_layoutのaxes=collect引数を使うことで、スケールの制御が可能である。\n通常の+では軸は同期された表現にならないが、上記の設定により同期された表現となる。\n\n\nCode\np1 + p2 + plot_layout(axes = \"collect\")\n\n\n\n\n\n\n\n\n\n同期はするが、ラベルを消さないことも可能である。\n\n\nCode\np1 + (p2 + scale_y_continuous(position = \"right\")) + plot_layout(axis_titles = \"collect\")",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#adding-empty-area",
    "href": "contents/libs/R/patchwork/working.html#adding-empty-area",
    "title": "patchwork",
    "section": "3.6 Adding empty area",
    "text": "3.6 Adding empty area\n\n\nCode\np1 + plot_spacer() + p2 + plot_spacer() + p3 + plot_spacer()\n\n\n\n\n\n\n\n\n\n\n\nCode\n(p1 + plot_spacer() + p2) / (plot_spacer() + p3 + plot_spacer())",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#controlling-the-grid",
    "href": "contents/libs/R/patchwork/working.html#controlling-the-grid",
    "title": "patchwork",
    "section": "3.7 Controlling the grid",
    "text": "3.7 Controlling the grid\n\n\nCode\np1 + p2 + p3 + p4 + \n  plot_layout(widths = c(2, 1), byrow = FALSE, ncol = 2)",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#moving-beyond-the-grid",
    "href": "contents/libs/R/patchwork/working.html#moving-beyond-the-grid",
    "title": "patchwork",
    "section": "3.8 Moving beyond the grid",
    "text": "3.8 Moving beyond the grid\nより柔軟なレイアウトの指定方法である\n\n\nCode\nlayout &lt;- \"\n##BBBB\nAACCDD\n##CCDD\n\"\np1 + p2 + p3 + p4 + \n  plot_layout(design = layout)\n\n\n\n\n\n\n\n\n\nareaconstructorを使うことでさらにプログラミックなものを作成することが可能である。\n\n\nCode\nlayout &lt;- c(\n  area(t = 2, l = 1, b = 5, r = 4),\n  area(t = 1, l = 3, b = 3, r = 5), \n  area(t = 3, l = 2, b = 4, r = 2.5)\n)\np1 + p2 + p3 + \n  plot_layout(design = layout)",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#fixed-aspect-plots",
    "href": "contents/libs/R/patchwork/working.html#fixed-aspect-plots",
    "title": "patchwork",
    "section": "3.9 Fixed aspect plots",
    "text": "3.9 Fixed aspect plots\n固定軸でのプロットアセンブリは特別なケースである。coord_fixed, coord_polar, coord_sf。\nデフォルトな固定軸に合わせられる。\n\n\nCode\np_fixed &lt;- ggplot(mtcars) + \n  geom_point(aes(hp, disp)) + \n  ggtitle('Plot F') + \n  coord_fixed()\np_fixed + p1 + p2 + p3\n\n\n\n\n\n\n\n\n\n合わせたくないときには、幅を指定すればよい。アスペクト比が維持されていることがわかるｓｓ\n\n\nCode\np_fixed + p1 + p2 + p3 + plot_layout(widths = 1)",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#avoiding-alignment",
    "href": "contents/libs/R/patchwork/working.html#avoiding-alignment",
    "title": "patchwork",
    "section": "3.10 Avoiding alignment",
    "text": "3.10 Avoiding alignment\nたとえば、captionが長いがそれに合わせて他のグラフについて、余白を調整したくないときがある。 そのときには次のような処理をおこなう。\n\n\nCode\np2mod &lt;- p2 + labs(x = \"This is such a long\\nand important label that\\nit has to span many lines\")\np1 | p2mod\n\n\n\n\n\n\n\n\n\n\n\nCode\nfree(p1) | p2mod\n\n\n\n\n\n\n\n\n\n\n\nCode\np1 + inset_element(p2, left = 0.6, bottom = 0.6, right = 1, top = 1)",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#insets",
    "href": "contents/libs/R/patchwork/working.html#insets",
    "title": "patchwork",
    "section": "3.11 Insets",
    "text": "3.11 Insets\n\n\nCode\np1 + inset_element(p2, left = 0.6, bottom = 0.6, right = 1, top = 1)\n\n\n\n\n\n\n\n\n\nnpcという変数を使うことで、エリアの座標を0, 1にした相対値で指定することが可能となる\n\n\nCode\n\np1 + inset_element(\n  p2, \n  left = 0.5, \n  bottom = 0.5, \n  right = unit(1, 'npc') - unit(1, 'cm'), \n  top = unit(1, 'npc') - unit(1, 'cm')\n)",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#adding-non-ggplot-content",
    "href": "contents/libs/R/patchwork/working.html#adding-non-ggplot-content",
    "title": "patchwork",
    "section": "4.1 Adding non-ggplot content",
    "text": "4.1 Adding non-ggplot content\n\n\nCode\np1 + grid::textGrob('Some really important text')",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#stacking-and-packing",
    "href": "contents/libs/R/patchwork/working.html#stacking-and-packing",
    "title": "patchwork",
    "section": "4.2 Stacking and packing",
    "text": "4.2 Stacking and packing\n\n\nCode\np1 + gridExtra::tableGrob(mtcars[1:10, c('mpg', 'disp')])\n\n\n\n\n\n\n\n\n\n通常のプロットでも組合せていいみたい。\n\n\nCode\np1 + ~plot(mtcars$mpg, mtcars$disp, main = 'Plot 2')",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/patchwork/working.html#modfiying-patches",
    "href": "contents/libs/R/patchwork/working.html#modfiying-patches",
    "title": "patchwork",
    "section": "4.3 Modfiying patches",
    "text": "4.3 Modfiying patches\nグラフ全体でテーマを変換するときに\n\n\nCode\npatchwork &lt;- p3 / (p1 | p2)\npatchwork & theme_minimal()",
    "crumbs": [
      "R",
      "pathcwork",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/02_examples.html",
    "href": "contents/libs/R/plotly/02_examples.html",
    "title": "02 Examples",
    "section": "",
    "text": "#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.0\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function",
    "crumbs": [
      "R",
      "plotly",
      "02 Examples"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/02_examples.html#折れ線",
    "href": "contents/libs/R/plotly/02_examples.html#折れ線",
    "title": "02 Examples",
    "section": "1.1 折れ線",
    "text": "1.1 折れ線\n\n\nCode\nlibrary(plotly) \n\nlibrary(plotly)\n\n# サンプルデータ\nx &lt;- c(1, 2, 3, 4, 5)\ny &lt;- c(10, 11, 12, 13, 14)\ny2 &lt;- c(14, 13, 12, 11, 10)\n\n# プロットの作成\np &lt;- plot_ly(x = x, y = y, type = 'scatter', mode = 'lines', name = 'データ1') %&gt;%\n  add_trace(x = x, y = y2, type = 'scatter', mode = 'lines', name = 'データ2') %&gt;%\n  layout(\n    title = list(text = \"カスタマイズされたプロット\", font = list(size = 24)),\n    xaxis = list(\n      title = \"X軸\", \n      titlefont = list(size = 18),\n      tickangle = 45,  # X軸のラベルを45度回転\n      showgrid = FALSE,  # X軸のグリッド線を非表示\n      range = c(0, 6),  # X軸の範囲を指定\n      tickvals = c(1, 2, 3, 4, 5),  # 特定の目盛りを表示\n      ticktext = c(\"A\", \"B\", \"C\", \"D\", \"E\")  # 目盛りのラベルを変更\n    ),\n    yaxis = list(\n      title = \"Y軸\",\n      titlefont = list(size = 18),\n      showgrid = TRUE,  # Y軸のグリッド線を表示\n      range = c(9, 15)  # Y軸の範囲を指定\n    ),\n    plot_bgcolor = 'lightgray',  # プロット領域の背景色\n    paper_bgcolor = 'white',     # 全体の背景色\n    margin = list(l = 60, r = 60, t = 60, b = 60)  # マージンの調整\n  )\n\n# プロットの表示\np",
    "crumbs": [
      "R",
      "plotly",
      "02 Examples"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/02_examples.html#散布図",
    "href": "contents/libs/R/plotly/02_examples.html#散布図",
    "title": "02 Examples",
    "section": "1.2 散布図",
    "text": "1.2 散布図\n\n\nCode\nlibrary(plotly)\n\n# サンプルデータ\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- rnorm(100)\nz &lt;- rnorm(100)\ncategory &lt;- sample(c(\"A\", \"B\", \"C\"), 100, replace = TRUE)\n\n# 散布図の作成\np &lt;- plot_ly(\n  x = x,\n  y = y,\n  mode = 'markers',  # 散布図をマーカーとして表示\n  type = 'scatter',\n  marker = list(\n    color = z,            # マーカーの色をzに基づいて設定\n    size = 10,            # マーカーのサイズ\n    opacity = 0.7,        # マーカーの透明度\n    colorbar = list(title = \"Z値\")  # カラーバーのタイトル\n  ),\n  text = paste(\"Category: \", category),  # ツールチップに表示するテキスト\n  hoverinfo = 'text'  # ツールチップの表示情報\n) %&gt;%\n  layout(\n    title = list(text=\"散布図のカスタマイズ例\", size=40),  # タイトル\n    xaxis = list(\n      title = \"X軸\",              # X軸のラベル\n      titlefont = list(size = 18),  # X軸ラベルのフォントサイズ\n      tickangle = 45,             # X軸の目盛りラベルを45度回転\n      showgrid = TRUE,            # X軸のグリッド線を表示\n      gridcolor = 'lightgray',    # グリッド線の色\n      zeroline = FALSE            # X軸のゼロラインを非表示\n    ),\n    yaxis = list(\n      title = \"Y軸\",              # Y軸のラベル\n      titlefont = list(size = 18),  # Y軸ラベルのフォントサイズ\n      showgrid = TRUE,            # Y軸のグリッド線を表示\n      gridcolor = 'lightgray',    # グリッド線の色\n      zeroline = FALSE            # Y軸のゼロラインを非表示\n    ),\n    showlegend = TRUE,             # 凡例を表示\n    legend = list(\n      title = 'カテゴリー',      # 凡例のタイトル\n      x = 0.8, y = 0.95,         # 凡例の位置（相対座標）\n      bgcolor = 'rgba(255, 255, 255, 0.5)',  # 凡例の背景色\n      bordercolor = 'black',     # 凡例のボーダー色\n      borderwidth = 1            # 凡例のボーダー幅\n    ),\n    plot_bgcolor = 'lightgray',   # プロット領域の背景色\n    paper_bgcolor = 'white',      # 全体の背景色\n    margin = list(l = 60, r = 60, t = 60, b = 60)  # マージンの調整\n  )\n\n# プロットの表示\np\n\n\n\n\n\n\n\n\nCode\nlibrary(plotly)\n\n# サンプルデータ\ncategories &lt;- c(\"A\", \"B\", \"C\", \"D\", \"E\")\nvalues &lt;- c(10, 14, 18, 24, 30)\n\n# 棒グラフの作成\np &lt;- plot_ly(\n  x = categories,\n  y = values,\n  type = 'bar',   # 棒グラフのタイプ\n  marker = list(\n    color = 'rgba(255, 99, 132, 0.6)',  # 棒の色\n    line = list(\n      color = 'rgba(255, 99, 132, 1)',   # 棒の外枠の色\n      width = 2                           # 外枠の幅\n    )\n  )\n) %&gt;%\n  layout(\n    title = list(text=\"棒グラフのカスタマイズ例\", size=40), # タイトル\n    xaxis = list(\n      title = \"カテゴリ\",              # X軸のラベル\n      titlefont = list(size = 18),      # X軸ラベルのフォントサイズ\n      tickangle = 45,                   # X軸の目盛りラベルを45度回転\n      showgrid = TRUE,                  # X軸のグリッド線を表示\n      gridcolor = 'lightgray'           # グリッド線の色\n    ),\n    yaxis = list(\n      title = \"値\",                    # Y軸のラベル\n      titlefont = list(size = 18),      # Y軸ラベルのフォントサイズ\n      showgrid = TRUE,                  # Y軸のグリッド線を表示\n      gridcolor = 'lightgray'           # グリッド線の色\n    ),\n    showlegend = FALSE,                 # 凡例は表示しない\n    plot_bgcolor = 'lightgray',         # プロット領域の背景色\n    paper_bgcolor = 'white',            # 全体の背景色\n    margin = list(l = 60, r = 60, t = 60, b = 60)  # マージンの調整\n  )\n\n# プロットの表示\np",
    "crumbs": [
      "R",
      "plotly",
      "02 Examples"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/02_examples.html#横棒グラフ",
    "href": "contents/libs/R/plotly/02_examples.html#横棒グラフ",
    "title": "02 Examples",
    "section": "1.3 横棒グラフ",
    "text": "1.3 横棒グラフ\n\n\nCode\n# 必要なライブラリをインストール＆読み込み\n# install.packages(\"plotly\") # 初回のみ必要\nlibrary(plotly)\n\n# データの定義\ndata &lt;- data.frame(\n  Category = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n  Value = c(30, 20, 50, 40, 60)\n)\n\n# 横棒グラフの作成\nplot &lt;- plot_ly(\n  data = data,\n  x = ~Value,                # 横軸のデータ\n  y = ~Category,             # 縦軸のデータ\n  type = 'bar',              # 棒グラフを指定\n  orientation = 'h',         # 横棒グラフにする\n  marker = list(color = 'rgba(255,99,71,0.6)') # 色の設定\n) %&gt;%\n  # ツールチップとホバー情報のカスタマイズ\n  add_trace(\n    text = ~paste(\"カテゴリ:\", Category, \"&lt;br&gt;値:\", Value),\n    hoverinfo = \"text\"\n  ) %&gt;%\n  # レイアウトの設定\n  layout(\n    title = \"横棒グラフの例\",             # グラフのタイトル\n    xaxis = list(title = \"値\"),          # 横軸のラベル\n    yaxis = list(title = \"カテゴリ\"),    # 縦軸のラベル\n    margin = list(l = 100, t = 100, b = 100),              # 左のマージンを調整\n    barmode = 'stack',                   # スタック表示（必要に応じて変更可能）\n    showlegend = TRUE                    # 凡例を表示\n  ) %&gt;%\n  # アニメーション設定（必要に応じて）\n  animation_opts(\n    frame = 100, \n    easing = 'linear', \n    redraw = FALSE\n  )\n\n# プロットを表示\nplot",
    "crumbs": [
      "R",
      "plotly",
      "02 Examples"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/02_examples.html#色を変える棒グラフ",
    "href": "contents/libs/R/plotly/02_examples.html#色を変える棒グラフ",
    "title": "02 Examples",
    "section": "1.4 色を変える棒グラフ",
    "text": "1.4 色を変える棒グラフ\n\n\nCode\nlibrary(plotly)\n\ndf &lt;- data.frame(\n  Category = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n  Value = c(30, 20, 50, 40, 60)\n)\n\n# \"C\"だけを別トレースとして分ける\ndf_highlight &lt;- subset(df, Category == \"C\")\ndf_others    &lt;- subset(df, Category != \"C\")\n\nplot_ly() %&gt;%\n  # Cの部分だけのトレース\n  add_trace(\n    data = df_highlight,\n    x = ~Category,\n    y = ~Value,\n    type = \"bar\",\n    marker = list(color = \"red\"),\n    name = \"特別な棒\"\n  ) %&gt;%\n  # それ以外の部分のトレース\n  add_trace(\n    data = df_others,\n    x = ~Category,\n    y = ~Value,\n    type = \"bar\",\n    marker = list(color = \"blue\"),\n    name = \"その他の棒\"\n  ) %&gt;%\n  layout(\n    title = \"複数トレースで色を変える例\",\n    xaxis = list(title = \"Category\"),\n    yaxis = list(title = \"Value\"),\n    barmode = \"group\"  # 棒を横並びに表示（必要に応じて \"stack\" など）\n  )",
    "crumbs": [
      "R",
      "plotly",
      "02 Examples"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/02_examples.html#ヒートマップ",
    "href": "contents/libs/R/plotly/02_examples.html#ヒートマップ",
    "title": "02 Examples",
    "section": "1.5 ヒートマップ",
    "text": "1.5 ヒートマップ\n\n\nCode\n# ライブラリを読み込み（必要に応じてインストール）\n# install.packages(\"plotly\")\nlibrary(plotly)\n\n# サンプルとなる行列データを作成\nset.seed(123)  # 乱数シードで再現性を確保\ndata_matrix &lt;- matrix(\n  rnorm(100, mean = 5, sd = 2),  # 平均5、標準偏差2の正規乱数を100個生成\n  nrow = 10,                     # 行数\n  ncol = 10                      # 列数\n)\n\n# 行名と列名を設定\nrownames(data_matrix) &lt;- paste0(\"Row\", 1:10)\ncolnames(data_matrix) &lt;- paste0(\"Col\", 1:10)\n\n# ヒートマップの作成\np &lt;- plot_ly(\n  x = colnames(data_matrix),       # x軸ラベルに列名を使用\n  y = rownames(data_matrix),       # y軸ラベルに行名を使用\n  z = data_matrix,                 # ヒートマップの値\n  type = \"heatmap\",                # ヒートマップを指定\n  colorscale = \"Viridis\",          # カラースケール（\"Blues\", \"Reds\"なども可）\n  hovertemplate = paste0(\n    \"行: %{y}&lt;br&gt;\",\n    \"列: %{x}&lt;br&gt;\",\n    \"値: %{z:.2f}&lt;extra&gt;&lt;/extra&gt;\"\n  )\n  # hovertemplate で表示内容を自由にカスタマイズ\n  # %{z:.2f} として小数点以下2桁まで表示\n) %&gt;%\n  # カラーバー（凡例）の設定\n  colorbar(\n    title = \"値\",  # カラーバーのタイトル\n    len = 0.8,     # カラーバーの長さ（全体比）\n    x = 1.05       # カラーバーをグラフの右側にオフセット\n  ) %&gt;%\n  # レイアウトの設定\n  layout(\n    title = \"ヒートマップの例\",\n    xaxis = list(\n      title = \"列\",\n      tickangle = 45  # x軸ラベルを45度回転\n    ),\n    yaxis = list(\n      title = \"行\",\n      autorange = \"reversed\"  # 通常のイメージに合わせ、y軸を上から下へ\n    ),\n    margin = list(\n      b = 100,  # 下マージン\n      l = 100,  # 左マージン\n      t = 50,   # 上マージン\n      r = 120   # 右マージン（カラーバーと被らないように調整）\n    ),\n    plot_bgcolor = \"#f0f0f0\"  # 背景色の設定, \n    ,   hoverlabel = list(\n      bgcolor = \"white\",           # 背景色\n      bordercolor = \"black\",       # 枠線の色\n      font = list(size = 36),      # フォント設定\n      align = \"left\",              # テキストの配置（\"left\", \"right\", \"auto\"）\n      namelength = -1,             # Hover時のトレース名省略をしない\n      pad = list(l = 124, r = 10, t = 15, b = 15)  # パディングを設定\n    )\n  )\n\n# グラフを表示\np",
    "crumbs": [
      "R",
      "plotly",
      "02 Examples"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/02_examples.html#世界地図",
    "href": "contents/libs/R/plotly/02_examples.html#世界地図",
    "title": "02 Examples",
    "section": "1.6 世界地図",
    "text": "1.6 世界地図\n\n\nCode\n# 必要に応じてインストール\n# install.packages(\"plotly\")\nlibrary(plotly)\n\n# サンプルデータ: 日本の主な都市と人口（ざっくりした数字）\ndf &lt;- data.frame(\n  city       = c(\"Tokyo\", \"Osaka\", \"Kyoto\", \"Sapporo\", \"Fukuoka\"),\n  latitude   = c(35.6895, 34.6937, 35.0116, 43.0642, 33.5904),\n  longitude  = c(139.6917, 135.5022, 135.7681, 141.3468, 130.4017),\n  population = c(14, 2.7, 1.5, 1.9, 1.6)  # 単位: 百万人\n)\n\nfig &lt;- plot_ly() %&gt;%\n  add_trace(\n    type = \"scattergeo\",\n    mode = \"markers\",\n    lat = ~df$latitude,\n    lon = ~df$longitude,\n    text = ~paste0(\n      df$city,\n      \"&lt;br&gt;人口: \",\n      df$population, \" 百万人\"\n    ),\n    hoverinfo = \"text\",\n    marker = list(\n      size = ~df$population * 5,  # 人口に応じた点の大きさ\n      color = \"red\",\n      line = list(width = 1, color = \"black\")\n    )\n  ) %&gt;%\n  layout(\n    title = \"日本主要都市の人口\",\n    geo = list(\n      scope = \"world\",         # \"asia\" を指定するとアジア中心など範囲を限定可\n      resolution = 50,         # 地図の解像度\n      showland = TRUE,         # 陸地の表示\n      landcolor = \"rgb(243, 243, 243)\",\n      countrycolor = \"rgb(204, 204, 204)\"\n    )\n  )\n\nfig",
    "crumbs": [
      "R",
      "plotly",
      "02 Examples"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/02_examples.html#コロプレス",
    "href": "contents/libs/R/plotly/02_examples.html#コロプレス",
    "title": "02 Examples",
    "section": "1.7 コロプレス",
    "text": "1.7 コロプレス\n\n\nCode\nlibrary(plotly)\n\nfig &lt;- plot_ly(\n  type = \"choropleth\",\n  locations = c(\"Japan\", \"China\", \"United States\"),\n  locationmode = \"country names\",  # \"ISO-3\" など別の指定も可\n  z = c(126, 1400, 331),           # 各国に対応する数値（人口など）\n  text = c(\"日本\", \"中国\", \"アメリカ\"),   # ホバー時に表示するテキスト\n  colorscale = \"Reds\",\n  marker = list(line = list(color = \"gray\", width = 0.5))\n) %&gt;%\n  layout(\n    title = \"世界の人口の一例\",\n    geo = list(\n      showframe = FALSE,\n      showcoastlines = TRUE\n    )\n  )\n\nfig",
    "crumbs": [
      "R",
      "plotly",
      "02 Examples"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/02_examples.html#ggplotly",
    "href": "contents/libs/R/plotly/02_examples.html#ggplotly",
    "title": "02 Examples",
    "section": "1.8 ggplotly",
    "text": "1.8 ggplotly\n\n\nCode\n# install.packages(\"sf\")\n# install.packages(\"ggplot2\")\n# install.packages(\"plotly\")\nlibrary(sf)\n#&gt; Warning: package 'sf' was built under R version 4.3.3\n#&gt; Linking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE\nlibrary(ggplot2)\nlibrary(plotly)\n\nnc &lt;- st_read(system.file(\"shape/nc.shp\", package = \"sf\"))\n#&gt; Reading layer `nc' from data source \n#&gt;   `C:\\Users\\suzuk\\AppData\\Local\\R\\cache\\R\\renv\\cache\\v5\\R-4.3\\x86_64-w64-mingw32\\sf\\1.0-17\\453a7d0263eae87a7831242a74fe9b60\\sf\\shape\\nc.shp' \n#&gt;   using driver `ESRI Shapefile'\n#&gt; Simple feature collection with 100 features and 14 fields\n#&gt; Geometry type: MULTIPOLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n#&gt; Geodetic CRS:  NAD27\n\n# ggplot2で描画\np &lt;- ggplot(nc) +\n  geom_sf(aes(fill = BIR74), color = \"white\") +\n  scale_fill_viridis_c(option = \"viridis\") +\n  labs(title = \"ノースカロライナ州の郡別データ (BIR74)\")\n\n# plotlyでインタラクティブ化\nggplotly(p)",
    "crumbs": [
      "R",
      "plotly",
      "02 Examples"
    ]
  },
  {
    "objectID": "contents/libs/R/plotly/02_examples.html#subplot",
    "href": "contents/libs/R/plotly/02_examples.html#subplot",
    "title": "02 Examples",
    "section": "1.9 subplot",
    "text": "1.9 subplot\n\n\nCode\n# 必要に応じてインストール\n# install.packages(\"plotly\")\nlibrary(plotly)\n\n# 1. データ準備\nset.seed(123)\nn &lt;- 200\nx &lt;- rnorm(n, mean = 50, sd = 10)\ny &lt;- rnorm(n, mean = 30, sd =  5)\n\n# 2. 中央の散布図\np_scatter &lt;- plot_ly() %&gt;%\n  add_markers(\n    x = ~x,\n    y = ~y,\n    marker = list(color = \"blue\", opacity = 0.6),\n    name = \"Scatter\"\n  ) %&gt;%\n  layout(\n    xaxis = list(showgrid = FALSE),\n    yaxis = list(showgrid = FALSE)\n  )\n\n# 3. 上側のヒストグラム（x軸の分布）\np_xhist &lt;- plot_ly() %&gt;%\n  add_histogram(\n    x = ~x,\n    marker = list(color = \"blue\", opacity = 0.6),\n    showlegend = FALSE\n  ) %&gt;%\n  layout(\n    yaxis = list(showticklabels = FALSE),  # y軸目盛りラベルを消す\n    margin = list(b = 20)                  # 下マージンを少し空ける\n  )\n\n# 4. 左側のヒストグラム（y軸の分布）\np_yhist &lt;- plot_ly() %&gt;%\n  add_histogram(\n    y = ~y,\n    marker = list(color = \"blue\", opacity = 0.6),\n    showlegend = FALSE\n  ) %&gt;%\n  layout(\n    xaxis = list(showticklabels = FALSE),  # x軸目盛りラベルを消す\n    margin = list(l = 20)                  # 左マージンを少し空ける\n  )\n\np_empty &lt;- plotly_empty(type = \"scatter\", mode = \"markers\") %&gt;%\n  layout(\n    xaxis = list(showgrid = FALSE, zeroline = FALSE, visible = FALSE),\n    yaxis = list(showgrid = FALSE, zeroline = FALSE, visible = FALSE)\n  )\n\n# 5. subplot() でレイアウトを組み合わせる\n#    2×2グリッド: 上左(xヒスト), 上右(空), 下左(yヒスト), 下右(散布図)\nfig &lt;- subplot(\n  p_xhist,      p_empty,\n   p_scatter, p_yhist,\n  nrows = 2,\n  shareX = FALSE,    # x軸を共有\n  shareY = FALSE,    # y軸を共有\n  widths = c(0.7, 0.3),   # 2列の幅の比率\n  heights = c(0.3, 0.7)   # 2行の高さの比率\n)\n\n# 6. 追加レイアウト調整（マージンや軸ラベルなど）\nfig &lt;- fig %&gt;%\n  layout(\n    showlegend = FALSE,\n    margin = list(l = 50, r = 50, t = 30, b = 50),\n    # 軸の domain を指定すると、位置の微調整ができる\n    xaxis = list(domain = c(0.0, 0.7)),    # 下段左領域\n    xaxis2 = list(domain = c(0.0, 0.7)),   # 上段左領域 (p_xhist)\n    yaxis = list(domain = c(0.3, 1.0)),    # 上段\n    yaxis2 = list(domain = c(0.0, 0.3), side = \"right\"), # 下段\n    xaxis2 = list(title = \"X values\"),\n    yaxis2 = list(title = \"Y values\")\n  )\n\n# 7. プロットを表示\nfig",
    "crumbs": [
      "R",
      "plotly",
      "02 Examples"
    ]
  },
  {
    "objectID": "contents/libs/R/pointblank/cheetsheet.html",
    "href": "contents/libs/R/pointblank/cheetsheet.html",
    "title": "pointblank",
    "section": "",
    "text": "Code\n\nglimpse(penguins)\n#&gt; Rows: 344\n#&gt; Columns: 8\n#&gt; $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n#&gt; $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n#&gt; $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n#&gt; $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n#&gt; $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n#&gt; $ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n#&gt; $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\n\n\n\nCode\n\nagent &lt;- create_agent(tbl = ~ penguins, tbl_name = \"penguins\")\n\n\n\n\n\n\n\nCode\nagent &lt;- \n\n  agent |&gt;\n  \n  # 値がある\n  col_exists(columns = c(bill_length_mm)) |&gt; \n\n  # 数値型\n  col_is_numeric(columns = vars(bill_length_mm)) |&gt; \n\n  # NULLを含まない\n  col_vals_not_null(columns = vars(species)) |&gt;\n\n  # 特定の値の範囲\n  col_vals_between(columns = vars(bill_length_mm), left = 30, right = 60) |&gt;\n\n  # 特定の水準の範囲\n  col_vals_in_set(columns = vars(island), set = c(\"Biscoe\", \"Dream\", \"Torgersen\")) |&gt;\n\n  # ある値より大きい\n  col_vals_gte(columns = vars(body_mass_g), 0) |&gt; \n\n  # 条件付きバリデーション\n  col_vals_gte(\n    columns = vars(bill_length_mm), \n    value = 35, \n    preconditions = ~ . %&gt;% filter(species == \"Adelie\")\n\n  )\n\n\n\n\n\n\n\nCode\niagent &lt;- \n  agent |&gt; \n  interrogate()\n\n\n\n\nCode\niagent\n\n\n\n\n\n\n\n\nPointblank Validation\n\n\n\n\n[2025-07-26|17:43:42]\n\n\ntibble penguins\n\n\n\n\n\n\nSTEP\nCOLUMNS\nVALUES\nTBL\nEVAL\nUNITS\nPASS\nFAIL\nW\nS\nN\nEXT\n\n\n\n\n\n\n1\n\n\n\n\ncol_exists\n\n      \n\n\n col_exists()\n\n\n▮bill_length_mm\n\n—\n\n\n      \n\n\n✓\n1\n1\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n2\n\n\n\n\ncol_is_numeric\n\n      \n\n\n col_is_numeric()\n\n\n▮bill_length_mm\n\n—\n\n\n      \n\n\n✓\n1\n1\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n3\n\n\n\n\ncol_vals_not_null\n\n       \n\n\n col_vals_not_null()\n\n\n▮species\n\n—\n\n\n      \n\n\n✓\n344\n344\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n4\n\n\n\n\ncol_vals_between\n\n     \n\n\n col_vals_between()\n\n\n▮bill_length_mm\n\n\n[30, 60]\n\n\n\n      \n\n\n✓\n344\n342\n0.99\n2\n0.01\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n\n5\n\n\n\n\ncol_vals_in_set\n\n     \n\n\n col_vals_in_set()\n\n\n▮island\n\n\nBiscoe, Dream, Torgersen\n\n\n\n      \n\n\n✓\n344\n344\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n6\n\n\n\n\ncol_vals_gte\n\n     \n\n\n col_vals_gte()\n\n\n▮body_mass_g\n\n\n0\n\n\n\n      \n\n\n✓\n344\n342\n0.99\n2\n0.01\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n\n7\n\n\n\n\ncol_vals_gte\n\n     \n\n\n col_vals_gte()\n\n\n▮bill_length_mm\n\n\n35\n\n\n\n        \n\n\n✓\n152\n142\n0.93421\n10\n0.06579\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n2025-07-26 17:43:43 JST &lt; 1 s 2025-07-26 17:43:43 JST\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nyaml_write(agent, filename = file.path(cur_dir, \"penguins_validation_v1.yaml\"))\n#&gt; ✔ The agent YAML file has been written to\n#&gt; `C:/Users/suzuk/Dropbox/R/Workspace/RTipsSite/contents/libs/R/pointblank/penguins_validation_v1.yaml`\n\n\n\n\nCode\nfile.path(cur_dir, \"penguins_validation_v1.yaml\") |&gt; \n  yaml_read_agent() |&gt; \n  interrogate()\n\n\n\n\n\n\n\n\nPointblank Validation\n\n\n\n\n[2025-07-26|17:43:42]\n\n\ntibble penguins\n\n\n\n\n\n\nSTEP\nCOLUMNS\nVALUES\nTBL\nEVAL\nUNITS\nPASS\nFAIL\nW\nS\nN\nEXT\n\n\n\n\n\n\n1\n\n\n\n\ncol_exists\n\n      \n\n\n col_exists()\n\n\n▮bill_length_mm\n\n—\n\n\n      \n\n\n✓\n1\n1\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n2\n\n\n\n\ncol_is_numeric\n\n      \n\n\n col_is_numeric()\n\n\n▮bill_length_mm\n\n—\n\n\n      \n\n\n✓\n1\n1\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n3\n\n\n\n\ncol_vals_not_null\n\n       \n\n\n col_vals_not_null()\n\n\n▮species\n\n—\n\n\n      \n\n\n✓\n344\n344\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n4\n\n\n\n\ncol_vals_between\n\n     \n\n\n col_vals_between()\n\n\n▮bill_length_mm\n\n\n[30, 60]\n\n\n\n      \n\n\n✓\n344\n342\n0.99\n2\n0.01\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n\n5\n\n\n\n\ncol_vals_in_set\n\n     \n\n\n col_vals_in_set()\n\n\n▮island\n\n\nBiscoe, Dream, Torgersen\n\n\n\n      \n\n\n✓\n344\n344\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n6\n\n\n\n\ncol_vals_gte\n\n     \n\n\n col_vals_gte()\n\n\n▮body_mass_g\n\n\n0\n\n\n\n      \n\n\n✓\n344\n342\n0.99\n2\n0.01\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n\n7\n\n\n\n\ncol_vals_gte\n\n     \n\n\n col_vals_gte()\n\n\n▮bill_length_mm\n\n\n35\n\n\n\n        \n\n\n✓\n152\n142\n0.93421\n10\n0.06579\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n2025-07-26 17:43:44 JST &lt; 1 s 2025-07-26 17:43:44 JST",
    "crumbs": [
      "R",
      "pointblank",
      "pointblank"
    ]
  },
  {
    "objectID": "contents/libs/R/pointblank/cheetsheet.html#overview",
    "href": "contents/libs/R/pointblank/cheetsheet.html#overview",
    "title": "pointblank",
    "section": "",
    "text": "Code\n\nglimpse(penguins)\n#&gt; Rows: 344\n#&gt; Columns: 8\n#&gt; $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n#&gt; $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n#&gt; $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n#&gt; $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n#&gt; $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n#&gt; $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n#&gt; $ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n#&gt; $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…",
    "crumbs": [
      "R",
      "pointblank",
      "pointblank"
    ]
  },
  {
    "objectID": "contents/libs/R/pointblank/cheetsheet.html#ステップ1検証エージェントを作成",
    "href": "contents/libs/R/pointblank/cheetsheet.html#ステップ1検証エージェントを作成",
    "title": "pointblank",
    "section": "",
    "text": "Code\n\nagent &lt;- create_agent(tbl = ~ penguins, tbl_name = \"penguins\")",
    "crumbs": [
      "R",
      "pointblank",
      "pointblank"
    ]
  },
  {
    "objectID": "contents/libs/R/pointblank/cheetsheet.html#ステップ2検証ルールの作成",
    "href": "contents/libs/R/pointblank/cheetsheet.html#ステップ2検証ルールの作成",
    "title": "pointblank",
    "section": "",
    "text": "Code\nagent &lt;- \n\n  agent |&gt;\n  \n  # 値がある\n  col_exists(columns = c(bill_length_mm)) |&gt; \n\n  # 数値型\n  col_is_numeric(columns = vars(bill_length_mm)) |&gt; \n\n  # NULLを含まない\n  col_vals_not_null(columns = vars(species)) |&gt;\n\n  # 特定の値の範囲\n  col_vals_between(columns = vars(bill_length_mm), left = 30, right = 60) |&gt;\n\n  # 特定の水準の範囲\n  col_vals_in_set(columns = vars(island), set = c(\"Biscoe\", \"Dream\", \"Torgersen\")) |&gt;\n\n  # ある値より大きい\n  col_vals_gte(columns = vars(body_mass_g), 0) |&gt; \n\n  # 条件付きバリデーション\n  col_vals_gte(\n    columns = vars(bill_length_mm), \n    value = 35, \n    preconditions = ~ . %&gt;% filter(species == \"Adelie\")\n\n  )",
    "crumbs": [
      "R",
      "pointblank",
      "pointblank"
    ]
  },
  {
    "objectID": "contents/libs/R/pointblank/cheetsheet.html#ステップ3検証の実行",
    "href": "contents/libs/R/pointblank/cheetsheet.html#ステップ3検証の実行",
    "title": "pointblank",
    "section": "",
    "text": "Code\niagent &lt;- \n  agent |&gt; \n  interrogate()\n\n\n\n\nCode\niagent\n\n\n\n\n\n\n\n\nPointblank Validation\n\n\n\n\n[2025-07-26|17:43:42]\n\n\ntibble penguins\n\n\n\n\n\n\nSTEP\nCOLUMNS\nVALUES\nTBL\nEVAL\nUNITS\nPASS\nFAIL\nW\nS\nN\nEXT\n\n\n\n\n\n\n1\n\n\n\n\ncol_exists\n\n      \n\n\n col_exists()\n\n\n▮bill_length_mm\n\n—\n\n\n      \n\n\n✓\n1\n1\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n2\n\n\n\n\ncol_is_numeric\n\n      \n\n\n col_is_numeric()\n\n\n▮bill_length_mm\n\n—\n\n\n      \n\n\n✓\n1\n1\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n3\n\n\n\n\ncol_vals_not_null\n\n       \n\n\n col_vals_not_null()\n\n\n▮species\n\n—\n\n\n      \n\n\n✓\n344\n344\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n4\n\n\n\n\ncol_vals_between\n\n     \n\n\n col_vals_between()\n\n\n▮bill_length_mm\n\n\n[30, 60]\n\n\n\n      \n\n\n✓\n344\n342\n0.99\n2\n0.01\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n\n5\n\n\n\n\ncol_vals_in_set\n\n     \n\n\n col_vals_in_set()\n\n\n▮island\n\n\nBiscoe, Dream, Torgersen\n\n\n\n      \n\n\n✓\n344\n344\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n6\n\n\n\n\ncol_vals_gte\n\n     \n\n\n col_vals_gte()\n\n\n▮body_mass_g\n\n\n0\n\n\n\n      \n\n\n✓\n344\n342\n0.99\n2\n0.01\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n\n7\n\n\n\n\ncol_vals_gte\n\n     \n\n\n col_vals_gte()\n\n\n▮bill_length_mm\n\n\n35\n\n\n\n        \n\n\n✓\n152\n142\n0.93421\n10\n0.06579\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n2025-07-26 17:43:43 JST &lt; 1 s 2025-07-26 17:43:43 JST",
    "crumbs": [
      "R",
      "pointblank",
      "pointblank"
    ]
  },
  {
    "objectID": "contents/libs/R/pointblank/cheetsheet.html#ステップ4定義の書き出し",
    "href": "contents/libs/R/pointblank/cheetsheet.html#ステップ4定義の書き出し",
    "title": "pointblank",
    "section": "",
    "text": "Code\nyaml_write(agent, filename = file.path(cur_dir, \"penguins_validation_v1.yaml\"))\n#&gt; ✔ The agent YAML file has been written to\n#&gt; `C:/Users/suzuk/Dropbox/R/Workspace/RTipsSite/contents/libs/R/pointblank/penguins_validation_v1.yaml`\n\n\n\n\nCode\nfile.path(cur_dir, \"penguins_validation_v1.yaml\") |&gt; \n  yaml_read_agent() |&gt; \n  interrogate()\n\n\n\n\n\n\n\n\nPointblank Validation\n\n\n\n\n[2025-07-26|17:43:42]\n\n\ntibble penguins\n\n\n\n\n\n\nSTEP\nCOLUMNS\nVALUES\nTBL\nEVAL\nUNITS\nPASS\nFAIL\nW\nS\nN\nEXT\n\n\n\n\n\n\n1\n\n\n\n\ncol_exists\n\n      \n\n\n col_exists()\n\n\n▮bill_length_mm\n\n—\n\n\n      \n\n\n✓\n1\n1\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n2\n\n\n\n\ncol_is_numeric\n\n      \n\n\n col_is_numeric()\n\n\n▮bill_length_mm\n\n—\n\n\n      \n\n\n✓\n1\n1\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n3\n\n\n\n\ncol_vals_not_null\n\n       \n\n\n col_vals_not_null()\n\n\n▮species\n\n—\n\n\n      \n\n\n✓\n344\n344\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n4\n\n\n\n\ncol_vals_between\n\n     \n\n\n col_vals_between()\n\n\n▮bill_length_mm\n\n\n[30, 60]\n\n\n\n      \n\n\n✓\n344\n342\n0.99\n2\n0.01\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n\n5\n\n\n\n\ncol_vals_in_set\n\n     \n\n\n col_vals_in_set()\n\n\n▮island\n\n\nBiscoe, Dream, Torgersen\n\n\n\n      \n\n\n✓\n344\n344\n1\n0\n0\n—\n—\n—\n—\n\n\n\n\n6\n\n\n\n\ncol_vals_gte\n\n     \n\n\n col_vals_gte()\n\n\n▮body_mass_g\n\n\n0\n\n\n\n      \n\n\n✓\n344\n342\n0.99\n2\n0.01\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n\n7\n\n\n\n\ncol_vals_gte\n\n     \n\n\n col_vals_gte()\n\n\n▮bill_length_mm\n\n\n35\n\n\n\n        \n\n\n✓\n152\n142\n0.93421\n10\n0.06579\n—\n—\n—\n\n\nCSV\n\n\n\n\n\n2025-07-26 17:43:44 JST &lt; 1 s 2025-07-26 17:43:44 JST",
    "crumbs": [
      "R",
      "pointblank",
      "pointblank"
    ]
  },
  {
    "objectID": "contents/libs/R/purrr/cheetsheet.html",
    "href": "contents/libs/R/purrr/cheetsheet.html",
    "title": "cheetsheet",
    "section": "",
    "text": "blog\n\n\n\n\nmap処理をマルチコアや分散システムにより拡張することができる\nバージョンは1.1.0以降が必要である\nin_parallel関数を使うことで、mapの処理を並列化することができる\nmirai::deamonでコア数を調整する\n\n\n\nCode\npackageVersion(\"purrr\")\n#&gt; [1] '1.1.0'\n\n\n\n\n\nまずは結果が同じになることを確認する。\n\n\nCode\nlibrary(purrr)\nlibrary(mirai)\n#&gt; Warning: パッケージ 'mirai' はバージョン 4.5.1 の R の下で造られました\n\n# Set up parallel processing (6 background processes)\ndaemons(6)\n#&gt; [1] 6\n\n# Sequential version\nx &lt;- mtcars |&gt; map_dbl(\\(x) mean(x))\n\n# paralle version\ny &lt;- mtcars |&gt; map_dbl(in_parallel(\\(x) mean(x)))\ndaemons(0)\n#&gt; [1] 0\n\n\nidentical(x, y)\n#&gt; [1] TRUE\n\n\n\n\n\nパラレル化したときには, オブジェクトは明示的に渡す必要があるし, ライブラリは明示的にロードする必要がある\n\n\nCode\nlibrary(purrr)\nlibrary(mirai)\n\n# Set up max parallel processes\ncores &lt;- as.integer(Sys.getenv(\"NUMBER_OF_PROCESSORS\"))\ndaemons(cores)\n#&gt; [1] 12\n\n# Define a slow model fitting function\nslow_lm &lt;- function(formula, data) {\n  Sys.sleep(5)  # Simulate computational complexity\n  lm(formula, data)\n}\n\n# Fit models to different subsets of data in parallel\nmodels &lt;- mtcars |&gt;\n  split(mtcars$cyl) |&gt;\n  # printは効かない\n  imap(in_parallel(\\(df, idx) {print(idx); slow_lm(mpg ~ wt + hp, data = df)}, slow_lm = slow_lm))\n\n# Extract R-squared values\nmodels |&gt; \n  map(summary) |&gt; \n  map_dbl(\"r.squared\")\n#&gt;         4         6         8 \n#&gt; 0.6807065 0.5889239 0.4970692\n#&gt;         4         6         8 \n#&gt; 0.6807065 0.5889239 0.4970692\n\ndaemons(0)\n#&gt; [1] 0\n\n\n\n\n\n\nパラレルが有効であるとき\n1イテレーションに, 100マイクロ秒から1ミリ秒以上かかる\nCPU集約的な計算を行っている\nI/Oバウンドな操作で, 並列性の恩恵を受けることができる\nプロセス間で渡されるデータが過度に大きくない\nまた, deamonsはモジュールなどで呼び出さず, 必ずユーザーに呼び出させる"
  },
  {
    "objectID": "contents/libs/R/purrr/cheetsheet.html#はじめに",
    "href": "contents/libs/R/purrr/cheetsheet.html#はじめに",
    "title": "cheetsheet",
    "section": "",
    "text": "map処理をマルチコアや分散システムにより拡張することができる\nバージョンは1.1.0以降が必要である\nin_parallel関数を使うことで、mapの処理を並列化することができる\nmirai::deamonでコア数を調整する\n\n\n\nCode\npackageVersion(\"purrr\")\n#&gt; [1] '1.1.0'"
  },
  {
    "objectID": "contents/libs/R/purrr/cheetsheet.html#the-power-of-in_parallel",
    "href": "contents/libs/R/purrr/cheetsheet.html#the-power-of-in_parallel",
    "title": "cheetsheet",
    "section": "",
    "text": "まずは結果が同じになることを確認する。\n\n\nCode\nlibrary(purrr)\nlibrary(mirai)\n#&gt; Warning: パッケージ 'mirai' はバージョン 4.5.1 の R の下で造られました\n\n# Set up parallel processing (6 background processes)\ndaemons(6)\n#&gt; [1] 6\n\n# Sequential version\nx &lt;- mtcars |&gt; map_dbl(\\(x) mean(x))\n\n# paralle version\ny &lt;- mtcars |&gt; map_dbl(in_parallel(\\(x) mean(x)))\ndaemons(0)\n#&gt; [1] 0\n\n\nidentical(x, y)\n#&gt; [1] TRUE"
  },
  {
    "objectID": "contents/libs/R/purrr/cheetsheet.html#real-world-example-parallel-model-fitting",
    "href": "contents/libs/R/purrr/cheetsheet.html#real-world-example-parallel-model-fitting",
    "title": "cheetsheet",
    "section": "",
    "text": "パラレル化したときには, オブジェクトは明示的に渡す必要があるし, ライブラリは明示的にロードする必要がある\n\n\nCode\nlibrary(purrr)\nlibrary(mirai)\n\n# Set up max parallel processes\ncores &lt;- as.integer(Sys.getenv(\"NUMBER_OF_PROCESSORS\"))\ndaemons(cores)\n#&gt; [1] 12\n\n# Define a slow model fitting function\nslow_lm &lt;- function(formula, data) {\n  Sys.sleep(5)  # Simulate computational complexity\n  lm(formula, data)\n}\n\n# Fit models to different subsets of data in parallel\nmodels &lt;- mtcars |&gt;\n  split(mtcars$cyl) |&gt;\n  # printは効かない\n  imap(in_parallel(\\(df, idx) {print(idx); slow_lm(mpg ~ wt + hp, data = df)}, slow_lm = slow_lm))\n\n# Extract R-squared values\nmodels |&gt; \n  map(summary) |&gt; \n  map_dbl(\"r.squared\")\n#&gt;         4         6         8 \n#&gt; 0.6807065 0.5889239 0.4970692\n#&gt;         4         6         8 \n#&gt; 0.6807065 0.5889239 0.4970692\n\ndaemons(0)\n#&gt; [1] 0"
  },
  {
    "objectID": "contents/libs/R/purrr/cheetsheet.html#when-to-use-parallel-processing",
    "href": "contents/libs/R/purrr/cheetsheet.html#when-to-use-parallel-processing",
    "title": "cheetsheet",
    "section": "",
    "text": "パラレルが有効であるとき\n1イテレーションに, 100マイクロ秒から1ミリ秒以上かかる\nCPU集約的な計算を行っている\nI/Oバウンドな操作で, 並列性の恩恵を受けることができる\nプロセス間で渡されるデータが過度に大きくない\nまた, deamonsはモジュールなどで呼び出さず, 必ずユーザーに呼び出させる"
  },
  {
    "objectID": "contents/libs/R/sf/working.html",
    "href": "contents/libs/R/sf/working.html",
    "title": "sf",
    "section": "",
    "text": "Code\ncur_dir     &lt;- here::here(\"contents/libs/R/sf\")\ndataset_dir &lt;- here::here(cur_dir, \"dataset\")\noutput_dir  &lt;- here::here(cur_dir, \"output\")",
    "crumbs": [
      "R",
      "sf",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/sf/working.html#idw",
    "href": "contents/libs/R/sf/working.html#idw",
    "title": "sf",
    "section": "2.1 IDW",
    "text": "2.1 IDW\nデータを作成する。\n\n\nCode\nlocations &lt;- \n  tibble(\n    longitude = c(10, 10.5, 11),\n    latitude = c(10, 14.5, 18),\n    measurement = c(100, 150, 120)\n  ) |&gt;\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\nregion &lt;- st_as_sf(data.frame(\n  x = c(10, 10, 20, 20),\n  y = c(10, 20, 20, 10)\n), coords = c(\"x\", \"y\"), crs = 4326)\n\n\n逆距離補間をおこなう。\n\n\nCode\ngrid &lt;- st_make_grid(region, what = \"centers\")\nidw_result &lt;- idw(formula = measurement ~ 1, locations, grid)\n#&gt; [inverse distance weighted interpolation]\nidw_result\n#&gt; Simple feature collection with 100 features and 2 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 10.5 ymin: 10.5 xmax: 19.5 ymax: 19.5\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    var1.pred var1.var          geometry\n#&gt; 1   101.6566       NA POINT (10.5 10.5)\n#&gt; 2   106.8246       NA POINT (11.5 10.5)\n#&gt; 3   112.7551       NA POINT (12.5 10.5)\n#&gt; 4   116.9918       NA POINT (13.5 10.5)\n#&gt; 5   119.5808       NA POINT (14.5 10.5)\n#&gt; 6   121.1055       NA POINT (15.5 10.5)\n#&gt; 7   122.0098       NA POINT (16.5 10.5)\n#&gt; 8   122.5585       NA POINT (17.5 10.5)\n#&gt; 9   122.9007       NA POINT (18.5 10.5)\n#&gt; 10  123.1201       NA POINT (19.5 10.5)\n\n\n\n\nCode\nplot(\n  st_make_grid(\n    region, \n    cellsize = 1,\n    what = \"polygons\")\n)\nplot(select(idw_result, var1.pred), add = TRUE)\nplot(locations, add = TRUE, col = \"red\", cex = 3, pch = 19)\n\n\n\n\n\n\n\n\n\nラスタ化する.\n\n\nCode\nplot(\n  rasterize(\n    idw_result, \n    rast(\n      extent = st_bbox(region), \n      resolution = 1\n    ),\n    field = \"var1.pred\"\n  )\n)\n\n\n\n\n\n\n\n\n\n距離の次数を変更してみる。\n\n\nCode\n# IDW補間の実行、距離のべき乗を3に設定\nidw_result_power3 &lt;- idw(formula = measurement ~ 1, locations, grid, idp = 3)\n#&gt; [inverse distance weighted interpolation]\nidw_result_power3\n#&gt; Simple feature collection with 100 features and 2 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 10.5 ymin: 10.5 xmax: 19.5 ymax: 19.5\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 10 features:\n#&gt;    var1.pred var1.var          geometry\n#&gt; 1   100.2868       NA POINT (10.5 10.5)\n#&gt; 2   102.7593       NA POINT (11.5 10.5)\n#&gt; 3   108.0653       NA POINT (12.5 10.5)\n#&gt; 4   113.3959       NA POINT (13.5 10.5)\n#&gt; 5   117.2178       NA POINT (14.5 10.5)\n#&gt; 6   119.6376       NA POINT (15.5 10.5)\n#&gt; 7   121.1204       NA POINT (16.5 10.5)\n#&gt; 8   122.0318       NA POINT (17.5 10.5)\n#&gt; 9   122.6014       NA POINT (18.5 10.5)\n#&gt; 10  122.9651       NA POINT (19.5 10.5)",
    "crumbs": [
      "R",
      "sf",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/sf/working.html#st_rasterize",
    "href": "contents/libs/R/sf/working.html#st_rasterize",
    "title": "sf",
    "section": "3.1 st_rasterize",
    "text": "3.1 st_rasterize\nstarsオブジェクトなどをラスターのテンプレートとして、空間データをラスター化する関数です。 leafletでみると少しズレているように見えるけど、QGISでみるとズレていない。\n\n\nCode\npath_to_polygon &lt;- here(dataset_dir, \"meshes_bursa2320.geojson\")\npolygon &lt;- st_read(path_to_polygon)\n#&gt; Reading layer `meshes_bursa2320' from data source \n#&gt;   `C:\\Users\\suzuk\\Dropbox\\R\\Workspace\\RTipsSite\\contents\\libs\\sf\\dataset\\meshes_bursa2320.geojson' \n#&gt;   using driver `GeoJSON'\n#&gt; Simple feature collection with 174928 features and 0 fields\n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 337290.5 ymin: 4383603 xmax: 493290.5 ymax: 4497853\n#&gt; Projected CRS: ED50 / TM30\nst_write(st_centroid(polygon), here(output_dir, \"centroid.geojson\"), delete_dsn = TRUE)\n#&gt; Deleting source `C:/Users/suzuk/Dropbox/R/Workspace/RTipsSite/contents/libs/sf/output/centroid.geojson' using driver `GeoJSON'\n#&gt; Writing layer `centroid' to data source \n#&gt;   `C:/Users/suzuk/Dropbox/R/Workspace/RTipsSite/contents/libs/sf/output/centroid.geojson' using driver `GeoJSON'\n#&gt; Writing 174928 features with 0 fields and geometry type Point.\nraster_template &lt;- st_as_stars(\n  st_bbox(polygon), \n  dx = set_units(250, \"m\"), \n  dy = set_units(250, \"m\"))\nrasterized &lt;- st_rasterize(polygon, raster_template)\nrasterized |&gt; write_stars(here(output_dir, \"rasterized.tif\"))\nrasterized\n#&gt; stars object with 2 dimensions and 1 attribute\n#&gt; attribute(s):\n#&gt;     Min. 1st Qu.  Median     Mean  3rd Qu.   Max.\n#&gt; ID     0       0 32344.5 53652.55 103636.2 174928\n#&gt; dimension(s):\n#&gt;   from  to  offset delta      refsys point x/y\n#&gt; x    1 624  337291   250 ED50 / TM30 FALSE [x]\n#&gt; y    1 457 4497853  -250 ED50 / TM30 FALSE [y]\n\n\n\n\nCode\nxx  &lt;- st_transform(st_centroid(head(polygon, 1000)), 4326)\nlng &lt;- st_coordinates(xx)[1:1000, 1]\nlat &lt;- st_coordinates(xx)[1:1000, 2]\nleaflet(\n    # options = leafletOptions(crs = leafletCRS(crsClass = \"L.CRS.EPSG3857\"))\n  ) |&gt; \n  addTiles() |&gt;\n  addCircleMarkers(lng = lng, lat = lat, radius = .1, color = \"blue\") |&gt; \n  addPolygons(data = st_transform(head(polygon, 1000), 4326), color = \"red\") |&gt;  \n  addStarsImage(rasterized, colors = \"RdPu\", opacity = 0.9, project = TRUE)",
    "crumbs": [
      "R",
      "sf",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/sf/working.html#st_make_grid",
    "href": "contents/libs/R/sf/working.html#st_make_grid",
    "title": "sf",
    "section": "3.2 st_make_grid",
    "text": "3.2 st_make_grid\n\n空間データをグリッドに変換する関数です。\n以下のように、グリッドのセルの大きさを指定することができます。\n\n\n\nCode\n# 例として、ある地域のポリゴンを定義\nregion &lt;- st_as_sf(data.frame(\n  x = c(10, 10, 20, 20),\n  y = c(10, 20, 20, 10)\n), coords = c(\"x\", \"y\"), crs = 4326)\n\n\n\n\nCode\ngrid &lt;- st_make_grid(region, cellsize = 1, what = \"polygons\")\ngrid\n#&gt; Geometry set for 100 features \n#&gt; Geometry type: POLYGON\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 10 ymin: 10 xmax: 20 ymax: 20\n#&gt; Geodetic CRS:  WGS 84\n#&gt; First 5 geometries:\n#&gt; POLYGON ((10 10, 11 10, 11 11, 10 11, 10 10))\n#&gt; POLYGON ((11 10, 12 10, 12 11, 11 11, 11 10))\n#&gt; POLYGON ((12 10, 13 10, 13 11, 12 11, 12 10))\n#&gt; POLYGON ((13 10, 14 10, 14 11, 13 11, 13 10))\n#&gt; POLYGON ((14 10, 15 10, 15 11, 14 11, 14 10))\n\n\n\n\nCode\nplot(grid)\nplot(region, add = TRUE)\n# ポイントデータを作成することもできる\nplot(\n  st_make_grid(region, cellsize = 1, what = \"centers\"),\n  col = \"red\", \n  add = TRUE\n)\nplot(\n  st_make_grid(region, cellsize = 1, what = \"corners\"),\n  col = \"blue\", \n  add = TRUE\n)\n\n\n\n\n\n\n\n\n\nセルサイズはXYで変更することもできる。\n\n\nCode\nplot(grid)\nplot(\n  st_make_grid(region, cellsize = c(1, 2), what = \"centers\"),\n  col = \"red\", \n  add = TRUE\n)",
    "crumbs": [
      "R",
      "sf",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/tidyverse/02_dplyr.html",
    "href": "contents/libs/R/tidyverse/02_dplyr.html",
    "title": "02 dplyr",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/tidyverse\")\nCode\npackages &lt;- c(\n  \"tidyverse\", \n  \"magick\",\n  \"ggplot2\", \n  \"readr\", \n  \"tibble\", \n  \"tidyr\", \n  \"forcats\", \n  \"stringr\",\n  \"lubridate\", \n  \"here\", \n  \"systemfonts\", \n  \"magick\", \n  \"scales\", \n  \"grid\",\n  \"grDevices\", \n  \"colorspace\", \n  \"viridis\", \n  \"RColorBrewer\", \n  \"rcartocolor\",\n  \"scico\", \n  \"ggsci\", \n  \"ggthemes\", \n  \"nord\", \n  \"MetBrewer\", \n  \"ggrepel\",\n  \"ggforce\",\n  \"ggtext\", \n  \"ggfittext\",\n  \"ggdist\", \n  \"ggbeeswarm\", \n  \"gghalves\", \n  \"patchwork\", \n  \"palmerpenguins\", \n  \"rnaturalearth\", \n  \"sf\", \n  \"rmapshaper\", \n  \"devtools\", \n  \"extrafont\"\n) |&gt; lapply(\\(x) library(x, character.only = TRUE))\nlibrary(cowplot)\n\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "tidyverse",
      "02 dplyr"
    ]
  },
  {
    "objectID": "contents/libs/R/tidyverse/02_dplyr.html#メモ",
    "href": "contents/libs/R/tidyverse/02_dplyr.html#メモ",
    "title": "02 dplyr",
    "section": "1.1 メモ",
    "text": "1.1 メモ\n\n等式、不等式、ローリング、オーバラップなどがある\njoin_by()の中で四則演算などの計算はできない\n左テーブルの引数が式の左、右テーブルの引数が式の右になる\nx$, y$がそれぞれのテーブルを指す\n\n\n1.1.1 ローリングジョイン\n前向き、後ろ向きに近い値にジョインする方法である。\ncloset(x&gt;=y)のようにすることで、xの値がyの値に近いものをジョインすることができる。\n常に左テーブルがプライマリーのテーブルであるとする。\n\n\n1.1.2 オーバラップジョイン\nbetweenは協会をどうするのかも決めることが出来る。",
    "crumbs": [
      "R",
      "tidyverse",
      "02 dplyr"
    ]
  },
  {
    "objectID": "contents/libs/R/tidyverse/02_dplyr.html#examples",
    "href": "contents/libs/R/tidyverse/02_dplyr.html#examples",
    "title": "02 dplyr",
    "section": "1.2 Examples",
    "text": "1.2 Examples\n\n\nCode\nsales &lt;- tibble(\n  id = c(1L, 1L, 1L, 2L, 2L), \n  sale_date = as.Date(c('2018-12-31', '2019-01-02', '2019-01-05', \"2019-01-04\", \"2019-01-01\"))\n)\nsales\n#&gt; # A tibble: 5 × 2\n#&gt;      id sale_date \n#&gt;   &lt;int&gt; &lt;date&gt;    \n#&gt; 1     1 2018-12-31\n#&gt; 2     1 2019-01-02\n#&gt; 3     1 2019-01-05\n#&gt; 4     2 2019-01-04\n#&gt; 5     2 2019-01-01\n\n\n\n\nCode\npromos &lt;- tibble(\n  id = c(1L, 1L, 2L), \n  promo_date = as.Date(c(\"2019-01-01\", \"2019-01-05\", \"2019-01-02\"))\n)\npromos\n#&gt; # A tibble: 3 × 2\n#&gt;      id promo_date\n#&gt;   &lt;int&gt; &lt;date&gt;    \n#&gt; 1     1 2019-01-01\n#&gt; 2     1 2019-01-05\n#&gt; 3     2 2019-01-02\n\n\n等式結合は次のとおり。\n\n\nCode\nby &lt;- join_by(id, sale_date == promo_date)\nleft_join(sales, promos, by)\n#&gt; # A tibble: 5 × 2\n#&gt;      id sale_date \n#&gt;   &lt;int&gt; &lt;date&gt;    \n#&gt; 1     1 2018-12-31\n#&gt; 2     1 2019-01-02\n#&gt; 3     1 2019-01-05\n#&gt; 4     2 2019-01-04\n#&gt; 5     2 2019-01-01\n\n\n不等式結合を行う。\n\n\nCode\n# idが同じでsale_dateがpromo_date以上のときに結合\nby &lt;- join_by(id, sale_date &gt;= promo_date)\nleft_join(sales, promos, by)\n#&gt; # A tibble: 6 × 3\n#&gt;      id sale_date  promo_date\n#&gt;   &lt;int&gt; &lt;date&gt;     &lt;date&gt;    \n#&gt; 1     1 2018-12-31 NA        \n#&gt; 2     1 2019-01-02 2019-01-01\n#&gt; 3     1 2019-01-05 2019-01-01\n#&gt; 4     1 2019-01-05 2019-01-05\n#&gt; 5     2 2019-01-04 2019-01-02\n#&gt; 6     2 2019-01-01 NA\n\n\nclosetにすると条件にマッチするもののうち最も近い値のみが残る。\n\n\nCode\nby &lt;- join_by(id, closest(sale_date &gt;= promo_date))\nleft_join(sales, promos, by)\n#&gt; # A tibble: 5 × 3\n#&gt;      id sale_date  promo_date\n#&gt;   &lt;int&gt; &lt;date&gt;     &lt;date&gt;    \n#&gt; 1     1 2018-12-31 NA        \n#&gt; 2     1 2019-01-02 2019-01-01\n#&gt; 3     1 2019-01-05 2019-01-05\n#&gt; 4     2 2019-01-04 2019-01-02\n#&gt; 5     2 2019-01-01 NA\n\n\n不等式の条件を変更してみる。\n\n\nCode\nby &lt;- join_by(id, closest(sale_date &gt; promo_date))\nleft_join(sales, promos, by)\n#&gt; # A tibble: 5 × 3\n#&gt;      id sale_date  promo_date\n#&gt;   &lt;int&gt; &lt;date&gt;     &lt;date&gt;    \n#&gt; 1     1 2018-12-31 NA        \n#&gt; 2     1 2019-01-02 2019-01-01\n#&gt; 3     1 2019-01-05 2019-01-01\n#&gt; 4     2 2019-01-04 2019-01-02\n#&gt; 5     2 2019-01-01 NA\n\n\nここからは別の例を示す。\n\n\nCode\nsegments &lt;- tibble(\n  segment_id = 1:4,\n  chromosome = c(\"chr1\", \"chr2\", \"chr2\", \"chr1\"),\n  start = c(140, 210, 380, 230),\n  end = c(150, 240, 415, 280)\n)\nsegments\n#&gt; # A tibble: 4 × 4\n#&gt;   segment_id chromosome start   end\n#&gt;        &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1          1 chr1         140   150\n#&gt; 2          2 chr2         210   240\n#&gt; 3          3 chr2         380   415\n#&gt; 4          4 chr1         230   280\n\n\n\n\nCode\nreference &lt;- tibble(\n  reference_id = 1:4,\n  chromosome = c(\"chr1\", \"chr1\", \"chr2\", \"chr2\"),\n  start = c(100, 200, 300, 415),\n  end = c(150, 250, 399, 450)\n)\nreference\n#&gt; # A tibble: 4 × 4\n#&gt;   reference_id chromosome start   end\n#&gt;          &lt;int&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1            1 chr1         100   150\n#&gt; 2            2 chr1         200   250\n#&gt; 3            3 chr2         300   399\n#&gt; 4            4 chr2         415   450\n\n\nreferenceの[start, end]にstartが包含されるsegmentをすべて見つける。\n\n\nCode\nby &lt;- join_by(chromosome, between(start, start, end))\nfull_join(segments, reference, by)\n#&gt; # A tibble: 5 × 7\n#&gt;   segment_id chromosome start.x end.x reference_id start.y end.y\n#&gt;        &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;        &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1          1 chr1           140   150            1     100   150\n#&gt; 2          2 chr2           210   240           NA      NA    NA\n#&gt; 3          3 chr2           380   415            3     300   399\n#&gt; 4          4 chr1           230   280            2     200   250\n#&gt; 5         NA chr2            NA    NA            4     415   450\n\n\nテーブルの順番を変更するときには、byを調整する必要がある\n\n\nCode\nby &lt;- join_by(chromosome, between(y$start, x$start, x$end))\nfull_join(reference, segments, by)\n#&gt; # A tibble: 5 × 7\n#&gt;   reference_id chromosome start.x end.x segment_id start.y end.y\n#&gt;          &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;      &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1            1 chr1           100   150          1     140   150\n#&gt; 2            2 chr1           200   250          4     230   280\n#&gt; 3            3 chr2           300   399          3     380   415\n#&gt; 4            4 chr2           415   450         NA      NA    NA\n#&gt; 5           NA chr2            NA    NA          2     210   240\n\n\nsegmentのうちstartとendの区間がすべて、referenceのstartとendの区間に包含されるケースを特定する。\n\n\nCode\nby &lt;- join_by(chromosome, within(x$start, x$end, y$start, y$end))\ninner_join(segments, reference, by)\n#&gt; # A tibble: 1 × 7\n#&gt;   segment_id chromosome start.x end.x reference_id start.y end.y\n#&gt;        &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;        &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1          1 chr1           140   150            1     100   150\n\n\n今度はオーバーラップするケースを特定する。\n\n\nCode\nby &lt;- join_by(chromosome, overlaps(x$start, x$end, y$start, y$end))\nfull_join(segments, reference, by)\n#&gt; # A tibble: 5 × 7\n#&gt;   segment_id chromosome start.x end.x reference_id start.y end.y\n#&gt;        &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;        &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1          1 chr1           140   150            1     100   150\n#&gt; 2          2 chr2           210   240           NA      NA    NA\n#&gt; 3          3 chr2           380   415            3     300   399\n#&gt; 4          3 chr2           380   415            4     415   450\n#&gt; 5          4 chr1           230   280            2     200   250\n\n\nバウンダリを調整する。右端を含まないようにしたので, 415の右端と415の左端はかさらなくなる。\n\n\nCode\nby &lt;- join_by(chromosome, overlaps(x$start, x$end, y$start, y$end, bounds = \"[)\"))\nfull_join(segments, reference, by)\n#&gt; # A tibble: 5 × 7\n#&gt;   segment_id chromosome start.x end.x reference_id start.y end.y\n#&gt;        &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;        &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1          1 chr1           140   150            1     100   150\n#&gt; 2          2 chr2           210   240           NA      NA    NA\n#&gt; 3          3 chr2           380   415            3     300   399\n#&gt; 4          4 chr1           230   280            2     200   250\n#&gt; 5         NA chr2            NA    NA            4     415   450",
    "crumbs": [
      "R",
      "tidyverse",
      "02 dplyr"
    ]
  },
  {
    "objectID": "contents/libs/R/treemapify/working.html",
    "href": "contents/libs/R/treemapify/working.html",
    "title": "treemapify",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/libs/R/treemapify\")\nCode\nlibrary(treemapify)\nlibrary(showtext)\nlibrary(dplyr)\nlibrary(ggplot2)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "R",
      "treemapify",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/treemapify/working.html#sample-data",
    "href": "contents/libs/R/treemapify/working.html#sample-data",
    "title": "treemapify",
    "section": "2.1 sample data",
    "text": "2.1 sample data\n\n\nCode\ngroup &lt;- paste(\"Group\", 1:9)\nsubgroup &lt;- c(\"A\", \"C\", \"B\", \"A\", \"A\",\n              \"C\", \"C\", \"B\", \"B\")\nvalue &lt;- c(7, 25, 50, 5, 16,\n           18, 30, 12, 41)\n\ndf &lt;- data.frame(group, subgroup, value) \ndf\n#&gt;     group subgroup value\n#&gt; 1 Group 1        A     7\n#&gt; 2 Group 2        C    25\n#&gt; 3 Group 3        B    50\n#&gt; 4 Group 4        A     5\n#&gt; 5 Group 5        A    16\n#&gt; 6 Group 6        C    18\n#&gt; 7 Group 7        C    30\n#&gt; 8 Group 8        B    12\n#&gt; 9 Group 9        B    41",
    "crumbs": [
      "R",
      "treemapify",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/treemapify/working.html#fill-by-categorical",
    "href": "contents/libs/R/treemapify/working.html#fill-by-categorical",
    "title": "treemapify",
    "section": "2.2 Fill by categorical",
    "text": "2.2 Fill by categorical\n\n\nCode\nggplot(df, aes(area = value, fill = group)) +\n  geom_treemap()",
    "crumbs": [
      "R",
      "treemapify",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/treemapify/working.html#fill-by-the-numerical-variable",
    "href": "contents/libs/R/treemapify/working.html#fill-by-the-numerical-variable",
    "title": "treemapify",
    "section": "2.3 Fill by the numerical variable",
    "text": "2.3 Fill by the numerical variable\n\n\nCode\n\nggplot(df, aes(area = value, fill = value)) +\n  geom_treemap()",
    "crumbs": [
      "R",
      "treemapify",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/treemapify/working.html#with-label",
    "href": "contents/libs/R/treemapify/working.html#with-label",
    "title": "treemapify",
    "section": "2.4 with label",
    "text": "2.4 with label\n\n\nCode\nggplot(df, aes(area = value, fill = group, label = value)) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"white\",\n                    place = \"centre\",\n                    size = 15)",
    "crumbs": [
      "R",
      "treemapify",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/treemapify/working.html#label-grow",
    "href": "contents/libs/R/treemapify/working.html#label-grow",
    "title": "treemapify",
    "section": "2.5 label grow",
    "text": "2.5 label grow\n\n\nCode\nggplot(df, aes(area = value, fill = value, label = group)) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"white\",\n                    place = \"centre\",\n                    size = 15,\n                    grow = TRUE)",
    "crumbs": [
      "R",
      "treemapify",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/treemapify/working.html#sub-group",
    "href": "contents/libs/R/treemapify/working.html#sub-group",
    "title": "treemapify",
    "section": "2.6 sub group",
    "text": "2.6 sub group\n\n\nCode\nggplot(df, aes(area = value, fill = value,\n               label = group, subgroup = subgroup)) +\n  geom_treemap() +\n  geom_treemap_subgroup_border(colour = \"white\", size = 5) +\n  geom_treemap_subgroup_text(place = \"centre\", grow = TRUE,\n                             alpha = 0.25, colour = \"black\",\n                             fontface = \"italic\") +\n  geom_treemap_text(colour = \"white\", place = \"centre\",\n                    size = 15, grow = TRUE)",
    "crumbs": [
      "R",
      "treemapify",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/libs/R/treemapify/working.html#color-customize",
    "href": "contents/libs/R/treemapify/working.html#color-customize",
    "title": "treemapify",
    "section": "2.7 color customize",
    "text": "2.7 color customize\n\n\nCode\nggplot(df, aes(area = value, fill = group, label = value)) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"white\",\n                    place = \"centre\",\n                    size = 15) +\n  scale_fill_brewer(palette = \"Blues\")",
    "crumbs": [
      "R",
      "treemapify",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch01_回帰分析の目的.html#身長と体重",
    "href": "contents/books/01_Rによる実証分析_2e/ch01_回帰分析の目的.html#身長と体重",
    "title": "ch01 回帰分析の目的",
    "section": "2.1 身長と体重",
    "text": "2.1 身長と体重\nある年代の人の身長と体重をプロットとすると「正の相関」が見られる。 一方で、この相関からは「体重を増やせば身長が伸びる」という因果関係を得ることはできない。 よく使われる例であるが、これを別の言い方に変えると、相関はマクロな指標のため入力も出力も統計的であるが、 因果は個別の人についても適用される。\n\n\n\n\n\n\nNote\n\n\n\nある集団の平均的な体重が増えるとも身長も増えるのが相関であり、ある人のことについて言及できるのが因果である",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch01 回帰分析の目的"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch01_回帰分析の目的.html#疑似相関",
    "href": "contents/books/01_Rによる実証分析_2e/ch01_回帰分析の目的.html#疑似相関",
    "title": "ch01 回帰分析の目的",
    "section": "2.2 疑似相関",
    "text": "2.2 疑似相関\n次のグラフは、一見因果関係がありそうな２つの支出であるが、これは経済自体が成長している時期であるため、 どのような指標とも医療への支出額が相関関係をもつ。 統計的には「所得」という因子が交絡しているということになる。\n\ndata(\"USPersonalExpenditure\")\nUSPersonalExpenditure &lt;-\n    USPersonalExpenditure |&gt;\n    t() |&gt; \n    as.data.frame() \n\nUSPersonalExpenditure |&gt; \n    ggplot(aes(x = `Food and Tobacco`, y = `Medical and Health`)) +\n    geom_path() + \n    geom_point(size = 10) + \n    labs(\n        x = \"食料品およびタバコへの支出総額(Food and tobacco)\", \n        y = \"医療および健康への支出総額\"\n    ) \n\n\n\n\n\n\n\n\nその他のカラムとの関係をみてみる。\n\n\nCode\npairs  &lt;- combn(syms(names(USPersonalExpenditure)), 2, simplify = FALSE)\ngraphs &lt;- \n    pairs |&gt; \n    map(\\(x) {\n        ggplot(USPersonalExpenditure, aes(!!x[[1]], !!x[[2]])) + \n            geom_path() + \n            geom_point(size = 5)\n    }) |&gt; \n    list_modify(ncol = 2)\n\ndo.call(grid.arrange, graphs)\n\n\n\n\n\n\n\n\n\nこのように一般にはデータからだけでは因果関係に言及することは困難である。 データ分析のまえに事前にフィールドワークや文献調査をおこない信頼できる因果関係を仮説立てた上で、その仮説をモデルにより検証する。\nただし近年では古典的なビッグデータを使った因果特定の研究もある。",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch01 回帰分析の目的"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#独立性と相関係数",
    "href": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#独立性と相関係数",
    "title": "ch03 確率論の基礎",
    "section": "2.1 独立性と相関係数",
    "text": "2.1 独立性と相関係数\n無相関であるとういことは、独立を意味しないことの例を示す。\n\n\nCode\nx &lt;- rnorm(100000, 50, 10)\nz &lt;- - ((x - 50) ** 2) / 10\ncor(x, z)\n#&gt; [1] 0.00199626\n\n\n\n\nCode\nplot(x, z)",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch03 確率論の基礎"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#確率密度",
    "href": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#確率密度",
    "title": "ch03 確率論の基礎",
    "section": "2.2 確率密度",
    "text": "2.2 確率密度\n確率変数\\(X\\)がある値を取る確率はゼロになってしまう。そこで、ある値に収束するように、確率関数を微分した確率密度関数を使う。",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch03 確率論の基礎"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#データによる条件付き期待値の推定",
    "href": "contents/books/01_Rによる実証分析_2e/ch03_確率論の基礎.html#データによる条件付き期待値の推定",
    "title": "ch03 確率論の基礎",
    "section": "2.3 データによる条件付き期待値の推定",
    "text": "2.3 データによる条件付き期待値の推定\n1976年にアメリカ合衆国の男性労働者を対象に実施された賃金調査結果データの1部分を扱う。educが教育年数であり、experは職業経験根数、wageは1時間あたり賃金です。\n\n\nCode\npath &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap03/wage.csv\")\nwage &lt;- read_csv(path, show_col_types = FALSE)\n\nwage |&gt; \n    head() |&gt; \n    paged_table()\n\n\n\n  \n\n\n\n\n\nCode\nsummary(wage)\n#&gt;       educ           exper             wage       \n#&gt;  Min.   : 1.00   Min.   : 0.000   Min.   : 100.0  \n#&gt;  1st Qu.:12.00   1st Qu.: 6.000   1st Qu.: 394.2  \n#&gt;  Median :13.00   Median : 8.000   Median : 537.5  \n#&gt;  Mean   :13.26   Mean   : 8.856   Mean   : 577.3  \n#&gt;  3rd Qu.:16.00   3rd Qu.:11.000   3rd Qu.: 708.8  \n#&gt;  Max.   :18.00   Max.   :23.000   Max.   :2404.0\n\n\n\n\nCode\nplot(wage)\n\n\n\n\n\n\n\n\n\nこのデータに対して、educ = 12の個人だけを対象にサマリーする。とおもったけど面倒であるので、ここではすべてを対象にする。 これを見るだけでも、どうやら教育年数により収入の平均値が変化していそうなことがわかる。\n\n\nCode\nwage_summary &lt;- \n    wage |&gt; \n    group_by(educ) |&gt; \n    skim() |&gt; \n    as_tibble() |&gt; \n    filter(skim_variable == \"wage\")\n\nwage_summary |&gt; paged_table()\n\n\n\n  \n\n\n\n\n\nCode\nwage_summary |&gt; \n    ggplot(aes(x = educ, y = numeric.mean)) +\n    geom_linerange(aes(ymin = numeric.p25, ymax = numeric.p75), lwd = 2) + \n    geom_point(color = \"red\", size = 5)",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch03 確率論の基礎"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch05_推測統計の基礎.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch05_推測統計の基礎.html",
    "title": "ch05 推測統計の基礎",
    "section": "",
    "text": "1 Setup\n\n\n2 平均値の検定\n\n\nCode\ndata_path &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap05/distributions.csv\")\nsimdata &lt;- read_csv(data_path, show_col_types = FALSE)\nsimdata\n#&gt; # A tibble: 100 × 2\n#&gt;    distA  distB\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1  2.83  0.816\n#&gt;  2  2.66  0.755\n#&gt;  3  2.64 -0.971\n#&gt;  4  1.31 -0.572\n#&gt;  5  2.29  0.450\n#&gt;  6  1.44  0.848\n#&gt;  7  2.56  0.130\n#&gt;  8  1.30 -0.640\n#&gt;  9  1.39  0.196\n#&gt; 10  1.27  0.817\n#&gt; # ℹ 90 more rows\n\n\n\n\nCode\nsimdata |&gt; \n    summarise(mean_A = mean(distA), var_A = var(distA)) |&gt; \n    mutate(t.value = sqrt(100) / sqrt(var_A) * mean_A)\n#&gt; # A tibble: 1 × 3\n#&gt;   mean_A var_A t.value\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;\n#&gt; 1   2.04 0.373    33.4\n\n\n\n\n3 回帰係数の検定\n線形モデルでシミュレーションして回帰係数の値をみてみる。 次の結果からモデル通りの値が出ていることがわかる。\n\n\nCode\nX &lt;- rnorm(1000, 0, 1)\nY &lt;- 1 + 5 * X + rnorm(1000, 0, 1)\nbeta1 &lt;- lm(Y ~ X)$coefficients[2]\nbeta1\n#&gt;        X \n#&gt; 4.998162\n\n\n上記の推定の流れをモンテカルロシミュレーションする。だいたい5に近い値が出ていることがわかる。\n\n\nCode\nn_simulations &lt;- 1000\nbetas &lt;- replicate(n_simulations, {\n    X &lt;- rnorm(1000, 0, 1)\n    Y &lt;- 1 + 5 * X + rnorm(1000, 0, 1)\n    lm(Y ~ X)$coefficients[2]\n})\n\nhist(betas)\n\n\n\n\n\n\n\n\n\n分析例としてwageデータを使う.\n\n\nCode\nwagedata &lt;- read_csv(here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap03/wage.csv\"), show_col_types = FALSE)\nfit &lt;- \n    wagedata |&gt; \n    lm(log(wage) ~ educ + exper, data = _) \n\nfit |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = log(wage) ~ educ + exper, data = wagedata)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -1.93442 -0.26396  0.02404  0.27287  1.42863 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 4.666034   0.063790   73.15   &lt;2e-16 ***\n#&gt; educ        0.093168   0.003612   25.80   &lt;2e-16 ***\n#&gt; exper       0.040657   0.002334   17.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.4017 on 3007 degrees of freedom\n#&gt; Multiple R-squared:  0.1813, Adjusted R-squared:  0.1808 \n#&gt; F-statistic:   333 on 2 and 3007 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nconfint(fit) \n#&gt;                  2.5 %     97.5 %\n#&gt; (Intercept) 4.54095799 4.79111090\n#&gt; educ        0.08608627 0.10024978\n#&gt; exper       0.03608017 0.04523456\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch05 推測統計の基礎"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#欠落変数",
    "href": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#欠落変数",
    "title": "ch07 内生変数と外生変数",
    "section": "2.1 欠落変数",
    "text": "2.1 欠落変数\nもとのモデルは内生変数がない次の状態を考える。この状態であれば通常の回帰分析で推定することができる。\n$$\nY_i = _0 + 1X{1i} + 2X{2i} + _i\n$$\nここで\\(X_2\\)が観測できない状態を考える。\n$$ \\[\\begin{align}\n\nY_i &= \\alpha_0 + \\beta_1X_{1i} + \\eta_i \\\\\n\\alpha_0 &= \\beta_0 + \\beta_2\\textrm{E}[X_{2i}] \\\\\n\\eta_i &= \\beta_2(X_{2i}-\\textrm{E}[X_{2i}])+\\epsilon_i\n\n\n\\end{align}\\] $$\n単回帰モデルで正しく\\(\\beta\\)を推定するためには、\\(X_{i}\\)が誤差項と外生的変数であることが求められる。 下記の結果をみると\\(X_{1i}\\)は外生的であるため欠落したままでは回帰分析は適していない\n\\[\n\\begin{align}\n\\textrm{E}(X_{1i}) &= \\textrm{E}\\{X_{1i}[\\beta_2(X_{2i}-\\textrm{E}[X_{2i}])+\\epsilon_i]\\} \\\\\n&=\\beta_2\\textrm{E}{X_{1i}(X_{2i}-\\textrm{E}[X_{2i}])} + E[X_{1i}\\epsilon_i]\\\\\n&=\\beta_2\\textrm{Cov}(X_{1i}., X_{2i})\n\\end{align}\n\\]",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch07 内生変数と外生変数"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#測定誤差",
    "href": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#測定誤差",
    "title": "ch07 内生変数と外生変数",
    "section": "2.2 測定誤差",
    "text": "2.2 測定誤差\n測定誤差があると、過小バイアスがかかる。ただし符号は変化しない。\n\n\nCode\nn &lt;- 200\ne &lt;- rnorm(n)\nX &lt;- rnorm(n)\nu &lt;- runif(n, -1, 1)\nW &lt;- X + u \nb0 &lt;- 1\nb1 &lt;- 2\nY &lt;- b0 + X * b1 + e\n\nlm(\n    Y ~ W\n)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = Y ~ W)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            W  \n#&gt;       1.040        1.463",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch07 内生変数と外生変数"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#同時性",
    "href": "contents/books/01_Rによる実証分析_2e/ch07_外生変数と内生変数.html#同時性",
    "title": "ch07 内生変数と外生変数",
    "section": "2.3 同時性",
    "text": "2.3 同時性\nこれも通常の回帰分析では正しく推定することは出来ない。",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch07 内生変数と外生変数"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch09_マッチング法.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch09_マッチング法.html",
    "title": "ch09 マッチング法",
    "section": "",
    "text": "1 Setup\n\n\n2 計算練習\nRを使って実際にマッチング法から因果分析を行っていく。\n\n\nCode\npath &lt;- here(cur_dir, \"data/R_EmpiricalAnalysis_csv/chap09/wage_training.csv\")\nwagedata &lt;- read_csv(path, show_col_types = FALSE)\nwagedata |&gt; \n    head() |&gt; \n    paged_table()\n\n\n\n  \n\n\n\n\nwagea：研修期間後の賃金\nD：研修参加の有無\nyears：経験年数\nwageb：研修以前の賃金\n\n\n\nCode\nwagedata |&gt; \n    summarise(\n        mean_treated = mean(wagea[D == 1]), \n        mean_controlled = mean(wagea[D == 0]), \n        mean_diff = mean_treated - mean_controlled\n    )\n#&gt; # A tibble: 1 × 3\n#&gt;   mean_treated mean_controlled mean_diff\n#&gt;          &lt;dbl&gt;           &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1         25.7            26.8     -1.13\n\n\n上記の結果をみると、研修をおこなった人の方が賃金が低いという結果が出ている。 これは、他の要因がコントロールされている状況下では、その通りであるが実際には、 次でみるように研修参加が経験年数などと相関していることがわかる。 経験年数などは賃金と相関することが高いと考えられるため、 この値を制御した上で比較する必要がある。\n\n\nCode\nlibrary(correlation)\nwagedata |&gt; \n    select(-wagea) |&gt; \n    correlation() |&gt; \n    paged_table()\n\n\n\n  \n\n\n\nそこでマッチングを行うことにする。\n\n\nCode\nm.out &lt;- matchit(D ~ years + wageb,\n                 data     = wagedata,\n                 method   = \"nearest\",\n                 distance = \"scaled_euclidean\",\n                 replace  = TRUE)\nm.out\n#&gt; A matchit object\n#&gt;  - method: 1:1 nearest neighbor matching with replacement\n#&gt;  - distance: Scaled Euclidean\n#&gt;  - number of obs.: 800 (original), 362 (matched)\n#&gt;  - target estimand: ATT\n#&gt;  - covariates: years, wageb\n\n\nバランスさせる前と後で統計量が変わっていることがわかる。 バランスさせると統計量が非常に近くなっていることがわかる。 注意点としてはUnmatchedなデータが多数発生してることは忘れないことにする。\nStd. Mean DiffというのはMean TreatedとMeaan Controlの差を、 treatedの標準偏差で割った値である。\n\n\nCode\nm.out |&gt; \n    summary()\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = D ~ years + wageb, data = wagedata, method = \"nearest\", \n#&gt;     distance = \"scaled_euclidean\", replace = TRUE)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;       Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\n#&gt; years        8.2311       10.9377         -0.4970     0.6556    0.1044   0.1722\n#&gt; wageb       24.4874       26.6637         -0.4734     0.6734    0.0989   0.2094\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;       Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean eCDF Max\n#&gt; years        8.2311        8.2101          0.0039     1.0147    0.0044   0.0210\n#&gt; wageb       24.4874       24.4958         -0.0018     0.9905    0.0008   0.0042\n#&gt;       Std. Pair Dist.\n#&gt; years          0.0239\n#&gt; wageb          0.0037\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All            562.       238\n#&gt; Matched (ESS)   88.78     238\n#&gt; Matched        124.       238\n#&gt; Unmatched      438.         0\n#&gt; Discarded        0.         0\n\n\n\n\nCode\nwagedata |&gt; group_by(D) |&gt; summarise(across(c(years), .fns = list(mean = mean, sd = sd)))\n#&gt; # A tibble: 2 × 3\n#&gt;       D years_mean years_sd\n#&gt;   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1     0      10.9      6.73\n#&gt; 2     1       8.23     5.45\n\n\nStd. Mean Diffの絶対値をプロットしたものをラブプロットという。 ラブプロットでマッチングがうまくいっているのかどうかを判定することができる。\n\n\nCode\nm.out |&gt; \n    summary() |&gt; \n    plot(xlim = c(0, 1.5))\n\n\n\n\n\n\n\n\n\nマッチングに使われたデータは次である。重複については重みで管理されているみたいです。 このあたりは詳しい使い方を調べる必要がある。\n\n\nCode\nmatch.data(m.out) \n#&gt; # A tibble: 362 × 7\n#&gt;    wagea     D years wageb  educ female weights\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n#&gt;  1    19     1     4    18    17      1   1    \n#&gt;  2    20     1     2    18    15      1   1    \n#&gt;  3    36     1    14    34    14      1   1    \n#&gt;  4    18     0     4    18    11      1   2.61 \n#&gt;  5    29     0    19    29    15      0   0.521\n#&gt;  6    21     1     3    20    18      1   1    \n#&gt;  7    31     0    13    31    15      1   1.04 \n#&gt;  8    26     0    13    26    14      0   2.61 \n#&gt;  9    35     0    19    34    15      1   0.521\n#&gt; 10    33     0    13    33    15      1   0.521\n#&gt; # ℹ 352 more rows\n\n\n\n\nCode\nmatch.data(m.out) |&gt; \n    group_by(D) |&gt; \n    skim() |&gt; \n    as_tibble() |&gt; \n    paged_table()\n\n\n\n  \n\n\n\nこの状態であれば通常の回帰分析を使うことで対象を推定することができる。\n\n\nCode\nmatch.data(m.out) |&gt; \n    lm(wagea ~ D + years + wageb, educ, female, data = _, weigths = weights) |&gt; \n    tidy()\n#&gt; Warning: In lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, \n#&gt;     ...) :\n#&gt;  extra argument 'weigths' will be disregarded\n#&gt; # A tibble: 4 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.48    0.209        7.10 7.67e- 12\n#&gt; 2 D             -0.386   0.0961      -4.01 7.40e-  5\n#&gt; 3 years          0.142   0.00948     14.9  8.02e- 39\n#&gt; 4 wageb          0.916   0.0102      89.8  1.48e-232\n\n\nまた、傾向スコアによるマッチングも行える。\n\n\nCode\nm.out &lt;- matchit(D ~ years + wageb,\n                 data     = wagedata,\n                 method   = \"nearest\",\n                 distance = \"glm\",\n                 replace  = TRUE)\n\nm.out %&gt;% \n  match.data() %&gt;% \n  lm(wagea ~ D,\n       data = .,\n     weights = weights) %&gt;% \n  tidy()\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)   25.1       0.369     68.0  3.92e-221\n#&gt; 2 D              0.613     0.479      1.28 2.01e-  1\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch09 マッチング法"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/ch11_操作変数法.html",
    "href": "contents/books/01_Rによる実証分析_2e/ch11_操作変数法.html",
    "title": "ch11 操作変数法",
    "section": "",
    "text": "1 Setup\n\n\n2 はじめに\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "ch11 操作変数法"
    ]
  },
  {
    "objectID": "contents/books/01_Rによる実証分析_2e/index.html",
    "href": "contents/books/01_Rによる実証分析_2e/index.html",
    "title": "Rによる実証分析 2nd",
    "section": "",
    "text": "1 はじめに\n\nRによる実証分析 2eの読書ノートです\n計量経済学をバックグラウンドとして回帰分析の基礎から解説しています\n応用として因果推論についても解説しています\n出版社HP\n著者サポートページ\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "Rによる実証分析2e",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch02_確率と回帰の概要.html",
    "href": "contents/books/02_因果推論ミックステープ/ch02_確率と回帰の概要.html",
    "title": "ch02 確率と回帰の概要",
    "section": "",
    "text": "Note\n\n\n\nランダム過程とは、何度も繰り返し可能であり、そのたびに異なるアウトカムが得られる過程である。 標本空間とはランダム過程における起こり得るアウトカムの集合である。\n\n\n\n\n\\(x\\)の値ごとに分割した母集団の誤差項の期待値に関する、次のような仮定がある。\n$$\n(u x) = (u)\n$$ さらに、回帰分析における自明な仮定として次がある。\n\\[\n\\textrm{E}(u) = 0\n\\]\nよって、回帰分析の文脈でいえば説明変数で条件付けることで、誤差項の期待値はすべて０となる、という解釈になる。\n\n\n\n\n\n\nNote\n\n\n\n残差と誤差項は異なる。残差とは、標本値と推定値の差であり、誤差項とはアウトカムに影響を与える要因のうちモデルが補足できない全て要因を含んだ値である。実態としては残差は計算することが出来るが、誤差項は含めることができない。\n\n\n\n\nCode\nset.seed(1)\ntb &lt;- tibble(\nx = rnorm(10000),\nu = rnorm(10000),\ny = 5.5*x + 12*u\n)\n\nreg_tb &lt;- tb %&gt;%\nlm(y ~ x, .) %&gt;%\nprint()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, data = .)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            x  \n#&gt;    -0.04991      5.55690\nreg_tb$coefficients\n#&gt; (Intercept)           x \n#&gt; -0.04990882  5.55690164\ntb &lt;- tb %&gt;%\nmutate(\nyhat1 = predict(lm(y ~ x, .)),\nyhat2 = 0.0732608 + 5.685033*x,\nuhat1 = residuals(lm(y ~ x, .)),\nuhat2 = y - yhat2\n)\nsummary(tb[-1:-3])\n#&gt;      yhat1               yhat2              uhat1              uhat2         \n#&gt;  Min.   :-20.45096   Min.   :-20.7982   Min.   :-51.5275   Min.   :-51.5247  \n#&gt;  1st Qu.: -3.79189   1st Qu.: -3.7550   1st Qu.: -8.1520   1st Qu.: -8.2751  \n#&gt;  Median : -0.13842   Median : -0.0173   Median : -0.1727   Median : -0.3147  \n#&gt;  Mean   : -0.08624   Mean   :  0.0361   Mean   :  0.0000   Mean   : -0.1223  \n#&gt;  3rd Qu.:  3.71578   3rd Qu.:  3.9258   3rd Qu.:  7.9778   3rd Qu.:  7.8506  \n#&gt;  Max.   : 21.12342   Max.   : 21.7348   Max.   : 44.7176   Max.   : 44.4416\ntb %&gt;%\nlm(y ~ x, .) %&gt;%\nggplot(aes(x=x, y=y)) +\nggtitle(\"OLS Regression Line\") +\ngeom_point(size = 0.05, color = \"black\", alpha = 0.5) +\ngeom_smooth(method = lm, color = \"black\") +\nannotate(\"text\", x = -1.5, y = 30, color = \"red\",\nlabel = paste(\"Intercept = \", -0.0732608)) +\nannotate(\"text\", x = 1.5, y = -30, color = \"blue\",\nlabel = paste(\"Slope =\", 5.685033))\n#&gt; `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch02 確率と回帰の概要"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch02_確率と回帰の概要.html#平均独立",
    "href": "contents/books/02_因果推論ミックステープ/ch02_確率と回帰の概要.html#平均独立",
    "title": "ch02 確率と回帰の概要",
    "section": "",
    "text": "\\(x\\)の値ごとに分割した母集団の誤差項の期待値に関する、次のような仮定がある。\n$$\n(u x) = (u)\n$$ さらに、回帰分析における自明な仮定として次がある。\n\\[\n\\textrm{E}(u) = 0\n\\]\nよって、回帰分析の文脈でいえば説明変数で条件付けることで、誤差項の期待値はすべて０となる、という解釈になる。\n\n\n\n\n\n\nNote\n\n\n\n残差と誤差項は異なる。残差とは、標本値と推定値の差であり、誤差項とはアウトカムに影響を与える要因のうちモデルが補足できない全て要因を含んだ値である。実態としては残差は計算することが出来るが、誤差項は含めることができない。\n\n\n\n\nCode\nset.seed(1)\ntb &lt;- tibble(\nx = rnorm(10000),\nu = rnorm(10000),\ny = 5.5*x + 12*u\n)\n\nreg_tb &lt;- tb %&gt;%\nlm(y ~ x, .) %&gt;%\nprint()\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y ~ x, data = .)\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)            x  \n#&gt;    -0.04991      5.55690\nreg_tb$coefficients\n#&gt; (Intercept)           x \n#&gt; -0.04990882  5.55690164\ntb &lt;- tb %&gt;%\nmutate(\nyhat1 = predict(lm(y ~ x, .)),\nyhat2 = 0.0732608 + 5.685033*x,\nuhat1 = residuals(lm(y ~ x, .)),\nuhat2 = y - yhat2\n)\nsummary(tb[-1:-3])\n#&gt;      yhat1               yhat2              uhat1              uhat2         \n#&gt;  Min.   :-20.45096   Min.   :-20.7982   Min.   :-51.5275   Min.   :-51.5247  \n#&gt;  1st Qu.: -3.79189   1st Qu.: -3.7550   1st Qu.: -8.1520   1st Qu.: -8.2751  \n#&gt;  Median : -0.13842   Median : -0.0173   Median : -0.1727   Median : -0.3147  \n#&gt;  Mean   : -0.08624   Mean   :  0.0361   Mean   :  0.0000   Mean   : -0.1223  \n#&gt;  3rd Qu.:  3.71578   3rd Qu.:  3.9258   3rd Qu.:  7.9778   3rd Qu.:  7.8506  \n#&gt;  Max.   : 21.12342   Max.   : 21.7348   Max.   : 44.7176   Max.   : 44.4416\ntb %&gt;%\nlm(y ~ x, .) %&gt;%\nggplot(aes(x=x, y=y)) +\nggtitle(\"OLS Regression Line\") +\ngeom_point(size = 0.05, color = \"black\", alpha = 0.5) +\ngeom_smooth(method = lm, color = \"black\") +\nannotate(\"text\", x = -1.5, y = 30, color = \"red\",\nlabel = paste(\"Intercept = \", -0.0732608)) +\nannotate(\"text\", x = 1.5, y = -30, color = \"blue\",\nlabel = paste(\"Slope =\", 5.685033))\n#&gt; `geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch02 確率と回帰の概要"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html",
    "href": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html",
    "title": "ch04 潜在アウトカム因果モデル",
    "section": "",
    "text": "潜在アウトカムとは、次のスイッチング方程式により決定される。\n\\[\nY_i = D_i Y_i^1 + (1-D_i)Y_i^0\n\\]\nここで\\(D_i\\)はそのユニットが処置を受けたときには\\(1\\), 受けなかった場合は\\(0\\)となる。 \\(D_i=1\\)のときには\\(Y_i=Y_i^1\\)となり、\\(D_i=0\\)のときには\\(Y_i=Y_i^0\\)となる。\nこの表記を用いて、ユニット固有の処置効果を２つの状態の差として定義する。\n\\[\n\\delta_i = Y_i^1 - Y_i^0\n\\]\nこの式が意味するところはあるユニットに対する処置効果を知るには、 処置が施された場合の結果、施されていない場合の結果のどちらも必要であり、 現実には知り得ない情報っであるということである.\n\n\n個体ごとの効果を知ることは出来ないので、 平均的な処置効果\\(ATE(Average treatment effect)\\)を調べることにする。\n\\[\n\\begin{align}\nATE &= E[\\delta_i]\\\\\n&=E[Y_i^1 - Y_i^0] \\\\\n&=E[Y_i^1] - E[Y_i^0]\n\\end{align}\n\\]\nATEを考えることで処置効果を推定することができる。 ただし、いずれにせよあるユニットにはどちらかしか観測できないのに注意する。\n次に関心があるのは処置群の平均処置効果である\\(ATT(average treatment effect for the treated group)\\)である。処置群においても、処置効果はユニットごとに異なるので、平均的にどの程度の効果があるのかを知りたいということになる。\n\\[\n\\begin{align}\nATT &= E[\\delta_i\\mid D_i = 1]\\\\\n&=E[Y_i^1 - Y_i^0\\mid D_i = 1] \\\\\n&=E[Y_i^1\\mid D_i = 1] - E[Y_i^0\\mid D_i = 1]\n\\end{align}\n\\]\n最期に関心があるのは、コントロール群、または未処理群に対する平均処置効果\\(ATU(average treatement effect for the untreated)\\)である。\n\\[\n\\begin{align}\nATU &= E[\\delta_i\\mid D_i = 0]\\\\\n&=E[Y_i^1 - Y_i^0\\mid D_i = 0] \\\\\n&=E[Y_i^1\\mid D_i = 0] - E[Y_i^0\\mid D_i = 0]\n\\end{align}\n\\]\n研究ではこれらの効果を 観測できる情報からどのように推定するのかが重要となる。\nところで, \\(ATE\\)の推定値として２つの処置群の単純差(Simple difference in mean outcomes: SDO)を考える。この値はデータから推定できる値である。\n\\[\nATE =　E[Y_i^1\\mid D=1] - E[Y_i^0\\mid D=0]\n\\]\nこの統計処理自体は正しいものの、誤解を招きかねないことには注意が必要である。 処置の割り当てがランダムでないときには、処置群とコントロール群に根本的な差が生じ津ことが知られている。\n\\[\n\\begin{align}\nE[Y_i^1\\mid D_i = 0] - E[Y_i^0\\mid D_i = 0] &= ATE + E[Y^0\\mid D=1]-E[Y^0\\mid D=0]+(1-\\pi)(ATT - ATU)\n\\end{align}\n\\]\n上記の結果は下の数式を整理したものである。\n\\[\n\\begin{align}\nATE&=\\pi ATT + (1-\\pi)ATU\\\\\n&=\\pi E[Y^1\\mid D=1]-\\pi E[Y^0\\mid D = 1] + (1-\\pi)E[Y^1\\mid D=0]-(1-\\pi)E[Y^0\\mid D = 0]\\\\\n&=\\{\\pi E[Y^1\\mid D=1]+(1-\\pi)E[Y^1\\mid D=0]\\}-{\\pi E[Y^0\\mid D = 1]+(1-\\pi)E[Y^0\\mid D = 0]}\n\\end{align}\n\\]\n\\(\\pi\\)は処置を受けた人の割合である。第一式は\\(ATT\\)の定義から、 処置を受けた人、受けていない人の処置効果に分会しているということになる。\nこれを見ると、単純な差にはATEだけでなく２つの項がある。 まずは次の式は選択バイアスと呼ばれるものである。 処置を受けていない場合の平均的なアウトカムが、 処置を受けているかどうかで変わるのでこのように呼ばれる。\n\\[\nE[Y^0\\mid D=1]-E[Y^0\\mid D=0]\n\\]\n次の式は異質性のある処置効果にともなうバイアスである。\n\\[\n(1-\\pi)(ATT - ATU)\n\\]\n上記２つの項は反実仮想を含んでいるため計算することが出来ない。 いずれにせよバイアスを含んでいることはわかる。\n\n\n\nSDOを使ってATEを推定するにあたり最も信頼できるのは、 処置を受けるのかどうかとアウトカムが独立であるときである。\n\\[\n(Y^1, Y^0) \\mathop{\\perp\\!\\!\\!\\!\\perp} D\n\\]\nランダム化によって実現したいのは、次の状態である。\n\\[\n\\begin{align}\nE[Y^1\\mid D=1]-E[Y^1\\mid D=0]&=0\\\\\nE[Y^0\\mid D=1]-E[Y^0\\mid D=0]&=0\n\\end{align}\n\\] ランダム化により明らかに選択バイアスを除くことができる。\nまた異質性のある処置効果についても取り除くことができる。\n\\[\nATT-ATU=E[Y^1\\mid D=1]-E[Y^0\\mid  D=1]-E[Y^1\\mid D=0]+E[Y^0\\mid D=0]\n\\]\n\n\n\nRubinはこの種の計算をいつくかの仮定が伴うとして、その仮定をSUTVAとまとめています。具体的には次の３つです。\n\n処置の均質性\n処置の外部性がないこと\n\nユニット間で影響しあわないこと\n\n一般均衡効果がないこと",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch04 潜在アウトカム因果モデル"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#平均処置効果",
    "href": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#平均処置効果",
    "title": "ch04 潜在アウトカム因果モデル",
    "section": "",
    "text": "個体ごとの効果を知ることは出来ないので、 平均的な処置効果\\(ATE(Average treatment effect)\\)を調べることにする。\n\\[\n\\begin{align}\nATE &= E[\\delta_i]\\\\\n&=E[Y_i^1 - Y_i^0] \\\\\n&=E[Y_i^1] - E[Y_i^0]\n\\end{align}\n\\]\nATEを考えることで処置効果を推定することができる。 ただし、いずれにせよあるユニットにはどちらかしか観測できないのに注意する。\n次に関心があるのは処置群の平均処置効果である\\(ATT(average treatment effect for the treated group)\\)である。処置群においても、処置効果はユニットごとに異なるので、平均的にどの程度の効果があるのかを知りたいということになる。\n\\[\n\\begin{align}\nATT &= E[\\delta_i\\mid D_i = 1]\\\\\n&=E[Y_i^1 - Y_i^0\\mid D_i = 1] \\\\\n&=E[Y_i^1\\mid D_i = 1] - E[Y_i^0\\mid D_i = 1]\n\\end{align}\n\\]\n最期に関心があるのは、コントロール群、または未処理群に対する平均処置効果\\(ATU(average treatement effect for the untreated)\\)である。\n\\[\n\\begin{align}\nATU &= E[\\delta_i\\mid D_i = 0]\\\\\n&=E[Y_i^1 - Y_i^0\\mid D_i = 0] \\\\\n&=E[Y_i^1\\mid D_i = 0] - E[Y_i^0\\mid D_i = 0]\n\\end{align}\n\\]\n研究ではこれらの効果を 観測できる情報からどのように推定するのかが重要となる。\nところで, \\(ATE\\)の推定値として２つの処置群の単純差(Simple difference in mean outcomes: SDO)を考える。この値はデータから推定できる値である。\n\\[\nATE =　E[Y_i^1\\mid D=1] - E[Y_i^0\\mid D=0]\n\\]\nこの統計処理自体は正しいものの、誤解を招きかねないことには注意が必要である。 処置の割り当てがランダムでないときには、処置群とコントロール群に根本的な差が生じ津ことが知られている。\n\\[\n\\begin{align}\nE[Y_i^1\\mid D_i = 0] - E[Y_i^0\\mid D_i = 0] &= ATE + E[Y^0\\mid D=1]-E[Y^0\\mid D=0]+(1-\\pi)(ATT - ATU)\n\\end{align}\n\\]\n上記の結果は下の数式を整理したものである。\n\\[\n\\begin{align}\nATE&=\\pi ATT + (1-\\pi)ATU\\\\\n&=\\pi E[Y^1\\mid D=1]-\\pi E[Y^0\\mid D = 1] + (1-\\pi)E[Y^1\\mid D=0]-(1-\\pi)E[Y^0\\mid D = 0]\\\\\n&=\\{\\pi E[Y^1\\mid D=1]+(1-\\pi)E[Y^1\\mid D=0]\\}-{\\pi E[Y^0\\mid D = 1]+(1-\\pi)E[Y^0\\mid D = 0]}\n\\end{align}\n\\]\n\\(\\pi\\)は処置を受けた人の割合である。第一式は\\(ATT\\)の定義から、 処置を受けた人、受けていない人の処置効果に分会しているということになる。\nこれを見ると、単純な差にはATEだけでなく２つの項がある。 まずは次の式は選択バイアスと呼ばれるものである。 処置を受けていない場合の平均的なアウトカムが、 処置を受けているかどうかで変わるのでこのように呼ばれる。\n\\[\nE[Y^0\\mid D=1]-E[Y^0\\mid D=0]\n\\]\n次の式は異質性のある処置効果にともなうバイアスである。\n\\[\n(1-\\pi)(ATT - ATU)\n\\]\n上記２つの項は反実仮想を含んでいるため計算することが出来ない。 いずれにせよバイアスを含んでいることはわかる。",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch04 潜在アウトカム因果モデル"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#独立性の仮定",
    "href": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#独立性の仮定",
    "title": "ch04 潜在アウトカム因果モデル",
    "section": "",
    "text": "SDOを使ってATEを推定するにあたり最も信頼できるのは、 処置を受けるのかどうかとアウトカムが独立であるときである。\n\\[\n(Y^1, Y^0) \\mathop{\\perp\\!\\!\\!\\!\\perp} D\n\\]\nランダム化によって実現したいのは、次の状態である。\n\\[\n\\begin{align}\nE[Y^1\\mid D=1]-E[Y^1\\mid D=0]&=0\\\\\nE[Y^0\\mid D=1]-E[Y^0\\mid D=0]&=0\n\\end{align}\n\\] ランダム化により明らかに選択バイアスを除くことができる。\nまた異質性のある処置効果についても取り除くことができる。\n\\[\nATT-ATU=E[Y^1\\mid D=1]-E[Y^0\\mid  D=1]-E[Y^1\\mid D=0]+E[Y^0\\mid D=0]\n\\]",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch04 潜在アウトカム因果モデル"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#sutva",
    "href": "contents/books/02_因果推論ミックステープ/ch04_潜在アウトカム因果モデル.html#sutva",
    "title": "ch04 潜在アウトカム因果モデル",
    "section": "",
    "text": "Rubinはこの種の計算をいつくかの仮定が伴うとして、その仮定をSUTVAとまとめています。具体的には次の３つです。\n\n処置の均質性\n処置の外部性がないこと\n\nユニット間で影響しあわないこと\n\n一般均衡効果がないこと",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch04 潜在アウトカム因果モデル"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch06_回帰不連続デザイン.html",
    "href": "contents/books/02_因果推論ミックステープ/ch06_回帰不連続デザイン.html",
    "title": "ch06 回帰不連続デザイン",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch06 回帰不連続デザイン"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch08_パネルデータ.html",
    "href": "contents/books/02_因果推論ミックステープ/ch08_パネルデータ.html",
    "title": "ch08 パネルデータ",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch08 パネルデータ"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/ch10_合成コントロール法.html",
    "href": "contents/books/02_因果推論ミックステープ/ch10_合成コントロール法.html",
    "title": "ch10 合成コントロール",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "ch10 合成コントロール"
    ]
  },
  {
    "objectID": "contents/books/02_因果推論ミックステープ/index.html",
    "href": "contents/books/02_因果推論ミックステープ/index.html",
    "title": "因果推論ミックステープ",
    "section": "",
    "text": "1 はじめに\n\n『因果推論ミックステープ』の読書ノートです\n因果推論の初中級レベルとのことです\n本書をきっかけとして計量経済学についても学習してもらいたいとのことです\n本書はサイバーエージェントのゼミが誕生したとのこと\n\n非常にすごいことだと思います\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "因果推論ミックステープ",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch02_プログラミングの基礎.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch02_プログラミングの基礎.html",
    "title": "ch02 プログラミングの基礎",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch02 プログラミングの基礎"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch04_母数の推定のシミュレーション.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch04_母数の推定のシミュレーション.html",
    "title": "ch04 母数の推定",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch04  母数の推定"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch06_適切な検定のためのサンプルサイズ設計.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch06_適切な検定のためのサンプルサイズ設計.html",
    "title": "ch06 サンプルサイズ",
    "section": "",
    "text": "統計的検定において、エラー確率を管理するには、 次のことを検定する前に決めておく必要がある。\n\n確率モデルと検定する母数\nその母数についての帰無仮説\n\\(\\alpha\\)と\\(\\beta\\)\n計算する検定統計量\nサンプルサイズ",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch06 サンプルサイズ"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch06_適切な検定のためのサンプルサイズ設計.html#標本のt検定のサンプルサイズ",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/ch06_適切な検定のためのサンプルサイズ設計.html#標本のt検定のサンプルサイズ",
    "title": "ch06 サンプルサイズ",
    "section": "2.1 1標本のt検定のサンプルサイズ",
    "text": "2.1 1標本のt検定のサンプルサイズ\n\n小さめのサンプルサイズnを適当に決める\n見積もった効果量\\(\\delta_0\\)と\\(n\\)から非心度\\(\\lambda\\)を計算する\n帰無頒布の自由度を求めて\\(\\alpha\\)の臨界値を計算する\n臨界値と非心度からタイプ２エラーを求める\nタイプ２が\\(\\beta\\)を下回っていればそこで終了しそうでないなあ\\(n\\)を増やす\n\n\n\nCode\nt2e_ttest &lt;- function(alpah, delta, n) {\n  df &lt;- n - 1\n  lambda &lt;- delta * sqrt(n)\n  cv &lt;- qt(p = 1 - alpha / 2, df = df)\n  type2error &lt;- pt(q = cv, df = df, ncp = lambda)\n  return(type2error)\n}\n\n# 設定と準備\nalpha &lt;- .05\nbeta &lt;- .2\ndelta &lt;- .5\n\niter &lt;- 1000\n\nfor (n in 5:iter) {\n  type2error &lt;- t2e_ttest(alpha, delta, n)\n  if (type2error &lt; beta) {\n    break\n  }\n}\n\nprint(n)\n#&gt; [1] 34\n\nprint(type2error)\n#&gt; [1] 0.1922233",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "ch06 サンプルサイズ"
    ]
  },
  {
    "objectID": "contents/books/03_数値シミュレーションで身につける統計の仕組み/index.html",
    "href": "contents/books/03_数値シミュレーションで身につける統計の仕組み/index.html",
    "title": "数値シミュレーションで身につける統計の仕組み",
    "section": "",
    "text": "1 はじめに\n\n『統計シミュレーションで身につける統計の仕組み』の読書ノートです\n誤植が多い図書なので次のサポートページをよくみること\nサポートページ\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "R",
      "数値シミュレーションで学ぶ統計の仕組み",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "この枠組みでは因果推論を「欠測データの問題」として扱う．\n\n\n20人の学生を対象にした数学の試験結果を考える． 10人は補習講義を受けており，試験結果から補習講義の効果を知りたいとする． しかし，補習講義を受けた人は受けていない場合の結果はなく，その逆もまた然りである． つまり，補習講義の有無による個々人の試験結果の違いの分布を得ることは出来ない．\n補習講義の有無がランダムに割り付けられている場合には評価が出来るものの， 補習講義を受けるの者はそれまでの成績が悪い場合である．\n仮にすべての生徒に対して補修講義の有無が観測されているならば 単に平均値を比較することでよい．．\n\n\n\n補習講義を受けたどうかを「補修講義を受ける確率」として考えるとことで 補習講義の有無をまとめて表現することが可能となる． 具体的には\\(Y_i(0)\\)を補習講義を受けていない場合の潜在的結果， \\(Y_i(1)\\)を補習講義を受けた場合の潜在的結果といして，次の式でモデル化する.\n\\[\n\\begin{align}\nY_i &= (1-T_i)Y_i(0) + T_iY_i(1) \\\\\nT_i &\\in {0, 1}\n\\end{align}\n\\]\n\n\nCode\npath   &lt;- \"./causality/data02.csv\"\ndata02 &lt;- read_csv(path, locale = locale(encoding = \"UTF-8\"), show_col_types = FALSE)\ndata02 |&gt; summary()\n#&gt;        x1              y3             t1            y0              y1       \n#&gt;  Min.   :58.00   Min.   :61.0   Min.   :0.0   Min.   :72.00   Min.   :61.00  \n#&gt;  1st Qu.:76.25   1st Qu.:75.0   1st Qu.:0.0   1st Qu.:75.00   1st Qu.:74.25  \n#&gt;  Median :83.50   Median :76.5   Median :0.0   Median :77.00   Median :75.50  \n#&gt;  Mean   :81.95   Mean   :76.6   Mean   :0.3   Mean   :77.79   Mean   :73.83  \n#&gt;  3rd Qu.:87.25   3rd Qu.:80.0   3rd Qu.:1.0   3rd Qu.:80.00   3rd Qu.:76.75  \n#&gt;  Max.   :96.00   Max.   :87.0   Max.   :1.0   Max.   :87.00   Max.   :80.00  \n#&gt;                                               NA's   :6       NA's   :14     \n#&gt;       y0t            y1t       \n#&gt;  Min.   :52.0   Min.   :61.00  \n#&gt;  1st Qu.:69.5   1st Qu.:79.25  \n#&gt;  Median :75.0   Median :84.50  \n#&gt;  Mean   :73.8   Mean   :83.85  \n#&gt;  3rd Qu.:78.5   3rd Qu.:89.00  \n#&gt;  Max.   :87.0   Max.   :97.00  \n#&gt; \n\n\ny0, y1が条件付きの値，つまり実際の結果であり，y0t, y1tがもしどちらも観測可能である場合の結果である． y0t, y1tが観測できるときにはt検定などで比較が可能である.\n\n\n\n個体因果結果とは\\(\\tau_i = Y_i(1) - Y_i(0)\\)であり，つまりは個体差である. 通常はどちらか一方しか観測できないのでこの値を算出することは出来ない. また推測も出来ない.\n\n\nCode\nwith(data02, {\n    # 個体ごとの因果効果(理想的)\n    print(y1t - y0t)\n})\n#&gt;  [1]  8  9 10 13  9  9 11 12 10 10  9 10 10  9 12 12 10  9 10  9\n\n\n\n\n\n個体因果効果は観測も推測も出来ないので， 個体の母集団に対する平均的な因果効果を考えることになる． つまり正しく推定が行えていないことがわかる.\n\n\nCode\nwith(data02, {\n    # 母集団への平均的な効果の推定\n    print(mean(y1t, na.rm = TRUE) - mean(y0t, na.rm = TRUE))\n    # しかし，この値をこのまま実際の結果に当てはめると結果が大きく異なる\n    print(mean(y1, na.rm = TRUE) - mean(y0, na.rm = TRUE))\n})\n#&gt; [1] 10.05\n#&gt; [1] -3.952381\n\n\n\n\n\nある処置が取られた対象に対してどれくらい影響があったのかについて知る． これは処置群の平均処置効果と呼ばれるものである. 次の式で定義される. これは処置を受けた人の処置を受けていなかったときの結果\\(E[Y_i(0)|T_i=1\\)が含まれている． このような値を推定することも出来るのが統計的因果推論の面白さになる.\n\\[\n\\tau_{\\text{ATT}}=E[Y_i(1)-Y_i(0)|T_i=1]=E[Y_i(1)|T_i=1]-E[Y_i(0)|T_i=1]\n\\]\nこれを計算してみるとかなりよい値を推定する.\n\n\nCode\nwith(subset(data02, t1==1), {\n    print(mean(y1t)-mean(y0t))\n})\n#&gt; [1] 9.333333\n\n\n\n\n\n処置効果２におけるナイーブな推定は大きく外れていた． このような場合交絡因子を疑うことが重要な点である． 今回の場合には，補習講義を受けたかどうかは入学時の成績が考慮されており， この部分が交絡している．\n処置と結果変数のどちらにも影響する変数が交絡因子である． たとえば，補習授業の効果を知りたいとき，補習講義を受けるかどうかがテストの点で決まり， 効果を測るときにもテストの点を使うとすると，学力が交絡しており，補習講義の効果を適切に推定することが困難になる.\n平均処置効果を適切似推定するには２つの比較可能な集団を作る必要がある． 比較可能な集団とは共変量の分布は異なるが，原因変数と結果変数だけ異なる である．\n\n\n\n観測されない交絡を統制するにはどうすればよいのか． 理想はランダムサンプリングでの割り付けである. 一般に無作為割り付けがされている実験を実験研究といい，無作為割り付けされていない実験を観察研究という．\n\n\n\n内的妥当性とはある限られた条件での因果である．外的妥当性とは条件が変更された場合における妥当性である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#具体例",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#具体例",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "20人の学生を対象にした数学の試験結果を考える． 10人は補習講義を受けており，試験結果から補習講義の効果を知りたいとする． しかし，補習講義を受けた人は受けていない場合の結果はなく，その逆もまた然りである． つまり，補習講義の有無による個々人の試験結果の違いの分布を得ることは出来ない．\n補習講義の有無がランダムに割り付けられている場合には評価が出来るものの， 補習講義を受けるの者はそれまでの成績が悪い場合である．\n仮にすべての生徒に対して補修講義の有無が観測されているならば 単に平均値を比較することでよい．．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#理論",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#理論",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "補習講義を受けたどうかを「補修講義を受ける確率」として考えるとことで 補習講義の有無をまとめて表現することが可能となる． 具体的には\\(Y_i(0)\\)を補習講義を受けていない場合の潜在的結果， \\(Y_i(1)\\)を補習講義を受けた場合の潜在的結果といして，次の式でモデル化する.\n\\[\n\\begin{align}\nY_i &= (1-T_i)Y_i(0) + T_iY_i(1) \\\\\nT_i &\\in {0, 1}\n\\end{align}\n\\]\n\n\nCode\npath   &lt;- \"./causality/data02.csv\"\ndata02 &lt;- read_csv(path, locale = locale(encoding = \"UTF-8\"), show_col_types = FALSE)\ndata02 |&gt; summary()\n#&gt;        x1              y3             t1            y0              y1       \n#&gt;  Min.   :58.00   Min.   :61.0   Min.   :0.0   Min.   :72.00   Min.   :61.00  \n#&gt;  1st Qu.:76.25   1st Qu.:75.0   1st Qu.:0.0   1st Qu.:75.00   1st Qu.:74.25  \n#&gt;  Median :83.50   Median :76.5   Median :0.0   Median :77.00   Median :75.50  \n#&gt;  Mean   :81.95   Mean   :76.6   Mean   :0.3   Mean   :77.79   Mean   :73.83  \n#&gt;  3rd Qu.:87.25   3rd Qu.:80.0   3rd Qu.:1.0   3rd Qu.:80.00   3rd Qu.:76.75  \n#&gt;  Max.   :96.00   Max.   :87.0   Max.   :1.0   Max.   :87.00   Max.   :80.00  \n#&gt;                                               NA's   :6       NA's   :14     \n#&gt;       y0t            y1t       \n#&gt;  Min.   :52.0   Min.   :61.00  \n#&gt;  1st Qu.:69.5   1st Qu.:79.25  \n#&gt;  Median :75.0   Median :84.50  \n#&gt;  Mean   :73.8   Mean   :83.85  \n#&gt;  3rd Qu.:78.5   3rd Qu.:89.00  \n#&gt;  Max.   :87.0   Max.   :97.00  \n#&gt; \n\n\ny0, y1が条件付きの値，つまり実際の結果であり，y0t, y1tがもしどちらも観測可能である場合の結果である． y0t, y1tが観測できるときにはt検定などで比較が可能である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果1個体因果結果",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果1個体因果結果",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "個体因果結果とは\\(\\tau_i = Y_i(1) - Y_i(0)\\)であり，つまりは個体差である. 通常はどちらか一方しか観測できないのでこの値を算出することは出来ない. また推測も出来ない.\n\n\nCode\nwith(data02, {\n    # 個体ごとの因果効果(理想的)\n    print(y1t - y0t)\n})\n#&gt;  [1]  8  9 10 13  9  9 11 12 10 10  9 10 10  9 12 12 10  9 10  9",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果2平均処置効果",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果2平均処置効果",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "個体因果効果は観測も推測も出来ないので， 個体の母集団に対する平均的な因果効果を考えることになる． つまり正しく推定が行えていないことがわかる.\n\n\nCode\nwith(data02, {\n    # 母集団への平均的な効果の推定\n    print(mean(y1t, na.rm = TRUE) - mean(y0t, na.rm = TRUE))\n    # しかし，この値をこのまま実際の結果に当てはめると結果が大きく異なる\n    print(mean(y1, na.rm = TRUE) - mean(y0, na.rm = TRUE))\n})\n#&gt; [1] 10.05\n#&gt; [1] -3.952381",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果3処置群の平均処置効果",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#処置効果3処置群の平均処置効果",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "ある処置が取られた対象に対してどれくらい影響があったのかについて知る． これは処置群の平均処置効果と呼ばれるものである. 次の式で定義される. これは処置を受けた人の処置を受けていなかったときの結果\\(E[Y_i(0)|T_i=1\\)が含まれている． このような値を推定することも出来るのが統計的因果推論の面白さになる.\n\\[\n\\tau_{\\text{ATT}}=E[Y_i(1)-Y_i(0)|T_i=1]=E[Y_i(1)|T_i=1]-E[Y_i(0)|T_i=1]\n\\]\nこれを計算してみるとかなりよい値を推定する.\n\n\nCode\nwith(subset(data02, t1==1), {\n    print(mean(y1t)-mean(y0t))\n})\n#&gt; [1] 9.333333",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#交絡因子",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#交絡因子",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "処置効果２におけるナイーブな推定は大きく外れていた． このような場合交絡因子を疑うことが重要な点である． 今回の場合には，補習講義を受けたかどうかは入学時の成績が考慮されており， この部分が交絡している．\n処置と結果変数のどちらにも影響する変数が交絡因子である． たとえば，補習授業の効果を知りたいとき，補習講義を受けるかどうかがテストの点で決まり， 効果を測るときにもテストの点を使うとすると，学力が交絡しており，補習講義の効果を適切に推定することが困難になる.\n平均処置効果を適切似推定するには２つの比較可能な集団を作る必要がある． 比較可能な集団とは共変量の分布は異なるが，原因変数と結果変数だけ異なる である．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#無作為抽出と無作為割り付け",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#無作為抽出と無作為割り付け",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "観測されない交絡を統制するにはどうすればよいのか． 理想はランダムサンプリングでの割り付けである. 一般に無作為割り付けがされている実験を実験研究といい，無作為割り付けされていない実験を観察研究という．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#内的妥当性と外的妥当性",
    "href": "contents/books/05_統計的因果推論の理論と実際/102_潜在的結果変数の枠組み.html#内的妥当性と外的妥当性",
    "title": "潜在的結果変数の枠組み",
    "section": "",
    "text": "内的妥当性とはある限られた条件での因果である．外的妥当性とは条件が変更された場合における妥当性である.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "潜在的結果変数の枠組み"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html",
    "title": "標準誤差と信頼区間",
    "section": "",
    "text": "本章では標準誤差とは何か，信頼区間とはなにか，具体的に検討したのち２標本t検定の メカニズムについて解説する.\n無作為に割り付けされている場合には，線形回帰分析で定量化しt.testで検定が行える．\n\n\n標本平均が従う分布の標準偏差である. いつも思うけどこれくらいの計算も出来なくなるのではないか，という一末の不安がある． これは何度も計算すれば解消さるのだろうか・・・\n\n\n\n信頼区間について納得できる説明がされている．t.testのやり方について説明されている. そういえば，\\(t\\)検定の場合，自由度は\\(n-1\\)とするけど，標本平均と標本分散を使っているので自由度\\(n-2\\)のような気がするのだけど，どうなのだろうか？",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "標準誤差と信頼区間"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html#標準誤差",
    "href": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html#標準誤差",
    "title": "標準誤差と信頼区間",
    "section": "",
    "text": "標本平均が従う分布の標準偏差である. いつも思うけどこれくらいの計算も出来なくなるのではないか，という一末の不安がある． これは何度も計算すれば解消さるのだろうか・・・",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "標準誤差と信頼区間"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html#信頼区間",
    "href": "contents/books/05_統計的因果推論の理論と実際/104_推測統計の基礎：標準誤差と信頼区間.html#信頼区間",
    "title": "標準誤差と信頼区間",
    "section": "",
    "text": "信頼区間について納得できる説明がされている．t.testのやり方について説明されている. そういえば，\\(t\\)検定の場合，自由度は\\(n-1\\)とするけど，標本平均と標本分散を使っているので自由度\\(n-2\\)のような気がするのだけど，どうなのだろうか？",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "標準誤差と信頼区間"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/106_図で理解する重回帰モデルの基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/106_図で理解する重回帰モデルの基礎.html",
    "title": "重回帰分析の基礎",
    "section": "",
    "text": "チョコレートの消費量とノーベル章受賞者の数が回帰分析により有意な関係があると 算出されたとき，因果推論を考えるにはまず交絡因子の探索が必要となる． つまり，チョコレートの消費量とノーベル章受賞者のどちらにも影響を与えると考えられる 変数を探すことになる．\nそのような変数が明示できなければ，なんらかの因果関係があると解釈を進めていくことが出来る.\n交絡因子についての検討はデータ分析担当者の責任のもと行うべきであるとともに， 後に検証可能な形で分析結果を共有してくことが必須である.\n重回帰モデルを考えたときある説明変数Aと別の説明変数Bに相関があるときには， AとBが交絡していることとなり， AとBの相関を除いた上で回帰分析することが望ましい． 共分散分析ではそのような場合の因果推論に役に立つ．\n重回帰分析における回帰係数とは当該変数以外で説明できる当該変数の分散を除いた， 当該変数の分散によるものである． 他の変数から受ける影響を統計的に除外， つまり交絡を取り除いて推定してると考えることが出来る.\n\n\nCode\n# 共分散分析\n# 重回帰分析は残差に対して単回帰を繰り返すことと同じ\n\ndata03 &lt;- read_csv(\"./causality/data03.csv\", locale = locale(encoding = \"UTF-8\"))\n#&gt; Rows: 20 Columns: 5\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; dbl (5): x1, y3, t1, y0t, y1t\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nmodel1 &lt;- lm(t1 ~ x1, data = data03)\net1    &lt;- resid(model1)\nmodel2 &lt;- lm(y3 ~ et1, data = data03)\nsummary(model2)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y3 ~ et1, data = data03)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -8.409 -4.328 -0.250  4.828  6.910 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)   77.250      1.236  62.475  &lt; 2e-16 ***\n#&gt; et1            9.816      2.758   3.559  0.00224 ** \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.53 on 18 degrees of freedom\n#&gt; Multiple R-squared:  0.413,  Adjusted R-squared:  0.3804 \n#&gt; F-statistic: 12.67 on 1 and 18 DF,  p-value: 0.002242\n\n\n\n\n処置の割り付けが無作為化されてていても， 実際にはたまたま共変量がうまくバランスしていないことがある． そのような時にも共変量を考慮した モデル化を行い共分散分析を行うことで，バランシングできることがある．\nまた処置の無作為割り付け自体は成功しており，交絡因子を気にする必要がない場合でも， 結果辺陬の変動を説明することに寄与する変数があれば，共分散分析のモデルに取り入れることが望ましい. 不偏性をもったまま決定係数を減らすことが出来る.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重回帰分析の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/106_図で理解する重回帰モデルの基礎.html#実験研究における共分散分析の活用",
    "href": "contents/books/05_統計的因果推論の理論と実際/106_図で理解する重回帰モデルの基礎.html#実験研究における共分散分析の活用",
    "title": "重回帰分析の基礎",
    "section": "",
    "text": "処置の割り付けが無作為化されてていても， 実際にはたまたま共変量がうまくバランスしていないことがある． そのような時にも共変量を考慮した モデル化を行い共分散分析を行うことで，バランシングできることがある．\nまた処置の無作為割り付け自体は成功しており，交絡因子を気にする必要がない場合でも， 結果辺陬の変動を説明することに寄与する変数があれば，共分散分析のモデルに取り入れることが望ましい. 不偏性をもったまま決定係数を減らすことが出来る.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "重回帰分析の基礎"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html",
    "title": "最小二乗法の仮定と診断2",
    "section": "",
    "text": "ここで紹介する家庭は次の三つである.\n\n完全な多重共線性が存在しない\n誤差項の分散均一の仮定\n誤差項の世紀性の仮定\n\n\n\n変数X1とX2との相関が強くなることで，パラメータの不偏性には影響ないが， 相関が強くなるにつれて標準誤差が大きくなることになる． 結果的に検定が棄却されにくくなる．\n\n\nVIFによる診断がある.\n\\[\n\\text{VIF}=\\frac{1}{1-R_j^2}\n\\]\nここで\\(R_j^2\\)は説明変数\\(j\\)を除いた他の説明変数で，説明変数\\(j\\)を重回帰分析したときの， 決定係数である．決定係数が大きいとき，つまり残りの変数で説明できるときには，VIFが大きくなる． VIFが大きいことは多重共線性の存在を疑うことになる.\n交絡因子として取り込んだ共変量同士で多重共線性があっても， 着目した変数との多重共線性がなければ，偏回帰係数の推定には影響がない． ただし共変量の回帰係数は解釈可能ではないということに注意する.\n\n\n\n\nたとえば，年収と食費の関係を調べたとする． 年収が少ない場合には食費に使える予算はほぼ一定であること対して， 年収が増えると食費に使う予算のばらつきが大きくなったとする． これは年収が増えることで食費に対する贅沢が可能となり， 質素派と贅沢派という個人の属性が影響し分散が大きくなり， 年収で食費を回帰した場合には分散不均一の仮定を満たさなくなる．\n不均一分散はそれ自体が発見であるし，様々なモデルが既に開発されている．\n\n\n分散が不均一であっても推定量の不偏性には影響しない．一方で標準誤差が大きくなる. これにより最小二乗推定量が最良線形不偏推定量を満たさないことになる.\n\n\n\nブルーシュ・ペイガン検定を行うことでよい. Rではlmtest::bptestを使うことで出来る.\n\n\n\n説明変数が分散に与える影響がわかっている場合，加重最小二乗法を使うことで， 不均一分散に対処することが可能である.\n関数系の推定は作法があるのでそれに従うこと.\n\n\nCode\ndata08b &lt;- read_csv(\"./causality/data08b.csv\", show_col_types = FALSE)\nsummary(data08b)\n#&gt;        y1                y2                 x1           \n#&gt;  Min.   :-2.4949   Min.   :-8.17685   Min.   :-0.997053  \n#&gt;  1st Qu.:-0.5390   1st Qu.:-0.52864   1st Qu.:-0.477269  \n#&gt;  Median : 0.1584   Median :-0.02624   Median :-0.008905  \n#&gt;  Mean   : 0.1779   Mean   : 0.18978   Mean   : 0.012520  \n#&gt;  3rd Qu.: 0.8890   3rd Qu.: 0.79394   3rd Qu.: 0.525134  \n#&gt;  Max.   : 3.3866   Max.   :10.32879   Max.   : 0.997758\n\n\n\n\nCode\n# 加重最小二乗法\nmodel5 &lt;- lm(y2 ~ x1, weights = 1/ exp(1.5 * x1), data = data08b)\nmodel5\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y2 ~ x1, data = data08b, weights = 1/exp(1.5 * x1))\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;      0.1689       0.4748\n\n\nなお関数系は一般に知ることが出来ないので， 通常は下記の実行可能一般化最小二乗法における関数系の推定手順を用いた分析が行われている。\n\n\nCode\nmodel4    &lt;- lm(y2 ~ x1, data = data08b)\nlog_resid &lt;- log(residuals(model4) ** 2)\nmodel6 &lt;- lm(log_resid ~ x1, data = data08b)\nhhat1  &lt;- predict(model6)\nhhat2  &lt;- exp(hhat1)\nmodel7 &lt;- lm(y2 ~ x1, weights = 1/ hhat2, data = data08b)\nsummary(model7)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y2 ~ x1, data = data08b, weights = 1/hhat2)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.9655 -1.2527 -0.0158  1.2804  5.8643 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.11808    0.04006   2.947  0.00328 ** \n#&gt; x1           0.38348    0.05509   6.961  6.1e-12 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.883 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.04631,    Adjusted R-squared:  0.04535 \n#&gt; F-statistic: 48.46 on 1 and 998 DF,  p-value: 6.102e-12\n\n\n\n\n\n\nここまでの５つの仮定が満たされていればこれは通常問題にならない． 一般に誤差項の正規性は問題になるが， 誤差項に正規性があるのかどうかは， 最小二乗推定量が最良線形不偏推定量であるのかどうかには影響がない．\n本来正規性について気にすべき問題は， 誤差項でなくパラメータの推定量の正規性である． パラメータの推定値は標本平均であるため中心極限定理からデータサイズに依存して， 正規分布となる．一般にはデータサイズが３０以上で正規分布として扱うことができるとされている．\n一方で誤差項の正規性はそれなりに意味を持つ． つまり，すべての共変量をモデルに取り込むことは現実的ではなく， 多くの場合には誤差項として表現することになる． 誤差項で様々な共変量をまとめて表現することにあるため， 誤差項が正規性を持つことは適切なモデリングが行えているのかの重要な指標となる.\n次の例では，データがベータ分布に従っている場合，データが正規分布に従っている場合の ２つの例で回帰分析を行い，その際の誤差項の分布を比べてみる. また誤差項をジャック・ベラの正規性検定をおこなう. (ライブラリが入らずできななかった)\nベータ分布に従うものは偏りが大きいことがグラフから見て取れる.\n\n\nCode\ndata08c &lt;- read_csv(\"./causality/data08c.csv\", show_col_types = FALSE)\nmodel1  &lt;- lm(y1 ~ x1, data = data08c)\nresid1  &lt;- residuals(model1)\nmodel2  &lt;- lm(y2 ~ x1, data = data08c)\nresid2  &lt;- residuals(model2)\n\n\n\n\nCode\nhist(resid1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nhist(resid2)",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断2"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#完全な多重共線性が存在しない",
    "href": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#完全な多重共線性が存在しない",
    "title": "最小二乗法の仮定と診断2",
    "section": "",
    "text": "変数X1とX2との相関が強くなることで，パラメータの不偏性には影響ないが， 相関が強くなるにつれて標準誤差が大きくなることになる． 結果的に検定が棄却されにくくなる．\n\n\nVIFによる診断がある.\n\\[\n\\text{VIF}=\\frac{1}{1-R_j^2}\n\\]\nここで\\(R_j^2\\)は説明変数\\(j\\)を除いた他の説明変数で，説明変数\\(j\\)を重回帰分析したときの， 決定係数である．決定係数が大きいとき，つまり残りの変数で説明できるときには，VIFが大きくなる． VIFが大きいことは多重共線性の存在を疑うことになる.\n交絡因子として取り込んだ共変量同士で多重共線性があっても， 着目した変数との多重共線性がなければ，偏回帰係数の推定には影響がない． ただし共変量の回帰係数は解釈可能ではないということに注意する.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断2"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#誤差項の分散均一性",
    "href": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#誤差項の分散均一性",
    "title": "最小二乗法の仮定と診断2",
    "section": "",
    "text": "たとえば，年収と食費の関係を調べたとする． 年収が少ない場合には食費に使える予算はほぼ一定であること対して， 年収が増えると食費に使う予算のばらつきが大きくなったとする． これは年収が増えることで食費に対する贅沢が可能となり， 質素派と贅沢派という個人の属性が影響し分散が大きくなり， 年収で食費を回帰した場合には分散不均一の仮定を満たさなくなる．\n不均一分散はそれ自体が発見であるし，様々なモデルが既に開発されている．\n\n\n分散が不均一であっても推定量の不偏性には影響しない．一方で標準誤差が大きくなる. これにより最小二乗推定量が最良線形不偏推定量を満たさないことになる.\n\n\n\nブルーシュ・ペイガン検定を行うことでよい. Rではlmtest::bptestを使うことで出来る.\n\n\n\n説明変数が分散に与える影響がわかっている場合，加重最小二乗法を使うことで， 不均一分散に対処することが可能である.\n関数系の推定は作法があるのでそれに従うこと.\n\n\nCode\ndata08b &lt;- read_csv(\"./causality/data08b.csv\", show_col_types = FALSE)\nsummary(data08b)\n#&gt;        y1                y2                 x1           \n#&gt;  Min.   :-2.4949   Min.   :-8.17685   Min.   :-0.997053  \n#&gt;  1st Qu.:-0.5390   1st Qu.:-0.52864   1st Qu.:-0.477269  \n#&gt;  Median : 0.1584   Median :-0.02624   Median :-0.008905  \n#&gt;  Mean   : 0.1779   Mean   : 0.18978   Mean   : 0.012520  \n#&gt;  3rd Qu.: 0.8890   3rd Qu.: 0.79394   3rd Qu.: 0.525134  \n#&gt;  Max.   : 3.3866   Max.   :10.32879   Max.   : 0.997758\n\n\n\n\nCode\n# 加重最小二乗法\nmodel5 &lt;- lm(y2 ~ x1, weights = 1/ exp(1.5 * x1), data = data08b)\nmodel5\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y2 ~ x1, data = data08b, weights = 1/exp(1.5 * x1))\n#&gt; \n#&gt; Coefficients:\n#&gt; (Intercept)           x1  \n#&gt;      0.1689       0.4748\n\n\nなお関数系は一般に知ることが出来ないので， 通常は下記の実行可能一般化最小二乗法における関数系の推定手順を用いた分析が行われている。\n\n\nCode\nmodel4    &lt;- lm(y2 ~ x1, data = data08b)\nlog_resid &lt;- log(residuals(model4) ** 2)\nmodel6 &lt;- lm(log_resid ~ x1, data = data08b)\nhhat1  &lt;- predict(model6)\nhhat2  &lt;- exp(hhat1)\nmodel7 &lt;- lm(y2 ~ x1, weights = 1/ hhat2, data = data08b)\nsummary(model7)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = y2 ~ x1, data = data08b, weights = 1/hhat2)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.9655 -1.2527 -0.0158  1.2804  5.8643 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  0.11808    0.04006   2.947  0.00328 ** \n#&gt; x1           0.38348    0.05509   6.961  6.1e-12 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 1.883 on 998 degrees of freedom\n#&gt; Multiple R-squared:  0.04631,    Adjusted R-squared:  0.04535 \n#&gt; F-statistic: 48.46 on 1 and 998 DF,  p-value: 6.102e-12",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断2"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#誤差項の正規性",
    "href": "contents/books/05_統計的因果推論の理論と実際/108_最小二乗法による重回帰モデルの仮定と診断2.html#誤差項の正規性",
    "title": "最小二乗法の仮定と診断2",
    "section": "",
    "text": "ここまでの５つの仮定が満たされていればこれは通常問題にならない． 一般に誤差項の正規性は問題になるが， 誤差項に正規性があるのかどうかは， 最小二乗推定量が最良線形不偏推定量であるのかどうかには影響がない．\n本来正規性について気にすべき問題は， 誤差項でなくパラメータの推定量の正規性である． パラメータの推定値は標本平均であるため中心極限定理からデータサイズに依存して， 正規分布となる．一般にはデータサイズが３０以上で正規分布として扱うことができるとされている．\n一方で誤差項の正規性はそれなりに意味を持つ． つまり，すべての共変量をモデルに取り込むことは現実的ではなく， 多くの場合には誤差項として表現することになる． 誤差項で様々な共変量をまとめて表現することにあるため， 誤差項が正規性を持つことは適切なモデリングが行えているのかの重要な指標となる.\n次の例では，データがベータ分布に従っている場合，データが正規分布に従っている場合の ２つの例で回帰分析を行い，その際の誤差項の分布を比べてみる. また誤差項をジャック・ベラの正規性検定をおこなう. (ライブラリが入らずできななかった)\nベータ分布に従うものは偏りが大きいことがグラフから見て取れる.\n\n\nCode\ndata08c &lt;- read_csv(\"./causality/data08c.csv\", show_col_types = FALSE)\nmodel1  &lt;- lm(y1 ~ x1, data = data08c)\nresid1  &lt;- residuals(model1)\nmodel2  &lt;- lm(y2 ~ x1, data = data08c)\nresid2  &lt;- residuals(model2)\n\n\n\n\nCode\nhist(resid1)\n\n\n\n\n\n\n\n\n\n\n\nCode\nhist(resid2)",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "最小二乗法の仮定と診断2"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html",
    "title": "傾向スコア",
    "section": "",
    "text": "実験研究がよいのは，処置の有無を無作為に割り付けられることで， 共変量の分布が確率的に同じになることにより， 交絡が発生しないことである．\n観察研究における因果推論が難しいのは，処置群と統制群が傾向的に異なっている可能性のためである．\n傾向スコアによる分析は「準実験」と呼ばれる． 処置の無作為な割り付けが出来ない場合でも， 割り付けや比較対象の集団について研究者が何らかの統制を行うことで 無作為化をマネすデザインのことをいう．\n傾向スコアマッチングとは傾向スコアが同じペアを同一だと見なし， 処置群の処置効果を測るための手法とするものである. 共変量で調整されていれば，結果の違いは，共変量以外の要因からなり， それは主として処置効果からくるものと考えられる． この考え方の妥当性は，これまでと同じように条件付き独立性が満たされるという考え方になる.\n\\[\n\\{Y(1), Y(0)\\} \\perp T |X\n\\] 上記のマッチングがうまく出来れば, 本来観測不可能である「処置群の処置効果(ATT)」を推定することが可能となる．傾向スコアによりATEを推定するには，スコアごとの層別解析をなんらかの方法で平均すればよい.\n\\[\n\\tau_{ATT}=E[Y_i(1)-Y_i(0)|T_i=1]=E[Y_i(1)|T_i=1]-E[T_i(0)|T_i=1]\n\\] なお，これまで調べていたのは「処置の平均効果(ATE)」である．これは，処置と結果変数を独立となるようにすることで，推定が行える.\n\\[\n\\tau_{ATE}=E[Y_i(1)-Y_i(0)|T_i]=E[Y_i(1)|T_i]-E[T_i(0)|T_i]=E[Y_i(1)]-E[T_i(0)]\n\\]\nとはいえ，まずはATTとATEは異なるということをまずは知っておく必要がある．\n\n\n共変量が単変量の場合には共分散分析により解析することが出来た． しかし，通常共変量は多変量である．これに対して傾向スコアが開発された． 傾向スコアとはバランシングスコアの１つであり， 観測される共変量\\(X\\)の関数\\(b(X)\\)が与えられたときの\\(X\\)の条件つき分布が， 処置群と統制群において同じとなる関数である.\nとにかくバランシングスコアを見るようにして，バランシングスコアが悪いときに，マッチングを使ったATT推定を行ってはならない. マッチしたからといってどの程度マッチに意味があるかは，別であるよね，という話だと理解した．\n\n\nCode\ndata10a &lt;- read_csv(\"./causality/data10a.csv\", show_col_types = FALSE)\n\n# 平均処置効果\nwith(data10a, {\n    print(mean(y1t) - mean(y0t))\n})\n#&gt; [1] 9.95\n\n# 処置群の平均処置効果\nwith(data10a, {\n    print(mean(y1t[t1==1])-mean(y0t[t1==1]))   \n})\n#&gt; [1] 9.090909\n\n# ナイーブな平均処置効果\nwith(data10a, {\n    print(mean(y1t[t1==1])-mean(y0t[t1==0]))   \n})\n#&gt; [1] 17.39394\n\n\nナイーブな平均処置結果が使えないことがよくわかる.\nこのような結果が起こるのは共変量の分布が異なることによる. 処置群と統制群で平均値や分散が同じになることを指して「バランスが取れている」ということになる．\n\n\nCode\ndata10a |&gt; \n    group_by(t1) |&gt;\n    summarise(quantile = list(stack(quantile(x1)))) |&gt; \n    unnest(quantile) |&gt; \n    pivot_wider(id_cols = t1, names_from = \"ind\", values_from = \"values\")\n#&gt; # A tibble: 2 × 6\n#&gt;      t1  `0%` `25%` `50%` `75%` `100%`\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1     0    66  70      74    78     88\n#&gt; 2     1    73  77.5    83    89     92\n\n\n\n\n\n共変量が多変量である場合を考える. 多変量である場合には上記のように 共変量の分布が同一であるのかを確認することは容易ではない．\n\n\nCode\ndata10b &lt;- read_csv(\"./causality/data10b.csv\", show_col_types = FALSE)\nprint(data10b |&gt; head())\n#&gt; # A tibble: 6 × 8\n#&gt;     y0t   y1t    t1     y    x1    x2    x3   ps1\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    63    74     0    63    66    76    75  0.12\n#&gt; 2    55    70     0    55    70    75    55  0.05\n#&gt; 3    59    69     0    59    70    60    73  0.19\n#&gt; 4    71    77     1    77    73    76    82  0.43\n#&gt; 5    73    78     0    73    73    79    78  0.35\n#&gt; 6    66    77     0    66    74    72    79  0.41\n\n\nそこで傾向スコアを考える．傾向スコアとは，共変量\\(X\\)が与えられたとき， 処置に割り付けらあれる確率と定義される．\n\\[\ne(X) = \\text{Pr}(T_i=1|X)\n\\]\n傾向スコアは最も粗いバランシングスコアである． ここで粗いとは，共変量の情報を一つの値に集約しているという意味である． 最も細かいバランシングスコアは共変量が取り得る値の組合せになる.\n傾向スコアが同程度で異なる処置がされたデータの組合せを選び， 処置群の処置群の平均処置効果を求めることになる\n\n\n\n\n定理１（バランシング）：処置の割り付け\\(T\\)を観測された共変量\\(X\\)は，傾向スコア\\(e(X)\\)が和えられたとき条件付き独立である．すなわち傾向スコアが\\(e(X)\\)が同じであれば処置群と統制群で共変量の分布は同じである.\n\n\\[\nX \\perp T|e(X)\n\\]\n\n定理２（条件付き独立）：傾向スコア\\(e(X)\\)が与えられれば潜在的結果変数\\({Y(1), Y(2)}\\)と割り付け変数\\(T\\)は条件付き独立である．すなわり傾向スコアが同じ個体であれば，処置への割り付けえは無作為と見なせる\n\n\\[\n\\{Y(0), Y(1)\\} \\perp T|e(X)\n\\]\n上記の定理はこれまでに使っていた条件付き独立性（無交絡性）と条件付き正直性である．\n\\[\n\\{Y(1), Y(0)\\} \\perp T|X\\\\\n0 &lt; \\text{Pr}(T_i=1|X) &lt; 1\n\\]\n条件付き独立性が述べているのは観測された共変量のみが処置の割り付けに影響を与えている ということであり，潜在的結果変数とは独立ということである. 二つ目の式で述べていることは実用的には傾向スコアが０になっても１になってもいけない ということである．処置群と統制群のどちらにも観測値があるということを意味している．\n何がいいたいのかというと観測されない共変量\\(\\mathcal{U}\\)がないという想定だし， それが合った場合にどのような結果となるのかは何もいえない. それを調べる場合には別のモデルを使う必要がある.\n\n\n\n省略\n\n\n\n\n\nCode\ndata03 &lt;- read_csv(\"./causality/data03.csv\", show_col_types = FALSE)\nsummary(data03)\n#&gt;        x1              y3              t1           y0t             y1t       \n#&gt;  Min.   :70.00   Min.   :63.00   Min.   :0.0   Min.   :62.00   Min.   :71.00  \n#&gt;  1st Qu.:73.75   1st Qu.:73.75   1st Qu.:0.0   1st Qu.:66.50   1st Qu.:75.50  \n#&gt;  Median :80.00   Median :77.00   Median :0.5   Median :71.00   Median :81.50  \n#&gt;  Mean   :80.00   Mean   :77.25   Mean   :0.5   Mean   :72.20   Mean   :82.00  \n#&gt;  3rd Qu.:86.25   3rd Qu.:82.00   3rd Qu.:1.0   3rd Qu.:78.75   3rd Qu.:88.75  \n#&gt;  Max.   :90.00   Max.   :91.00   Max.   :1.0   Max.   :82.00   Max.   :92.00\n\n\n\n\nCode\n# 傾向スコア\npsmodel &lt;- glm(t1 ~ x1, family = binomial(link = \"logit\"), data = data03)\nps3 &lt;- round(psmodel$fitted.values, 4)\nps4 &lt;- c(rep(.8, 5), rep(.6, 5), rep(.4, 5), rep(.2, 5))\n\nn1 &lt;- 1000\nx1 &lt;- runif(n1, -10, 10)\ne1 &lt;- rlogis(n1, location = 0, scale = 1)\ntstar &lt;- .5 + 1.1 * x1 + e1\nt1 &lt;- NULL\nt1[tstar &gt;  0] &lt;- 1\nt1[tstar &lt;= 0] &lt;- 0\n\n\ndf2 &lt;- with(data03, data.frame(x1, y3, t1, ps3, ps4))\nprint(df2)\n#&gt;    x1 y3 t1    ps3 ps4\n#&gt; 1  70 74  1 0.7751 0.8\n#&gt; 2  70 63  0 0.7751 0.8\n#&gt; 3  70 73  1 0.7751 0.8\n#&gt; 4  70 71  1 0.7751 0.8\n#&gt; 5  70 74  1 0.7751 0.8\n#&gt; 6  75 67  0 0.6499 0.6\n#&gt; 7  75 77  1 0.6499 0.6\n#&gt; 8  75 68  0 0.6499 0.6\n#&gt; 9  75 77  1 0.6499 0.6\n#&gt; 10 75 78  1 0.6499 0.6\n#&gt; 11 85 88  1 0.3501 0.4\n#&gt; 12 85 77  0 0.3501 0.4\n#&gt; 13 85 76  0 0.3501 0.4\n#&gt; 14 85 86  1 0.3501 0.4\n#&gt; 15 85 78  0 0.3501 0.4\n#&gt; 16 90 81  0 0.2249 0.2\n#&gt; 17 90 91  1 0.2249 0.2\n#&gt; 18 90 82  0 0.2249 0.2\n#&gt; 19 90 82  0 0.2249 0.2\n#&gt; 20 90 82  0 0.2249 0.2\n\n\n\n\n\n傾向スコアとは，共変量\\(X\\)が与えられたときに処置に割り付けられる確率であり， ロジスティック回帰モデルによる確率の予測値である。 実務ではもろもろ処理をパッケージで済ます．\n\n\nCode\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nm.out &lt;- matchit(t1 ~ x1, data = data03)\nps5 &lt;- m.out$model$fitted.values\nsummary(ps3)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.2249  0.3188  0.5000  0.5000  0.6812  0.7751\nsummary(ps5)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.2249  0.3188  0.5000  0.5000  0.6812  0.7751\ncor(ps3, ps5)\n#&gt; [1] 1\n\n\nパッケージで算出した傾向スコアとロジスティックス回帰モデルで算出した傾向スコアが 一致していることがわかる.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#バランシングスコア",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#バランシングスコア",
    "title": "傾向スコア",
    "section": "",
    "text": "共変量が単変量の場合には共分散分析により解析することが出来た． しかし，通常共変量は多変量である．これに対して傾向スコアが開発された． 傾向スコアとはバランシングスコアの１つであり， 観測される共変量\\(X\\)の関数\\(b(X)\\)が与えられたときの\\(X\\)の条件つき分布が， 処置群と統制群において同じとなる関数である.\nとにかくバランシングスコアを見るようにして，バランシングスコアが悪いときに，マッチングを使ったATT推定を行ってはならない. マッチしたからといってどの程度マッチに意味があるかは，別であるよね，という話だと理解した．\n\n\nCode\ndata10a &lt;- read_csv(\"./causality/data10a.csv\", show_col_types = FALSE)\n\n# 平均処置効果\nwith(data10a, {\n    print(mean(y1t) - mean(y0t))\n})\n#&gt; [1] 9.95\n\n# 処置群の平均処置効果\nwith(data10a, {\n    print(mean(y1t[t1==1])-mean(y0t[t1==1]))   \n})\n#&gt; [1] 9.090909\n\n# ナイーブな平均処置効果\nwith(data10a, {\n    print(mean(y1t[t1==1])-mean(y0t[t1==0]))   \n})\n#&gt; [1] 17.39394\n\n\nナイーブな平均処置結果が使えないことがよくわかる.\nこのような結果が起こるのは共変量の分布が異なることによる. 処置群と統制群で平均値や分散が同じになることを指して「バランスが取れている」ということになる．\n\n\nCode\ndata10a |&gt; \n    group_by(t1) |&gt;\n    summarise(quantile = list(stack(quantile(x1)))) |&gt; \n    unnest(quantile) |&gt; \n    pivot_wider(id_cols = t1, names_from = \"ind\", values_from = \"values\")\n#&gt; # A tibble: 2 × 6\n#&gt;      t1  `0%` `25%` `50%` `75%` `100%`\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1     0    66  70      74    78     88\n#&gt; 2     1    73  77.5    83    89     92",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコア-1",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコア-1",
    "title": "傾向スコア",
    "section": "",
    "text": "共変量が多変量である場合を考える. 多変量である場合には上記のように 共変量の分布が同一であるのかを確認することは容易ではない．\n\n\nCode\ndata10b &lt;- read_csv(\"./causality/data10b.csv\", show_col_types = FALSE)\nprint(data10b |&gt; head())\n#&gt; # A tibble: 6 × 8\n#&gt;     y0t   y1t    t1     y    x1    x2    x3   ps1\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    63    74     0    63    66    76    75  0.12\n#&gt; 2    55    70     0    55    70    75    55  0.05\n#&gt; 3    59    69     0    59    70    60    73  0.19\n#&gt; 4    71    77     1    77    73    76    82  0.43\n#&gt; 5    73    78     0    73    73    79    78  0.35\n#&gt; 6    66    77     0    66    74    72    79  0.41\n\n\nそこで傾向スコアを考える．傾向スコアとは，共変量\\(X\\)が与えられたとき， 処置に割り付けらあれる確率と定義される．\n\\[\ne(X) = \\text{Pr}(T_i=1|X)\n\\]\n傾向スコアは最も粗いバランシングスコアである． ここで粗いとは，共変量の情報を一つの値に集約しているという意味である． 最も細かいバランシングスコアは共変量が取り得る値の組合せになる.\n傾向スコアが同程度で異なる処置がされたデータの組合せを選び， 処置群の処置群の平均処置効果を求めることになる",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコア定理",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコア定理",
    "title": "傾向スコア",
    "section": "",
    "text": "定理１（バランシング）：処置の割り付け\\(T\\)を観測された共変量\\(X\\)は，傾向スコア\\(e(X)\\)が和えられたとき条件付き独立である．すなわち傾向スコアが\\(e(X)\\)が同じであれば処置群と統制群で共変量の分布は同じである.\n\n\\[\nX \\perp T|e(X)\n\\]\n\n定理２（条件付き独立）：傾向スコア\\(e(X)\\)が与えられれば潜在的結果変数\\({Y(1), Y(2)}\\)と割り付け変数\\(T\\)は条件付き独立である．すなわり傾向スコアが同じ個体であれば，処置への割り付けえは無作為と見なせる\n\n\\[\n\\{Y(0), Y(1)\\} \\perp T|e(X)\n\\]\n上記の定理はこれまでに使っていた条件付き独立性（無交絡性）と条件付き正直性である．\n\\[\n\\{Y(1), Y(0)\\} \\perp T|X\\\\\n0 &lt; \\text{Pr}(T_i=1|X) &lt; 1\n\\]\n条件付き独立性が述べているのは観測された共変量のみが処置の割り付けに影響を与えている ということであり，潜在的結果変数とは独立ということである. 二つ目の式で述べていることは実用的には傾向スコアが０になっても１になってもいけない ということである．処置群と統制群のどちらにも観測値があるということを意味している．\n何がいいたいのかというと観測されない共変量\\(\\mathcal{U}\\)がないという想定だし， それが合った場合にどのような結果となるのかは何もいえない. それを調べる場合には別のモデルを使う必要がある.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコアのモデル化",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコアのモデル化",
    "title": "傾向スコア",
    "section": "",
    "text": "省略",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコアの算出例",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#傾向スコアの算出例",
    "title": "傾向スコア",
    "section": "",
    "text": "Code\ndata03 &lt;- read_csv(\"./causality/data03.csv\", show_col_types = FALSE)\nsummary(data03)\n#&gt;        x1              y3              t1           y0t             y1t       \n#&gt;  Min.   :70.00   Min.   :63.00   Min.   :0.0   Min.   :62.00   Min.   :71.00  \n#&gt;  1st Qu.:73.75   1st Qu.:73.75   1st Qu.:0.0   1st Qu.:66.50   1st Qu.:75.50  \n#&gt;  Median :80.00   Median :77.00   Median :0.5   Median :71.00   Median :81.50  \n#&gt;  Mean   :80.00   Mean   :77.25   Mean   :0.5   Mean   :72.20   Mean   :82.00  \n#&gt;  3rd Qu.:86.25   3rd Qu.:82.00   3rd Qu.:1.0   3rd Qu.:78.75   3rd Qu.:88.75  \n#&gt;  Max.   :90.00   Max.   :91.00   Max.   :1.0   Max.   :82.00   Max.   :92.00\n\n\n\n\nCode\n# 傾向スコア\npsmodel &lt;- glm(t1 ~ x1, family = binomial(link = \"logit\"), data = data03)\nps3 &lt;- round(psmodel$fitted.values, 4)\nps4 &lt;- c(rep(.8, 5), rep(.6, 5), rep(.4, 5), rep(.2, 5))\n\nn1 &lt;- 1000\nx1 &lt;- runif(n1, -10, 10)\ne1 &lt;- rlogis(n1, location = 0, scale = 1)\ntstar &lt;- .5 + 1.1 * x1 + e1\nt1 &lt;- NULL\nt1[tstar &gt;  0] &lt;- 1\nt1[tstar &lt;= 0] &lt;- 0\n\n\ndf2 &lt;- with(data03, data.frame(x1, y3, t1, ps3, ps4))\nprint(df2)\n#&gt;    x1 y3 t1    ps3 ps4\n#&gt; 1  70 74  1 0.7751 0.8\n#&gt; 2  70 63  0 0.7751 0.8\n#&gt; 3  70 73  1 0.7751 0.8\n#&gt; 4  70 71  1 0.7751 0.8\n#&gt; 5  70 74  1 0.7751 0.8\n#&gt; 6  75 67  0 0.6499 0.6\n#&gt; 7  75 77  1 0.6499 0.6\n#&gt; 8  75 68  0 0.6499 0.6\n#&gt; 9  75 77  1 0.6499 0.6\n#&gt; 10 75 78  1 0.6499 0.6\n#&gt; 11 85 88  1 0.3501 0.4\n#&gt; 12 85 77  0 0.3501 0.4\n#&gt; 13 85 76  0 0.3501 0.4\n#&gt; 14 85 86  1 0.3501 0.4\n#&gt; 15 85 78  0 0.3501 0.4\n#&gt; 16 90 81  0 0.2249 0.2\n#&gt; 17 90 91  1 0.2249 0.2\n#&gt; 18 90 82  0 0.2249 0.2\n#&gt; 19 90 82  0 0.2249 0.2\n#&gt; 20 90 82  0 0.2249 0.2",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#rパッケージによる傾向スコアのモデル化",
    "href": "contents/books/05_統計的因果推論の理論と実際/110_傾向スコア.html#rパッケージによる傾向スコアのモデル化",
    "title": "傾向スコア",
    "section": "",
    "text": "傾向スコアとは，共変量\\(X\\)が与えられたときに処置に割り付けられる確率であり， ロジスティック回帰モデルによる確率の予測値である。 実務ではもろもろ処理をパッケージで済ます．\n\n\nCode\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nm.out &lt;- matchit(t1 ~ x1, data = data03)\nps5 &lt;- m.out$model$fitted.values\nsummary(ps3)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.2249  0.3188  0.5000  0.5000  0.6812  0.7751\nsummary(ps5)\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;  0.2249  0.3188  0.5000  0.5000  0.6812  0.7751\ncor(ps3, ps5)\n#&gt; [1] 1\n\n\nパッケージで算出した傾向スコアとロジスティックス回帰モデルで算出した傾向スコアが 一致していることがわかる.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコア"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html",
    "title": "傾向スコアによる層化解析法および重み付き法",
    "section": "",
    "text": "ここまでに傾向スコアマッチングによりATTを推定できること， 仮定が満たされているならば共分散分析によりATEが推定できることも確認した． しかし，ATEを推定するための共分散分析の仮定を満たすことは現実的には難しい.\nそこで登場するのが傾向スコアを用いた層化解析手法である.\n\n\nまず層化抽出とは，母集団をいくつかのグループに分けて， それぞれのグループで無作為抽出する方法である． このときのグループは調べる共変量が似通った値になるように調整する． たとえば「県民性」を調べたいときには 日本全国からサンプリングするよりも４７都道府県別でサンプリングする方がよいのは 直感的に理解できる． 層別は「意味がある」特性が行うことが重要である．\n層別サンプリングは交絡因子を使って層にわけることで， 交絡因子の影響を受けるするという考えであり， フィッシャーの三原則の「局所管理」にあたる作法である．\n層ごとの推定値の情報をまとめたものを全体の推定値とする． \\(K\\)層に分けた場合のATEは次式で求められる.\n\\[\n\\begin{align}\n\\hat{\\tau}_{\\text{ATE}}&=\\sum_{k=1}^{K}\\frac{n_k}{N}[\\overline{Y_k}(1)-\\overline{Y_k}(0)]\\\\\n\\text{var}(\\hat{\\tau}_{\\text{ATE}})&=\\sum_{k=1}^{K}\\left(\\frac{n_k}{N}\\right)^2[\\overline{Y_k}(1)-\\overline{Y_k}(0)]\n\\end{align}\n\\]\n交絡変数をなくそうとして多変量で層別にすると， パターンは膨大になるので，傾向スコアで層別するのがやはり基本となる．\n\n\n\n\nCode\ndata11 &lt;- read_csv(\"./causality/data11.csv\", show_col_types = FALSE)\nwith(data11, {\n    print(\n        mean(y1t) - mean(y0t)\n    )\n})\n#&gt; [1] 3.755947\n\n\n\n\nCode\ndata11 |&gt; head()\n#&gt; # A tibble: 6 × 10\n#&gt;      y0t   y1t     y3    t1     x1     x2      x3      x4    x5     x6\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 12.7   23.2  12.7       0  2.75   3.08   1.74   1.04    0.708 -0.411\n#&gt; 2 61.6   64.7  64.7       1  0.640 -1.20  -0.0539 1.67    2.11   2.47 \n#&gt; 3 -0.275 -1.97 -0.275     0 -0.473  0.747  0.961  0.956   0.223 -1.06 \n#&gt; 4 25.8   38.4  25.8       0  1.27  -0.828 -0.962  0.00882 0.668  1.81 \n#&gt; 5 24.8   21.0  24.8       0  0.640 -0.492  0.630  0.114   0.936  0.886\n#&gt; 6 46.8   48.3  48.3       1  0.846 -0.855  0.159  1.05    1.29   2.05\n\n\n上記がATEのバイアスのない推定値である． この値が算出できるのかを傾向モデルを使った層別サンプリングで試す．\n\n\nCode\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nsub    &lt;- 5\nm.out2 &lt;- matchit(\n    t1 ~ x1 + x2 + x3 + x4 + x5 + x6, \n    data      = data11, \n    method    = \"subclass\", \n    subclass  = sub, \n    estimand  = \"ATE\", \n    min.n     = 2\n)\nm.data2 &lt;- match.data(m.out2)\nm.data2\n#&gt; # A tibble: 1,000 × 13\n#&gt;        y0t   y1t      y3    t1     x1     x2      x3       x4       x5     x6\n#&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1  12.7   23.2   12.7       0  2.75   3.08   1.74    1.04     0.708   -0.411\n#&gt;  2  61.6   64.7   64.7       1  0.640 -1.20  -0.0539  1.67     2.11     2.47 \n#&gt;  3  -0.275 -1.97  -0.275     0 -0.473  0.747  0.961   0.956    0.223   -1.06 \n#&gt;  4  25.8   38.4   25.8       0  1.27  -0.828 -0.962   0.00882  0.668    1.81 \n#&gt;  5  24.8   21.0   24.8       0  0.640 -0.492  0.630   0.114    0.936    0.886\n#&gt;  6  46.8   48.3   48.3       1  0.846 -0.855  0.159   1.05     1.29     2.05 \n#&gt;  7  31.0   34.6   34.6       1  0.680 -0.106  0.304   0.362    1.66     0.992\n#&gt;  8  53.3   54.3   54.3       1  1.70   2.02   2.23    2.88     1.58     1.22 \n#&gt;  9  23.6   20.6   23.6       0 -0.752 -0.567  0.440   1.56     0.686    0.263\n#&gt; 10 -17.8   11.4  -17.8       0  1.05   0.397 -2.04   -0.851   -0.00748 -0.232\n#&gt; # ℹ 990 more rows\n#&gt; # ℹ 3 more variables: distance &lt;dbl&gt;, weights &lt;dbl&gt;, subclass &lt;fct&gt;\n\n\n上記で層を割り当てることが出来たので， 層ごとに解析して，その結果を集約すれば終わりである.\n\n\nCode\nlibrary(lmtest); library(sandwich);\n#&gt; Warning: package 'lmtest' was built under R version 4.3.2\n#&gt; Loading required package: zoo\n#&gt; \n#&gt; Attaching package: 'zoo'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\nfits &lt;- \n    m.data2 |&gt; \n    group_by(subclass) |&gt; \n    summarise(\n        fit = list(lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = cur_data()))\n    )\n#&gt; Warning: There was 1 warning in `summarise()`.\n#&gt; ℹ In argument: `fit = list(lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data =\n#&gt;   cur_data()))`.\n#&gt; ℹ In group 1: `subclass = 1`.\n#&gt; Caused by warning:\n#&gt; ! `cur_data()` was deprecated in dplyr 1.1.0.\n#&gt; ℹ Please use `pick()` instead.\nclass_size &lt;- \n    m.data2 |&gt; \n    count(subclass, name = \"N\")\nfit_stats &lt;- \n    fits |&gt; \n    mutate(coef = map(fit, ~ tidy(.x))) |&gt; \n    select(!fit) |&gt; \n    unnest(coef) |&gt; \n    filter(term == \"t1\") \nate &lt;- \n    fit_stats|&gt; \n    left_join(class_size, by = \"subclass\") |&gt; \n    mutate( \n        psvar = std.error ^ 2\n    ) |&gt; \n    select(\n        subclass, \n        psp = estimate, \n        psvar, \n        nps = N, \n        p.value\n    )\n    \nate\n#&gt; # A tibble: 5 × 5\n#&gt;   subclass   psp psvar   nps  p.value\n#&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1 1        8.21  0.401   200 5.16e-28\n#&gt; 2 2        4.48  0.567   200 1.26e- 8\n#&gt; 3 3        3.82  0.488   200 1.44e- 7\n#&gt; 4 4        2.12  0.493   200 2.86e- 3\n#&gt; 5 5        0.114 0.411   200 8.58e- 1\n\n\n上記の結果を集約する.\n\n\nCode\nestimated &lt;- \n    ate |&gt; \n    summarise(\n        tauhat = sum(nps/sum(nps) * psp), \n        vartau = sum((nps/sum(nps)) ^ 2 * psvar), \n        settau = sqrt(vartau)\n    )\n\nestimated\n#&gt; # A tibble: 1 × 3\n#&gt;   tauhat vartau settau\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   3.75 0.0944  0.307\n\n\n\n\n\n\nStd.Mean Diffの値が0に近く, Var.Ratioの値が1に近ければ， バランシングが取れていることを意味する. ここでは出力結果を省略する.\n\n\nCode\nsummary(m.out2)\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, \n#&gt;     method = \"subclass\", estimand = \"ATE\", subclass = sub, min.n = 2)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.3584          0.6007     0.9897    0.1755\n#&gt; x1              0.9655        0.9919         -0.0280     0.7966    0.0210\n#&gt; x2              0.9596        0.9810         -0.0211     0.8841    0.0200\n#&gt; x3              1.1451        0.9162          0.2213     1.0303    0.0616\n#&gt; x4              1.2246        0.8986          0.3371     0.9352    0.1017\n#&gt; x5              1.2874        0.7913          0.4971     1.0065    0.1461\n#&gt; x6              1.2896        0.8382          0.4753     0.9166    0.1376\n#&gt;          eCDF Max\n#&gt; distance   0.3232\n#&gt; x1         0.0540\n#&gt; x2         0.0524\n#&gt; x3         0.1002\n#&gt; x4         0.1797\n#&gt; x5         0.2387\n#&gt; x6         0.2212\n#&gt; \n#&gt; Summary of Balance Across Subclasses\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.3902        0.3882          0.0148     1.0303    0.0095\n#&gt; x1              0.9917        0.9727          0.0202     0.7583    0.0190\n#&gt; x2              0.9844        0.9713          0.0129     0.9283    0.0213\n#&gt; x3              1.0157        1.0030          0.0123     1.0600    0.0107\n#&gt; x4              1.0413        1.0228          0.0191     0.9487    0.0205\n#&gt; x5              0.9994        0.9760          0.0235     1.0893    0.0100\n#&gt; x6              1.0112        1.0121         -0.0009     0.9771    0.0083\n#&gt;          eCDF Max\n#&gt; distance   0.0339\n#&gt; x1         0.0536\n#&gt; x2         0.0474\n#&gt; x3         0.0408\n#&gt; x4         0.0553\n#&gt; x5         0.0324\n#&gt; x6         0.0328\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All            611.       389\n#&gt; Matched (ESS)  570.38     325\n#&gt; Matched        611.       389\n#&gt; Unmatched        0.         0\n#&gt; Discarded        0.         0\n\n\n\n\nCode\ndiffa &lt;- abs(summary(m.out2)$sum.all[,3])\ndiffb &lt;- abs(summary(m.out2)$sum.acros[,3])\ndiff1 &lt;- rev(diffa)\ndiff2 &lt;- rev(diffb)\n\nmaxx    &lt;- max(diff1, diff2)\nlabels0 &lt;- rownames(summary(m.out2)$sum.all)\nlabels1 &lt;- rev(labels0)\n\ndotchart(diff1, xlim = c(0, maxx), labels = c(labels1))\nabline(v = .0, col = 8)\nabline(v = .1, col = 8)\nabline(v = .05, lty = 2, col = 8)\n\npar(new = TRUE)\ndotchart(diff2, xlim = c(0, maxx), labels = c(labels1), \n         pch = 16, xlab = \"Absolute Standardized Mean Difference\")\n\n\n\n\n\n\n\n\n\n\n\n\n調査標本における重み付けと似ている． 調査標本ではある個体がサンプリングされる確率が等確率にならない， つまり，ランダムサンプリングが実現できない現象である．\n例えば男子6000人，女子4000人の学校で1000人を対象にしたアンケートを行うとき， 女子ロッカーに関することのため女子の人数を多くしたアンケートをしたいと考える． このとき，男子120人，女子880人をサンプリングすると， 男子は120/6000なので0.02, 女子は880/4000なので0.22の確率でサンプリングされる．\nそこでサンプリングされた個人の意見はサンプリング確率の逆数， この場合だと男子は50人, 女子は4.8人分の価値があると解釈する． これが，重み付け法である．\n\n\n標本調査では抽出確率の逆数を使っていた． 傾向スコアは共変量\\(X\\)があたてられたとき，処置に割り付けられる確率という意味であることから， その逆数を重みとして使うことは自然である． これを逆重み付け法(IPW)とおいう.\n無作為化実験から得られるであろう解析結果に一致するように， 処置群の個体に対して「傾向スコアの逆数」を乗じて， 統制群の個体に対して「１－傾向スコア」の逆数を乗じることで，データの重み付けを算出する． つまり出にくい方の値が出たときにはその価値を重視するというものとなる．\n\\[\nw_i = \\frac{T_i}{\\hat{e_i}}+\\frac{1-T_i}{1-\\hat{e_i}}\n\\]\n上記の計算は傾向スコアを算出したのちに，そのスコアを重みとした 回帰分析を行うことで算出が可能である.\n\n\nCode\nmodel1  &lt;- glm(t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, family = binomial(link = \"logit\"))\nps1     &lt;- model1$fitted.values\nweights &lt;- data11$t1 / ps1 + (1 - data11$t1) / (1 - ps1)\n\n\n\n\nCode\nmodel2 &lt;- lm(y3 ~ t1, data = data11, weights = weights)\nmodel3 &lt;- lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11, weights = weights)\n\n\n\n\nCode\ntidy(model2)\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    31.9       1.06     29.9  4.09e-141\n#&gt; 2 t1              3.20      1.51      2.13 3.38e-  2\n\n\n\n\nCode\ntidy(model3)\n#&gt; # A tibble: 8 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.36      0.341      3.98 7.44e-  5\n#&gt; 2 t1             3.82      0.311     12.3  2.66e- 32\n#&gt; 3 x1             1.54      0.191      8.05 2.29e- 15\n#&gt; 4 x2             0.454     0.198      2.30 2.19e-  2\n#&gt; 5 x3            -0.765     0.198     -3.87 1.15e-  4\n#&gt; 6 x4             7.64      0.208     36.7  2.87e-187\n#&gt; 7 x5             9.68      0.195     49.6  9.98e-271\n#&gt; 8 x6            11.3       0.187     60.2  0",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアによる層化解析法および重み付き法"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによる層化解析法",
    "href": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによる層化解析法",
    "title": "傾向スコアによる層化解析法および重み付き法",
    "section": "",
    "text": "まず層化抽出とは，母集団をいくつかのグループに分けて， それぞれのグループで無作為抽出する方法である． このときのグループは調べる共変量が似通った値になるように調整する． たとえば「県民性」を調べたいときには 日本全国からサンプリングするよりも４７都道府県別でサンプリングする方がよいのは 直感的に理解できる． 層別は「意味がある」特性が行うことが重要である．\n層別サンプリングは交絡因子を使って層にわけることで， 交絡因子の影響を受けるするという考えであり， フィッシャーの三原則の「局所管理」にあたる作法である．\n層ごとの推定値の情報をまとめたものを全体の推定値とする． \\(K\\)層に分けた場合のATEは次式で求められる.\n\\[\n\\begin{align}\n\\hat{\\tau}_{\\text{ATE}}&=\\sum_{k=1}^{K}\\frac{n_k}{N}[\\overline{Y_k}(1)-\\overline{Y_k}(0)]\\\\\n\\text{var}(\\hat{\\tau}_{\\text{ATE}})&=\\sum_{k=1}^{K}\\left(\\frac{n_k}{N}\\right)^2[\\overline{Y_k}(1)-\\overline{Y_k}(0)]\n\\end{align}\n\\]\n交絡変数をなくそうとして多変量で層別にすると， パターンは膨大になるので，傾向スコアで層別するのがやはり基本となる．\n\n\n\n\nCode\ndata11 &lt;- read_csv(\"./causality/data11.csv\", show_col_types = FALSE)\nwith(data11, {\n    print(\n        mean(y1t) - mean(y0t)\n    )\n})\n#&gt; [1] 3.755947\n\n\n\n\nCode\ndata11 |&gt; head()\n#&gt; # A tibble: 6 × 10\n#&gt;      y0t   y1t     y3    t1     x1     x2      x3      x4    x5     x6\n#&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 12.7   23.2  12.7       0  2.75   3.08   1.74   1.04    0.708 -0.411\n#&gt; 2 61.6   64.7  64.7       1  0.640 -1.20  -0.0539 1.67    2.11   2.47 \n#&gt; 3 -0.275 -1.97 -0.275     0 -0.473  0.747  0.961  0.956   0.223 -1.06 \n#&gt; 4 25.8   38.4  25.8       0  1.27  -0.828 -0.962  0.00882 0.668  1.81 \n#&gt; 5 24.8   21.0  24.8       0  0.640 -0.492  0.630  0.114   0.936  0.886\n#&gt; 6 46.8   48.3  48.3       1  0.846 -0.855  0.159  1.05    1.29   2.05\n\n\n上記がATEのバイアスのない推定値である． この値が算出できるのかを傾向モデルを使った層別サンプリングで試す．\n\n\nCode\nlibrary(MatchIt)\n#&gt; Warning: package 'MatchIt' was built under R version 4.3.2\n\nsub    &lt;- 5\nm.out2 &lt;- matchit(\n    t1 ~ x1 + x2 + x3 + x4 + x5 + x6, \n    data      = data11, \n    method    = \"subclass\", \n    subclass  = sub, \n    estimand  = \"ATE\", \n    min.n     = 2\n)\nm.data2 &lt;- match.data(m.out2)\nm.data2\n#&gt; # A tibble: 1,000 × 13\n#&gt;        y0t   y1t      y3    t1     x1     x2      x3       x4       x5     x6\n#&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n#&gt;  1  12.7   23.2   12.7       0  2.75   3.08   1.74    1.04     0.708   -0.411\n#&gt;  2  61.6   64.7   64.7       1  0.640 -1.20  -0.0539  1.67     2.11     2.47 \n#&gt;  3  -0.275 -1.97  -0.275     0 -0.473  0.747  0.961   0.956    0.223   -1.06 \n#&gt;  4  25.8   38.4   25.8       0  1.27  -0.828 -0.962   0.00882  0.668    1.81 \n#&gt;  5  24.8   21.0   24.8       0  0.640 -0.492  0.630   0.114    0.936    0.886\n#&gt;  6  46.8   48.3   48.3       1  0.846 -0.855  0.159   1.05     1.29     2.05 \n#&gt;  7  31.0   34.6   34.6       1  0.680 -0.106  0.304   0.362    1.66     0.992\n#&gt;  8  53.3   54.3   54.3       1  1.70   2.02   2.23    2.88     1.58     1.22 \n#&gt;  9  23.6   20.6   23.6       0 -0.752 -0.567  0.440   1.56     0.686    0.263\n#&gt; 10 -17.8   11.4  -17.8       0  1.05   0.397 -2.04   -0.851   -0.00748 -0.232\n#&gt; # ℹ 990 more rows\n#&gt; # ℹ 3 more variables: distance &lt;dbl&gt;, weights &lt;dbl&gt;, subclass &lt;fct&gt;\n\n\n上記で層を割り当てることが出来たので， 層ごとに解析して，その結果を集約すれば終わりである.\n\n\nCode\nlibrary(lmtest); library(sandwich);\n#&gt; Warning: package 'lmtest' was built under R version 4.3.2\n#&gt; Loading required package: zoo\n#&gt; \n#&gt; Attaching package: 'zoo'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\nfits &lt;- \n    m.data2 |&gt; \n    group_by(subclass) |&gt; \n    summarise(\n        fit = list(lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = cur_data()))\n    )\n#&gt; Warning: There was 1 warning in `summarise()`.\n#&gt; ℹ In argument: `fit = list(lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data =\n#&gt;   cur_data()))`.\n#&gt; ℹ In group 1: `subclass = 1`.\n#&gt; Caused by warning:\n#&gt; ! `cur_data()` was deprecated in dplyr 1.1.0.\n#&gt; ℹ Please use `pick()` instead.\nclass_size &lt;- \n    m.data2 |&gt; \n    count(subclass, name = \"N\")\nfit_stats &lt;- \n    fits |&gt; \n    mutate(coef = map(fit, ~ tidy(.x))) |&gt; \n    select(!fit) |&gt; \n    unnest(coef) |&gt; \n    filter(term == \"t1\") \nate &lt;- \n    fit_stats|&gt; \n    left_join(class_size, by = \"subclass\") |&gt; \n    mutate( \n        psvar = std.error ^ 2\n    ) |&gt; \n    select(\n        subclass, \n        psp = estimate, \n        psvar, \n        nps = N, \n        p.value\n    )\n    \nate\n#&gt; # A tibble: 5 × 5\n#&gt;   subclass   psp psvar   nps  p.value\n#&gt;   &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt;\n#&gt; 1 1        8.21  0.401   200 5.16e-28\n#&gt; 2 2        4.48  0.567   200 1.26e- 8\n#&gt; 3 3        3.82  0.488   200 1.44e- 7\n#&gt; 4 4        2.12  0.493   200 2.86e- 3\n#&gt; 5 5        0.114 0.411   200 8.58e- 1\n\n\n上記の結果を集約する.\n\n\nCode\nestimated &lt;- \n    ate |&gt; \n    summarise(\n        tauhat = sum(nps/sum(nps) * psp), \n        vartau = sum((nps/sum(nps)) ^ 2 * psvar), \n        settau = sqrt(vartau)\n    )\n\nestimated\n#&gt; # A tibble: 1 × 3\n#&gt;   tauhat vartau settau\n#&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1   3.75 0.0944  0.307",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアによる層化解析法および重み付き法"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによるバランシングの評価",
    "href": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによるバランシングの評価",
    "title": "傾向スコアによる層化解析法および重み付き法",
    "section": "",
    "text": "Std.Mean Diffの値が0に近く, Var.Ratioの値が1に近ければ， バランシングが取れていることを意味する. ここでは出力結果を省略する.\n\n\nCode\nsummary(m.out2)\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, \n#&gt;     method = \"subclass\", estimand = \"ATE\", subclass = sub, min.n = 2)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.4371        0.3584          0.6007     0.9897    0.1755\n#&gt; x1              0.9655        0.9919         -0.0280     0.7966    0.0210\n#&gt; x2              0.9596        0.9810         -0.0211     0.8841    0.0200\n#&gt; x3              1.1451        0.9162          0.2213     1.0303    0.0616\n#&gt; x4              1.2246        0.8986          0.3371     0.9352    0.1017\n#&gt; x5              1.2874        0.7913          0.4971     1.0065    0.1461\n#&gt; x6              1.2896        0.8382          0.4753     0.9166    0.1376\n#&gt;          eCDF Max\n#&gt; distance   0.3232\n#&gt; x1         0.0540\n#&gt; x2         0.0524\n#&gt; x3         0.1002\n#&gt; x4         0.1797\n#&gt; x5         0.2387\n#&gt; x6         0.2212\n#&gt; \n#&gt; Summary of Balance Across Subclasses\n#&gt;          Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; distance        0.3902        0.3882          0.0148     1.0303    0.0095\n#&gt; x1              0.9917        0.9727          0.0202     0.7583    0.0190\n#&gt; x2              0.9844        0.9713          0.0129     0.9283    0.0213\n#&gt; x3              1.0157        1.0030          0.0123     1.0600    0.0107\n#&gt; x4              1.0413        1.0228          0.0191     0.9487    0.0205\n#&gt; x5              0.9994        0.9760          0.0235     1.0893    0.0100\n#&gt; x6              1.0112        1.0121         -0.0009     0.9771    0.0083\n#&gt;          eCDF Max\n#&gt; distance   0.0339\n#&gt; x1         0.0536\n#&gt; x2         0.0474\n#&gt; x3         0.0408\n#&gt; x4         0.0553\n#&gt; x5         0.0324\n#&gt; x6         0.0328\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All            611.       389\n#&gt; Matched (ESS)  570.38     325\n#&gt; Matched        611.       389\n#&gt; Unmatched        0.         0\n#&gt; Discarded        0.         0\n\n\n\n\nCode\ndiffa &lt;- abs(summary(m.out2)$sum.all[,3])\ndiffb &lt;- abs(summary(m.out2)$sum.acros[,3])\ndiff1 &lt;- rev(diffa)\ndiff2 &lt;- rev(diffb)\n\nmaxx    &lt;- max(diff1, diff2)\nlabels0 &lt;- rownames(summary(m.out2)$sum.all)\nlabels1 &lt;- rev(labels0)\n\ndotchart(diff1, xlim = c(0, maxx), labels = c(labels1))\nabline(v = .0, col = 8)\nabline(v = .1, col = 8)\nabline(v = .05, lty = 2, col = 8)\n\npar(new = TRUE)\ndotchart(diff2, xlim = c(0, maxx), labels = c(labels1), \n         pch = 16, xlab = \"Absolute Standardized Mean Difference\")",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアによる層化解析法および重み付き法"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによる重み付け法",
    "href": "contents/books/05_統計的因果推論の理論と実際/112_傾向スコアによる層化解析法および重み付き法.html#傾向スコアによる重み付け法",
    "title": "傾向スコアによる層化解析法および重み付き法",
    "section": "",
    "text": "調査標本における重み付けと似ている． 調査標本ではある個体がサンプリングされる確率が等確率にならない， つまり，ランダムサンプリングが実現できない現象である．\n例えば男子6000人，女子4000人の学校で1000人を対象にしたアンケートを行うとき， 女子ロッカーに関することのため女子の人数を多くしたアンケートをしたいと考える． このとき，男子120人，女子880人をサンプリングすると， 男子は120/6000なので0.02, 女子は880/4000なので0.22の確率でサンプリングされる．\nそこでサンプリングされた個人の意見はサンプリング確率の逆数， この場合だと男子は50人, 女子は4.8人分の価値があると解釈する． これが，重み付け法である．\n\n\n標本調査では抽出確率の逆数を使っていた． 傾向スコアは共変量\\(X\\)があたてられたとき，処置に割り付けられる確率という意味であることから， その逆数を重みとして使うことは自然である． これを逆重み付け法(IPW)とおいう.\n無作為化実験から得られるであろう解析結果に一致するように， 処置群の個体に対して「傾向スコアの逆数」を乗じて， 統制群の個体に対して「１－傾向スコア」の逆数を乗じることで，データの重み付けを算出する． つまり出にくい方の値が出たときにはその価値を重視するというものとなる．\n\\[\nw_i = \\frac{T_i}{\\hat{e_i}}+\\frac{1-T_i}{1-\\hat{e_i}}\n\\]\n上記の計算は傾向スコアを算出したのちに，そのスコアを重みとした 回帰分析を行うことで算出が可能である.\n\n\nCode\nmodel1  &lt;- glm(t1 ~ x1 + x2 + x3 + x4 + x5 + x6, data = data11, family = binomial(link = \"logit\"))\nps1     &lt;- model1$fitted.values\nweights &lt;- data11$t1 / ps1 + (1 - data11$t1) / (1 - ps1)\n\n\n\n\nCode\nmodel2 &lt;- lm(y3 ~ t1, data = data11, weights = weights)\nmodel3 &lt;- lm(y3 ~ t1 + x1 + x2 + x3 + x4 + x5 + x6, data = data11, weights = weights)\n\n\n\n\nCode\ntidy(model2)\n#&gt; # A tibble: 2 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    31.9       1.06     29.9  4.09e-141\n#&gt; 2 t1              3.20      1.51      2.13 3.38e-  2\n\n\n\n\nCode\ntidy(model3)\n#&gt; # A tibble: 8 × 5\n#&gt;   term        estimate std.error statistic   p.value\n#&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n#&gt; 1 (Intercept)    1.36      0.341      3.98 7.44e-  5\n#&gt; 2 t1             3.82      0.311     12.3  2.66e- 32\n#&gt; 3 x1             1.54      0.191      8.05 2.29e- 15\n#&gt; 4 x2             0.454     0.198      2.30 2.19e-  2\n#&gt; 5 x3            -0.765     0.198     -3.87 1.15e-  4\n#&gt; 6 x4             7.64      0.208     36.7  2.87e-187\n#&gt; 7 x5             9.68      0.195     49.6  9.98e-271\n#&gt; 8 x6            11.3       0.187     60.2  0",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "傾向スコアによる層化解析法および重み付き法"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html",
    "title": "操作変数の非遵守への対処",
    "section": "",
    "text": "操作変数法による応用として，実験兼研究における無作為割り付けが守られない場合の対処法を考える．\n\n\n小学生の学力にテレビの視聴が与える影響を計りたいとする． 理論的には，無作為にサンプリングした小学生に，無作為にテレビの視聴時間を割り当てることで， 計測することができる．\nしかし，このような実験は必ずしも対象者が条件を守るとは限らない． 実験者が監視することは出来ないので，たとえば「視聴時間０」と割り付けられた対象者が， 本当にテレビを見ていないのかを管理することが出来ない． このような状況を非遵守という．\n\n\n処置の割り付け変数\\(T_i\\)とは別に，新たに\\(D_i\\)を個体\\(i\\)が 実際に処置を受けたかどうか表す変数として定義する． たとえば\\(D_i(T_i=0)=1\\)とは処理が割り付けられていないが実際には処置を受けたということを 表す．\\(T_i\\)と\\(D_i\\)の組合せで各個人がどのような振る舞いをするのかが分類できる.\n\n\n\n\n\n\nCode\ndata14 &lt;- read_csv(\"./causality/data14.csv\", show_col_types = FALSE)\ndata14 |&gt; head()\n#&gt; # A tibble: 6 × 7\n#&gt;      y3    t1    d1   y0t   y1t   d0t   d1t\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    70     0     0    70    79     0     1\n#&gt; 2    74     0     0    74    82     0     1\n#&gt; 3    69     0     0    69    78     0     1\n#&gt; 4    81     0     0    81    89     0     1\n#&gt; 5    75     0     0    75    75     0     0\n#&gt; 6    69     0     0    69    69     0     0\n\n\n\n\n\n単調性は次式で表される．これは一言でいうと，処置が割り付けられたときにおこなわず， 割り付けられていないときにおこなう，という天邪鬼が存在していない，という仮定である.\n\\[\nD_i(T_i=1)\\ge D_i(T_i=0)\n\\]\n天邪鬼は実際には少ないため，上記の仮定は妥当なものと考えられる． これ以外の常に処置を受ける人，常に処置を受けない人は，処置効果を求められないので， 効果を計測することが出来ない． このため結局は，遵守者のみを対象にした推定をおこなうことになる.\n\\[\nCACE = E[Y_i(1)-Y_i(0)|C]\n\\]\n\n\nCode\n# d1tは処置が割り付けられたときに実際に取る行動\n# d0tは処理が割り付けられなかったときに実際に取る行動\n# d1t==1 & d0t ==0 は遵守者を抽出していることになる\nwith(subset(data14, d1t==1 & d0t==0), {\n    print(\n        mean(y1t) - mean(y0t)\n    )\n})\n#&gt; [1] 7.875\n\n\nとは言え遵守者を抽出出来ないので意味はない気もする．\n\n\n\n小学生のTVの話に戻ると，実際に制御することは出来ないが， TVを見ること，見ないことの奨励をすることは出来，奨励するのかどうかは 無作為化することが出来る．\n基本的に過小推定することしか出来ない． しかし，過小であることがわかっているので，それはそれで意味がある値となる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数の非遵守への対処"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#非遵守",
    "href": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#非遵守",
    "title": "操作変数の非遵守への対処",
    "section": "",
    "text": "小学生の学力にテレビの視聴が与える影響を計りたいとする． 理論的には，無作為にサンプリングした小学生に，無作為にテレビの視聴時間を割り当てることで， 計測することができる．\nしかし，このような実験は必ずしも対象者が条件を守るとは限らない． 実験者が監視することは出来ないので，たとえば「視聴時間０」と割り付けられた対象者が， 本当にテレビを見ていないのかを管理することが出来ない． このような状況を非遵守という．\n\n\n処置の割り付け変数\\(T_i\\)とは別に，新たに\\(D_i\\)を個体\\(i\\)が 実際に処置を受けたかどうか表す変数として定義する． たとえば\\(D_i(T_i=0)=1\\)とは処理が割り付けられていないが実際には処置を受けたということを 表す．\\(T_i\\)と\\(D_i\\)の組合せで各個人がどのような振る舞いをするのかが分類できる.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数の非遵守への対処"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#使用するデータ",
    "href": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#使用するデータ",
    "title": "操作変数の非遵守への対処",
    "section": "",
    "text": "Code\ndata14 &lt;- read_csv(\"./causality/data14.csv\", show_col_types = FALSE)\ndata14 |&gt; head()\n#&gt; # A tibble: 6 × 7\n#&gt;      y3    t1    d1   y0t   y1t   d0t   d1t\n#&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1    70     0     0    70    79     0     1\n#&gt; 2    74     0     0    74    82     0     1\n#&gt; 3    69     0     0    69    78     0     1\n#&gt; 4    81     0     0    81    89     0     1\n#&gt; 5    75     0     0    75    75     0     0\n#&gt; 6    69     0     0    69    69     0     0",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数の非遵守への対処"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#単調性の仮定と推定対象者",
    "href": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#単調性の仮定と推定対象者",
    "title": "操作変数の非遵守への対処",
    "section": "",
    "text": "単調性は次式で表される．これは一言でいうと，処置が割り付けられたときにおこなわず， 割り付けられていないときにおこなう，という天邪鬼が存在していない，という仮定である.\n\\[\nD_i(T_i=1)\\ge D_i(T_i=0)\n\\]\n天邪鬼は実際には少ないため，上記の仮定は妥当なものと考えられる． これ以外の常に処置を受ける人，常に処置を受けない人は，処置効果を求められないので， 効果を計測することが出来ない． このため結局は，遵守者のみを対象にした推定をおこなうことになる.\n\\[\nCACE = E[Y_i(1)-Y_i(0)|C]\n\\]\n\n\nCode\n# d1tは処置が割り付けられたときに実際に取る行動\n# d0tは処理が割り付けられなかったときに実際に取る行動\n# d1t==1 & d0t ==0 は遵守者を抽出していることになる\nwith(subset(data14, d1t==1 & d0t==0), {\n    print(\n        mean(y1t) - mean(y0t)\n    )\n})\n#&gt; [1] 7.875\n\n\nとは言え遵守者を抽出出来ないので意味はない気もする．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数の非遵守への対処"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#無作為化奨励デザインと４つの推定量",
    "href": "contents/books/05_統計的因果推論の理論と実際/114_操作変数法による非遵守への対処.html#無作為化奨励デザインと４つの推定量",
    "title": "操作変数の非遵守への対処",
    "section": "",
    "text": "小学生のTVの話に戻ると，実際に制御することは出来ないが， TVを見ること，見ないことの奨励をすることは出来，奨励するのかどうかは 無作為化することが出来る．\n基本的に過小推定することしか出来ない． しかし，過小であることがわかっているので，それはそれで意味がある値となる．",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "操作変数の非遵守への対処"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/119_欠測データ処理の基礎.html",
    "href": "contents/books/05_統計的因果推論の理論と実際/119_欠測データ処理の基礎.html",
    "title": "欠損データ処理",
    "section": "",
    "text": "MCAR(完全にランダム)\nMAR(条件付けたときにランダム)\nNMAR(ランダムとは言えない情況)\n\nMCARは欠測値を削除して良い． NMARのときには何もできない． MARのときは条件を探す必要がある.\nMARは次式で表される．つまり変数\\(X\\)の欠損を表す\\(K\\)は変数\\(Y\\)で条件づけたときに， 変数\\(X\\)の値によらない． たとえば性別で条件づけることで体重に関する回答の欠損の有無は 体重によらないと考えられる．\n\\[\nX \\perp K_i|Y\n\\]\nまあ色々あるけど多重代入法から傾向スコアマッチングによる効果推定がデフォルトでよい.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "欠損データ処理"
    ]
  },
  {
    "objectID": "contents/books/05_統計的因果推論の理論と実際/119_欠測データ処理の基礎.html#欠測のメカニズム",
    "href": "contents/books/05_統計的因果推論の理論と実際/119_欠測データ処理の基礎.html#欠測のメカニズム",
    "title": "欠損データ処理",
    "section": "",
    "text": "MCAR(完全にランダム)\nMAR(条件付けたときにランダム)\nNMAR(ランダムとは言えない情況)\n\nMCARは欠測値を削除して良い． NMARのときには何もできない． MARのときは条件を探す必要がある.\nMARは次式で表される．つまり変数\\(X\\)の欠損を表す\\(K\\)は変数\\(Y\\)で条件づけたときに， 変数\\(X\\)の値によらない． たとえば性別で条件づけることで体重に関する回答の欠損の有無は 体重によらないと考えられる．\n\\[\nX \\perp K_i|Y\n\\]\nまあ色々あるけど多重代入法から傾向スコアマッチングによる効果推定がデフォルトでよい.",
    "crumbs": [
      "R",
      "統計的因果推論の理論と実際",
      "欠損データ処理"
    ]
  },
  {
    "objectID": "contents/books/06_Pythonではじめる数理最適化/ch02_チュートリアル.html",
    "href": "contents/books/06_Pythonではじめる数理最適化/ch02_チュートリアル.html",
    "title": "数理最適化チュートリアル",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\n\n\n\n#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.3 (tags/v3.12.3:f6650f9, Apr  9 2024, 14:05:25) [MSC v.1938 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.4\n#&gt; \n#&gt; NOTE: Python version was forced by RETICULATE_PYTHON\n\n\n1 はじめに\n二次方程式、線形計画問題を例としたPythonによる数理最適化のチュートリアルである。 必要なライブラリはrequirements.txtに記載されている。\n必要なデータはPyOptBook以下の各章のディレクトリに格納されている。\n\n\n2 連立1次方程式の解\nPuLPを使って連立1次方程式を解く。\n\n1個120円のりんごと1個150円のなしをあわせて10個購入したら、代金は1440円であった。りんごとなしの個数を求めよ。\n\n\n\nCode\nimport pulp\n\n# 問題の定義\nproblem = pulp.LpProblem('SLE', pulp.LpMaximize)\n\n# 変数の定義\nx = pulp.LpVariable('x', lowBound=0, cat='Continuous')\ny = pulp.LpVariable('y', lowBound=0, cat='Continuous')\n\n# 制約条件\nproblem += x + y == 10\nproblem += 120*x + 150*y == 1440\n\n# 最適化\nstatus = problem.solve()\n\n# 結果の表示\nprint(f'りんごの個数: {x.value()}')\n#&gt; りんごの個数: 2.0\nprint(f'なしの個数: {y.value()}')\n#&gt; なしの個数: 8.0\n\n\n\n\n3 線形計画問題\n\n\nCode\nproblem = pulp.LpProblem('LP', pulp.LpMaximize)\n\nx = pulp.LpVariable('x', lowBound=0, cat='Continuous')\ny = pulp.LpVariable('y', lowBound=0, cat='Continuous')\n\nproblem += 1 * x + 3 * y &lt;= 30\nproblem += 2 * x + 1 * y &lt;= 40\nproblem += x &gt;= 0\nproblem += y &gt;= 0\nproblem += x + 2 * y\n\nproblem.solve()\n#&gt; 1\n\n# 解の状態について表示する\nprint(f\"Status: {pulp.LpStatus[status]}\")\n#&gt; Status: Optimal\nprint(f\"x: {x.value()}, y: {y.value()}, Objective: {problem.objective.value()}\")\n#&gt; x: 18.0, y: 4.0, Objective: 26.0\n\n\n\n\n4 規模の大きな問題\n変数が増えた場合にまとめて定義する方法について示す。 ファイルに記述しておいて、その情報を使って変数の定義などをおこなう。\n\n\nCode\nimport subprocess\n\nsubprocess.run([\"head\", \"-n\", \"10\", \"./unshare/PyOptBook/2.tutorial/requires.csv\"])\n\n\n上記のようにファイルに記述しておいて、その情報を使って変数の定義などをおこなう。\n\n\nCode\nimport pandas as pd\nfrom pathlib import Path\n\ndata_dir = Path(\"./unshare/PyOptBook/2.tutorial\")\nstock_df = pd.read_csv(data_dir / \"stocks.csv\")\nprint(stock_df)\n#&gt;     m  stock\n#&gt; 0  m1     35\n#&gt; 1  m2     22\n#&gt; 2  m3     27\n\nrequire_df = pd.read_csv(data_dir / \"requires.csv\")\nprint(require_df)\n#&gt;      p   m  require\n#&gt; 0   p1  m1        2\n#&gt; 1   p1  m2        0\n#&gt; 2   p1  m3        1\n#&gt; 3   p2  m1        3\n#&gt; 4   p2  m2        2\n#&gt; 5   p2  m3        0\n#&gt; 6   p3  m1        0\n#&gt; 7   p3  m2        2\n#&gt; 8   p3  m3        2\n#&gt; 9   p4  m1        2\n#&gt; 10  p4  m2        2\n#&gt; 11  p4  m3        2\n\ngain_df = pd.read_csv(data_dir / \"gains.csv\")\nprint(gain_df)\n#&gt;     p  gain\n#&gt; 0  p1     3\n#&gt; 1  p2     4\n#&gt; 2  p3     4\n#&gt; 3  p4     5\n\n\nデータを読み込んだので、前処理として変数のリストを作成する。\n\n\nCode\nP = gain_df['p'].tolist()\nprint(P)\n#&gt; ['p1', 'p2', 'p3', 'p4']\n\nM = stock_df['m'].tolist()\nprint(M)\n#&gt; ['m1', 'm2', 'm3']\n\n\n定数を定義する。\n\n\nCode\nstock = {row.m:row.stock for row in stock_df.itertuples()}\nprint(stock)\n#&gt; {'m1': 35, 'm2': 22, 'm3': 27}\n\nrequire = {(row.p, row.m): row.require for row in require_df.itertuples()}\nprint(require)\n#&gt; {('p1', 'm1'): 2, ('p1', 'm2'): 0, ('p1', 'm3'): 1, ('p2', 'm1'): 3, ('p2', 'm2'): 2, ('p2', 'm3'): 0, ('p3', 'm1'): 0, ('p3', 'm2'): 2, ('p3', 'm3'): 2, ('p4', 'm1'): 2, ('p4', 'm2'): 2, ('p4', 'm3'): 2}\n\ngain = {row.p: row.gain for row in gain_df.itertuples()}\nprint(gain)\n#&gt; {'p1': 3, 'p2': 4, 'p3': 4, 'p4': 5}\n\n\n問題を定義する。\n\n\nCode\nimport pulp\nproblem = pulp.LpProblem('LP2', pulp.LpMaximize)\nproblem\n#&gt; LP2:\n#&gt; MAXIMIZE\n#&gt; None\n#&gt; VARIABLES\n\n\n変数を定義する。次のように辞書を使ってまとめて定義することができる。\n\n\nCode\nimport pulp\nx = pulp.LpVariable.dicts('x', P, cat='Continuous')\nx\n#&gt; {'p1': x_p1, 'p2': x_p2, 'p3': x_p3, 'p4': x_p4}\n\n\n上記の定義は次の定義の仕方と同じである。\n\n\nCode\nx = {}\nfor p in P:\n    x[p] = pulp.LpVariable(f'x_{p}', cat='Continuous')\n\n\n制約式を定義する。ここまでに定義してきた変数を使って制約式を定義する。\n\n\nCode\nfor p in P:\n    problem += x[p] &gt;= 0\n\nfor m in M:\n    problem += pulp.lpSum([require[(p, m)] * x[p] for p in P]) &lt;= stock[m]\n\nprint(problem)\n#&gt; LP2:\n#&gt; MAXIMIZE\n#&gt; None\n#&gt; SUBJECT TO\n#&gt; _C1: x_p1 &gt;= 0\n#&gt; \n#&gt; _C2: x_p2 &gt;= 0\n#&gt; \n#&gt; _C3: x_p3 &gt;= 0\n#&gt; \n#&gt; _C4: x_p4 &gt;= 0\n#&gt; \n#&gt; _C5: 2 x_p1 + 3 x_p2 + 2 x_p4 &lt;= 35\n#&gt; \n#&gt; _C6: 2 x_p2 + 2 x_p3 + 2 x_p4 &lt;= 22\n#&gt; \n#&gt; _C7: x_p1 + 2 x_p3 + 2 x_p4 &lt;= 27\n#&gt; \n#&gt; VARIABLES\n#&gt; x_p1 free Continuous\n#&gt; x_p2 free Continuous\n#&gt; x_p3 free Continuous\n#&gt; x_p4 free Continuous\n\n\n目的関数の定義をおこなう。\n\n\nCode\nproblem += pulp.lpSum([gain[p] * x[p] for p in P])\nprint(problem)\n#&gt; LP2:\n#&gt; MAXIMIZE\n#&gt; 3*x_p1 + 4*x_p2 + 4*x_p3 + 5*x_p4 + 0\n#&gt; SUBJECT TO\n#&gt; _C1: x_p1 &gt;= 0\n#&gt; \n#&gt; _C2: x_p2 &gt;= 0\n#&gt; \n#&gt; _C3: x_p3 &gt;= 0\n#&gt; \n#&gt; _C4: x_p4 &gt;= 0\n#&gt; \n#&gt; _C5: 2 x_p1 + 3 x_p2 + 2 x_p4 &lt;= 35\n#&gt; \n#&gt; _C6: 2 x_p2 + 2 x_p3 + 2 x_p4 &lt;= 22\n#&gt; \n#&gt; _C7: x_p1 + 2 x_p3 + 2 x_p4 &lt;= 27\n#&gt; \n#&gt; VARIABLES\n#&gt; x_p1 free Continuous\n#&gt; x_p2 free Continuous\n#&gt; x_p3 free Continuous\n#&gt; x_p4 free Continuous\n\n\n問題を解く。\n\n\nCode\nstatus = problem.solve()\nprint(f\"Status: {pulp.LpStatus[status]}\")\n#&gt; Status: Optimal\nfor p in P:\n    print(f\"{p}: {x[p].value()}\")\n#&gt; p1: 12.142857\n#&gt; p2: 3.5714286\n#&gt; p3: 7.4285714\n#&gt; p4: 0.0\n\nprint(f\"Objective: {problem.objective.value()}\")\n#&gt; Objective: 80.42857099999999\n\n\n\n\n5 整数問題\n上記の問題は変数が実数でおこなわれた。ここでは、変数が整数である場合について示す。\n\n\nCode\n\ndata_dir = Path(\"./unshare/PyOptBook/2.tutorial\")\nstock_df = pd.read_csv(data_dir / \"stocks.csv\")\nrequire_df = pd.read_csv(data_dir / \"requires.csv\")\ngain_df = pd.read_csv(data_dir / \"gains.csv\")\n\nP = gain_df['p'].tolist()\nM = stock_df['m'].tolist()\n\nstock = {row.m:row.stock for row in stock_df.itertuples()}\nrequire = {(row.p, row.m): row.require for row in require_df.itertuples()}\ngain = {row.p: row.gain for row in gain_df.itertuples()}\n\nproblem = pulp.LpProblem('IP', pulp.LpMaximize)\n\nx = pulp.LpVariable.dicts('x', P, cat='Integer')\n\nfor p in P:\n    problem += x[p] &gt;= 0\n\nfor m in M:\n    problem += pulp.lpSum([require[(p, m)] * x[p] for p in P]) &lt;= stock[m]\n\nproblem += pulp.lpSum([gain[p] * x[p] for p in P])\n\nstatus = problem.solve()\nprint(f\"Status: {pulp.LpStatus[status]}\")\n#&gt; Status: Optimal\nfor p in P:\n    print(f\"{p}: {x[p].value()}\")\n#&gt; p1: 13.0\n#&gt; p2: 3.0\n#&gt; p3: 7.0\n#&gt; p4: -0.0\n\nprint(f\"Objective: {problem.objective.value()}\")\n#&gt; Objective: 79.0\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる数理最適化 2nd",
      "数理最適化チュートリアル"
    ]
  },
  {
    "objectID": "contents/books/06_Pythonではじめる数理最適化/ch04_クーポン.html",
    "href": "contents/books/06_Pythonではじめる数理最適化/ch04_クーポン.html",
    "title": "クーポン",
    "section": "",
    "text": "#&gt; python:         C:/pyenv/py312/Scripts/python.exe\n#&gt; libpython:      C:/Program Files/Python312/python312.dll\n#&gt; pythonhome:     C:/pyenv/py312\n#&gt; version:        3.12.0 (tags/v3.12.0:0fb18b0, Oct  2 2023, 13:03:39) [MSC v.1935 64 bit (AMD64)]\n#&gt; Architecture:   64bit\n#&gt; numpy:          C:/pyenv/py312/Lib/site-packages/numpy\n#&gt; numpy_version:  1.26.0\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function",
    "crumbs": [
      "Python",
      "Pythonではじめる数理最適化 2nd",
      "クーポン"
    ]
  },
  {
    "objectID": "contents/books/06_Pythonではじめる数理最適化/ch04_クーポン.html#会員データ",
    "href": "contents/books/06_Pythonではじめる数理最適化/ch04_クーポン.html#会員データ",
    "title": "クーポン",
    "section": "3.1 会員データ",
    "text": "3.1 会員データ\n\n\nCode\ndata_dir = Path(\"unshare\", \"PyOptBook\", \"4.coupon\")\n\ncust_df = pd.read_csv(data_dir / \"customers.csv\")\nprint(cust_df.head())\n#&gt;    customer_id   age_cat freq_cat\n#&gt; 0            1  age20~34    freq2\n#&gt; 1            2  age35~49    freq0\n#&gt; 2            3  age35~49    freq0\n#&gt; 3            4    age~19    freq0\n#&gt; 4            5  age35~49    freq0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nクロス集計でセグメントを確認する。\n\n#&gt; freq_cat  freq0  freq1  freq2  freq3~\n#&gt; age_cat                              \n#&gt; age~19      200    150     50     100\n#&gt; age20~34    600    450    150     300\n#&gt; age35~49    800    600    200     400\n#&gt; age50~      400    300    100     200\n\nヒートマップで可視化する。\n\n\nCode\nsns.heatmap(cust_pivot_df, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.show()",
    "crumbs": [
      "Python",
      "Pythonではじめる数理最適化 2nd",
      "クーポン"
    ]
  },
  {
    "objectID": "contents/books/06_Pythonではじめる数理最適化/ch04_クーポン.html#来店率データ",
    "href": "contents/books/06_Pythonではじめる数理最適化/ch04_クーポン.html#来店率データ",
    "title": "クーポン",
    "section": "3.2 来店率データ",
    "text": "3.2 来店率データ\n\n\nCode\nprob_df = pd.read_csv(data_dir / 'visit_probability.csv')\nprob_df.head()\n#&gt;     age_cat freq_cat  segment_id  prob_dm1  prob_dm2  prob_dm3\n#&gt; 0    age~19    freq0           1      0.07      0.12      0.29\n#&gt; 1    age~19    freq1           2      0.21      0.30      0.58\n#&gt; 2    age~19    freq2           3      0.28      0.39      0.74\n#&gt; 3    age~19   freq3~           4      0.35      0.45      0.77\n#&gt; 4  age20~34    freq0           5      0.11      0.17      0.37\n\n\n来店率のセグメントごとのヒートマップを作成する。\n\n\nCode\nfig = plt.figure(figsize=(10, 10))\nvalue_columns = [\"prob_dm1\", \"prob_dm2\", \"prob_dm3\"]\nfor i, col in enumerate(value_columns):\n    ax = fig.add_subplot(3, 1, i+1)\n    sns.heatmap(\n        prob_df.pivot_table(index = \"age_cat\", columns = \"freq_cat\", values = col),\n        annot=True,\n        fmt=\".2f\",\n        cmap=\"Blues\",\n        ax=ax\n    )\n    ax.set_title(col)\n\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Python",
      "Pythonではじめる数理最適化 2nd",
      "クーポン"
    ]
  },
  {
    "objectID": "contents/books/06_Pythonではじめる数理最適化/index.html",
    "href": "contents/books/06_Pythonではじめる数理最適化/index.html",
    "title": "Pythonではじめる数理最適化",
    "section": "",
    "text": "1 はじめに\n数理最適化とは現実問題の解決を目的とするオペレーションズリサーチの一分野です。\n数理モデルとは対象を数学によって記述したモデルを指しており、模型と見本の二つの意味がある。現実の問題を簡略化した模型を通して現象を理解し、見本に習って意思決定につなげるという一連の使い方をする。\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる数理最適化 2nd",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/12_いきなりPython/index.html",
    "href": "contents/books/12_いきなりPython/index.html",
    "title": "いきなりPython",
    "section": "",
    "text": "1 はじめに\n\nPythonの基礎からアプリまで\nサポートページ\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "いきなりPython",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/13_教員のため教育データ分析/index.html",
    "href": "contents/books/13_教員のため教育データ分析/index.html",
    "title": "はじめに",
    "section": "",
    "text": "1 はじめに\n\nこのノートは教員のための教育データ分析に関する学習ノートです。\nサンプルデータはこちらからダウンロードできます\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "note",
      "教員のための教育データ分析",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/14_入門統計学/index.html",
    "href": "contents/books/14_入門統計学/index.html",
    "title": "はじめに",
    "section": "",
    "text": "1 はじめに\n\nこのノートは教員のための教育データ分析に関する学習ノートです。\nサンプルデータはこちらからダウンロードできます\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "note",
      "入門統計学",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/15_プロになるためのWeb技術入門/index.html",
    "href": "contents/books/15_プロになるためのWeb技術入門/index.html",
    "title": "はじめに",
    "section": "",
    "text": "1 はじめに\n\nこのノートは『プロになるためのWeb技術入門』に関するノートです\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "note",
      "プロになるためのWeb技術入門",
      "Introduction"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html",
    "href": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html",
    "title": "Pythonの基礎",
    "section": "",
    "text": "リスト・タプルの使い方\nfor,　多重ループ\nインデックスとスライス\n\n\n\nCode\n!curl https://gist.githubusercontent.com/koshian2/731c26ce148e664192877510cbb08888/raw/9f3b31a6bb4abfb2bccc3365baa18f3434e98f9d/anko.txt -o ./dat/anko.txt\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100    80  100    80    0     0    462      0 --:--:-- --:--:-- --:--:--   462\n\n\n\n\nCode\nwith open(\"dat/anko.txt\", encoding=\"utf-8\") as fp:\n    anko_data = fp.read().split(\"\\n\")\n\nprint(anko_data)\n\n\n['こしあん', 'つぶあん', 'しろあん', 'ごまあん', 'うぐいすあん', '栗あん']\n\n\n\n\nCode\nfor anko in anko_data:\n    x = anko + \"はうまい\"\n    print(x)\n\n\nこしあんはうまい\nつぶあんはうまい\nしろあんはうまい\nごまあんはうまい\nうぐいすあんはうまい\n栗あんはうまい\n\n\n\n\nCode\nfor anko in anko_data:\n    for x in anko:\n        print(x)\n    print(\"-\" * 5)\n\n\nこ\nし\nあ\nん\n-----\nつ\nぶ\nあ\nん\n-----\nし\nろ\nあ\nん\n-----\nご\nま\nあ\nん\n-----\nう\nぐ\nい\nす\nあ\nん\n-----\n栗\nあ\nん\n-----",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "Pythonの基礎"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html#学ぶこと",
    "href": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html#学ぶこと",
    "title": "Pythonの基礎",
    "section": "",
    "text": "リスト・タプルの使い方\nfor,　多重ループ\nインデックスとスライス\n\n\n\nCode\n!curl https://gist.githubusercontent.com/koshian2/731c26ce148e664192877510cbb08888/raw/9f3b31a6bb4abfb2bccc3365baa18f3434e98f9d/anko.txt -o ./dat/anko.txt\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100    80  100    80    0     0    462      0 --:--:-- --:--:-- --:--:--   462\n\n\n\n\nCode\nwith open(\"dat/anko.txt\", encoding=\"utf-8\") as fp:\n    anko_data = fp.read().split(\"\\n\")\n\nprint(anko_data)\n\n\n['こしあん', 'つぶあん', 'しろあん', 'ごまあん', 'うぐいすあん', '栗あん']\n\n\n\n\nCode\nfor anko in anko_data:\n    x = anko + \"はうまい\"\n    print(x)\n\n\nこしあんはうまい\nつぶあんはうまい\nしろあんはうまい\nごまあんはうまい\nうぐいすあんはうまい\n栗あんはうまい\n\n\n\n\nCode\nfor anko in anko_data:\n    for x in anko:\n        print(x)\n    print(\"-\" * 5)\n\n\nこ\nし\nあ\nん\n-----\nつ\nぶ\nあ\nん\n-----\nし\nろ\nあ\nん\n-----\nご\nま\nあ\nん\n-----\nう\nぐ\nい\nす\nあ\nん\n-----\n栗\nあ\nん\n-----",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "Pythonの基礎"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html#問題",
    "href": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html#問題",
    "title": "Pythonの基礎",
    "section": "2 問題",
    "text": "2 問題\n\n\nCode\n# question 6\nanko_b = [[\"こしあん\", \"大福\"], [\"ごまあん\", \"あんぱん\"]]\nprint(anko_b)\n\n\n[['こしあん', '大福'], ['ごまあん', 'あんぱん']]\n\n\n\n\nCode\nfor anko in anko_b:\n    for x in anko:\n        print(x, end = \" \")\n    print (\"\\n\" +  \"-\" * 4)\n\n\nこしあん 大福 \n----\nごまあん あんぱん \n----\n\n\n\n\nCode\nfor anko in anko_b:\n    print(anko[1])\n\n\n大福\nあんぱん",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "Pythonの基礎"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html#リストの要素の追加削除とパフォーマンス",
    "href": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html#リストの要素の追加削除とパフォーマンス",
    "title": "Pythonの基礎",
    "section": "3 リストの要素の追加・削除とパフォーマンス",
    "text": "3 リストの要素の追加・削除とパフォーマンス\n\nPythonのリストは連結リストのイメージなので末尾への要素追加はコストが小さい\n末尾へはappend，任意の箇所へはinsert\n\n\n\nCode\nanko_list = [\"つぶあん\", \"こしあん\"]\nanko_list.append(\"しろあん\")\nprint(anko_list)\n\n\n['つぶあん', 'こしあん', 'しろあん']\n\n\n\n\nCode\nanko_list.insert(1, \"かぼちゃあん\")\nprint(anko_list)\n\n\n['つぶあん', 'かぼちゃあん', 'こしあん', 'しろあん']\n\n\n\n3.1 パフォーマンスの比較\n\n\nCode\nfrom mypy.mymodule import TimeExecute\n\n\n\n\nCode\nx = list(range(int(1e7)))\nwith TimeExecute():\n    for i in range(int(1e4)):\n        x.append(i)\n    \n\n\n27.925 [msec]\n\n\n\n\nCode\nx = list(range(int(1e7)))\nwith TimeExecute():\n    for i in range(int(1e4)):\n        x.insert(0, i)\n\n\n104406.729 [msec]\n\n\n上記を観るとinsertがバリ遅いのがわかる.",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "Pythonの基礎"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html#横長の画像を抽出",
    "href": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html#横長の画像を抽出",
    "title": "Pythonの基礎",
    "section": "4 横長の画像を抽出",
    "text": "4 横長の画像を抽出\n花の画像のデータセットであるTF_Flowersを使う. このデータセットには, デイジー，タンポポ，バラ，ひまわり，チューリップの５種類の花が格納されている.\n\n\nCode\n!curl https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz -o ./dat/flower_photos.tgz\n\n\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n  0  218M    0  3268    0     0   3258      0 19:30:31  0:00:01 19:30:30  3261\n  2  218M    2 5982k    0     0  3226k      0  0:01:09  0:00:01  0:01:08 3225k\n  4  218M    4 9070k    0     0  3178k      0  0:01:10  0:00:02  0:01:08 3177k\n  6  218M    6 13.9M    0     0  3701k      0  0:01:00  0:00:03  0:00:57 3701k\n  8  218M    8 19.4M    0     0  4092k      0  0:00:54  0:00:04  0:00:50 4091k\n 10  218M   10 22.5M    0     0  3940k      0  0:00:56  0:00:05  0:00:51 4753k\n 11  218M   11 25.8M    0     0  3862k      0  0:00:57  0:00:06  0:00:51 4099k\n 14  218M   14 32.2M    0     0  4206k      0  0:00:53  0:00:07  0:00:46 4792k\n 16  218M   16 36.9M    0     0  4260k      0  0:00:52  0:00:08  0:00:44 4689k\n 19  218M   19 42.3M    0     0  4394k      0  0:00:50  0:00:09  0:00:41 4687k\n 21  218M   21 45.8M    0     0  4322k      0  0:00:51  0:00:10  0:00:41 4769k\n 22  218M   22 49.4M    0     0  4253k      0  0:00:52  0:00:11  0:00:41 4785k\n 23  218M   23 51.2M    0     0  4080k      0  0:00:54  0:00:12  0:00:42 3880k\n 25  218M   25 55.7M    0     0  4123k      0  0:00:54  0:00:13  0:00:41 3880k\n 30  218M   30 66.6M    0     0  4593k      0  0:00:48  0:00:14  0:00:34 4987k\n 36  218M   36 79.6M    0     0  5144k      0  0:00:43  0:00:15  0:00:28 6929k\n 40  218M   40 88.0M    0     0  5186k      0  0:00:43  0:00:17  0:00:26 7209k\n 43  218M   43 94.6M    0     0  5427k      0  0:00:41  0:00:17  0:00:24 8891k\n 48  218M   48  105M    0     0  5748k      0  0:00:38  0:00:18  0:00:20 10.0M\n 53  218M   53  116M    0     0  6023k      0  0:00:37  0:00:19  0:00:18 10.0M\n 61  218M   61  133M    0     0  6537k      0  0:00:34  0:00:20  0:00:14 10.7M\n 65  218M   65  143M    0     0  6712k      0  0:00:33  0:00:21  0:00:12 12.3M\n 73  218M   73  159M    0     0  7139k      0  0:00:31  0:00:22  0:00:09 12.9M\n 76  218M   76  168M    0     0  7199k      0  0:00:31  0:00:23  0:00:08 12.3M\n 83  218M   83  181M    0     0  7471k      0  0:00:29  0:00:24  0:00:05 12.8M\n 87  218M   87  189M    0     0  7523k      0  0:00:29  0:00:25  0:00:04 11.3M\n 90  218M   90  197M    0     0  7520k      0  0:00:29  0:00:26  0:00:03 10.8M\n 93  218M   93  203M    0     0  7474k      0  0:00:29  0:00:27  0:00:02 9003k\n 95  218M   95  208M    0     0  7385k      0  0:00:30  0:00:28  0:00:02 8283k\n 96  218M   96  210M    0     0  7222k      0  0:00:30  0:00:29  0:00:01 5979k\n 97  218M   97  211M    0     0  7027k      0  0:00:31  0:00:30  0:00:01 4465k\n 97  218M   97  212M    0     0  6826k      0  0:00:32  0:00:31  0:00:01 3106k\n 97  218M   97  212M    0     0  6632k      0  0:00:33  0:00:32  0:00:01 1944k\n 97  218M   97  213M    0     0  6444k      0  0:00:34  0:00:33  0:00:01 1019k\n 97  218M   97  213M    0     0  6268k      0  0:00:35  0:00:34  0:00:01  573k\n 97  218M   97  213M    0     0  6102k      0  0:00:36  0:00:35  0:00:01  405k\n 98  218M   98  214M    0     0  5949k      0  0:00:37  0:00:36  0:00:01  350k\n 98  218M   98  214M    0     0  5801k      0  0:00:38  0:00:37  0:00:01  341k\n 98  218M   98  214M    0     0  5663k      0  0:00:39  0:00:38  0:00:01  368k\n 98  218M   98  215M    0     0  5532k      0  0:00:40  0:00:39  0:00:01  436k\n 99  218M   99  216M    0     0  5425k      0  0:00:41  0:00:40  0:00:01  552k\n 99  218M   99  217M    0     0  5324k      0  0:00:41  0:00:41 --:--:--  711k\n100  218M  100  218M    0     0  5295k      0  0:00:42  0:00:42 --:--:--  880k\n\n\n\n\nCode\n!tar -zxvf ./dat/flower_photos.tgz -C ./dat/\n\n\n_photos/dandelion/14060367700_fe87e99b6a_m.jpg\nx flower_photos/dandelion/17147436650_c94ae24004_n.jpg\nx flower_photos/dandelion/151385302_f8980a257f_n.jpg\nx flower_photos/dandelion/10779476016_9130714dc0.jpg\nx flower_photos/dandelion/18876985840_7531dc8e6a.jpg\nx flower_photos/dandelion/8497389500_45636fdd14.jpg\nx flower_photos/dandelion/4645101643_9c9d9df13e.jpg\nx flower_photos/dandelion/8842317179_d59cf218cb_n.jpg\nx flower_photos/dandelion/7270523166_b62fc9e5f1_m.jpg\nx flower_photos/dandelion/18687587599_3dd4fdf255.jpg\nx flower_photos/dandelion/3487229452_73e3004858.jpg\nx flower_photos/dandelion/4669006062_6b3d260037_n.jpg\nx flower_photos/dandelion/483097906_2c35054346.jpg\nx flower_photos/dandelion/16656127943_2f70926b6c.jpg\nx flower_photos/dandelion/921252114_91e334b950.jpg\nx flower_photos/dandelion/16953818045_fea21c8bf8.jpg\nx flower_photos/dandelion/14292205986_da230467ef.jpg\nx flower_photos/dandelion/5749815755_12f9214649_n.jpg\nx flower_photos/dandelion/5772194932_60b833091f.jpg\nx flower_photos/dandelion/3472437817_7902b3d984_n.jpg\nx flower_photos/dandelion/4573204407_babff0dce4_n.jpg\nx flower_photos/dandelion/13734221225_0e04edc6b6.jpg\nx flower_photos/dandelion/4134441089_c8c1e6132a.jpg\nx flower_photos/dandelion/425800274_27dba84fac_n.jpg\nx flower_photos/dandelion/19443674130_08db1d9578_m.jpg\nx flower_photos/dandelion/9517326597_5d116a0166.jpg\nx flower_photos/dandelion/5705695593_d79286ac0d.jpg\nx flower_photos/dandelion/138132145_782763b84f_m.jpg\nx flower_photos/dandelion/18238604119_a5689980ee_n.jpg\nx flower_photos/dandelion/19435491090_7af558e17e.jpg\nx flower_photos/dandelion/16241101274_334b54731e.jpg\nx flower_photos/dandelion/17080000869_a80e767f4a_m.jpg\nx flower_photos/dandelion/5725836812_a7d1c5540d_m.jpg\nx flower_photos/dandelion/17457028309_95514c8d02_n.jpg\nx flower_photos/dandelion/3461986955_29a1abc621.jpg\nx flower_photos/dandelion/14313509432_6f2343d6c8_m.jpg\nx flower_photos/dandelion/19691175559_ef12b8b354_n.jpg\nx flower_photos/dandelion/4556178143_e0d32c0a86_n.jpg\nx flower_photos/dandelion/4258272073_f616d1e575_m.jpg\nx flower_photos/dandelion/3533167406_e9f4cf10bb_m.jpg\nx flower_photos/dandelion/2481428401_bed64dd043.jpg\nx flower_photos/dandelion/8952484062_31d1d97e45.jpg\nx flower_photos/dandelion/17220096449_0e535989f0_n.jpg\nx flower_photos/dandelion/510677438_73e4b91c95_m.jpg\nx flower_photos/dandelion/5740633858_8fd54c23c9_n.jpg\nx flower_photos/dandelion/2401343175_d2a892cf25_n.jpg\nx flower_photos/dandelion/7291185504_b740bbeba4_m.jpg\nx flower_photos/dandelion/8267315764_129f2e1d77_m.jpg\nx flower_photos/dandelion/2229906591_e953785d13.jpg\nx flower_photos/dandelion/5651310874_c8be336c2b.jpg\nx flower_photos/dandelion/4258272381_65bd4b8191_m.jpg\nx flower_photos/dandelion/2490828907_5094017933_m.jpg\nx flower_photos/dandelion/5654859907_c2be3b0f1e_n.jpg\nx flower_photos/dandelion/3856725141_0db85f466d_n.jpg\nx flower_photos/dandelion/578938011_34918b1468.jpg\nx flower_photos/dandelion/4510938552_6f7bae172a_n.jpg\nx flower_photos/dandelion/8981659922_7b1be892e7_m.jpg\nx flower_photos/dandelion/8748402330_c00f9fbf7f_n.jpg\nx flower_photos/dandelion/7267547016_c8903920bf.jpg\nx flower_photos/dandelion/7808430998_31ba639031_n.jpg\nx flower_photos/dandelion/14048849371_ec9dbafaeb_m.jpg\nx flower_photos/dandelion/5776879272_95008399c3.jpg\nx flower_photos/dandelion/14761980161_2d6dbaa4bb_m.jpg\nx flower_photos/dandelion/18304194360_2a4a0be631_m.jpg\nx flower_photos/dandelion/3005677730_2662753d3f_m.jpg\nx flower_photos/dandelion/2516714633_87f28f0314.jpg\nx flower_photos/dandelion/17747738311_5014b1f77f.jpg\nx flower_photos/dandelion/3664916269_29f07c7c7b.jpg\nx flower_photos/dandelion/17388697431_0d84c427d1_n.jpg\nx flower_photos/dandelion/14845607659_1be18c5d7f.jpg\nx flower_photos/dandelion/2518321294_dde5aa7c20_m.jpg\nx flower_photos/dandelion/17029965300_8e755c2214_n.jpg\nx flower_photos/dandelion/3458770076_17ed3a1225.jpg\nx flower_photos/dandelion/4278757393_bca8415ed4_n.jpg\nx flower_photos/dandelion/14306875733_61d71c64c0_n.jpg\nx flower_photos/dandelion/8083321316_f62ea76f72_n.jpg\nx flower_photos/dandelion/4633323785_20676ff914_m.jpg\nx flower_photos/dandelion/3533075436_0954145b9f_m.jpg\nx flower_photos/dandelion/16375088191_2bf2916b53.jpg\nx flower_photos/dandelion/7116950607_49b19102ba_n.jpg\nx flower_photos/dandelion/3584414925_1e6c4b61db_n.jpg\nx flower_photos/dandelion/21657726011_2c94e341bc_n.jpg\nx flower_photos/dandelion/15819121091_26a5243340_n.jpg\nx flower_photos/dandelion/17280886635_e384d91300_n.jpg\nx flower_photos/dandelion/19426575569_4b53c0b726.jpg\nx flower_photos/dandelion/494108764_e00178af6e.jpg\nx flower_photos/dandelion/13967344688_aa629dcdee_n.jpg\nx flower_photos/dandelion/7719263062_3c8a307a5d.jpg\nx flower_photos/dandelion/17574213074_f5416afd84.jpg\nx flower_photos/dandelion/4588529727_4a79c61577.jpg\nx flower_photos/dandelion/16691236594_4287cea9d6_n.jpg\nx flower_photos/dandelion/10200780773_c6051a7d71_n.jpg\nx flower_photos/dandelion/16495282564_d8c34d6a2e_m.jpg\nx flower_photos/dandelion/138166590_47c6cb9dd0.jpg\nx flower_photos/dandelion/4632251871_9f324a7bb5.jpg\nx flower_photos/dandelion/6897671808_57230e04c5_n.jpg\nx flower_photos/dandelion/140951103_69847c0b7c.jpg\nx flower_photos/dandelion/8475769_3dea463364_m.jpg\nx flower_photos/dandelion/7132677385_bcbdcc6001.jpg\nx flower_photos/dandelion/8738317694_eca2ce3bfc_n.jpg\nx flower_photos/dandelion/8647874151_aac8db2588_m.jpg\nx flower_photos/dandelion/7232035352_84a39e99ba_n.jpg\nx flower_photos/dandelion/177851662_b2622b4238_n.jpg\nx flower_photos/dandelion/8723679596_391a724d4f_m.jpg\nx flower_photos/dandelion/4634716478_1cbcbee7ca.jpg\nx flower_photos/dandelion/2489438981_4eb60ef98f_m.jpg\nx flower_photos/dandelion/2780702427_312333ef33.jpg\nx flower_photos/dandelion/2542908888_25a1c78ff0.jpg\nx flower_photos/dandelion/19526570282_1d1e71b0f3_m.jpg\nx flower_photos/dandelion/22196426956_eca94f6faa_m.jpg\nx flower_photos/dandelion/5875763050_82f32f2eed_m.jpg\nx flower_photos/dandelion/17047231499_bd66c23641.jpg\nx flower_photos/dandelion/13652698934_d258a6ee8c.jpg\nx flower_photos/dandelion/1413979148_b40d63db90_m.jpg\nx flower_photos/dandelion/4550784336_584d7a65de_m.jpg\nx flower_photos/dandelion/10043234166_e6dd915111_n.jpg\nx flower_photos/dandelion/5416388641_c66d52d2ff_m.jpg\nx flower_photos/dandelion/14805304536_c321a7b061_n.jpg\nx flower_photos/dandelion/17346385582_7ba433dbbe.jpg\nx flower_photos/dandelion/3554992110_81d8c9b0bd_m.jpg\nx flower_photos/dandelion/149782934_21adaf4a21.jpg\nx flower_photos/dandelion/4858372040_52216eb0bd.jpg\nx flower_photos/dandelion/3451646670_3eff7094b7_n.jpg\nx flower_photos/dandelion/3418355347_2bdcca592a.jpg\nx flower_photos/dandelion/19440660848_c789227129_m.jpg\nx flower_photos/dandelion/3502447188_ab4a5055ac_m.jpg\nx flower_photos/dandelion/4226758402_a1b75ce3ac_n.jpg\nx flower_photos/dandelion/8720503800_cab5c62a34.jpg\nx flower_photos/dandelion/7262863194_682209e9fb.jpg\nx flower_photos/dandelion/14886963928_d4856f1eb6_n.jpg\nx flower_photos/dandelion/6994933428_307b092ce7_m.jpg\nx flower_photos/dandelion/5446666484_365f3be83a_n.jpg\nx flower_photos/dandelion/4558562689_c8e2ab9f10.jpg\nx flower_photos/dandelion/80846315_d997645bea_n.jpg\nx flower_photos/dandelion/3562861685_8b8d747b4d.jpg\nx flower_photos/dandelion/4560663938_3557a1f831.jpg\nx flower_photos/dandelion/645330051_06b192b7e1.jpg\nx flower_photos/dandelion/14648777167_1d92d403c9_n.jpg\nx flower_photos/dandelion/4696437766_85952d0196.jpg\nx flower_photos/dandelion/6983120596_8b9f084ac2_n.jpg\nx flower_photos/dandelion/751941983_58e1ae3957_m.jpg\nx flower_photos/dandelion/4684022752_89631bd98e_n.jpg\nx flower_photos/dandelion/19602790836_912d38aaa8.jpg\nx flower_photos/dandelion/7222962522_36952a67b6_n.jpg\nx flower_photos/dandelion/6994925894_030e157fe0.jpg\nx flower_photos/dandelion/4635296297_9ce69e4a6e.jpg\nx flower_photos/dandelion/1297972485_33266a18d9.jpg\nx flower_photos/dandelion/14439618952_470224b89b_n.jpg\nx flower_photos/dandelion/8744249948_36cb1969f8_m.jpg\nx flower_photos/dandelion/8791577794_7573712cb4_n.jpg\nx flower_photos/dandelion/15219268336_f2460fca88_m.jpg\nx flower_photos/dandelion/2674176237_e265ea64cc_n.jpg\nx flower_photos/dandelion/7368435774_0045b9dc4e.jpg\nx flower_photos/dandelion/16766166609_ccb8344c9f_m.jpg\nx flower_photos/dandelion/4713958242_fbcfe9a61b_m.jpg\nx flower_photos/dandelion/5655177340_78fc36ce59_m.jpg\nx flower_photos/dandelion/3761310831_41b5eba622_n.jpg\nx flower_photos/dandelion/4708723476_a1b476a373.jpg\nx flower_photos/dandelion/15297244181_011883a631_m.jpg\nx flower_photos/dandelion/17146644679_11aff3045c.jpg\nx flower_photos/dandelion/61242541_a04395e6bc.jpg\nx flower_photos/dandelion/4496277750_8c34256e28.jpg\nx flower_photos/dandelion/7368449232_c99f49b2e6_n.jpg\nx flower_photos/dandelion/13916196427_50a611008f.jpg\nx flower_photos/dandelion/176284193_8fa1710431_m.jpg\nx flower_photos/dandelion/2538797744_deb53ac253.jpg\nx flower_photos/dandelion/8181477_8cb77d2e0f_n.jpg\nx flower_photos/dandelion/18243329421_771b4d938e.jpg\nx flower_photos/dandelion/3491333876_e3fed43c0d.jpg\nx flower_photos/dandelion/8756906129_b05a1b26f2.jpg\nx flower_photos/dandelion/3393060921_2328b752f4.jpg\nx flower_photos/dandelion/8689302100_be76a16ccc_n.jpg\nx flower_photos/dandelion/7950901292_2dea05f9a2_n.jpg\nx flower_photos/dandelion/8981828144_4b66b4edb6_n.jpg\nx flower_photos/dandelion/3451637528_b245144675_n.jpg\nx flower_photos/dandelion/19067907051_16d530c7d2.jpg\nx flower_photos/dandelion/7243478942_30bf542a2d_m.jpg\nx flower_photos/dandelion/18232119726_cef27eaaac_n.jpg\nx flower_photos/dandelion/6400843175_ef07053f8f_m.jpg\nx flower_photos/dandelion/2076141453_c63801962a_m.jpg\nx flower_photos/dandelion/2522454811_f87af57d8b.jpg\nx flower_photos/dandelion/145173479_7d04346c20.jpg\nx flower_photos/dandelion/136011860_44ca0b2835_n.jpg\nx flower_photos/dandelion/9200211647_be34ce978b.jpg\nx flower_photos/dandelion/15139657325_74031c44fc.jpg\nx flower_photos/dandelion/9111669902_9471c3a49c_n.jpg\nx flower_photos/dandelion/18889216716_cd67aec890_n.jpg\nx flower_photos/dandelion/6972675188_37f1f1d6f6.jpg\nx flower_photos/dandelion/9726260379_4e8ee66875_m.jpg\nx flower_photos/dandelion/8737699225_19e0c9f0fa_m.jpg\nx flower_photos/dandelion/10486992895_20b344ce2d_n.jpg\nx flower_photos/dandelion/21195621914_a5bdbb203d.jpg\nx flower_photos/dandelion/1273326361_b90ea56d0d_m.jpg\nx flower_photos/dandelion/8969938579_4c2032dd96_n.jpg\nx flower_photos/dandelion/2661585172_94707236be_m.jpg\nx flower_photos/dandelion/14373114081_7922bcf765_n.jpg\nx flower_photos/dandelion/5643666851_dc3f42399d_m.jpg\nx flower_photos/dandelion/15547944931_c1e095b185.jpg\nx flower_photos/dandelion/6968202872_cfcb5b77fb.jpg\nx flower_photos/dandelion/4500964841_b1142b50fb_n.jpg\nx flower_photos/dandelion/4629844753_4e02015d29_m.jpg\nx flower_photos/dandelion/9011235009_58c7b244c1_n.jpg\nx flower_photos/dandelion/11296320473_1d9261ddcb.jpg\nx flower_photos/dandelion/6954604340_d3223ed296_m.jpg\nx flower_photos/dandelion/493696003_f93ffb3abd_n.jpg\nx flower_photos/dandelion/17020815734_81e8db8008_m.jpg\nx flower_photos/dandelion/3499837275_5f24d2f8bf_n.jpg\nx flower_photos/dandelion/6901435398_b3192ff7f8_m.jpg\nx flower_photos/dandelion/2395009660_295c8ffd67_m.jpg\nx flower_photos/dandelion/7295618968_c08a326cc1_m.jpg\nx flower_photos/dandelion/8270191872_61e47ae3b8_m.jpg\nx flower_photos/dandelion/144686365_d7e96941ee_n.jpg\nx flower_photos/dandelion/5623492051_8e5ce438bd.jpg\nx flower_photos/dandelion/18996760154_58d3c48604.jpg\nx flower_photos/dandelion/18803577858_fd0036e1f5_m.jpg\nx flower_photos/dandelion/18996957833_0bd71fbbd4_m.jpg\nx flower_photos/dandelion/4573886524_5161482ca7_n.jpg\nx flower_photos/dandelion/14185089716_2a48298d17.jpg\nx flower_photos/dandelion/9188647508_3b56e62f69.jpg\nx flower_photos/dandelion/151979452_9832f08b69.jpg\nx flower_photos/dandelion/3454102259_957ecd0a9b.jpg\nx flower_photos/dandelion/2330343016_23acc484ee.jpg\nx flower_photos/dandelion/8759118120_9eac064e38_n.jpg\nx flower_photos/dandelion/8935456132_8dc4d3b679_n.jpg\nx flower_photos/dandelion/4560613196_91a04f8dcf_m.jpg\nx flower_photos/dandelion/4528742654_99d233223b_m.jpg\nx flower_photos/dandelion/15002906952_cab2cb29cf.jpg\nx flower_photos/dandelion/3554435478_1a7ab743e9_n.jpg\nx flower_photos/dandelion/477207005_6327db8393_m.jpg\nx flower_photos/dandelion/3581252194_8c976d333a_n.jpg\nx flower_photos/dandelion/20983660733_06b35b9eb8.jpg\nx flower_photos/dandelion/5607669502_ccd2a76668_n.jpg\nx flower_photos/dandelion/7148085703_b9e8bcd6ca_n.jpg\nx flower_photos/dandelion/9595369280_dd88b61814.jpg\nx flower_photos/dandelion/5033866477_a77cccba49_m.jpg\nx flower_photos/dandelion/14002252932_64d5cbdac7.jpg\nx flower_photos/dandelion/7465850028_cdfaae235a_n.jpg\nx flower_photos/dandelion/17903104293_9138439e76.jpg\nx flower_photos/dandelion/14914603395_b271ffab56_n.jpg\nx flower_photos/dandelion/510897767_918260db93.jpg\nx flower_photos/dandelion/7280221020_98b473b20d_n.jpg\nx flower_photos/dandelion/8880158802_6e10a452c7_m.jpg\nx flower_photos/dandelion/4953240903_a121fba81f_m.jpg\nx flower_photos/dandelion/5628296138_9031791fab.jpg\nx flower_photos/dandelion/14021281124_89cc388eac_n.jpg\nx flower_photos/dandelion/1353279846_7e6b87606d.jpg\nx flower_photos/dandelion/3446018470_0c40e73ed6_m.jpg\nx flower_photos/dandelion/20165867412_fc45d31698_m.jpg\nx flower_photos/dandelion/1667963621_c76d570af3_n.jpg\nx flower_photos/dandelion/8681388520_c697dee897_n.jpg\nx flower_photos/dandelion/8929523512_c87897b84e.jpg\nx flower_photos/dandelion/9818247_e2eac18894.jpg\nx flower_photos/dandelion/10294487385_92a0676c7d_m.jpg\nx flower_photos/dandelion/7218569994_de7045c0c0.jpg\nx flower_photos/dandelion/5829610661_8439ba4a77_n.jpg\nx flower_photos/dandelion/19443726008_8c9c68efa7_m.jpg\nx flower_photos/dandelion/9759608055_9ab623d193.jpg\nx flower_photos/dandelion/2831102668_eb65cd40b9_n.jpg\nx flower_photos/dandelion/14003401241_543535b385.jpg\nx flower_photos/dandelion/141340262_ca2e576490.jpg\nx flower_photos/dandelion/8327657321_2cbceec396_n.jpg\nx flower_photos/dandelion/2503034372_db7867de51_m.jpg\nx flower_photos/dandelion/4164845062_1fd9b3f3b4.jpg\nx flower_photos/dandelion/10683189_bd6e371b97.jpg\nx flower_photos/dandelion/10477378514_9ffbcec4cf_m.jpg\nx flower_photos/dandelion/7164500544_332b75aa3b.jpg\nx flower_photos/dandelion/2462379970_6bd5560f4c_m.jpg\nx flower_photos/dandelion/4633514720_22e82c5f7c_m.jpg\nx flower_photos/dandelion/451965300_619b781dc9_m.jpg\nx flower_photos/dandelion/501987276_744448580c_m.jpg\nx flower_photos/dandelion/4573204385_9b71e96b35_m.jpg\nx flower_photos/dandelion/14469481104_d0e29f7ffd.jpg\nx flower_photos/dandelion/151861297_55b10a03a6_n.jpg\nx flower_photos/dandelion/7401173270_ebaf04c9b0_n.jpg\nx flower_photos/dandelion/2683330456_0f7bbce110_m.jpg\nx flower_photos/dandelion/8749577087_dc2521615f_n.jpg\nx flower_photos/dandelion/19621170705_30bf8bf0ba.jpg\nx flower_photos/dandelion/8663932737_0a603ab718_n.jpg\nx flower_photos/dandelion/14053173516_a00150a919_m.jpg\nx flower_photos/dandelion/3476759348_a0d34a4b59_n.jpg\nx flower_photos/dandelion/459633569_5ddf6bc116_m.jpg\nx flower_photos/dandelion/8717787983_c83bdf39fe_n.jpg\nx flower_photos/dandelion/8376558865_19c5cd6fd6_n.jpg\nx flower_photos/dandelion/7998106328_c3953f70e9_n.jpg\nx flower_photos/dandelion/8831808134_315aedb37b.jpg\nx flower_photos/dandelion/13386618495_3df1f1330d.jpg\nx flower_photos/dandelion/4714026966_93846ddb74_m.jpg\nx flower_photos/dandelion/459748276_69101b0cec_n.jpg\nx flower_photos/dandelion/5727534342_419604c177_n.jpg\nx flower_photos/dandelion/139124974_9e3ba69f6c.jpg\nx flower_photos/dandelion/8209318399_ae72aefdb5.jpg\nx flower_photos/dandelion/5045509402_6e052ce443.jpg\nx flower_photos/dandelion/4557781241_0060cbe723_n.jpg\nx flower_photos/dandelion/5767676943_4f9c7323f3_n.jpg\nx flower_photos/dandelion/2465442759_d4532a57a3.jpg\nx flower_photos/dandelion/10437652486_aa86c14985.jpg\nx flower_photos/dandelion/9939430464_5f5861ebab.jpg\nx flower_photos/dandelion/5607983792_f8b8766ff7.jpg\nx flower_photos/dandelion/3465599902_14729e2b1b_n.jpg\nx flower_photos/dandelion/3393564906_f2df184b76_n.jpg\nx flower_photos/dandelion/1128626197_3f52424215_n.jpg\nx flower_photos/dandelion/2453532367_fc373df4de.jpg\nx flower_photos/dandelion/13807932364_673b7f1c1c_n.jpg\nx flower_photos/dandelion/6983113346_21551e1b52_n.jpg\nx flower_photos/dandelion/3518608454_c3fd3c311c_m.jpg\nx flower_photos/dandelion/7308600792_27cff2f73f.jpg\nx flower_photos/dandelion/14740350060_a489d9fa06.jpg\nx flower_photos/dandelion/510874382_f7e3435043.jpg\nx flower_photos/dandelion/8642679391_0805b147cb_m.jpg\nx flower_photos/dandelion/15378782362_4161b23af7_m.jpg\nx flower_photos/dandelion/2467980325_237b14c737_m.jpg\nx flower_photos/dandelion/4523239455_9c31a06aaf_n.jpg\nx flower_photos/dandelion/17619402434_15b2ec2d79.jpg\nx flower_photos/dandelion/9646730031_f3d5014416_n.jpg\nx flower_photos/dandelion/7165651120_2279ebf6d1.jpg\nx flower_photos/dandelion/5129135346_3fa8e804d8_n.jpg\nx flower_photos/dandelion/4290112545_3528055993_m.jpg\nx flower_photos/dandelion/8963359346_65ca69c59d_n.jpg\nx flower_photos/dandelion/8915661673_9a1cdc3755_m.jpg\nx flower_photos/dandelion/3372748508_e5a4eacfcb_n.jpg\nx flower_photos/dandelion/2569516382_9fd7097b9b.jpg\nx flower_photos/dandelion/4573886520_09c984ecd8_m.jpg\nx flower_photos/dandelion/5140791232_52f2c5b41d_n.jpg\nx flower_photos/dandelion/5762590366_5cf7a32b87_n.jpg\nx flower_photos/dandelion/8223949_2928d3f6f6_n.jpg\nx flower_photos/dandelion/13887031789_97437f246b.jpg\nx flower_photos/dandelion/10946896405_81d2d50941_m.jpg\nx flower_photos/dandelion/14085038920_2ee4ce8a8d.jpg\nx flower_photos/dandelion/8707349105_6d06b543b0.jpg\nx flower_photos/dandelion/23192507093_2e6ec77bef_n.jpg\nx flower_photos/dandelion/8805314187_1aed702082_n.jpg\nx flower_photos/dandelion/4607183665_3472643bc8.jpg\nx flower_photos/dandelion/17161833794_e1d92259d2_m.jpg\nx flower_photos/dandelion/4586018734_6de9c513c2.jpg\nx flower_photos/dandelion/3513200808_390f1d63a7_m.jpg\nx flower_photos/dandelion/2521827947_9d237779bb_n.jpg\nx flower_photos/dandelion/17077940105_d2cd7b9ec4_n.jpg\nx flower_photos/dandelion/8684925862_d736e153bf_n.jpg\nx flower_photos/dandelion/5605502523_05acb00ae7_n.jpg\nx flower_photos/dandelion/8980460785_b5e6842e59_n.jpg\nx flower_photos/dandelion/158988663_6fe055fcb4.jpg\nx flower_photos/dandelion/18342918441_b1bb69a2fd_n.jpg\nx flower_photos/dandelion/506659320_6fac46551e.jpg\nx flower_photos/dandelion/8989067485_aab399460b_n.jpg\nx flower_photos/dandelion/136999986_e410a68efb_n.jpg\nx flower_photos/dandelion/7280217714_fb9ffccf2d_n.jpg\nx flower_photos/dandelion/126012913_edf771c564_n.jpg\nx flower_photos/dandelion/11405573_24a8a838cc_n.jpg\nx flower_photos/dandelion/14012247974_69ac128799.jpg\nx flower_photos/dandelion/8979087213_28f572174c.jpg\nx flower_photos/dandelion/3469112805_6cc8640236.jpg\nx flower_photos/dandelion/8739657154_6db14796c9.jpg\nx flower_photos/dandelion/13881700933_69a750d418_n.jpg\nx flower_photos/dandelion/18587334446_ef1021909b_n.jpg\nx flower_photos/dandelion/340190928_d77bf4d615.jpg\nx flower_photos/dandelion/2502627784_4486978bcf.jpg\nx flower_photos/dandelion/18215579866_94b1732f24.jpg\nx flower_photos/dandelion/501987288_c69c4e0c90_m.jpg\nx flower_photos/dandelion/3844111216_742ea491a0.jpg\nx flower_photos/dandelion/19586799286_beb9d684b5.jpg\nx flower_photos/dandelion/5760890854_c3e009bc8a_n.jpg\nx flower_photos/dandelion/14614655810_9910e6dbd6_n.jpg\nx flower_photos/dandelion/9472854850_fc9e1db673.jpg\nx flower_photos/dandelion/15644450971_6a28298454_n.jpg\nx flower_photos/dandelion/5674707921_1ffd141bab_n.jpg\nx flower_photos/dandelion/6888894675_524a6accab_n.jpg\nx flower_photos/dandelion/3297108443_0393d04dfc_m.jpg\nx flower_photos/dandelion/5596093561_09b0301136_n.jpg\nx flower_photos/dandelion/1386449001_5d6da6bde6.jpg\nx flower_photos/dandelion/8740218495_23858355d8_n.jpg\nx flower_photos/dandelion/2478018280_1be353ca8c_m.jpg\nx flower_photos/dandelion/7141013005_d2f168c373.jpg\nx flower_photos/dandelion/14211880544_5d1f9d5aa8_n.jpg\nx flower_photos/dandelion/455728598_c5f3e7fc71_m.jpg\nx flower_photos/dandelion/6900157914_c3387c11d8.jpg\n\n\n上記のデータセットから「ひまわり」かつ「横長」の画像ファイル一覧を作成する.\n\n\nCode\nfrom PIL import Image\nimport glob\n\nsunflower_imgs = sorted(glob.glob(\"./dat/flower_photos/sunflowers/*.jpg\"))\nwide_imgs = []\nfor f in sunflower_imgs:\n    w, h = Image.open(f).size\n    if (w &gt; h):\n        wide_imgs.append(f)\n\n\n\n500 330\n320 240\n320 240\n240 231\n213 240\n320 277\n320 240\n201 240\n320 232\n320 240\n320 213\n500 333\n180 240\n180 240\n320 214\n500 330\n320 240\n500 332\n500 332\n159 240\n500 375\n320 214\n240 234\n240 222\n500 335\n500 375\n500 330\n500 335\n320 212\n320 212\n500 332\n320 229\n320 240\n320 234\n320 240\n320 240\n180 240\n240 240\n320 225\n240 240\n320 240\n500 292\n320 240\n500 332\n240 217\n240 237\n234 240\n320 240\n320 229\n320 252\n320 286\n320 252\n500 325\n500 333\n500 333\n209 240\n320 284\n320 245\n320 219\n320 239\n320 243\n320 239\n180 240\n320 213\n500 325\n500 321\n500 281\n500 281\n240 221\n180 240\n500 281\n320 212\n320 252\n320 252\n500 293\n320 252\n320 243\n320 218\n180 240\n500 331\n320 240\n320 200\n640 204\n320 212\n320 247\n320 285\n320 253\n240 240\n240 229\n320 273\n240 228\n320 256\n240 213\n240 231\n320 270\n240 221\n320 239\n320 239\n240 240\n320 240\n320 240\n320 240\n500 281\n320 240\n320 212\n320 240\n500 333\n240 220\n240 240\n500 332\n238 240\n159 240\n500 271\n320 264\n178 240\n320 259\n240 234\n500 281\n500 316\n240 240\n320 252\n320 226\n320 240\n320 229\n160 240\n240 180\n320 224\n500 333\n320 228\n500 333\n320 228\n320 228\n500 333\n320 200\n500 333\n320 200\n320 240\n174 240\n320 240\n320 240\n500 333\n320 240\n500 375\n500 375\n320 240\n320 240\n320 240\n320 240\n500 332\n500 332\n500 332\n159 240\n159 240\n500 332\n240 240\n320 240\n500 331\n500 331\n500 291\n240 240\n320 213\n320 212\n320 240\n320 240\n500 331\n500 331\n240 240\n320 200\n192 240\n180 240\n500 281\n500 375\n500 281\n240 210\n195 240\n169 240\n320 200\n500 375\n240 240\n320 240\n500 375\n320 240\n500 375\n500 375\n320 200\n500 323\n500 313\n320 240\n500 375\n240 240\n240 240\n226 240\n500 331\n500 331\n500 331\n320 240\n320 240\n500 334\n500 313\n320 238\n500 248\n320 262\n320 224\n500 375\n500 333\n320 213\n500 331\n500 331\n320 217\n320 213\n500 331\n500 331\n500 331\n500 375\n320 223\n320 213\n500 281\n320 231\n500 375\n500 375\n500 375\n320 240\n320 240\n179 240\n160 240\n225 240\n320 240\n229 240\n320 213\n170 240\n320 240\n180 240\n240 216\n320 240\n202 240\n320 240\n320 200\n500 331\n500 333\n240 238\n500 400\n500 334\n320 200\n320 240\n500 334\n500 333\n500 333\n500 334\n320 238\n320 240\n500 375\n386 320\n194 240\n465 288\n500 375\n180 240\n500 333\n320 249\n320 240\n320 240\n320 240\n320 240\n320 240\n180 240\n500 333\n320 240\n320 213\n184 240\n320 240\n320 234\n500 334\n159 240\n500 334\n500 301\n500 333\n320 240\n500 375\n500 375\n320 240\n320 240\n320 278\n320 213\n500 334\n320 239\n500 333\n500 376\n500 375\n500 335\n240 232\n176 240\n240 219\n500 332\n320 213\n320 240\n320 240\n180 240\n320 240\n240 219\n320 240\n500 325\n166 240\n320 208\n500 333\n320 239\n320 240\n320 240\n320 240\n500 319\n500 334\n500 333\n320 213\n320 213\n320 225\n192 240\n320 240\n320 240\n320 240\n320 240\n500 375\n500 375\n500 333\n320 240\n320 240\n320 213\n500 400\n500 333\n240 192\n320 240\n320 220\n320 220\n320 240\n159 240\n320 222\n500 281\n500 333\n320 213\n320 266\n500 333\n500 348\n500 348\n500 335\n500 332\n500 334\n320 247\n320 237\n320 240\n320 240\n320 240\n500 398\n320 251\n320 187\n320 213\n500 375\n180 240\n320 240\n320 240\n180 240\n320 240\n500 333\n320 271\n180 240\n320 240\n180 240\n320 240\n320 240\n320 240\n500 333\n159 240\n500 290\n320 224\n320 261\n500 346\n500 246\n500 346\n500 346\n320 240\n180 240\n500 375\n198 240\n320 240\n320 240\n320 213\n320 240\n320 240\n500 290\n500 312\n320 238\n320 238\n320 238\n320 238\n320 238\n320 238\n320 238\n500 332\n500 332\n320 208\n320 240\n320 240\n320 229\n320 240\n320 240\n320 248\n320 213\n320 240\n500 281\n320 240\n320 213\n500 332\n320 212\n320 213\n500 314\n500 317\n166 240\n320 240\n500 333\n320 240\n320 240\n180 240\n320 240\n320 240\n320 240\n320 264\n320 232\n500 320\n180 240\n500 333\n500 333\n500 333\n500 333\n500 333\n500 333\n500 333\n500 291\n500 292\n159 240\n500 333\n159 240\n500 333\n500 333\n500 333\n159 240\n159 240\n500 333\n500 333\n500 333\n500 333\n500 235\n500 333\n500 333\n500 333\n500 333\n500 333\n180 240\n320 240\n500 333\n500 334\n320 239\n320 241\n320 242\n320 240\n181 240\n500 333\n500 333\n320 240\n320 240\n320 247\n500 333\n500 333\n500 333\n320 240\n180 240\n180 240\n180 240\n240 240\n159 240\n179 240\n500 332\n320 240\n320 240\n320 240\n320 240\n180 240\n180 240\n500 333\n320 213\n180 240\n180 240\n180 240\n500 313\n320 240\n500 333\n320 240\n320 240\n500 334\n500 333\n180 240\n500 286\n320 240\n500 334\n500 334\n500 334\n320 240\n320 218\n320 210\n320 213\n500 333\n320 213\n320 213\n500 334\n320 213\n161 240\n320 240\n180 240\n320 240\n320 240\n320 240\n320 240\n320 240\n320 240\n320 240\n320 240\n320 240\n320 240\n320 240\n320 240\n500 331\n500 331\n320 240\n320 213\n320 251\n320 213\n500 333\n320 252\n500 333\n500 333\n320 238\n500 337\n320 213\n180 240\n161 240\n500 332\n500 332\n500 332\n500 332\n500 332\n500 332\n320 240\n172 240\n180 240\n180 240\n180 240\n320 213\n240 240\n320 240\n320 240\n500 333\n500 333\n320 216\n500 338\n500 332\n500 332\n500 332\n500 332\n159 240\n500 332\n159 240\n159 240\n500 332\n500 332\n320 240\n320 240\n180 240\n500 400\n180 240\n320 240\n500 350\n500 333\n180 240\n180 240\n320 213\n500 333\n159 240\n320 213\n320 240\n320 240\n320 240\n320 240\n320 240\n159 240\n500 234\n180 240\n500 333\n500 333\n500 333\n500 331\n500 333\n320 256\n221 240\n320 212\n500 332\n500 332\n320 212\n500 341\n240 240\n168 240\n320 217\n320 213\n240 240\n320 227\n320 240\n320 240\n500 331\n500 331\n320 211\n500 331\n500 331\n320 213\n320 215\n500 332\n500 332\n320 215\n180 240\n214 240\n240 228\n320 236\n500 332\n320 213\n320 216\n500 333\n500 257\n320 240\n500 333\n500 343\n500 333\n500 333\n500 333\n320 213\n320 213\n500 333\n320 240\n320 213\n500 333\n240 240\n152 240\n500 318\n159 240\n320 240\n320 268\n320 213\n500 333\n500 331\n500 332\n320 249\n320 240\n320 240\n320 240\n320 236\n500 332\n320 240\n320 212\n500 332\n320 240\n320 284\n320 274\n500 332\n500 332\n320 247\n240 217\n320 282\n500 333\n500 336\n500 266\n214 240\n320 240\n500 333\n500 333\n320 213\n240 240\n160 240\n160 240\n159 240\n159 240\n500 333\n320 240\n500 333\n500 332\n500 318\n500 333\n320 213\n500 333\n500 327\n320 213\n500 332\n240 240\n207 240\n320 240\n320 212\n320 240\n159 240\n\n\n\n\nCode\nprint(\"横長画像：\", len(wide_imgs), \"/\", len(sunflower_imgs))\n\n\n横長画像： 582 / 699\n\n\nリストをインデックスで削除するpop, 要素で削除するremoveがある． どちらも末尾の操作でないときには非常に遅い.",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "Pythonの基礎"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html#演習問題２",
    "href": "contents/books/07_NumpyInfinity/ch01_Pythonの基礎.html#演習問題２",
    "title": "Pythonの基礎",
    "section": "5 演習問題２",
    "text": "5 演習問題２\n\n\nCode\n# question12\nanko_list = [\"こしあん\", \"ごまあん\", \"栗あん\", \"かぼちゃあん\"]\nkeep_anko_type = (\"こしあん\", \"栗あん\")\nfor anko in anko_list:\n    if (anko in keep_anko_type):\n        print(anko) \n\n\n\n\nこしあん\n栗あん\n\n\n\n\nCode\n# question18\nnum_list = [i + 10 for i in range(10)]\nnum_list[slice(0, len(num_list), 2)]\n\n\n[10, 12, 14, 16, 18]\n\n\n\n\nCode\n# quesition 19\nnum_list[slice(2, 6, 3)]\n\n\n[12, 15]\n\n\n\n\nCode\n# question 20\nnum_list[slice(7, 0, -3)]\n\n\n[17, 14, 11]\n\n\n\n\nCode\n# question 21\nanko_char = []\nfor anko in anko_list:\n    anko_char += list(anko)\nanko_char\n\n\n['こ',\n 'し',\n 'あ',\n 'ん',\n 'ご',\n 'ま',\n 'あ',\n 'ん',\n '栗',\n 'あ',\n 'ん',\n 'か',\n 'ぼ',\n 'ち',\n 'ゃ',\n 'あ',\n 'ん']\n\n\n\n\nCode\n# question 22\nanko_char.remove(\"栗\")\nanko_char\n\n\n['こ',\n 'し',\n 'あ',\n 'ん',\n 'ご',\n 'ま',\n 'あ',\n 'ん',\n 'あ',\n 'ん',\n 'か',\n 'ぼ',\n 'ち',\n 'ゃ',\n 'あ',\n 'ん']\n\n\n\n\nCode\n# question 23\nx = []\nfor y in anko_char:\n    if y not in (\"あ\", \"ん\"):\n        x += y\n\n\n\n\nCode\nx\n\n\n['こ', 'し', 'ご', 'ま', 'か', 'ぼ', 'ち', 'ゃ']\n\n\n\n\nCode\n# question 24\n!git clone https://github.com/koshian2/numpy_book\n\n\nCloning into 'numpy_book'...\n\n\n\n\nCode\nwith open(\"numpy_book/data/hokkaido_raw.txt\", encoding = \"UTF-8\") as f:\n    text = f.readlines()\n\n\n\n\nCode\ntext[:10]\n\n\n['空知,夕張市,7769\\n',\n '空知,岩見沢市,80410\\n',\n '空知,美唄市,21058\\n',\n '空知,芦別市,13204\\n',\n '空知,赤平市,9906\\n',\n '空知,三笠市,8302\\n',\n '空知,滝川市,39861\\n',\n '空知,砂川市,16848\\n',\n '空知,歌志内市,3130\\n',\n '空知,深川市,20422\\n']\n\n\n\n\nCode\nhokkaido = []\nfor t in text:\n    tl  = t.split(\",\")\n    ret = tl[0], tl[1], tl[1][-1], int(tl[2])\n    hokkaido.append(list(ret))\n\n\n\n\nCode\nhokkaido[:10]\n\n\n[['空知', '夕張市', '市', 7769],\n ['空知', '岩見沢市', '市', 80410],\n ['空知', '美唄市', '市', 21058],\n ['空知', '芦別市', '市', 13204],\n ['空知', '赤平市', '市', 9906],\n ['空知', '三笠市', '市', 8302],\n ['空知', '滝川市', '市', 39861],\n ['空知', '砂川市', '市', 16848],\n ['空知', '歌志内市', '市', 3130],\n ['空知', '深川市', '市', 20422]]\n\n\n\n\nCode\n# question 27\nfor city in hokkaido:\n    if city[0] == \"石狩\":\n        print(city)\n\n\n['石狩', '札幌市', '市', 1959313]\n['石狩', '江別市', '市', 119580]\n['石狩', '千歳市', '市', 97552]\n['石狩', '恵庭市', '市', 70049]\n['石狩', '北広島市', '市', 58265]\n['石狩', '石狩市', '市', 58288]\n['石狩', '当別町', '町', 15840]\n['石狩', '新篠津村', '村', 3033]\n\n\n\n\nCode\n# 30\nuniques = []\nfor x in zip(*hokkaido):\n    uniques.append(list(set(x)))\n\n\n\n\nCode\nshinkoukyoku = uniques[0]\nshinkoukyoku_pop = {}\nfor s in shinkoukyoku:\n    shinkoukyoku_pop[s] = 0\nfor city in hokkaido:\n    shinkoukyoku_pop[city[0]] += city[-1]\n\nshinkoukyoku_pop\n\n\n{'日高': 65586,\n '釧路': 227420,\n '後志': 206592,\n '留萌': 44638,\n '十勝': 336986,\n 'オホーツク': 277502,\n '宗谷': 62707,\n '檜山': 35119,\n '渡島': 389500,\n '胆振': 387621,\n '空知': 287802,\n '上川': 490316,\n '根室': 74053,\n '石狩': 2381920}\n\n\n\n\nCode\npop_top3 = {}\nfor k in shinkoukyoku_pop.keys():\n    pop_top3[k] = []\n\nfor city in hokkaido:\n    pop_top3[city[0]].append(city)\n\nfor k in shinkoukyoku_pop.keys():\n    pop_top3[k] = sorted(pop_top3[k], key = lambda x: x[-1], reverse=True)[:3]\n\npop_top3\n\n\n{'日高': [['日高', '新ひだか町', '町', 22242],\n  ['日高', '浦河町', '町', 12166],\n  ['日高', '日高町', '町', 11919]],\n '釧路': [['釧路', '釧路市', '市', 168086],\n  ['釧路', '釧路町', '町', 19573],\n  ['釧路', '厚岸町', '町', 9183]],\n '後志': [['後志', '小樽市', '市', 114425],\n  ['後志', '余市町', '町', 18564],\n  ['後志', '倶知安町', '町', 16892]],\n '留萌': [['留萌', '留萌市', '市', 20715],\n  ['留萌', '羽幌町', '町', 6796],\n  ['留萌', '増毛町', '町', 4222]],\n '十勝': [['十勝', '帯広市', '市', 166043],\n  ['十勝', '音更町', '町', 44342],\n  ['十勝', '幕別町', '町', 26636]],\n 'オホーツク': [['オホーツク', '北見市', '市', 116630],\n  ['オホーツク', '網走市', '市', 35039],\n  ['オホーツク', '紋別市', '市', 21582]],\n '宗谷': [['宗谷', '稚内市', '市', 33605],\n  ['宗谷', '枝幸町', '町', 8027],\n  ['宗谷', '豊富町', '町', 3891]],\n '檜山': [['檜山', 'せたな町', '町', 7743],\n  ['檜山', '江差町', '町', 7488],\n  ['檜山', '今金町', '町', 5178]],\n '渡島': [['渡島', '函館市', '市', 255308],\n  ['渡島', '北斗市', '市', 46031],\n  ['渡島', '七飯町', '町', 28148]],\n '胆振': [['胆振', '苫小牧市', '市', 171242],\n  ['胆振', '室蘭市', '市', 82977],\n  ['胆振', '登別市', '市', 47608]],\n '空知': [['空知', '岩見沢市', '市', 80410],\n  ['空知', '滝川市', '市', 39861],\n  ['空知', '美唄市', '市', 21058]],\n '上川': [['上川', '旭川市', '市', 334070],\n  ['上川', '名寄市', '市', 27277],\n  ['上川', '富良野市', '市', 21593]],\n '根室': [['根室', '根室市', '市', 25457],\n  ['根室', '中標津町', '町', 23392],\n  ['根室', '別海町', '町', 15006]],\n '石狩': [['石狩', '札幌市', '市', 1959313],\n  ['石狩', '江別市', '市', 119580],\n  ['石狩', '千歳市', '市', 97552]]}",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "Pythonの基礎"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html",
    "title": "行列の導入",
    "section": "",
    "text": "NumPy配列の結合：concatenate, stack\nNumPy配列のshapeとaxis, 集約関数\n行列積の導入",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#学ぶこと",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#学ぶこと",
    "title": "行列の導入",
    "section": "",
    "text": "NumPy配列の結合：concatenate, stack\nNumPy配列のshapeとaxis, 集約関数\n行列積の導入",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#配列の結合",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#配列の結合",
    "title": "行列の導入",
    "section": "2 配列の結合",
    "text": "2 配列の結合\n\n2.1 concatenate\n\n\nCode\na = [1, 2, 3]\nb = [100, 101, 102]\na + b\n\n\n[1, 2, 3, 100, 101, 102]\n\n\n\n\nCode\na.append(b)\na\n\n\n[1, 2, 3, [100, 101, 102]]\n\n\n\n\nCode\nimport numpy as np\n\nx = np.array([1, 2, 3])\ny = np.array([100, 101, 102])\n\n# numpyでは関数を使う\nnp.concatenate([x, y])\n\n\narray([  1,   2,   3, 100, 101, 102])\n\n\n\n\nCode\nnp.concatenate([x, x, y])\n\n\narray([  1,   2,   3,   1,   2,   3, 100, 101, 102])\n\n\n\n\nCode\nnp.concatenate([x] * 4)\n\n\narray([1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3])\n\n\n\n\nCode\nnp.array([x] * 4)\n\n\narray([[1, 2, 3],\n       [1, 2, 3],\n       [1, 2, 3],\n       [1, 2, 3]])\n\n\n\n\n2.2 ベストプラクティス\nappendを使ってリストにためてからnp.arrayを適用するのと，np.concatenateでNumpy配列を結合するのではどちらが早いのだろうか?\n\n\nCode\nfrom mypy.mymodule import TimeExecute\n\nchunk_size = 100\ndef pattern1():\n    x = []\n    for i in range(10000):\n        for j in range(chunk_size):\n            x.append(j)\n    x= np.array(x)\n\ndef pattern2():\n    x = []\n    for i in range(10000):\n        x += list(range(chunk_size))\n    x = np.array(x)\n\ndef pattern3():\n    x = []\n    for i in range(10000):\n        x.append(np.arange(chunk_size))\n    x = np.concatenate(x)\n\ndef pattern4():\n    x = np.array([])\n    for i in range(10000):\n        x = np.concatenate([x, np.arange(chunk_size)])\n\n\n\n\nCode\nwith TimeExecute():\n    pattern1()\n\n\n114.722 [msec]\n\n\n\n\nCode\nwith TimeExecute():\n    pattern2()\n\n\n85.800 [msec]\n\n\n\n\nCode\nwith TimeExecute():\n    pattern3()\n\n\n15.957 [msec]\n\n\n\n\nCode\nwith TimeExecute():\n    pattern4()\n\n\n13,534.473 [msec]\n\n\n基本的にはpattern3, つまりNumPy配列をリストに入れる作業の後に， まとめて結合させるのが良い． chunk_sizeが小さい場合には他のpatternが優勢なときもある.\n配列は要素の結合などのように素数が変化する処理には向いていないため， 結合回数を少なくすることがよい.",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#numpy配列の分割",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#numpy配列の分割",
    "title": "行列の導入",
    "section": "3 NumPy配列の分割",
    "text": "3 NumPy配列の分割\n\n3.1 スライス\n普通に働くので省略.\n\n\n3.2 np.split\nあまり使う機会はあにが指定した数に要素を当分する.\n\n\nCode\nx    = np.arange(8)\na, b = np.split(x, 2)\nprint(a)\nprint(b)\n\n\n[0 1 2 3]\n[4 5 6 7]",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#多次元リストと多次元配列",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#多次元リストと多次元配列",
    "title": "行列の導入",
    "section": "4 多次元リストと多次元配列",
    "text": "4 多次元リストと多次元配列\n\n\nCode\nx = np.array([np.arange(1, 4)] * 4) + np.array(np.arange(4) * 10).reshape(4, 1)\nx\n\n\narray([[ 1,  2,  3],\n       [11, 12, 13],\n       [21, 22, 23],\n       [31, 32, 33]])\n\n\n\n\nCode\n# 1行目\nx[0]\n\n\narray([1, 2, 3])\n\n\n\n\nCode\n# 1列目\nx[:, 0]\n\n\narray([ 1, 11, 21, 31])\n\n\n\n\nCode\n# 1要素\nx[0,0], x[0][0]\n\n\n(1, 1)\n\n\n\n\nCode\n# 1要素を配列で\nx[0, 0:1]\n\n\narray([1])\n\n\n\n4.1 行列の積み上げ\nnp.concatenateは末尾への結合であったが，np.stackは配列を積み重ねていくことが可能である.\n\n\nCode\nx1 = np.arange(3)\nx2 = 3 + np.arange(3)\nnp.concatenate([x1, x2])\n\n\narray([0, 1, 2, 3, 4, 5])\n\n\n\n\nCode\nnp.stack([x1, x2])\n\n\narray([[0, 1, 2],\n       [3, 4, 5]])",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#多次元配列のスライスの解釈",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#多次元配列のスライスの解釈",
    "title": "行列の導入",
    "section": "5 多次元配列のスライスの解釈",
    "text": "5 多次元配列のスライスの解釈\n\n\nCode\nanko_sell = np.array([[100, 50, 50], [50, 30, 100],[50, 50, 70], [100, 50, 100]])\nanko_sell\n\n\narray([[100,  50,  50],\n       [ 50,  30, 100],\n       [ 50,  50,  70],\n       [100,  50, 100]])\n\n\n\n\nCode\nanko_sell[0, ]\n\n\narray([100,  50,  50])\n\n\n\n\nCode\nanko_sell[:, 0]\n\n\narray([100,  50,  50, 100])\n\n\n\n\nCode\nanko_sell[::2, ::]\n\n\narray([[100,  50,  50],\n       [ 50,  50,  70]])\n\n\n\n5.1 行と列が反転したときのスライス\n\n\nCode\nannko_vertical = anko_sell.T\nannko_vertical\n\n\narray([[100,  50,  50, 100],\n       [ 50,  30,  50,  50],\n       [ 50, 100,  70, 100]])\n\n\n\n\n5.2 スライスと識別子\nスライスしたときに作成されるのは元の変数のViewである．\nスライスで作ったViewと元の変数は識別しが異なる，ということになる.\nただし配列を抜けるとそれは数値になるので，そこでも識別子は異なる.\n\n\nCode\nx = np.arange(5)\nprint(id(x[::]))\nprint(id(x[:]))\nprint(id(x))\n\n\n2627982381584\n2627982381584\n2627982384544\n\n\n\n\nCode\nx = np.array([[1, 2, 3], [11, 12, 13], [21, 22, 23]])\nprint(id(x[1,::]))\nprint(id(x[1,:]))\nprint(id(x[1,1]))\nprint(id(x[1]))\n\n\n2627982384544\n2627982384544\n2627981811504\n2627982384544",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#numpy配列のshape",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#numpy配列のshape",
    "title": "行列の導入",
    "section": "6 NumPy配列のshape",
    "text": "6 NumPy配列のshape\n\n\nCode\nx = np.arange(5)\nprint(x.shape)\nprint(len(x))\n\n\n(5,)\n5\n\n\n\n\nCode\nx = np.array([[1,2], [3,4], [5,6]])\nprint(x.shape)\nprint(len(x))\n\n\n(3, 2)\n3",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#集約関数",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#集約関数",
    "title": "行列の導入",
    "section": "7 集約関数",
    "text": "7 集約関数\n\n\nCode\nx = np.array([16, 12, 12, 11, 11])\nprint(np.sum(x)) # 62\nprint(np.prod(x)) # 278784p\nｐrint(np.mean(x)) # 12.4\nprint(np.std(x)) # 1.8547236990991407\nprint(np.median(x)) # 12.0\nprint(len(x)) # 5\nprint(np.max(x)) # 16\nprint(np.min(x)) # 11\nprint(np.argmax(x)) \nprint(np.argmin(x))\n\n\n62\n278784\n12.4\n1.8547236990991407\n12.0\n5\n16\n11\n0\n3\n\n\n集約関数ではないが，いわゆる最大値の管理，最小値の管理とう面で活用出来るのがnp.maxmium/minimumである．これらの関数はnp.clipと等価である.\n\n\nCode\nx = np.arange(10)\nprint(np.maximum(np.minimum(x, 7), 3)) # [3 3 3 3 4 5 6 7 7 7]\nprint(np.minimum(np.maximum(x, 3), 7))\n\n\n[3 3 3 3 4 5 6 7 7 7]\n[3 3 3 3 4 5 6 7 7 7]\n\n\n\n\nCode\nnp.clip(x, 3, 7)\n\n\narray([3, 3, 3, 3, 4, 5, 6, 7, 7, 7])",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#多次元配列と集約関数のaxis",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#多次元配列と集約関数のaxis",
    "title": "行列の導入",
    "section": "8 多次元配列と集約関数のaxis",
    "text": "8 多次元配列と集約関数のaxis\n\n\nCode\nanko_sell = np.array(\n    [[100, 50, 50], [50, 30, 100],[50, 50, 70], [100, 50, 100]])\nprint(np.sum(anko_sell)) \n\n\n800\n\n\n\n\nCode\n# 月単位\nprint(\"月単位\", np.sum(anko_sell, axis = 1))\n\n\n月単位 [200 180 170 250]\n\n\n\n\nCode\n# 商品単位\nprint(\"商品単位\", np.sum(anko_sell, axis = 0))\n\n\n商品単位 [300 180 320]",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#多次元配列の次元",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#多次元配列の次元",
    "title": "行列の導入",
    "section": "9 多次元配列の次元",
    "text": "9 多次元配列の次元\nnumpy配列の次元とはlen(x.shape)である.",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#多次元配列化出来ないリストのリスト",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#多次元配列化出来ないリストのリスト",
    "title": "行列の導入",
    "section": "10 多次元配列化出来ない「リストのリスト」",
    "text": "10 多次元配列化出来ない「リストのリスト」\nリストのリストは通常，NumPyヘア塚しても多次元配列にはならないが，入れ子になった内側のリストの要素数が全て同じ，つまり多次元リストに限定すれば多次元配列となる.\nとはいえ，下記とおり思わぬエラーが発生するもとであるので，基本的にはリストのリストか直接多次元配列を作成することはやめておいた方が良い.\n\n\nCode\nx = [[1, 2], [3, 4, 5], [6, 7, 8, 9]]\nprint(x)\nx = np.array(x)\nprint(x) # エラーにはならないが、多次元配列ではない\nprint(x.dtype) # object型になる\nprint(x.shape) # あたかも1次元配列のようにみなされる\nprint(x[:, 1]) # これはエラー\n\n\n[[1, 2], [3, 4, 5], [6, 7, 8, 9]]\n[list([1, 2]) list([3, 4, 5]) list([6, 7, 8, 9])]\nobject\n(3,)\n\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n&lt;ipython-input-35-7b13c4b5e3d2&gt; in &lt;module&gt;\n      5 print(x.dtype) # object型になる\n      6 print(x.shape) # あたかも1次元配列のようにみなされる\n----&gt; 7 print(x[:, 1]) # これはエラー\n\nIndexError: too many indices for array: array is 1-dimensional, but 2 were indexed",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#配列の結合でのaxis",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#配列の結合でのaxis",
    "title": "行列の導入",
    "section": "11 配列の結合でのaxis",
    "text": "11 配列の結合でのaxis",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#行列の導入",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#行列の導入",
    "title": "行列の導入",
    "section": "12 行列の導入",
    "text": "12 行列の導入\n\n\nCode\nsell_amount = np.array([[20, 10, 15], [15, 15, 20],[30, 25, 25]])\nprice = np.array([120, 120, 180])\nprint(price.shape) # (3, )\np = np.expand_dims(price, axis=-1) # 行列にするために軸を増やす。要素数1の軸を作る\nprint(p.shape) # (3, 1) これで行列になる\nresult = np.dot(sell_amount, p) # (3, 3) x (3, 1)の行列積\nprint(result.shape) # (3, 1)\nresult = result[:, 0] # 末尾の軸がいらないのでスライス\nprint(result.shape) # (3, ) ベクトルに戻ったprint(result\n\n\n(3,)\n(3, 1)\n(3, 1)\n(3,)",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#演習問題",
    "href": "contents/books/07_NumpyInfinity/ch03_行列の導入.html#演習問題",
    "title": "行列の導入",
    "section": "13 演習問題",
    "text": "13 演習問題\n\n13.1 q1:np.concatenate\n\n\nCode\na = np.arange(5)\nb = np.arange(3)\nnp.concatenate([a, b])\n\n\narray([0, 1, 2, 3, 4, 0, 1, 2])\n\n\n\n\nCode\na = np.arange(5)\nb = 0\nnp.concatenate([a, b])\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-42-3eb57bb2af36&gt; in &lt;module&gt;\n      1 a = np.arange(5)\n      2 b = 0\n----&gt; 3 np.concatenate([a, b])\n\n&lt;__array_function__ internals&gt; in concatenate(*args, **kwargs)\n\nValueError: all the input arrays must have same number of dimensions, but the array at index 0 has 1 dimension(s) and the array at index 1 has 0 dimension(s)\n\n\n\n\n\nCode\na = np.arange(5)\nb = np.arange(7)\nnp.concatenate([a,b], axis = 0)\n\n\narray([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 6])\n\n\n\n\nCode\nnp.concatenate([a, b], axis = 1)\n\n\n\n---------------------------------------------------------------------------\nAxisError                                 Traceback (most recent call last)\n&lt;ipython-input-45-3f904d3ecdb8&gt; in &lt;module&gt;\n----&gt; 1 np.concatenate([a, b], axis = 1)\n\n&lt;__array_function__ internals&gt; in concatenate(*args, **kwargs)\n\nAxisError: axis 1 is out of bounds for array of dimension 1\n\n\n\n\n\nCode\nnp.concatenate([np.arange(9) for _ in range(10)])\n\n\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3,\n       4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7,\n       8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2,\n       3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6,\n       7, 8])\n\n\n\n\n13.2 q2:np.stack\n\n\nCode\nnp.stack([np.arange(5) for i in range(2)], axis = 0)\n\n\narray([[0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4]])\n\n\n\n\nCode\nnp.vstack([np.arange(5) for i in range(2)])\n\n\narray([[0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4]])\n\n\n\n\nCode\nnp.stack([np.arange(5) for i in range(2)], axis = 1)\n\n\narray([[0, 0],\n       [1, 1],\n       [2, 2],\n       [3, 3],\n       [4, 4]])\n\n\n\n\nCode\nnp.stack([np.arange(i + 2) for i in range(4)], axis = 0)\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-52-39a67970cdd7&gt; in &lt;module&gt;\n----&gt; 1 np.stack([np.arange(i + 2) for i in range(4)], axis = 0)\n\n&lt;__array_function__ internals&gt; in stack(*args, **kwargs)\n\nC:\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py in stack(arrays, axis, out)\n    425     shapes = {arr.shape for arr in arrays}\n    426     if len(shapes) != 1:\n--&gt; 427         raise ValueError('all input arrays must have the same shape')\n    428 \n    429     result_ndim = arrays[0].ndim + 1\n\nValueError: all input arrays must have the same shape\n\n\n\n\n\nCode\nnp.stack([np.arange(2) for _ in range(8)], axis = 2)\n\n\n\n---------------------------------------------------------------------------\nAxisError                                 Traceback (most recent call last)\n&lt;ipython-input-54-1d9d105f408b&gt; in &lt;module&gt;\n----&gt; 1 np.stack([np.arange(2) for _ in range(8)], axis = 2)\n\n&lt;__array_function__ internals&gt; in stack(*args, **kwargs)\n\nC:\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py in stack(arrays, axis, out)\n    428 \n    429     result_ndim = arrays[0].ndim + 1\n--&gt; 430     axis = normalize_axis_index(axis, result_ndim)\n    431 \n    432     sl = (slice(None),) * axis + (_nx.newaxis,)\n\nAxisError: axis 2 is out of bounds for array of dimension 2\n\n\n\n\n\nCode\nnp.stack([np.arange(2) for _ in range(8)], axis = -1)\n\n\narray([[0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 1, 1, 1, 1, 1, 1]])\n\n\n\n\nCode\nnp.stack([np.arange(2) for _ in range(8)], axis = -2)\n\n\narray([[0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1]])\n\n\n\n\nCode\nnp.stack([np.arange(2) for _ in range(8)], axis = -3)\n\n\n\n---------------------------------------------------------------------------\nAxisError                                 Traceback (most recent call last)\n&lt;ipython-input-57-55463459fb3e&gt; in &lt;module&gt;\n----&gt; 1 np.stack([np.arange(2) for _ in range(8)], axis = -3)\n\n&lt;__array_function__ internals&gt; in stack(*args, **kwargs)\n\nC:\\anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py in stack(arrays, axis, out)\n    428 \n    429     result_ndim = arrays[0].ndim + 1\n--&gt; 430     axis = normalize_axis_index(axis, result_ndim)\n    431 \n    432     sl = (slice(None),) * axis + (_nx.newaxis,)\n\nAxisError: axis -3 is out of bounds for array of dimension 2\n\n\n\n\n\n13.3 q3:スライスのshape\n\n\nCode\nnp.random.seed(123)\nA = np.random.randn(16, 12)\nA.shape\n\n\n(16, 12)\n\n\n\n\nCode\nA[3] \n\n\narray([ 0.00284592,  0.68822271, -0.87953634,  0.28362732, -0.80536652,\n       -1.72766949, -0.39089979,  0.57380586,  0.33858905, -0.01183049,\n        2.39236527,  0.41291216])\n\n\n\n\nCode\nA[:, 2]\n\n\narray([ 0.2829785 , -0.44398196,  0.9071052 , -0.87953634, -1.29408532,\n        0.31427199, -2.12310035, -0.73246199,  0.37940061, -0.4276796 ,\n        0.12074736,  0.76094939,  0.4562709 ,  1.37725748, -1.04899168,\n        1.39535293])\n\n\n\n\nCode\nA[1:3, :].shape\n\n\n(2, 12)\n\n\n\n\nCode\nA[2:3, 4:8].shape\n\n\n(1, 4)\n\n\n\n\nCode\nA[4,6:7].shape\n\n\n(1,)\n\n\n\n\nCode\nA[::2, ::2].shape\n\n\n(8, 6)\n\n\n\n\nCode\nA[2::3, 1::4].shape\n\n\n(5, 3)\n\n\n\n\nCode\nA[-5::, -7:-4:2].shape\n\n\n(5, 2)\n\n\n\n\n13.4 q4:ベクトルの結合\n\n\nCode\nfrom mypy.mymodule import TimeExecute\nN = 10000\nwith TimeExecute():\n    x = []\n    for i in np.arange(1, N + 1):\n        x.append(np.full((i,), i))\n    y = np.concatenate(x)\n\n\n200.463 [msec]\n\n\n\n\n13.5 q5:ベクトルの行列化（１）\n\n\nCode\njan = np.array([100, 50, 50])\nfeb = np.array([50, 30, 100])\n\n# ベクトルの場合のaxisの指定は，\n# 指定された軸がゼロになるイメージで，そこに対してstackされる\nsells = np.stack([jan, feb], axis = 0)\nsells\n\n\narray([[100,  50,  50],\n       [ 50,  30, 100]])\n\n\n\n\n13.6 q6:ベクトルの行列化\n\n\nCode\nkoshian = np.array([100, 50])\ntsubuan = np.array([50, 30])\nkurian  = np.array([50, 100])\n\nsells = np.stack([koshian, tsubuan, kurian], axis = 1)\nsells\n\n\narray([[100,  50,  50],\n       [ 50,  30, 100]])\n\n\n\n\n13.7 q8:分散と標準偏差\n\n\nCode\nx = np.arange(100) ** 2\n\nx_mean = np.mean(x)\nx_mean\n\n\n3283.5\n\n\n\n\nCode\nnp.var(x), np.mean((x - x_mean) ** 2)\n\n\n(8721961.05, 8721961.05)\n\n\n\n\nCode\nnp.std(x), np.sqrt(np.mean((x - x_mean) ** 2))\n\n\n(2953.2966410436998, 2953.2966410436998)\n\n\n\n\n13.8 q9:平均値と中央時\n\n\nCode\ndata_file = \"numpy_book/data/income.npz\"\nincome = np.load(data_file)[\"income\"]\nincome\n\n\narray([ 675,  777, 1166, ...,  279,  315,  232])\n\n\n\n\nCode\nnp.mean(income), \\\nnp.median(income), \\\nnp.sum(income &gt;= np.mean(income)) / len(income)\n\n\n(555.0538, 426.0, 0.3805)\n\n\n\n\n13.9 q10: ポケモンの分析\n\n\nCode\ndata_file = \"numpy_book/data/pokemon.npz\"\npokemon   = np.load(data_file, allow_pickle = True)\nfor k, v in pokemon.items():\n    print(k, v)\n\n\nparams [[ 45  49  49  65  65  45]\n [ 60  62  63  80  80  60]\n [ 80  82  83 100 100  80]\n [ 39  52  43  60  50  65]\n [ 58  64  58  80  65  80]\n [ 78  84  78 109  85 100]\n [ 44  48  65  50  64  43]\n [ 59  63  80  65  80  58]\n [ 79  83 100  85 105  78]\n [ 45  30  35  20  20  45]\n [ 50  20  55  25  25  30]\n [ 60  45  50  90  80  70]\n [ 40  35  30  20  20  50]\n [ 45  25  50  25  25  35]\n [ 65  90  40  45  80  75]\n [ 40  45  40  35  35  56]\n [ 63  60  55  50  50  71]\n [ 83  80  75  70  70 101]\n [ 30  56  35  25  35  72]\n [ 55  81  60  50  70  97]\n [ 40  60  30  31  31  70]\n [ 65  90  65  61  61 100]\n [ 35  60  44  40  54  55]\n [ 60  95  69  65  79  80]\n [ 35  55  40  50  50  90]\n [ 60  90  55  90  80 110]\n [ 50  75  85  20  30  40]\n [ 75 100 110  45  55  65]\n [ 55  47  52  40  40  41]\n [ 70  62  67  55  55  56]\n [ 90  92  87  75  85  76]\n [ 46  57  40  40  40  50]\n [ 61  72  57  55  55  65]\n [ 81 102  77  85  75  85]\n [ 70  45  48  60  65  35]\n [ 95  70  73  95  90  60]\n [ 38  41  40  50  65  65]\n [ 73  76  75  81 100 100]\n [115  45  20  45  25  20]\n [140  70  45  85  50  45]\n [ 40  45  35  30  40  55]\n [ 75  80  70  65  75  90]\n [ 45  50  55  75  65  30]\n [ 60  65  70  85  75  40]\n [ 75  80  85 110  90  50]\n [ 35  70  55  45  55  25]\n [ 60  95  80  60  80  30]\n [ 60  55  50  40  55  45]\n [ 70  65  60  90  75  90]\n [ 10  55  25  35  45  95]\n [ 35 100  50  50  70 120]\n [ 40  45  35  40  40  90]\n [ 65  70  60  65  65 115]\n [ 50  52  48  65  50  55]\n [ 80  82  78  95  80  85]\n [ 40  80  35  35  45  70]\n [ 65 105  60  60  70  95]\n [ 55  70  45  70  50  60]\n [ 90 110  80 100  80  95]\n [ 40  50  40  40  40  90]\n [ 65  65  65  50  50  90]\n [ 90  95  95  70  90  70]\n [ 25  20  15 105  55  90]\n [ 40  35  30 120  70 105]\n [ 55  50  45 135  95 120]\n [ 70  80  50  35  35  35]\n [ 80 100  70  50  60  45]\n [ 90 130  80  65  85  55]\n [ 50  75  35  70  30  40]\n [ 65  90  50  85  45  55]\n [ 80 105  65 100  70  70]\n [ 40  40  35  50 100  70]\n [ 80  70  65  80 120 100]\n [ 40  80 100  30  30  20]\n [ 55  95 115  45  45  35]\n [ 80 120 130  55  65  45]\n [ 50  85  55  65  65  90]\n [ 65 100  70  80  80 105]\n [ 90  65  65  40  40  15]\n [ 95  75 110 100  80  30]\n [ 25  35  70  95  55  45]\n [ 50  60  95 120  70  70]\n [ 52  90  55  58  62  60]\n [ 35  85  45  35  35  75]\n [ 60 110  70  60  60 110]\n [ 65  45  55  45  70  45]\n [ 90  70  80  70  95  70]\n [ 80  80  50  40  50  25]\n [105 105  75  65 100  50]\n [ 30  65 100  45  25  40]\n [ 50  95 180  85  45  70]\n [ 30  35  30 100  35  80]\n [ 45  50  45 115  55  95]\n [ 60  65  60 130  75 110]\n [ 35  45 160  30  45  70]\n [ 60  48  45  43  90  42]\n [ 85  73  70  73 115  67]\n [ 30 105  90  25  25  50]\n [ 55 130 115  50  50  75]\n [ 40  30  50  55  55 100]\n [ 60  50  70  80  80 150]\n [ 60  40  80  60  45  40]\n [ 95  95  85 125  75  55]\n [ 50  50  95  40  50  35]\n [ 60  80 110  50  80  45]\n [ 50 120  53  35 110  87]\n [ 50 105  79  35 110  76]\n [ 90  55  75  60  75  30]\n [ 40  65  95  60  45  35]\n [ 65  90 120  85  70  60]\n [ 80  85  95  30  30  25]\n [105 130 120  45  45  40]\n [250   5   5  35 105  50]\n [ 65  55 115 100  40  60]\n [105  95  80  40  80  90]\n [ 30  40  70  70  25  60]\n [ 55  65  95  95  45  85]\n [ 45  67  60  35  50  63]\n [ 80  92  65  65  80  68]\n [ 30  45  55  70  55  85]\n [ 60  75  85 100  85 115]\n [ 40  45  65 100 120  90]\n [ 70 110  80  55  80 105]\n [ 65  50  35 115  95  95]\n [ 65  83  57  95  85 105]\n [ 65  95  57 100  85  93]\n [ 65 125 100  55  70  85]\n [ 75 100  95  40  70 110]\n [ 20  10  55  15  20  80]\n [ 95 125  79  60 100  81]\n [130  85  80  85  95  60]\n [ 48  48  48  48  48  48]\n [ 55  55  50  45  65  55]\n [130  65  60 110  95  65]\n [ 65  65  60 110  95 130]\n [ 65 130  60  95 110  65]\n [ 65  60  70  85  75  40]\n [ 35  40 100  90  55  35]\n [ 70  60 125 115  70  55]\n [ 30  80  90  55  45  55]\n [ 60 115 105  65  70  80]\n [ 80 105  65  60  75 130]\n [160 110  65  65 110  30]\n [ 90  85 100  95 125  85]\n [ 90  90  85 125  90 100]\n [ 90 100  90 125  85  90]\n [ 41  64  45  50  50  50]\n [ 61  84  65  70  70  70]\n [ 91 134  95 100 100  80]\n [106 110  90 154  90 130]\n [100 100 100 100 100 100]]\nparams_header ['HP' '攻撃' '防御' '特攻' '特防' '素早']\nnames ['フシギダネ' 'フシギソウ' 'フシギバナ' 'ヒトカゲ' 'リザード' 'リザードン' 'ゼニガメ' 'カメール' 'カメックス'\n 'キャタピー' 'トランセル' 'バタフリー' 'ビードル' 'コクーン' 'スピアー' 'ポッポ' 'ピジョン' 'ピジョット' 'コラッタ'\n 'ラッタ' 'オニスズメ' 'オニドリル' 'アーボ' 'アーボック' 'ピカチュウ' 'ライチュウ' 'サンド' 'サンドパン' 'ニドラン♀'\n 'ニドリーナ' 'ニドクイン' 'ニドラン♂' 'ニドリーノ' 'ニドキング' 'ピッピ' 'ピクシー' 'ロコン' 'キュウコン' 'プリン'\n 'プクリン' 'ズバット' 'ゴルバット' 'ナゾノクサ' 'クサイハナ' 'ラフレシア' 'パラス' 'パラセクト' 'コンパン'\n 'モルフォン' 'ディグダ' 'ダグトリオ' 'ニャース' 'ペルシアン' 'コダック' 'ゴルダック' 'マンキー' 'オコリザル'\n 'ガーディ' 'ウインディ' 'ニョロモ' 'ニョロゾ' 'ニョロボン' 'ケーシィ' 'ユンゲラー' 'フーディン' 'ワンリキー'\n 'ゴーリキー' 'カイリキー' 'マダツボミ' 'ウツドン' 'ウツボット' 'メノクラゲ' 'ドククラゲ' 'イシツブテ' 'ゴローン'\n 'ゴローニャ' 'ポニータ' 'ギャロップ' 'ヤドン' 'ヤドラン' 'コイル' 'レアコイル' 'カモネギ' 'ドードー' 'ドードリオ'\n 'パウワウ' 'ジュゴン' 'ベトベター' 'ベトベトン' 'シェルダー' 'パルシェン' 'ゴース' 'ゴースト' 'ゲンガー' 'イワーク'\n 'スリープ' 'スリーパー' 'クラブ' 'キングラー' 'ビリリダマ' 'マルマイン' 'タマタマ' 'ナッシー' 'カラカラ' 'ガラガラ'\n 'サワムラー' 'エビワラー' 'ベロリンガ' 'ドガース' 'マタドガス' 'サイホーン' 'サイドン' 'ラッキー' 'モンジャラ'\n 'ガルーラ' 'タッツー' 'シードラ' 'トサキント' 'アズマオウ' 'ヒトデマン' 'スターミー' 'バリヤード' 'ストライク'\n 'ルージュラ' 'エレブー' 'ブーバー' 'カイロス' 'ケンタロス' 'コイキング' 'ギャラドス' 'ラプラス' 'メタモン' 'イーブイ'\n 'シャワーズ' 'サンダース' 'ブースター' 'ポリゴン' 'オムナイト' 'オムスター' 'カブト' 'カブトプス' 'プテラ' 'カビゴン'\n 'フリーザー' 'サンダー' 'ファイヤー' 'ミニリュウ' 'ハクリュー' 'カイリュー' 'ミュウツー' 'ミュウ']\nnumbers [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n 145 146 147 148 149 150 151]\n\n\n\n\n13.10 q11:種族値合計\n\nparams:種族値\nparams_header:種族値のヘッダー\nnames:ポケモンの名前\nnumbers:ポケモン図鑑No.\n\n\n\nCode\ntotal_params = np.sum(pokemon[\"params\"], axis = 1)\nprint(f\"\"\"\n平均 = {np.mean(total_params)}, \n最大 = {np.max(total_params)}, \n最小 = {np.min(total_params)}, \n中央値 = {np.median(total_params)}\"\"\")\n\n\n\n平均 = 407.6423841059603, \n最大 = 680, \n最小 = 195, \n中央値 = 405.0\n\n\n\n\n13.11 q12:種族値の集約\n\n\nCode\nfor i, v in enumerate(pokemon[\"params_header\"]):\n    print(f\"{v}\")\n    data = pokemon[\"params\"][:, i]\n    print(f\"\"\"\n    平均 = {np.mean(data)}, \n    最大 = {np.max(data)}, \n    最小 = {np.min(data)}, \n    中央値 = {np.median(data)}\"\"\")  \n    print(\"-\"* 10)\n\n\nHP\n\n    平均 = 64.21192052980132, \n    最大 = 250, \n    最小 = 10, \n    中央値 = 60.0\n----------\n攻撃\n\n    平均 = 72.91390728476821, \n    最大 = 134, \n    最小 = 5, \n    中央値 = 70.0\n----------\n防御\n\n    平均 = 68.2251655629139, \n    最大 = 180, \n    最小 = 5, \n    中央値 = 65.0\n----------\n特攻\n\n    平均 = 67.13907284768212, \n    最大 = 154, \n    最小 = 15, \n    中央値 = 65.0\n----------\n特防\n\n    平均 = 66.08609271523179, \n    最大 = 125, \n    最小 = 20, \n    中央値 = 65.0\n----------\n素早\n\n    平均 = 69.06622516556291, \n    最大 = 150, \n    最小 = 15, \n    中央値 = 70.0\n----------\n\n\n\n\n13.12 q13:種族値のランキング\nnp.argsortにより，ソート結果がインデックスで表示されるので，わたしがやっているようにわざわざ構造体の 配列を作成して処理行う必要はなかった。\nとはいえ，いい勉強になった気がする.\n\n\nCode\ndtype  = [(\"name\", \"U10\"), (\"param\", int)]\nvalues = [(x, y) for x, y in zip(pokemon[\"names\"], total_params)]\ntbl = np.array(values, dtype = dtype)\n\n\n\n\nCode\ntbl.sort(order = \"param\")\n\n\n\n\nCode\n# worst 10\ntbl[:10]\n\n\narray([('キャタピー', 195), ('ビードル', 195), ('コイキング', 200), ('コクーン', 205),\n       ('トランセル', 205), ('ズバット', 245), ('ポッポ', 251), ('コラッタ', 253),\n       ('オニスズメ', 262), ('ディグダ', 265)],\n      dtype=[('name', '&lt;U10'), ('param', '&lt;i4')])\n\n\n\n\nCode\n# top10\ntbl[-10:]\n\n\narray([('ラプラス', 535), ('カビゴン', 540), ('ギャラドス', 540), ('ウインディ', 555),\n       ('サンダー', 580), ('ファイヤー', 580), ('フリーザー', 580), ('カイリュー', 600),\n       ('ミュウ', 600), ('ミュウツー', 680)],\n      dtype=[('name', '&lt;U10'), ('param', '&lt;i4')])\n\n\n\n\nCode\nhensachi = (total_params - np.mean(total_params)) * 10 / np.std(total_params) + 50\nnp.min(hensachi), np.max(hensachi)\n\n\n(28.638341700682517, 77.36053938826282)\n\n\n\n\n13.13 q15: 偏差値ソート\n\n\nCode\ndtype  = [(\"name\", \"U10\"), (\"param\", float)]\nvalues = [(x, y) for x, y in zip(pokemon[\"names\"], hensachi)]\ntbl = np.array(values, dtype = dtype)\ntbl.sort(order = \"param\")\ntbl[:10], tbl[-10:]\n\n\n(array([('キャタピー', 28.6383417 ), ('ビードル', 28.6383417 ),\n        ('コイキング', 29.1406324 ), ('コクーン', 29.6429231 ),\n        ('トランセル', 29.6429231 ), ('ズバット', 33.66124868),\n        ('ポッポ', 34.26399752), ('コラッタ', 34.4649138 ),\n        ('オニスズメ', 35.36903705), ('ディグダ', 35.67041147)],\n       dtype=[('name', '&lt;U10'), ('param', '&lt;f8')]),\n array([('ラプラス', 62.79410915), ('カビゴン', 63.29639985),\n        ('ギャラドス', 63.29639985), ('ウインディ', 64.80327194),\n        ('サンダー', 67.31472543), ('ファイヤー', 67.31472543),\n        ('フリーザー', 67.31472543), ('カイリュー', 69.32388822),\n        ('ミュウ', 69.32388822), ('ミュウツー', 77.36053939)],\n       dtype=[('name', '&lt;U10'), ('param', '&lt;f8')]))\n\n\n\n\nCode\nv = pokemon[\"params\"]\nhensachi2 = (v - np.mean(v, axis = 0, keepdims=True)) * 10  / np.std(v, axis = 0, keepdims=True) + 50\nfor i in np.arange(len(v)):\n    v = np.sort(hensachi2[:, i])\n    print(v[:10], v[-10:])\n\n\n[30.97512971 34.48448155 36.23915747 36.23915747 37.99383338 37.99383338\n 37.99383338 37.99383338 37.99383338 37.99383338] [ 64.31397216  64.31397216  64.31397216  64.66490734  67.823324\n  73.08735175  73.08735175  76.59670359  83.61540726 115.19957379]\n[24.53229698 26.40729609 30.15729429 30.15729429 32.03229339 33.90729249\n 33.90729249 35.78229159 35.78229159 35.78229159] [65.7822772  67.6572763  67.6572763  69.5322754  69.5322754  71.4072745\n 71.4072745  71.4072745  71.4072745  72.90727378]\n[26.43263987 30.16016829 32.0239325  33.8876967  35.75146091 35.75146091\n 35.75146091 35.75146091 37.61522512 37.61522512] [65.57168825 67.43545246 67.43545246 67.43545246 69.29921666 69.29921666\n 71.16298087 73.02674508 84.20933033 91.66438717]\n[31.6667079  33.42482241 33.42482241 33.42482241 35.18293692 35.18293692\n 35.18293692 35.18293692 36.94105143 36.94105143] [66.82899809 66.82899809 68.5871126  68.5871126  70.34522711 70.34522711\n 70.34522711 72.10334162 73.86145613 80.54229127]\n[30.89972952 30.89972952 30.89972952 32.97196752 32.97196752 32.97196752\n 32.97196752 32.97196752 32.97196752 35.04420551] [66.12777544 66.12777544 68.20001344 68.20001344 68.20001344 68.20001344\n 70.27225144 72.34448943 72.34448943 74.41672743]\n[29.92712204 31.78344527 31.78344527 33.6397685  33.6397685  33.6397685\n 35.49609173 35.49609173 35.49609173 35.49609173] [65.19726343 65.19726343 67.05358666 67.05358666 68.9099099  68.9099099\n 72.62255636 72.62255636 72.62255636 80.04784928]\n\n\n\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n&lt;ipython-input-47-aaef69bb8869&gt; in &lt;module&gt;\n      2 hensachi2 = (v - np.mean(v, axis = 0, keepdims=True)) * 10  / np.std(v, axis = 0, keepdims=True) + 50\n      3 for i in np.arange(len(v)):\n----&gt; 4     v = np.sort(hensachi2[:, i])\n      5     print(v[:10], v[-10:])\n\nIndexError: index 6 is out of bounds for axis 1 with size 6\n\n\n\n\n\nCode\ntotal_hensachi = np.sum(hensachi2, axis = 1)\ntotal_hensachi\n\n\narray([266.98670102, 299.40429343, 344.12475328, 263.22003676,\n       298.79723743, 346.6823034 , 265.79349106, 299.74828355,\n       346.5409814 , 220.16999211, 223.89110947, 295.41238503,\n       220.28287432, 224.00399168, 296.34781708, 241.47904633,\n       277.82687269, 326.259979  , 242.65493362, 302.69998443,\n       245.50993826, 312.69971633, 256.10173332, 315.75230634,\n       267.5884241 , 329.00172699, 259.72545478, 315.6260296 ,\n       250.22749063, 283.76783552, 336.38276957, 249.6874199 ,\n       283.97327047, 335.95995755, 268.41656752, 327.83337136,\n       260.32633   , 337.18064313, 246.35035403, 307.52527238,\n       239.55814096, 317.8189457 , 267.54547722, 295.39914637,\n       330.59939705, 254.9866355 , 299.94568838, 261.938432  ,\n       315.50231661, 247.73349359, 307.61783579, 256.0686326 ,\n       311.96920741, 266.98955604, 334.07024582, 262.08245686,\n       317.98303168, 277.99040793, 354.29585927, 259.8073959 ,\n       291.18529885, 338.57632737, 263.04675511, 296.5871    ,\n       334.27192089, 261.06306639, 298.87565143, 336.71070625,\n       258.66895779, 292.20930268, 329.89412356, 275.15142553,\n       341.59699116, 259.77333076, 293.31367565, 332.57117084,\n       301.1847966 , 334.72514149, 264.45312496, 329.97592473,\n       268.95002061, 320.70611944, 288.91902357, 263.6421557 ,\n       323.25537698, 269.78170242, 325.68227724, 268.83460026,\n       335.09636505, 261.26638008, 342.84711314, 262.25800801,\n       297.87059089, 333.48317378, 292.03877794, 271.89640979,\n       329.55509911, 269.21903289, 325.11960771, 271.2386319 ,\n       330.92738364, 268.52767877, 344.1570508 , 267.54307448,\n       307.556017  , 320.55194558, 320.53461105, 291.53774888,\n       274.61893999, 330.51951481, 275.67829623, 327.70551598,\n       313.91283669, 308.37167245, 330.98119238, 256.9246648 ,\n       310.75300162, 267.75434163, 316.11427103, 274.9234154 ,\n       342.00410519, 321.50439989, 335.16677147, 317.73967011,\n       330.85093913, 332.65387572, 334.92238078, 331.20024553,\n       222.58782106, 350.32911051, 346.90637896, 255.38234933,\n       269.79899305, 342.5982215 , 343.91963656, 345.10479332,\n       295.27882319, 280.04619583, 331.58032416, 280.53799966,\n       332.94137665, 340.17732426, 349.2644539 , 365.55530159,\n       363.45099885, 363.2798768 , 260.08221194, 304.80267179,\n       371.95806505, 399.76472594, 371.65554495])\n\n\n\n\nCode\ndtype  = [(\"name\", \"U10\"), (\"param\", float)]\nvalues = [(x, y) for x, y in zip(pokemon[\"names\"], total_hensachi)]\ntbl = np.array(values, dtype = dtype)\ntbl.sort(order = \"param\")\ntbl[:10], tbl[-10:]\n\n\n(array([('キャタピー', 220.16999211), ('ビードル', 220.28287432),\n        ('コイキング', 222.58782106), ('トランセル', 223.89110947),\n        ('コクーン', 224.00399168), ('ズバット', 239.55814096),\n        ('ポッポ', 241.47904633), ('コラッタ', 242.65493362),\n        ('オニスズメ', 245.50993826), ('プリン', 246.35035403)],\n       dtype=[('name', '&lt;U10'), ('param', '&lt;f8')]),\n array([('ラプラス', 346.90637896), ('カビゴン', 349.2644539 ),\n        ('ギャラドス', 350.32911051), ('ウインディ', 354.29585927),\n        ('ファイヤー', 363.2798768 ), ('サンダー', 363.45099885),\n        ('フリーザー', 365.55530159), ('ミュウ', 371.65554495),\n        ('カイリュー', 371.95806505), ('ミュウツー', 399.76472594)],\n       dtype=[('name', '&lt;U10'), ('param', '&lt;f8')]))\n\n\n\n\n13.14 q18：コサイン類似度\n\n\nCode\nidx_purin, *_ = np.where(pokemon[\"names\"] == \"プリン\")\nidx_purin\n\n\narray([38], dtype=int64)\n\n\n\n\nCode\nparams = pokemon[\"params\"]\nparams_norm = np.sqrt(np.sum(params * params, axis = 1))\nparams_norm\n\n\narray([131.53706702, 166.77229986, 215.43676566, 128.05858035,\n       166.88019655, 219.84085153, 130.03845585, 166.85023224,\n       217.86234186,  83.51646544,  89.86100378, 165.90660023,\n        83.81527307,  87.32124598, 167.25728684, 103.97595876,\n       143.64887748, 197.26885208, 110.88282103, 173.1906464 ,\n       113.67497526, 184.36919482, 119.59096956, 185.07295859,\n       137.65899898, 203.2855135 , 135.09256086, 192.35384062,\n       113.22102278, 149.72975656, 206.78249442, 112.53888217,\n       149.76314633, 207.28965242, 135.19985207, 199.94749311,\n       125.19984026, 208.06489372, 136.74794331, 195.63997547,\n       101.85774394, 186.74849397, 135.27749258, 164.84841522,\n       204.81699148, 121.75795662, 172.98843892, 125.59856687,\n       185.87630295, 126.58988901, 188.48076825, 127.08265027,\n       185.47236991, 131.36970731, 204.59227747, 131.81426326,\n       190.72231123, 144.74114826, 228.08989456, 130.3840481 ,\n       160.54594358, 209.88091862, 152.97058541, 184.52642087,\n       221.81073013, 132.19304066, 171.53716798, 214.18449991,\n       129.42179106, 164.62077633, 203.59273072, 147.73286703,\n       215.2324325 , 142.12670404, 174.78558293, 216.73716802,\n       171.17242769, 207.24381776, 141.33294025, 209.88091862,\n       144.3086969 , 198.30532015, 156.96177879, 136.19838472,\n       199.74984355, 135.        , 195.51214796, 141.50971698,\n       210.71307506, 139.19410907, 240.98755155, 143.70107863,\n       178.39562775, 214.59263734, 191.50718002, 140.14991973,\n       201.23866428, 154.19143945, 209.22475953, 145.08618129,\n       215.17434791, 137.20422734, 222.5982929 , 139.10427743,\n       181.72781845, 201.50186103, 197.14715316, 163.93596311,\n       147.30919863, 206.03397778, 158.35087622, 219.03196114,\n       278.02877549, 188.87826768, 206.27651345, 128.54960132,\n       185.87630295, 133.44661854, 185.30515373, 145.25839046,\n       216.56407828, 200.87309427, 209.40391591, 198.0530232 ,\n       204.05391444, 205.84703058, 212.13203436, 207.96634343,\n       102.71319292, 225.94689642, 224.44375687, 117.57550765,\n       133.51029923, 223.77444001, 223.77444001, 223.77444001,\n       164.84841522, 158.66631653, 212.77922831, 153.21553446,\n       208.266656  , 218.57492994, 243.20773014, 239.16521486,\n       239.06066176, 239.06066176, 123.70125303, 172.34268189,\n       248.31834407, 283.11128554, 244.94897428])\n\n\n\n\nCode\nparams_inner_dot = np.sum(params * params[idx_purin, :], axis = 1)\n\n\n\n\nCode\ncos_norm = params_inner_dot / params_norm / params_norm[idx_purin]\ncos_norm\n\n\narray([0.76775866, 0.77831184, 0.78579701, 0.73864701, 0.76838728,\n       0.77637519, 0.74399024, 0.75844481, 0.76630726, 0.83401157,\n       0.82191954, 0.76584406, 0.80050142, 0.81651448, 0.78042715,\n       0.7732884 , 0.81272924, 0.79941104, 0.6667547 , 0.72244497,\n       0.73786642, 0.75737419, 0.72490712, 0.75291367, 0.66933747,\n       0.73024536, 0.71859186, 0.76129129, 0.84610336, 0.83759717,\n       0.82222066, 0.80932011, 0.80786913, 0.80592149, 0.8686574 ,\n       0.85069317, 0.71199858, 0.75424095, 1.        , 0.9765127 ,\n       0.77357499, 0.79197344, 0.76355813, 0.78628622, 0.78994434,\n       0.73122464, 0.7640828 , 0.84132222, 0.78290352, 0.5040175 ,\n       0.61786215, 0.68620096, 0.73630981, 0.79740434, 0.80153485,\n       0.72120735, 0.75725961, 0.80710131, 0.81113599, 0.68705392,\n       0.77433481, 0.81269557, 0.57246217, 0.63506573, 0.67008084,\n       0.87403272, 0.84195341, 0.81770504, 0.82070661, 0.82513189,\n       0.8216333 , 0.65587028, 0.75596467, 0.65344228, 0.70079081,\n       0.74902925, 0.72733168, 0.7436442 , 0.91452425, 0.82227719,\n       0.62835978, 0.69695804, 0.76825596, 0.68188472, 0.71937494,\n       0.82742121, 0.8237992 , 0.89658695, 0.85807256, 0.62124011,\n       0.60613781, 0.64119439, 0.68763428, 0.71391812, 0.50117967,\n       0.78188538, 0.79799488, 0.60349924, 0.68068093, 0.64515359,\n       0.65081489, 0.7954741 , 0.83771742, 0.71758171, 0.71828369,\n       0.66321986, 0.6639597 , 0.86984039, 0.69250599, 0.73470111,\n       0.80931344, 0.8104588 , 0.90150245, 0.73367956, 0.83487283,\n       0.66130442, 0.71798941, 0.73841481, 0.82576632, 0.64438873,\n       0.70573078, 0.62707092, 0.73946533, 0.73384588, 0.74720596,\n       0.75934776, 0.72478458, 0.7305117 , 0.47167065, 0.80750155,\n       0.9049524 , 0.80605994, 0.79694329, 0.90520822, 0.70341541,\n       0.74671508, 0.80181232, 0.64293736, 0.73117197, 0.64671911,\n       0.71804682, 0.74942274, 0.92984708, 0.77280931, 0.79455989,\n       0.8014425 , 0.76821252, 0.78052379, 0.79497546, 0.79349323,\n       0.80605994])\n\n\n\n\nCode\ndtype  = [(\"name\", \"U10\"), (\"param\", float)]\nvalues = [(x, y) for x, y in zip(pokemon[\"names\"], cos_norm)]\ntbl = np.array(values, dtype = dtype)\ntbl.sort(order = \"param\")\ntbl[:10], tbl[-10:]\n\n\n(array([('コイキング', 0.47167065), ('イワーク', 0.50117967), ('ディグダ', 0.5040175 ),\n        ('ケーシィ', 0.57246217), ('クラブ', 0.60349924), ('パルシェン', 0.60613781),\n        ('ダグトリオ', 0.61786215), ('シェルダー', 0.62124011),\n        ('バリヤード', 0.62707092), ('コイル', 0.62835978)],\n       dtype=[('name', '&lt;U10'), ('param', '&lt;f8')]),\n array([('ベロリンガ', 0.86984039), ('ワンリキー', 0.87403272),\n        ('ベトベター', 0.89658695), ('ラッキー', 0.90150245), ('ラプラス', 0.9049524 ),\n        ('シャワーズ', 0.90520822), ('ヤドン', 0.91452425), ('カビゴン', 0.92984708),\n        ('プクリン', 0.9765127 ), ('プリン', 1.        )],\n       dtype=[('name', '&lt;U10'), ('param', '&lt;f8')]))\n\n\nなんかすべてのポケモンをまとめて計算出来そうな気がする.\n\n\nCode\n# ポケモン vs ポケモン のノルム表\ninner_prd = np.sum(params[np.newaxis, :, :] * params[:, np.newaxis, :], axis = 2)\ninner_prd.shape\n\n\n(151, 151)\n\n\n\n\nCode\nnorms = np.outer(params_norm, params_norm)\n\n\n\n\nCode\ncos_norm = inner_prd / norms\n\n\n\n\nCode\ncos_norm[idx_purin, :5]\n\n\narray([[0.76775866, 0.77831184, 0.78579701, 0.73864701, 0.76838728]])\n\n\n\n\n13.15 q19:画像入門\n\n\nCode\nimg = np.zeros((400, 400), dtype = np.uint8)\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nimg[100:200, 100:200] = 255\nplt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\nCode\nimg[200:350, 100:200] = 255\nplt.imshow(img, cmap = \"gray\", vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n13.16 q20:行列積の可換性\n\n\nCode\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nB = np.array([[-4, -3, -2], [-1, 0, 1], [2, 3, 4]])\n\n\n\n\nCode\nA.dot(B)\n\n\narray([[  0,   6,  12],\n       [ -9,   6,  21],\n       [-18,   6,  30]])\n\n\n\n\nCode\n\nB.dot(A)\n\n\narray([[-30, -39, -48],\n       [  6,   6,   6],\n       [ 42,  51,  60]])\n\n\n\n\n13.17 q22:あんこ商店\n\n\nCode\ndata_file = \"numpy_book/data/anko_shop.npz\"\nanko_sell = np.load(data_file)[\"sell\"]\nanko_sell.shape\n\n\n(100, 4)\n\n\n\n\nCode\nnontax_price = np.diag([120, 180, 270, 120])\nnontax_price\n\n\narray([[ 120,    0,    0,    0],\n       [   0,  180,    0,    0],\n       [   0,    0,  270,    0],\n       [   0,    0,    0, 1200]])\n\n\n\n\nCode\nanko_sell.dot(nontax_price).sum()\n\n\n287970\n\n\n\n\n13.18 q23:アファイン変換入門\n\n\nCode\ncharacter = np.load(\"numpy_book/data/ankochan.npz\")[\"points\"]\nprint(character.shape)\n\n\n(2, 3465)\n\n\n\n\nCode\nfig = plt.figure(figsize = (10, 8))\nax = fig.gca()\nax.plot(\n    character[0], \n    character[1], \n    linewidth = 0, \n    markersize = 2, \n    marker = \"o\"\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n13.19 q24:アファイン変換ー回転\n\n\nCode\ndef gen_pmat(theta):\n    c = np.cos(theta)\n    s = np.sin(theta)\n    return np.array([c, -s, s, c]).reshape((2, 2))\n\n\n\n\n\nCode\nthetas = [np.pi / 6, np.pi / 3, np.pi / 2, 4 * np.pi / 3]\n\nfig = plt.figure(figsize = (14, 12))\n\nfor i, theta in enumerate(thetas):\n    rotated_points = np.dot(gen_pmat(theta), character)\n    ax = fig.add_subplot(2, 2, i + 1)\n    ax.plot(\n        rotated_points[0], \n        rotated_points[1], \n        linewidth = 0, \n        markersize = 2, \n        marker = \"o\"\n    )\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n13.20 q25: せん断変形\n\n\nCode\ndef gen_skewmat(theta):\n    t = np.tan(theta)\n    return np.array([1, 0, t, 1]).reshape((2, 2))\n\n\n\n\nCode\nthetas = [np.pi / 6, np.pi / 3, np.pi / 2, 4 * np.pi / 3]\n\nfig = plt.figure(figsize = (14, 12))\n\nfor i, theta in enumerate(thetas):\n    rotated_points = np.dot(gen_skewmat(theta), character)\n    ax = fig.add_subplot(2, 2, i + 1)\n    ax.plot(\n        rotated_points[0], \n        rotated_points[1], \n        linewidth = 0, \n        markersize = 2, \n        marker = \"o\"\n    )\nplt.show()",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "行列の導入"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理_Q52.html",
    "href": "contents/books/07_NumpyInfinity/ch04_テンソルと画像処理_Q52.html",
    "title": "Q52：セピア化",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import (Image)\n\n\nグレースケール化したときに，白は白になり，黒はセピア色になる処理.\n\n\nCode\n## Q52\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as orig:\n    g = np.array(orig.convert(\"L\")) / 255.0\nimg = np.stack([g*255+(1-g)*107, (g*255)+(1-g)*74, g*255+(1-g)*43], axis=-1).astype(np.uint8)\nplt.imshow(img)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n0.1 Q53：CSSでのセピア化\n\n\nCode\ndef generate_sepia(alpha):\n    v = np.array([\n            393,  769,  189,  349, 686,  168,  272,  534,  131\n        ]).reshape((3,-1)) / 1000\n    a = np.array([\n            607, -769, -189, -349, 314, -168, -272, -534,  869\n        ]).reshape((3,-1)) / 1000\n    return v  + (1 - alpha) * a\n\n\n\n\nCode\nfig = plt.figure(figsize = (16, 16))\nsepia_rate = [.25, .5, .75, 1.]\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    flower = np.array(img)\n\nfor i, alpha in enumerate(sepia_rate, start = 1):\n    sepia = generate_sepia(alpha)\n    img   = np.clip(np.dot(flower, sepia.T), 0, 255).astype(np.uint8)\n    ax    = fig.add_subplot(2, 2, i)\n    ax.imshow(img, vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n0.2 Q54：トーンカーブ\n\\[\ny = \\frac{128}{128-a}(x-a)\n\\]\n\n\nCode\nfrom functools import partial\n\ndef tone(x, a):\n    return 255 / (255 - a) * (x.astype(float)  - a)\n\n\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as oimg:\n    flower = np.array(oimg)\n\nflower.shape\n\n\n(960, 1280, 3)\n\n\n\n\nCode\nfig = plt.figure(figsize = (12, 12))\na = [0, 10, 20, 40]\nfor i, v in enumerate(a, start = 1):\n    img = flower.copy()\n    img[..., 1] = np.clip(tone(flower[..., 1], v), 0, 255).astype(np.uint8)\n    ax = fig.add_subplot(2, 2, i)\n    ax.imshow(img, vmin = 0, vmax = 255)\n\n\n\n\n\n\n\n\n\n\n\n\n0.3 Q55：トーンカーブ（ガンマ補正）\n\n\nCode\ndef generate_gamma_tone(gamma):\n    def f(x, gamma):\n        return 255 * (x / 255) ** (1/gamma)\n    return partial(f, gamma = gamma)\n    \n\n\n\n\nCode\nx  = np.arange(256)\ngs = [.5, 1, 1.5, 2]\nfor g in gs:\n    gt = generate_gamma_tone(g)\n    y  = gt(x)\n    plt.plot(x, y, label = f\"{g}\")\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic01.jpg\") as oimg:\n    pic = np.array(oimg)\n\n\n\n\nCode\nfig = plt.figure(figsize = (16, 16))\nfor i, g in enumerate(gs, start = 1):\n    gt  = generate_gamma_tone(g)\n    img = np.clip(gt(pic.copy()), 0, 255).astype(np.uint8)\n    ax = fig.add_subplot(2, 2, i)\n    ax.imshow(img)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n0.4 Q56:トーンカーブ（S字曲線）\n\n\nCode\ndef sigmoid(x, a):\n    v1 = np.exp(-a * (2 * x - 1))\n    v2 = np.exp(-a)\n    y  = (1  + (1 - v1) / (1 + v1) * (1 + v2) / (1 - v2)) / 2\n    return y\n\n\n\n\n\nCode\na_seq = [1, 2, 4, 8]\n\n# 正規化していないとだめのようです\nx = np.arange(256) / 256\n\nfig = plt.figure(figsize = (12,12))\nfor i, a in enumerate(a_seq):\n    y = sigmoid(x, a)\n    plt.plot(x, y, label = f\"{a}\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig = plt.figure(figsize = (16, 16))\nfor i, a in enumerate(a_seq, start = 1):\n    y   = sigmoid(pic.copy() / 255, a)\n    img = np.clip(y * 255, 0, 255).astype(np.uint8)\n    ax = fig.add_subplot(2, 2, i)\n    ax.imshow(img)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n0.5 Q57:チャンネル別のカーブ\n\n\nCode\ndef apply_tone(x, a):\n    # 計算上値がuintでオバーフローするので最初に整数に変えておく\n    return np.clip(255 / (255 - a) * (x.astype(float) - a), 0, 255).astype(np.uint8)\n\n\n\n\nCode\na_seq = [0, 32, 64, 128]\nx = np.arange(256)\nfor i, a in enumerate(a_seq, start = 1):\n    y = np.clip(apply_tone(x, a), 0, 255).astype(np.uint8)\n    plt.plot(x, y, label = f\"{a}\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    flower = np.array(img)\n\n\n\n\nCode\nplt.imshow(flower)\n\n\n\n\n\n\n\n\n\n\n\nCode\nflower[...,1]\n\n\narray([[181, 178, 177, ..., 189, 191, 192],\n       [179, 177, 177, ..., 191, 192, 192],\n       [178, 179, 179, ..., 192, 193, 194],\n       ...,\n       [ 59,  56,  56, ..., 102, 103, 104],\n       [ 58,  57,  58, ..., 102, 102, 102],\n       [ 53,  55,  56, ..., 102, 101, 102]], dtype=uint8)\n\n\n\n\nCode\napply_tone(flower[...,1], 128)\n\n\narray([[106, 100,  98, ..., 122, 126, 128],\n       [102,  98,  98, ..., 126, 128, 128],\n       [100, 102, 102, ..., 128, 130, 132],\n       ...,\n       [  0,   0,   0, ...,   0,   0,   0],\n       [  0,   0,   0, ...,   0,   0,   0],\n       [  0,   0,   0, ...,   0,   0,   0]], dtype=uint8)\n\n\n\n\nCode\nfig = plt.figure(figsize = (12, 12))\n\na_seq = [0, 32, 64, 128]\n\nfor i, a in enumerate(a_seq, start = 1):\n    f = flower.copy()\n    f[..., 1] = apply_tone(f[..., 1], a).astype(np.uint8)\n    ax = fig.add_subplot(2, 2, i)\n    ax.imshow(f)\n\n\n\n\n\n\n\n\n\n\n\n0.6 Q58：色合わせていく世界\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic01.jpg\") as img:\n    pic_hsv = np.array(img.convert(\"HSV\"))\n\npic_hsv.shape\n\n\n(700, 1050, 3)\n\n\n\n\nCode\nfig = plt.figure(figsize = (12, 12))\n\na_seq = [0, 32, 64, 128]\n\nfor i, a in enumerate(a_seq, start = 1):\n    img  = pic_hsv.copy()\n    img[..., 1] = apply_tone(img[..., 1], a)\n    ax = fig.add_subplot(2, 2, i)\n    ax.imshow(Image.fromarray(img, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\n0.7 Q59：暗くなっていく世界，明るくなっていく世界\n\n\nCode\ndef up_tone(x, a):\n    # 計算上値がuintでオバーフローするので最初に整数に変えておく\n    return np.clip(255 / (255 - a) * (x.astype(int) - a), 0, 255).astype(np.uint8)\n\ndef down_tone(x, a):\n    return np.clip(255 / (255 - a) * x.astype(int), 0, 255).astype(np.uint8)\n\n\n\n\n\nCode\nfig = plt.figure(figsize=(12,12))\n\na_seq = [0, 32, 64, 128]\n\nfor i, a in enumerate(a_seq, start = 1):\n    img_up = pic_hsv.copy()\n    img_up[..., 2] = up_tone(img_up[...,2], a)\n    img_down = pic_hsv.copy()\n    img_down[..., 2] = down_tone(img_down[..., 2], a)\n    img = np.concatenate([img_up, img_down], axis = 1)\n    ax = fig.add_subplot(4, 1, i)\n    ax.imshow(Image.fromarray(img, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.8 Q60：レイヤーマスク\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    flower = np.array(img.convert(\"HSV\"))\n    print(flower.shape)\n\n\n(960, 1280, 3)\n\n\n\n\nCode\nH, W, c = flower.shape\nidy, idx = np.indices((H, W))\nmask1 = -0.75 * idx + 800  &lt;= idy\nmask2 = -0.75 * idx + 1200 &gt;= idy\nmask  = np.logical_and(mask1, mask2)\n\ngradation_line = np.linspace(0, 255, W).astype(np.uint8)\ngradation      = np.tile(gradation_line, (H, 1))\nflower[mask, 0] = gradation[mask]\n\nplt.imshow(Image.fromarray(flower, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\n0.9 Q61:レイヤーマスク\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic01.jpg\") as img:\n    pic_hsv = np.array(img.convert(\"HSV\"))\n    H, W, c = pic_hsv.shape\n    print(H, W)\n    print(pic_hsv[0])\n\nv = 255 / 127 * (pic_hsv[..., 2].astype(float) - 128)\nidy, idx = np.indices((H, W))\nm = 255 * (1 - .75 / H * np.sqrt(idy * idy + (idx - W) ** 2))\nv = np.clip(v + m, 0, 255).astype(np.uint8)\n\npic_hsv[..., 2] = v\nplt.imshow(Image.fromarray(pic_hsv, \"HSV\"))\n\n\n700 1050\n[[156  14 224]\n [134  13 221]\n [ 98  14 226]\n ...\n [134  14 232]\n [134  14 233]\n [134  14 233]]\n\n\n\n\n\n\n\n\n\n\n\n0.10 Q62：レイヤーマスク\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    back = np.array(img.convert(\"HSV\"))\n    grad = np.full_like(back, 255)\n    grad[..., 0] = np.linspace(0, 255, back.shape[1])[None, :].astype(np.uint8)\n    print(back.shape)\n\nwith Image.open(\"numpy_book/imgs/heart01.png\") as img:\n    heat = np.array(img)\n    print(heat.shape)\n\nmask = heat[..., 3] &gt; 0\nback[mask, :] = grad[mask, :]\nplt.imshow(Image.fromarray(back, \"HSV\"))\n\n\n\n(960, 1280, 3)\n(960, 1280, 4)\n\n\n\n\n\n\n\n\n\n\n\n0.11 Q63：レイヤーの合成\nここからは, 要素の値域が0-1として扱う.\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    base_img = np.array(img) / 255\n\nwith Image.open(\"numpy_book/imgs/heart01.png\") as img:\n    ref_img = np.array(img.convert(\"RGB\")) / 255\n\n\n\n\nCode\nnew_img = base_img * ref_img\nnew_img[:3]\n\n\narray([[[0.58039216, 0.70980392, 0.76862745],\n        [0.57647059, 0.69803922, 0.76862745],\n        [0.58039216, 0.69411765, 0.76470588],\n        ...,\n        [0.67058824, 0.74117647, 0.82745098],\n        [0.66666667, 0.74901961, 0.83137255],\n        [0.66666667, 0.75294118, 0.83529412]],\n\n       [[0.57254902, 0.70196078, 0.76862745],\n        [0.57254902, 0.69411765, 0.76470588],\n        [0.58039216, 0.69411765, 0.76470588],\n        ...,\n        [0.66666667, 0.74901961, 0.83137255],\n        [0.66666667, 0.75294118, 0.83529412],\n        [0.66666667, 0.75294118, 0.83529412]],\n\n       [[0.57647059, 0.69803922, 0.76862745],\n        [0.58823529, 0.70196078, 0.77254902],\n        [0.59607843, 0.70196078, 0.77647059],\n        ...,\n        [0.67058824, 0.75294118, 0.82745098],\n        [0.66666667, 0.75686275, 0.82745098],\n        [0.67058824, 0.76078431, 0.83137255]]])\n\n\n\n\nCode\nplt.imshow(new_img)\n\n\n\n\n\n\n\n\n\n\n\n0.12 Q64：レイヤーの合成モード\n\n\nCode\nnew_img = 1 - (1 - base_img) * (1 - ref_img)\nplt.imshow(new_img)\n\n\n\n\n\n\n\n\n\n\n\n0.13 Q65：スクリーンによる光の描画\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic01.jpg\") as img:\n    pic01 = np.array(img.convert(\"HSV\"))\n\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    plt.imshow(img)\n    pic02 = np.array(img) / 255\n\n\n\n\n\n\n\n\n\n\n\nCode\npic01[..., 2] = np.clip(255 / 191 * (pic01[..., 2] - 64.), 0, 255).astype(np.uint8)\nwith Image.fromarray(pic01, \"HSV\") as img:\n    pic01 = np.array(img.convert(\"RGB\")) / 255\n\nplt.imshow(pic01)\n\n\n\n\n\n\n\n\n\n\n\nCode\nH, W, _ = pic01.shape\nidy, idx = np.indices((H, W))\ngrad = np.clip(1 - .75 / H * np.sqrt(idy ** 2 + (idx - W) ** 2), 0, 1)\n\n\n\n\nCode\nimg = 1 - (1 - pic01) * (1 - grad[:, :, None])\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\nCode\nimg = 1 - (1 - pic02) * (1 - grad[:, :, None])\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n0.14 Q66：例あーの合成モード\n明るくなる合成モード.\n\nスクリーン\n覆い焼き（リニア加算）\n\n\n\nCode\nnew_img = np.clip(base_img + ref_img, 0, 1)\nplt.imshow(new_img)\n\n\n\n\n\n\n\n\n\n\n\n0.15 Q67：レイヤーの合成\n覆い焼きカラー．割り算のエラーになるので，適当なイプシロンをくえて置くこと.\n\n\nCode\nepsilon = 1e-8\nnew_img = np.clip(base_img / (1 - ref_img + epsilon), 0, 1)\nplt.imshow(new_img)\n\n\n\n\n\n\n\n\n\n\n\n0.16 Q68：覆い焼き二種類の比較\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    pic02 = np.array(img) / 255\n\nclr = np.array([.4, .2, .1])\nlinear = np.clip(pic02 + clr[None, None, :], 0, 1)\ncolor  = np.clip(pic02 / (1 - clr[None, None, :] + epsilon), 0, 1)\n\nplt.imshow(np.concatenate([linear, color], axis = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.17 Q69：レイヤーの合成モード\n比較は要素ごとに値の大きい方を採用する.\nカラー比較の場合にはRGB値の合計を比較する.\n\n\nCode\nout = np.clip(np.maximum(base_img, ref_img), 0, 1)\nplt.imshow(out)\n\n\n\n\n\n\n\n\n\n\n\nCode\nmask = np.sum(base_img, axis = 2, keepdims = True) &gt; np.sum(ref_img, axis = 2, keepdims = True)\nout  = mask * base_img + (1- mask) * ref_img\nplt.imshow(out)\n\n\n\n\n\n\n\n\n\n\n\n0.18 Q70：焼き込みリニア\n暗くなっていく合成である．\n白黒反転してから，リニアで合成.\n\n\nCode\nout = np.clip(base_img + ref_img - 1, 0, 1)\nplt.imshow(out)\n\n\n\n\n\n\n\n\n\n\n\n0.19 Q71：レイヤーの合成モード\n焼き込みカラー. 覆い焼きカラーとは異なり暗くなる.\n\\[\nO = 1 - \\frac{1-B}{R + \\epsilon}\n\\]\n\n\nCode\nout  = np.clip(1 - (1 - base_img) / (ref_img + epsilon), 0, 1)\nplt.imshow(out)\n\n\n\n\n\n\n\n\n\n\n\n0.20 Q72：レイヤーの合成モード（比較（暗），カラー比較（暗））\n\n\nCode\ncmp1 = np.minimum(base_img, ref_img)\nmask = np.sum(base_img, axis = 2, keepdims=True) &lt; np.sum(ref_img, axis = 2, keepdims=True)\ncmp2 = mask * base_img + (1 - mask) * ref_img\nplt.imshow(np.concatenate([cmp1, cmp2], axis = 1))\n\n\n\n\n\n\n\n\n\n\n\n0.21 Q73：焼き込み２種類比較\n覆い焼きカラーと，焼き込みを比較する.\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    pic02 = np.array(img)\n    plt.imshow(img)\n\npic02.shape\n\n\n\n\n\n\n\n\n\n\n\nCode\ntarget_color  = np.array([.8, .2, .1])\nyakikomi_linear = np.clip(pic02 / 255 + target_color[None, None, :] -1, 0, 1)\nplt.imshow(yakikomi_linear)\n\n\n\n\n\n\n\n\n\n\n\nCode\nyakikomi_color = np.clip(1 - (1 - pic02 / 255) / (epsilon + target_color[None, None, :]), 0, 1)\nplt.imshow(yakikomi_color)\n\n\n\n\n\n\n\n\n\n\n\n0.22 Q74：レイヤーの合成モード\nオーバーレイ.\n\n\nCode\nlayer_1 = 2 * base_img * ref_img\nlayer_2 = 1 - 2 * (1- base_img) * (1 -ref_img)\nmask    = base_img &lt; .5\nout     = np.clip(mask * layer_1 + (1-mask) * layer_2, 0, 1)\nplt.imshow(out)\n\n\n\n\n\n\n\n\n\n\n\n0.23 Q75：レイヤーの合成モード（ハードライト)\n\n\nCode\nlayer_1 = 2 * base_img * ref_img\nlayer_2 = 1 - 2 * (1- base_img) * (1 -ref_img)\nmask    = ref_img &lt; .5\nout     = np.clip(mask * layer_1 + (1-mask) * layer_2, 0, 1)\nplt.imshow(out)\n\n\n\n\n\n\n\n\n\n\n\n0.24 Q76：オーバーレイを使ったコントラスト補正\n同一画像でオーバーレイを行うとコントラストの強調が行える.\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic01.jpg\") as img:\n    pic01 = np.array(img.convert(\"RGBA\")) / 255\n    plt.imshow(pic01)\n\n\n\n\n\n\n\n\n\n\n\nCode\nlayer_1 = 2 * pic01 * pic01\nlayer_2 = 1 - 2 * (1- pic01) * (1 -pic01)\nmask    = pic01 &gt; .5\nout     = np.clip(mask * layer_1 + (1-mask) * layer_2, 0, 1)\nout[..., 3] = .9\nplt.imshow(out)\n\n\n\n\n\n\n\n\n\n\n\n0.25 Q77：ゲーミングあんこちゃん\n\n\nCode\ndef overlay(i1, i2):\n    layer_1 = 2 * i1 * i2\n    layer_2 = 1 - 2 * (1- i1) * (1 -i2)\n    mask    = i1 &lt; .5\n    out     = np.clip(mask * layer_1 + (1-mask) * layer_2, 0, 1)\n    return out\n\n\n\n\nCode\nwith Image.open(\"numpy_book/imgs/girl01.png\") as img:\n    girl = np.array(img.convert(\"RGB\")) / 255\n    plt.imshow(girl)\n    H, W, _ = girl.shape\n\n\n\n\n\n\n\n\n\n\n\nCode\ngrad_hsv         = np.full_like(girl, 255, dtype = np.uint8)\ngrad_hsv[..., 0] = np.linspace(0, 255, H)[:, None].astype(np.uint8)\nplt.imshow(Image.fromarray(grad_hsv, \"HSV\"))\n\n\n\n\n\n\n\n\n\n\n\nCode\ngrad_rgb = np.array(Image.fromarray(grad_hsv, \"HSV\").convert(\"RGB\")) / 255\nplt.imshow(overlay(girl, grad_rgb))\n\n\n\n\n\n\n\n\n\n\n\nCode\nshifts = 40 * np.arange(7)\nfig = plt.figure(figsize = (24, 24))\nfor i, s in enumerate(shifts, start = 1):\n    g = grad_hsv.copy()\n    g[..., 0] += s\n    g = np.array(Image.fromarray(g, \"HSV\").convert(\"RGB\")) / 255\n    img = overlay(girl, g)\n    ax = fig.add_subplot(4, 2, i)\n    ax.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n\n0.26 Q78：レイヤー合成モード（ソフトライト）\n\n\nCode\nlay1 = 2 * base_img * ref_img + base_img * base_img * (1 - 2 * ref_img)\nlay2 = 2 * base_img * (1 - ref_img) + np.sqrt(base_img) * (2 * ref_img - 1)\nmask = ref_img &lt; .5\nout  = mask * lay1 + (1 - mask) * lay2\nout  = np.clip(out, 0, 1)\nplt.imshow(out)\n\n\n\n\n\n\n\n\n\n\n\n0.27 Q81：ソフトライトを再帰的にかける\n\n\nCode\ndef softlight (img1, img2):\n    lay1 = 2 * img1 * img2 + img1 * img1 * (1 - 2 * img2)\n    lay2 = 2 * img1 * (1 - img2) + np.sqrt(img1) * (2 * img2 - 1)\n    mask = img2 &lt; .5\n    out  = mask * lay1 + (1 - mask) * lay2\n    out  = np.clip(out, 0, 1)\n    return out\n\n\n\n\nCode\ndef generate_mask (H, W, alpha):\n    idy, idx = np.indices((H, W))\n    r = np.sqrt((idy - H //2) ** 2 + (idx - W // 2) **2)\n    r /= (H // 2)\n    v = np.clip(1 - alpha * r, 0, 1)\n    return v\n\n\n\n\nCode\nfig = plt.figure()\nalpha = [0, .5, .75, 1, 1.5, 2]\n\nH, W, _  = base_img.shape\no = base_img.copy()\nfor i, a in enumerate(alpha, start = 1):\n    m = generate_mask(H, W, a)\n    o = softlight(o, m[..., None])\n    ax = fig.add_subplot(2, 3, i)\n    ax.imshow(o)\n\n\n\n\n\n\n\n\n\n\n\n0.28 Q81：禍々しい♥\nリニアーライトで合成.\n\n\nCode\ndef linearlight(img1, img2):\n    layer1 = img1 + 2 * img2 - 1\n    layer2 = img1 + 2 * (img2 - .5)\n    mask = img2 &lt; .5\n    out  = mask * layer1 + (1-mask) * layer2\n    out  = np.clip(out, 0, 1)\n    return out\n\n\n\n\nCode\nH, W, _ = ref_img.shape\nv = generate_mask(H, W, .65)\no = linearlight(ref_img, v[..., None])\nplt.imshow(o)\n\n\n\n\n\n\n\n\n\n\n\n0.29 Q83：ライトシリーズ比較\n\n\nCode\ndef hardlight(img1, img2):\n    lay1 = 2 * img1 * img2\n    lay2 = 1 - 2 * (1 - img1) * (1 - img2)\n    mask = img2 &lt; .5\n    out  = mask * lay1 + (1-mask) * lay2\n    out  = np.clip(out, 0, 1)\n    return out\n\n\ndef softlight (img1, img2):\n    lay1 = 2 * img1 * img2 + img1 * img1 * (1 - 2 * img2)\n    lay2 = 2 * img1 * (1 - img2) + np.sqrt(img1) * (2 * img2 - 1)\n    mask = img2 &lt; .5\n    out  = mask * lay1 + (1 - mask) * lay2\n    out  = np.clip(out, 0, 1)\n    return out\n\ndef linearlight(img1, img2):\n    layer1 = img1 + 2 * img2 - 1\n    layer2 = img1 + 2 * (img2 - .5)\n    mask = img2 &lt; .5\n    out  = mask * layer1 + (1-mask) * layer2\n    out  = np.clip(out, 0, 1)\n    return out\n\n\ndef vividlight(img1, img2):\n    epsilon = 1e-8\n    layer1  = 1 - (1 - img1) / (2 * img2 + epsilon)\n    layer2  = img1 / (1 - 2 * (img2 - .5) + epsilon)\n    mask = img2 &lt; .5\n    out  = mask * layer1 + (1-mask) * layer2\n    out  = np.clip(out, 0, 1)\n    return out\n\n\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    # 合成をするときには，0 ~ 1で合成を行う\n    back = np.array(img) / 255\n    H, W, _ = back.shape\n    plt.imshow(back)\n\n\n\n\n\n\n\n\n\n\n\nCode\nblue   = np.full((H, W, 3), np.array([.6, .8,  1]))\norange = np.full((H, W, 3), np.array([.4, .2, .1]))\nidy, idx = np.indices((H, W))\nmask   = idy + H / W * idx - H &lt;= 0\nfront  = mask[..., None] * blue + (1-mask[..., None]) * orange\nplt.imshow(front)\n\n\n\n\n\n\n\n\n\n\n\nCode\ndef plotter (img1, img2):\n    fig = plt.figure(figsize = (12, 12))\n    o1 = hardlight(img1, img2) \n    o4 = linearlight(img1, img2)\n    o2 = softlight(img1, img2)\n    o3 = vividlight(img1, img2)\n    for i, o in enumerate([o1, o2, o3, o4], start = 1):\n        ax = fig.add_subplot(2, 2, i)\n        ax.imshow(o)\n\n\n\n\nCode\nplotter(back, front)\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    pic02 = np.array(img) / 255\n    W, H, _ = pic02.shape\n\nwith Image.open(\"numpy_book/imgs/water.jpg\") as img:\n    water = np.array(img) / 255\n\n\n\n\nCode\nplotter(pic02, water)\n\n\n\n\n\n\n\n\n\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    pic02 = np.array(img) / 255\n    W, H, _ = pic02.shape\n\nwith Image.open(\"numpy_book/imgs/water2.jpg\") as img:\n    water = np.array(img) / 255\n\nplotter(pic02, water)\n\n\n\n\n\n\n\n\n\n\n\nCode\nplt.imshow(water)\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "Q52：セピア化"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html",
    "title": "",
    "section": "",
    "text": "CodeShow All CodeHide All Code\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import (Image, ImageDraw, ImageFont)\nimport cv2\nfrom mypy.blender import *\nfrom numpy_book import video_utils\nfrom scipy.interpolate import interp1d\n動画は(F, H, W, C)という４次元のテンソル担っており，ブロードキャストが必須である.",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#reshape",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#reshape",
    "title": "",
    "section": "1.1 reshape",
    "text": "1.1 reshape\n\n1.1.1 ベクトルと行列のやり取り\n\n\nCode\nx = np.arange(4).reshape(2, 2)\nx\n\n\narray([[0, 1],\n       [2, 3]])\n\n\n\n\nCode\n# １次元に戻すことも可能である.abs\nx.reshape(4)\n\n\narray([0, 1, 2, 3])\n\n\n\n\nCode\nx.reshape(-1)\n\n\narray([0, 1, 2, 3])\n\n\n\n\n1.1.2 テンソルと行列\n\n\nCode\n# 2 x 1 x 3 の画像をつくり，RGB値を入力している. \n# PILは(W, H)の順番で解像度を指定していることに注意する\nwith Image.new(\"RGB\", (1, 2), color = (255, 255, 0)) as img:\n    x = np.array(img).reshape(2, 3)\n    print(x)\n\n\n[[255 255   0]\n [255 255   0]]\n\n\n\n\nCode\nnp.array(img).shape\n\n\n(2, 1, 3)\n\n\n\n\nCode\nwith Image.new(\"RGB\", (871, 1137), color = (255, 255, 0)) as img:\n    x = np.array(img)\n    print(x.shape)\n    x = x.reshape(-1, 3)\n    print(x.shape)\n\n\n(1137, 871, 3)\n(990327, 3)\n\n\n\n\nCode\n1137 * 871\n\n\n990327\n\n\n画像を行列にするとなにが嬉しいのか，ということについて，まずカラーヒストグラムが挙げられる.\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    plt.imshow(img)\n    plt.show()\n    x = np.array(img).reshape(-1, 3) # データの次元 x RGB\n\nfig = plt.figure(figsize = (12, 4))\nfor i in range(3):\n    ax = fig.add_subplot(1, 3, 1 + i)\n    ax.hist(x[:,i], bins = 256, color = \"rgb\"[i])\n    ax.set_xlim((0, 255))\n\nplt.show()",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#軸を追加する関数",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#軸を追加する関数",
    "title": "",
    "section": "1.2 軸を追加する関数",
    "text": "1.2 軸を追加する関数\n\n\nCode\nx = np.expand_dims(np.arange(5), axis = -1)\nx.shape\n\n\n(5, 1)\n\n\n\n\nCode\nx = np.arange(5).reshape(5, 1)\nx.shape\n\n\n(5, 1)\n\n\n\n\nCode\nx = np.expand_dims(np.arange(5), axis = 0)\nx.shape\n\n\n(1, 5)\n\n\n\n\nCode\nx = np.arange(5).reshape(1, 5)\nx.shape\n\n\n(1, 5)\n\n\n上述したように軸を追加する方法には，reshape，expand_dimsがある。\nreshapeの方がやりやすいようにも見えるが，ディープラーニングのような複雑なモデルを扱うときには，expand_dimsを使う. また，stackでも大丈夫である.\nexpand_dimsは指定した軸に新しいshapeが加わる.",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#ブロードキャスト",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#ブロードキャスト",
    "title": "",
    "section": "1.3 ブロードキャスト",
    "text": "1.3 ブロードキャスト\n\n1.3.1 reshapeの計算コスト\nreshapeの計算コストは軽いので暗黙のブロードキャストが行われないように， shapeを合わせておくことが望ましい。\n\n\nCode\n%timeit -n 100000 np.arange(100)\n%timeit -n 100000 np.arange(100).reshape(1, 100)\n%timeit -n 100000 np.expand_dims(np.arange(100), axis = 0)\n\n\n551 ns ± 22.6 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n780 ns ± 8.39 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n3.82 µs ± 47.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n\n\n\n\n1.3.2 アンチパターン\n\n\nCode\n# どの軸で計算が行われているのかがわからない ？\nx = np.zeros((3, 3, 3)) + np.array([1, 2, 3])\nx\n\n\narray([[[1., 2., 3.],\n        [1., 2., 3.],\n        [1., 2., 3.]],\n\n       [[1., 2., 3.],\n        [1., 2., 3.],\n        [1., 2., 3.]],\n\n       [[1., 2., 3.],\n        [1., 2., 3.],\n        [1., 2., 3.]]])\n\n\n\n\nCode\nx = np.zeros((3, 3, 3)) + np.array([1, 2, 3]).reshape(1, 3, 1)\nx\n\n\narray([[[1., 1., 1.],\n        [2., 2., 2.],\n        [3., 3., 3.]],\n\n       [[1., 1., 1.],\n        [2., 2., 2.],\n        [3., 3., 3.]],\n\n       [[1., 1., 1.],\n        [2., 2., 2.],\n        [3., 3., 3.]]])",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#ブロードキャストを自在に操る",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#ブロードキャストを自在に操る",
    "title": "",
    "section": "1.4 ブロードキャストを自在に操る",
    "text": "1.4 ブロードキャストを自在に操る\n\n1.4.1 九九\n\n\nCode\na = np.arange(9).reshape(1, 9) + 1\nb = np.arange(9).reshape(9, 1) + 1\na * b\n\n\narray([[ 1,  2,  3,  4,  5,  6,  7,  8,  9],\n       [ 2,  4,  6,  8, 10, 12, 14, 16, 18],\n       [ 3,  6,  9, 12, 15, 18, 21, 24, 27],\n       [ 4,  8, 12, 16, 20, 24, 28, 32, 36],\n       [ 5, 10, 15, 20, 25, 30, 35, 40, 45],\n       [ 6, 12, 18, 24, 30, 36, 42, 48, 54],\n       [ 7, 14, 21, 28, 35, 42, 49, 56, 63],\n       [ 8, 16, 24, 32, 40, 48, 56, 64, 72],\n       [ 9, 18, 27, 36, 45, 54, 63, 72, 81]])\n\n\n\n\n1.4.2 縦横のグラデーション\n\n\nCode\nimg = np.zeros((256, 256, 3), dtype = np.uint8)\nimg[..., 0] = np.arange(256).reshape(-1, 1) # 縦のグラデーション\nimg[..., 1] = np.arange(256).reshape(1, -1) # 横のグラデーション\nplt.imshow(img)\n\n\n\n\n\n\n\n\n\n\n\n1.4.3 動画でのブロードキャスト\n\n\nCode\nframes = np.zeros((100, 256, 256, 3), dtype = np.uint8)\n# フレーム方向にグラデーションを追加している\nframes[..., 0] = \\\n    np.linspace(255, 0, frames.shape[0], dtype = np.uint8).reshape(-1, 1, 1)\nframes[..., ２] = \\\n    np.linspace(0, 255, frames.shape[0], dtype = np.uint8).reshape(-1, 1, 1)\n\nfig = plt.figure(figsize = (8, 8))\nfor i, f in enumerate([0, 30, 60, 99]):\n    ax = fig.add_subplot(2, 2, i + 1)\n    ax.imshow(frames[f])\n    ax.set_title(\"f =\" + str(f))\nplt.show()\n\n\n\n  File \"&lt;ipython-input-19-5cab0d0a9f5e&gt;\", line 5\n    frames[..., ２] = \\\n                ^\nSyntaxError: invalid character in identifier\n\n\n\n\n\n\n1.4.4",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#軸を増やすための書き方",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#軸を増やすための書き方",
    "title": "",
    "section": "1.5 軸を増やすための書き方",
    "text": "1.5 軸を増やすための書き方\n\n\nCode\nprint(\"- reshape -\")\n%timeit -n 1000000 np.arange(100).reshape(1, 100)\n%timeit -n 1000000 np.arange(100).reshape(100, 1)\nprint(\"- expand_dims -\")\n%timeit -n 1000000 np.expand_dims(np.arange(100),axis=0)\n%timeit -n 1000000 np.expand_dims(np.arange(100),axis=-1)\nprint(\"- np.newaxis -\")\n%timeit -n 1000000 np.arange(100)[np.newaxis, :]\n%timeit -n 1000000 np.arange(100)[:, np.newaxis]\nprint(\"- None -\")\n%timeit -n 1000000 np.arange(100)[None, :]\n%timeit -n 1000000 np.arange(100)[:, None]\n\n\n- reshape -\n811 ns ± 2.34 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n815 ns ± 1.65 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n- expand_dims -\n3.86 µs ± 49.3 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n3.89 µs ± 54.2 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n- np.newaxis -\n874 ns ± 20.3 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n864 ns ± 13.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n- None -\n824 ns ± 1.54 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n837 ns ± 22.6 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n\n\n上記を見ると，基本的にexpand_dimsはやや遅いが，繰り返し使うものでなければ，どれを使っても大差がないように思える.",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#手動ブロードキャスト",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#手動ブロードキャスト",
    "title": "",
    "section": "1.6 手動ブロードキャスト",
    "text": "1.6 手動ブロードキャスト\nなにも考えずとりあずブロードキャストしてみる.\n\n1.6.1 np.broadcast_to\n\n\nCode\nx = np.arange(3).reshape(3, 1)\nx = np.broadcast_to(x, (3, 2))\nx # axis = 1の方向にコピーさせている\n\n\narray([[0, 0],\n       [1, 1],\n       [2, 2]])\n\n\nブロードキャストした配列に対して，x[0,:]のような代入するとエラーになる。\nつまりは，計算はできるが，代入はできない.\n\n\nCode\nx[:1] = 1\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-46-6f4c93d065fe&gt; in &lt;module&gt;\n----&gt; 1 x[:1] = 1\n\nValueError: assignment destination is read-only\n\n\n\n\n\nCode\nx * 2\n\n\narray([[0, 0],\n       [2, 2],\n       [4, 4]])\n\n\nでは，どうするかというと，copy, np.repeatヲ使う.\n\n\nCode\nxc = x.copy()\nxc[0] = 10\nxc\n\n\narray([[10, 10],\n       [ 1,  1],\n       [ 2,  2]])\n\n\n\n\nCode\nx = np.arange(3).reshape(1, 3)\nx = np.repeat(x, 2, axis = 0)\nx\n\n\narray([[0, 1, 2],\n       [0, 1, 2]])\n\n\n数値計算らしい方法としては，１を乗じるものもある.\n\n\nCode\nx = np.arange(3).reshape(1, 3)\nx = x * np.ones((2, 1), dtype = x.dtype)\nx[0, :] = 1\nx\n\n\narray([[1, 1, 1],\n       [0, 1, 2]])\n\n\n速度的にはnp.repeatが早いのでこれを使うこと.",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#reshapeの黒魔術1-モノクロ画像の切り出し",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#reshapeの黒魔術1-モノクロ画像の切り出し",
    "title": "",
    "section": "1.7 reshapeの黒魔術(1) ：モノクロ画像の切り出し",
    "text": "1.7 reshapeの黒魔術(1) ：モノクロ画像の切り出し\nこれはすごい！！！！！！\n\n\nCode\nx = np.arange(36).reshape(6, 6)\nprint(\"元の行列\\n\", x)\n\n\n元の行列\n [[ 0  1  2  3  4  5]\n [ 6  7  8  9 10 11]\n [12 13 14 15 16 17]\n [18 19 20 21 22 23]\n [24 25 26 27 28 29]\n [30 31 32 33 34 35]]\n\n\n\n\nCode\n# 2 x 2 の小さい画像にreshaep\nx = x.reshape(3, 2, 3, 2)\nx = x.transpose([0, 2, 1, 3])\nx = x.reshape(-1, 2, 2)\n\nprint(\"切り出した行列\")\nfor i in range(x.shape[0]):\n    print(x[i])\n\n\n切り出した行列\n[[0 1]\n [6 7]]\n[[2 3]\n [8 9]]\n[[ 4  5]\n [10 11]]\n[[12 13]\n [18 19]]\n[[14 15]\n [20 21]]\n[[16 17]\n [22 23]]\n[[24 25]\n [30 31]]\n[[26 27]\n [32 33]]\n[[28 29]\n [34 35]]\n\n\nやっていることは次の図を見れば明らか.\n\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    x = np.array(img.convert(\"L\"))\n    H, W = x.shape\nplt.imshow(x, cmap = \"gray\", vmin = 0, vmax = 255)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nbatch_y, batch_x = 320, 320\nx = x.reshape(H // batch_y, batch_y, W // batch_x, batch_x)\nx = x.transpose(0, 2, 1, 3)\nx = x.reshape(-1, batch_y, batch_x)\nx.shape\n\n\n(12, 320, 320)\n\n\n\n\nCode\nfig = plt.figure(figsize = (12, 8))\nfor i in range(x.shape[0]):\n    ax = fig.add_subplot(3, 4, i + 1)\n    ax.imshow(x[i], cmap = \"gray\", vmin = 0, vmax = 255)\n\nplt.show()",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#reshapeの黒魔術カラー画像の切り出し",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#reshapeの黒魔術カラー画像の切り出し",
    "title": "",
    "section": "1.8 reshapeの黒魔術：カラー画像の切り出し",
    "text": "1.8 reshapeの黒魔術：カラー画像の切り出し\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    x = np.array(img)\n    H, W, C = x.shape\n    plt.imshow(x)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nbatch_y, batch_x = 320, 320\nx = x.reshape(H // batch_y, batch_y, W // batch_x, batch_x, 3)\nx = x.transpose(0, 2, 1, 3, 4)\nx = x.reshape(-1, batch_y, batch_x, 3)\nx.shape\n\n\n(12, 320, 320, 3)\n\n\n\n\nCode\nfig = plt.figure(figsize = (12, 8))\nfor i in range(x.shape[0]):\n    ax = fig.add_subplot(3, 4, i + 1)\n    ax.imshow(x[i])\n\nplt.show()",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#reshapeの黒魔術カラー画像の切り出し-1",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#reshapeの黒魔術カラー画像の切り出し-1",
    "title": "",
    "section": "1.9 reshapeの黒魔術カラー画像の切り出し",
    "text": "1.9 reshapeの黒魔術カラー画像の切り出し\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    x = np.array(img)\n    H, W, c = x.shape\n\nbatch_y, batch_x = 240, 320\nx = x.reshape(H // batch_y, batch_y, W // batch_x, batch_x, 3)\nx = x.transpose(0, 2, 1, 3, 4)\nx = x.reshape(-1, batch_y, batch_x, 3)\nx.shape\n\n\n(16, 240, 320, 3)\n\n\n\n\nCode\nfig = plt.figure(figsize = (12, 8))\nfor i in range(x.shape[0]):\n    ax = fig.add_subplot(4, 4, i + 1)\n    ax.imshow(x[i])\n\nplt.show()",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#colabで動画を表示する方法",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#colabで動画を表示する方法",
    "title": "",
    "section": "1.10 Colabで動画を表示する方法",
    "text": "1.10 Colabで動画を表示する方法\n\n\nCode\nfrom numpy_book import video_utils\n\nframes = np.zeros((256, 128, 128, 3), dtype = np.uint8)\n# Bにだけグラデーションを与える\n# framが変わるごとに青になっていく\nframes[..., 2] = np.arange(256)[:, None, None]\nvideo_utils.save_video(\"./dat/blue_grad.mp4\", frames, frame_rate = 24.)\n\n\n\n\nCode\n# jupyterでの動画の表示\nimport moviepy.editor\nmoviepy.editor.VideoFileClip(\"./dat/blue_grad.mp4\").ipython_display()\n\n\nt:   0%|          | 0/257 [00:00&lt;?, ?it/s, now=None]Moviepy - Building video __temp__.mp4.\nMoviepy - Writing video __temp__.mp4\n\nMoviepy - Done !\nMoviepy - video ready __temp__.mp4\n\n\nSorry, seems like your browser doesn't support HTML5 audio/video",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#演習問題",
    "href": "contents/books/07_NumpyInfinity/ch05_ブロードキャストの応用.html#演習問題",
    "title": "",
    "section": "1.11 演習問題",
    "text": "1.11 演習問題\n\n1.11.1 Q3:九九\n\n\nCode\nnp.arange(1, 10).reshape(9, -1) * np.arange(1, 10).reshape(-1, 9)\n\n\narray([[ 1,  2,  3,  4,  5,  6,  7,  8,  9],\n       [ 2,  4,  6,  8, 10, 12, 14, 16, 18],\n       [ 3,  6,  9, 12, 15, 18, 21, 24, 27],\n       [ 4,  8, 12, 16, 20, 24, 28, 32, 36],\n       [ 5, 10, 15, 20, 25, 30, 35, 40, 45],\n       [ 6, 12, 18, 24, 30, 36, 42, 48, 54],\n       [ 7, 14, 21, 28, 35, 42, 49, 56, 63],\n       [ 8, 16, 24, 32, 40, 48, 56, 64, 72],\n       [ 9, 18, 27, 36, 45, 54, 63, 72, 81]])\n\n\n\n\n1.11.2 Q4：真理値表\n論理関数もブロードキャストが適用されるようです.\n\n\nCode\nx = np.array([True, False]).reshape(2, 1)\n\nnp.logical_and(x, x.T)\n\n\narray([[ True, False],\n       [False, False]])\n\n\n\n\nCode\nnp.logical_or(x, x.T)\n\n\narray([[ True,  True],\n       [ True, False]])\n\n\n\n\nCode\nnp.logical_xor(x, x.T)\n\n\narray([[False,  True],\n       [ True, False]])\n\n\n\n\n1.11.3 Q5：駅間距離\n\n\nCode\ndist_from_depature = np.array([0, 1.1, 2.8, 4.7, 7])\nnp.abs(dist_from_depature[None, :] - dist_from_depature[:, None])\n\n\narray([[0. , 1.1, 2.8, 4.7, 7. ],\n       [1.1, 0. , 1.7, 3.6, 5.9],\n       [2.8, 1.7, 0. , 1.9, 4.2],\n       [4.7, 3.6, 1.9, 0. , 2.3],\n       [7. , 5.9, 4.2, 2.3, 0. ]])\n\n\n\n\n1.11.4 Q6：都市間の直線距離\n\n\nCode\nlonlat = np.array([\n    [43.06, 141.35], \n    [43.77, 142.37], \n    [45.40, 141.68], \n    [42.97, 144.38], \n    [43.33, 145.58], \n    [42.91, 143.18],\n    [42.34, 140.99], \n    [41.79, 140.74]\n])\n\n# 都市x都市x(lon, lat)\ndmat = \\\nnp.sqrt(\n    np.sum(\n        ((lonlat[None, :, :] - lonlat[:, None, :]) * np.array([111, 82])[None, None, :]) ** 2, \n        axis = 2\n    )\n)\n\n\nnp.round(dmat, 0)\n\n\narray([[  0., 115., 261., 249., 348., 151.,  85., 150.],\n       [115.,   0., 190., 187., 268., 116., 195., 257.],\n       [261., 190.,   0., 349., 394., 303., 344., 408.],\n       [249., 187., 349.,   0., 106.,  99., 287., 326.],\n       [348., 268., 394., 106.,   0., 202., 392., 432.],\n       [151., 116., 303.,  99., 202.,   0., 190., 236.],\n       [ 85., 195., 344., 287., 392., 190.,   0.,  64.],\n       [150., 257., 408., 326., 432., 236.,  64.,   0.]])\n\n\n\n\n1.11.5 Q7：グラデーション\n\n\nCode\nimg = np.full((256, 256, 3), 255, dtype = np.uint8)\nimg[..., 0] = np.linspace(80, 200, 256)[:, None].astype(np.uint8)\nimg[..., 1] = np.linspace(128, 255, 256)[:, None].astype(np.uint8)\nimg[..., 2] = np.linspace(255, 192, 256)[:, None].astype(np.uint8)\n\nwith Image.fromarray(img, \"HSV\") as img:\n    plt.imshow(img.convert(\"RGB\"))\n\n\n\n\n\n\n\n\n\n\n\n\n1.11.6 Q8：はじめまして動画\n\n\nCode\nframe = np.zeros((3, 255, 255, 3), dtype = np.uint8)\nframe[0, ...] = np.array([128, 255, 255], dtype = np.uint8)[None, None, None, :]\nframe[1, ...] = np.array([255, 128, 255], dtype = np.uint8)[None, None, None, :]\nframe[2, ...] = np.array([255, 255, 128], dtype = np.uint8)[None, None, None, :]\n\n\nvideo_utils.save_video(\"./dat/q08.mp4\", frame, frame_rate = 1)\n\n\n\n\n1.11.7 Q9：グラデーション動画\n\n\nCode\nframes = np.zeros((100, 256, 256, 3), dtype = np.uint8)\nframes[..., 0] = np.linspace(0, 255, 100, dtype = np.uint8)[:, None, None]\nframes[..., 1] = np.linspace(255, 0, 100, dtype = np.uint8)[:, None, None]\nframes[..., 2] = np.linspace(128, 255, 100, dtype = np.uint8)[:, None, None]\n\nvideo_utils.save_video(\"./dat/q09.mp4\", frames, frame_rate = 1)\n\n\n\n\n1.11.8 Q10：ルパン風タイトルジェネレーター\n\n\nCode\nfont = ImageFont.truetype(\"numpy_book/fonts/M_PLUS_1p/MPLUS1p-Bold.ttf\", 200)\nwith Image.new(\"RGB\", (256, 256), color = (0, 0, 0)) as canvas:\n    draw = ImageDraw.Draw(canvas)\n    text_size = draw.textsize(\"あ\", font = font)\n    ox, oy = (256 - text_size[0]) // 2, 0\n    draw.text((ox, oy), \"あ\", fill = (255, 255, 255), font = font)\n    plt.imshow(canvas)    \n\n\n\n\n\n\n\n\n\n\n\nCode\ntext_size\n\n\n(200, 225)\n\n\n\n\nCode\nframe_list = []\nfor s in list(\"こしあんは美味しい\") + [\"こしあんは\\n美味しい\"]:\n    with Image.new(\"RGB\", (256, 256), color = (0, 0, 0)) as canvas:\n        draw = ImageDraw.Draw(canvas)\n        font_size = np.where(len(s) &gt; 1, 40, 200)\n        font = ImageFont.truetype(\"numpy_book/fonts/M_PLUS_1p/MPLUS1p-Bold.ttf\", font_size)\n        text_size = draw.textsize(s, font = font)\n        ox, oy = (256 - text_size[0]) // 2, (256 - text_size[1]) // 2\n        draw.text((ox, oy), s, fill = (255, 255, 255), font = font)\n        frame_list.append(np.array(canvas))\n\nframe_list += [frame_list[-1]] * 11\nframes = np.stack(frame_list, axis = 0)\n\n\n\n\nCode\n# frame rateを6にすると出力された動画がおかしい・・・・\n# frame rateを6, 12, 24などとすると挙動は安定する\nvideo_utils.save_video(\"./dat/q10.mp4\", frames, frame_rate = 6)\n\n\n\n\nCode\nframes.shape\n\n\n(21, 256, 256, 3)\n\n\n\n\n1.11.9 Q11：動くグラデーション\n\n\nCode\nframe_list = []\n\nfor i in range(100):\n    canvas = np.full((256, 256, 3), 255, dtype = np.uint8)\n    canvas[..., 0] = np.linspace(2 * i, 2 * (i + 100), 256)[:, None]\n    with Image.fromarray(canvas, \"HSV\") as img:\n        rgb = np.array(img.convert(\"RGB\"))\n        frame_list.append(rgb)\n\nframes = np.stack(frame_list, axis = 0)\nvideo_utils.save_video(\"./dat/q11.mp4\", frames)\n\n\n\n\n1.11.10 Q12：動くゲーミングあんこちゃん\n\n\nCode\nwith Image.open(\"numpy_book/imgs/girl01.png\") as img:\n    girl = np.array(img.convert(\"RGB\"), dtype = np.float32) / 255.\n    H, W, _ = girl.shape\n\nframe_list = []\nfor i in range(300):\n    Rhsv = np.full_like(girl, 255, dtype = np.uint8)\n    Rhsv[..., 0] = np.linspace(4 * i, 4 * i + 256, H, dtype = np.uint8)[:, None]\n    with Image.fromarray(Rhsv, \"HSV\") as img:\n        Rrgb = np.array(img.convert(\"RGB\"), dtype = np.float32) / 255.\n    mixed_frame = overlay(girl, Rrgb)\n    frame_list.append((mixed_frame * 255).astype(np.uint8))\n\n\nframes = np.stack(frame_list, axis = 0)\nvideo_utils.save_video(\"./dat/q12.mp4\", frames)\n\n\n\n\n1.11.11 Q13：動画のハードライトブレンディング\n割としっかり時間が必要なのでツラ。\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    B = np.array(img.convert(\"RGB\"), dtype = np.float32) / 255.\n    Hb, Wb, _ = B.shape\n    print(Hb, Wb)\n\nwith Image.open(\"numpy_book/imgs/water2.jpg\") as img:\n    R = np.tile(np.array(img.convert(\"RGB\"), dtype = np.float32) / 255., (2, 2, 1))\n    Hr, Wr, _ = R.shape\n    print(Hr, Wr)\n\nframe_list = []\ndy, dx = Hb, Wb\nfor i in range(Hb):\n    oy, ox = (i, W // 2)\n    mixed  = (hardlight(B, R[oy:oy + dy, ox:ox + dx]) * 255).astype(np.uint8)\n    frame_list.append(mixed)\n\n\nframes = np.stack(frame_list, axis = 0)\nvideo_utils.save_video(\"./dat/q13.mp4\", frames)\n\n\n960 1280\n1920 2560\n\n\n\n\n1.11.12 Q14：ビビットライト・ブレンディング\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    B = np.array(img.convert(\"RGB\"), dtype = np.float32) / 255.\n    Hb, Wb, _ = B.shape\n    print(Hb, Wb)\n\nwith Image.open(\"numpy_book/imgs/water2.jpg\") as img:\n    R = np.tile(np.array(img.convert(\"RGB\"), dtype = np.float32) / 255., (2, 1, 1))\n    Hr, Wr, _ = R.shape\n    print(Hr, Wr)\n\nframe_list = []\ndy, dx = Hb, Wb\nfor i in range(Hb):\n    oy, ox = (i, 0)\n    mixed  = (vividlight(B, R[oy:oy + dy, ox:ox + dx]) * 255).astype(np.uint8)\n    frame_list.append(mixed)\n\n\nframes = np.stack(frame_list, axis = 0)\nvideo_utils.save_video(\"./dat/q14.mp4\", frames)\n\n\n960 1280\n1920 1280\n\n\n\n\n1.11.13 Q15：移動する円\n\n\nCode\nframe_list = []\nidy, idx = np.indices((320, 480))\ngb = np.full((320, 480, 2), 0, dtype = np.uint8)\n\nfor i in range(100):\n    ox, oy = (i * i) / 100, 160\n    mask = np.sqrt((idy - oy) ** 2 + (idx - ox) ** 2) &lt;= 100\n    r    = (mask * 255).astype(np.uint8)\n    rgb  = np.concatenate([r[..., None], gb], axis = 2)\n    frame_list.append(rgb)\n\nframes = np.stack(frame_list, axis = 0)\nvideo_utils.save_video(\"./dat/q15.mp4\", frames)\n\n\n\n\nCode\nframes.shape\n\n\n(50, 320, 480, 3)\n\n\n\n\n1.11.14 Q16：ひかるまりも\n\n\nCode\nH, W = 320, 480\nwith Image.open(\"numpy_book/imgs/water2.jpg\") as img:\n    water2_resized = img.resize((W, H), Image.NEAREST)\n    water2 = np.array(water2_resized)\nwith Image.open(\"numpy_book/imgs/moss.jpg\") as img:\n    moss_resized = img.resize((W, H), Image.NEAREST)\n    moss = np.array(moss_resized)\n\nidy, idx  = np.indices((H, W))\noy, ox, r = 160, 240, 100\nmask = np.sqrt((idy - oy) ** 2 + (idx - ox) ** 2) + np.random.randn(H, W) * 5 &lt;=  r\n\n\n\n\nCode\nn_frames = 100\ngrad = np.full((n_frames, H, W, 3), 255, dtype = np.uint8)\ngrad[...,0] = np.linspace(4,    32, n_frames, dtype = np.uint8)[:, None, None]\ngrad[...,1] = np.linspace(149, 209, n_frames, dtype = np.uint8)[:, None, None] \ngrad[...,2] = np.linspace( 80,   1, n_frames, dtype = np.uint8)[:, None, None]\ngrad = (.5 * np.sin(6 * np.arange(n_frames) * np.pi / 180) + .5)[:, None, None, None] * grad\n\nmoss = moss / 255.\ngrad = grad / 255.\nfor i in np.arange(n_frames):\n    grad[i, ...] = np.clip(1 - (1 - moss) / (1e-8 + grad[i, ...]), 0, 1)\n\n\n\n\nCode\nframe_list = []\nfor i in np.arange(n_frames):\n    out = np.logical_not(mask[..., None]) * water2 / 255. + mask[..., None] * grad[i, ...]\n    frame_list.append((255. * out).astype(np.uint8))\n\n\n\n\nCode\nframes = np.stack(frame_list, axis = 0)\nvideo_utils.save_video(\"./dat/q16.mp4\", frames)\n\n\n\n\n1.11.15 Q17：ダイナミックな行列のスライス\nnp.indicesのインデックスそのものを使いスライスを行う.\n\n\nCode\nx = np.arange(9).reshape(3, 3)\nidy, idx = np.indices(x.shape)\nx[idy, idx]\n\n\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n\n\n\nCode\n# 1行目を繰り返す\nx[idy * 0, idx]\n\n\narray([[0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2]])\n\n\n\n\nCode\nx = np.arange(100).reshape(10, 10)\nidy, idx = np.indices(x.shape)\nidx2 = np.concatenate([idx[:, ::2], idx[:, 1::2]], axis = 1)\nx[idy, idx2]\n\n\narray([[ 0,  2,  4,  6,  8,  1,  3,  5,  7,  9],\n       [10, 12, 14, 16, 18, 11, 13, 15, 17, 19],\n       [20, 22, 24, 26, 28, 21, 23, 25, 27, 29],\n       [30, 32, 34, 36, 38, 31, 33, 35, 37, 39],\n       [40, 42, 44, 46, 48, 41, 43, 45, 47, 49],\n       [50, 52, 54, 56, 58, 51, 53, 55, 57, 59],\n       [60, 62, 64, 66, 68, 61, 63, 65, 67, 69],\n       [70, 72, 74, 76, 78, 71, 73, 75, 77, 79],\n       [80, 82, 84, 86, 88, 81, 83, 85, 87, 89],\n       [90, 92, 94, 96, 98, 91, 93, 95, 97, 99]])\n\n\n\n\n1.11.16 Q18：ダイナミックなスライスとブロードキャスト\n\n\nCode\nx = np.arange(9).reshape(3, 3)\nx\n\n\narray([[0, 1, 2],\n       [3, 4, 5],\n       [6, 7, 8]])\n\n\n\n\nCode\nidy, idx = np.indices(x.shape)\nidx2 = np.tile(idx[None, ...], (3, 1, 1))\nidy2 = np.tile(np.array([0, 0, 1, 0, 1, 2, 1, 2, 2]).reshape(3, 3)[None, ...].transpose(2, 1, 0), (1,1,3))\n\nx[idy2, idx2]\n\n\narray([[[0, 1, 2],\n        [0, 1, 2],\n        [3, 4, 5]],\n\n       [[0, 1, 2],\n        [3, 4, 5],\n        [6, 7, 8]],\n\n       [[3, 4, 5],\n        [6, 7, 8],\n        [6, 7, 8]]])\n\n\n\n\n1.11.17 Q19：座標にノイズを乗せたエフェクト\n各ピクセルの座標にノイズを乗せて，そのノイズ幅をだんだん小さくしていくことで フェードインエフェクトを得たい。\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    pic = np.array(img)\n    H, W, _ = pic.shape\n\nidy, idx = np.indices((H, W))\nn_frame  = 100\nk        = np.linspace(10, 0, n_frame) ** 2\nnoize    = np.random.randn(H, W)\nidx_r = np.clip(idx[None, ...] + k[:, None, None] * noize[None, ...], 0, W-1).astype(int)\nidy_r = np.clip(idy[None, ...] + k[:, None, None] * noize[None, ...], 0, H-1).astype(int)\n\n\n\n\nCode\nframes = pic[idy_r, idx_r]\nvideo_utils.save_video(\"./dat/q19.mp4\", frames)\n\n\n\n\n1.11.18 Q20：座標にノイズを乗せたエフェクト\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    pic = np.array(img)\n    H, W, _ = pic.shape\n\nidy, idx = np.indices((H, W))\nn_frame  = 100\nk        = np.linspace(10, 0, n_frame) ** 2\nx_noize  = np.random.randn(n_frame, *idx.shape)\ny_noize  = np.random.randn(n_frame, *idy.shape)\nidx_r = np.clip(idx[None, ...] + k[:, None, None] * x_noize, 0, W-1).astype(int)\nidy_r = np.clip(idy[None, ...] + k[:, None, None] * y_noize, 0, H-1).astype(int)\nframes = pic[idy_r, idx_r]\nvideo_utils.save_video(\"./dat/q20.mp4\", frames)\n\n\n\n\n1.11.19 Q21：三角関数を載せた絵ジェクト\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    pic = np.array(img)\n    H, W, _ = pic.shape\n\nidy, idx = np.indices((H, W))\nn_frame  = 100\nk        = np.linspace(15, 0, n_frame, dtype = np.float32) ** 2\nx_noize  = np.cos(idy / H * 2 * np.pi * 8).astype(np.float32)\ny_noize  = np.sin(idx / W * 2 * np.pi * 5).astype(np.float32)\nidx_r = np.clip(idx[None, ...] + k[:, None, None] * x_noize[None, ...], 0, W-1).astype(int)\nidy_r = np.clip(idy[None, ...] + k[:, None, None] * y_noize[None, ...], 0, H-1).astype(int)\nframes = pic[idy_r, idx_r]\nvideo_utils.save_video(\"./dat/q21.mp4\", frames)\n\n\n\n\n1.11.20 Q21：三角関数を載せたエフェクト\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    pic = np.array(img)\n    H, W, _ = pic.shape\n\nidy, idx = np.indices((H, W))\nn_frame  = 100\nk        = np.linspace(3, 1, n_frame, dtype = np.float32) ** 2\nx_noize  = np.cos(idy / H * 2 * np.pi * 8).astype(np.float32)\ny_noize  = np.sin(idx / W * 2 * np.pi * 5).astype(np.float32)\nidx_r = np.clip(idx[None, ...] + k[:, None, None] * x_noize[None, ...], 0, W-1).astype(int)\nidy_r = np.clip(idy[None, ...] + k[:, None, None] * y_noize[None, ...], 0, H-1).astype(int)\nframes = pic[idy_r, idx_r]\nvideo_utils.save_video(\"./dat/q22.mp4\", frames)\n\n\n\n\n1.11.21 Q23：スライドしながら覆い焼く\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    img_resized = img.resize((img.width //2, img.height // 2), Image.NEAREST)\n    back = np.array(img_resized) / 255.\n    H, W, _ = back.shape\n\nprint(H, W)\n\n\n350 525\n\n\n\n\nCode\nrect = np.full((H, W, 3), 1)\n\n\n\n\nCode\nslide_per_frame = 3\nn_round = 2\nrect_sub_width = 100\nblack_front = np.zeros((H, W, 3), dtype = np.float32)\nframe_list  = []\norange = np.array([.8, .5, .3])[None, None, :]\nfor i in range(0, n_round * W, slide_per_frame):\n    black_front[...] = 0\n    using_idx = np.arange(i, i + rect_sub_width) % W\n    black_front[:, using_idx, :] = orange\n    out = np.clip(.3 * back / (1 - black_front + 1e-8), 0, 1)\n    out = (255 * out).astype(np.uint8)\n    frame_list.append(out)\n\n\n\n\nCode\nframes = np.stack(frame_list, axis = 0)\nvideo_utils.save_video(\"./dat/q23.mp4\", frames)\n\n\n\n\n1.11.22 Q24：トーンカーブとアニメーション\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic01.jpg\") as img:\n    img = img.resize((img.width //2, img.height // 2), Image.NEAREST)\n    back = np.array(img) / 255.\n    H, W, _ = back.shape\n    plt.imshow(back)\n\nprint(H, W)\n\n\n350 525\n\n\n\n\n\n\n\n\n\n\n\nCode\nn_frame  = 181\na_seq    = 1 - np.sin(np.pi * np.arange(n_frame) / 180)[:, None, None, None]\nidy, idx = np.indices((H, W))\nepsilon  = 1e-8\ny_tone   = (back[None, ...] - a_seq) / (1 - a_seq + epsilon)\n\n\n\n\nCode\nframes = (np.clip(y_tone, 0, 1) * 255).astype(np.uint8)\nvideo_utils.save_video(\"./dat/q24.mp4\", frames)\n\n\n\n\n1.11.23 Q25：動くスポットライトのアニメ\nスプライン補間を使うと，動くスポットライトのアニメーションが再現ｄけいる. 楕円形の白マスクを作成し，フレームに応じて位置を移動させて，スクリーンで合成する.\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    img = img.resize((img.width //2, img.height // 2), Image.NEAREST)\n    back = np.array(img) / 255.\n    H, W, _ = back.shape\n    plt.imshow(back)\n\nn_frame = 210\nkey_frame = np.arange(8) * 30\nkf_spot_y_center = np.array([0.8*H, 0.8*H, 0.2*H,0.2*H, 0.5*H, 0.5*H, 0.6*H, 0.6*H])\nkf_spot_x_center = np.array([0.2*W, 0.2*W, 0.3*W,0.3*W, 0.8*W, 0.8*W, 0.5*W, 0.5*W])\nf_interp_y = interp1d(np.linspace(0, n_frame, 8), kf_spot_y_center, kind = \"quadratic\")\nf_interp_x = interp1d(np.linspace(0, n_frame, 8), kf_spot_x_center, kind = \"quadratic\")\nidy, idx = np.indices((H, W))\n\nframe_list = []\nfor i in range(n_frame):\n    oy, ox = f_interp_y(i), f_interp_x(i)\n    d = np.sqrt(((idy - oy) / 80) ** 2 + ((idx - ox) / 150) ** 2)\n    v = np.clip(1 - d, 0, 1)\n    light = np.tile(v[..., None], (1, 1, 3))\n    out   = 1 - (1 - back) * (1 - light)\n    out   = (np.clip(out, 0, 1) * 255).astype(np.uint8)\n    frame_list.append(out)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nframes = np.stack(frame_list, axis = 0)\nvideo_utils.save_video(\"./dat/q25.mp4\", frames)\n\n\n\n\n\n1.11.24 Q26：懐かしいのスクリーンセ-バー\n\n\nCode\nwith Image.open(\"numpy_book/imgs/girl01.png\") as img:\n    img = img.convert(\"RGB\")\n    back = np.array(img) / 255.\n    H, W, _ = back.shape\n    plt.imshow(back)\n\nn_frame   = 210\nkey_frame = np.arange(8) * 30\nkf_spot_y_center = np.array([0.8*H, 0.2*H, 0.5*H,0.6*H, 0.9*H, 0.1*H, 0.8*H, 0.5*H])\nkf_spot_x_center = np.array([0.2*W, 0.3*W, 0.8*W,0.5*W, 0.4*W, 0.8*W, 0.9*W, 0.5*W])\nf_interp_y = interp1d(np.linspace(0, n_frame, 8), kf_spot_y_center, kind = \"quadratic\")\nf_interp_x = interp1d(np.linspace(0, n_frame, 8), kf_spot_x_center, kind = \"quadratic\")\nidy, idx = np.indices((H, W))\n\nR = 150\nframe_list = []\nfor i in range(n_frame):\n    oy, ox = f_interp_y(i), f_interp_x(i)\n    y, x = (idy - oy), (idx - ox)\n    r = np.sqrt(y ** 2 +  x ** 2)\n    theta   = np.arctan2(y, x)\n    theta_d = np.clip((R - r) * np.pi * 2 / R, 0, np.pi * 2)\n    theta_norm = theta + theta_d\n    x   = np.clip(r * np.cos(theta_norm) + ox, 0, W-1).astype(int)\n    y   = np.clip(r * np.sin(theta_norm) + oy, 0, H-1).astype(int)\n    uzu = back[y, x]\n    mask = r &gt; R\n    o    = mask[..., None] * back + (1-mask[..., None]) * uzu\n    frame_list.append((255 * o).astype(np.uint8))\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nframes = np.stack(frame_list, axis = 0)\nvideo_utils.save_video(\"./dat/q26.mp4\", frames)\n\n\n\n\nCode\nframes.shape\n\n\n(210, 348, 318, 4)\n\n\n\n\n1.11.25 Q27：微分可能なモザイク\nPILのリサイズを使わずに, reshapeだけでモザイクを描けることが可能となる.\n\n\nCode\nwith Image.open(\"numpy_book/imgs/flower01.jpg\") as img:\n    img = img.convert(\"RGB\")\n    x = np.array(img)\n    H, W, _ = x.shape\n    g = 32\n    resized = x.reshape(H // g, g, W // g, g, 3)\n\n\n\n\nCode\n\nfig = plt.figure(figsize = (24, 24))\nmosaic = np.tile(np.mean(resized, axis = (1, 3), keepdims = True), (1, g, 1, g, 1))\nmosaic = mosaic.reshape(H, W, 3).astype(np.uint8)\nplt.imshow(np.concatenate([x, mosaic], axis = 1))\n\n\n\n\n\n\n\n\n\n\n1.11.25.1 Q28：スライドするライト\n\n\nCode\nwith Image.open(\"numpy_book/imgs/pic02.jpg\") as img:\n    img = img.convert(\"RGB\")\n    x   = np.array(img, dtype = np.uint16)\n    H, W, _ = x.shape\n\nS = 175\nx_r = x.reshape(1, H // S, S, W // S, S, 3)\nx_r = x_r.transpose(0, 1, 3, 2, 4, 5)\nx_r = x_r.reshape(1, -1, S, S, 3)\n\nn_frame = int(H * W / S / S)\n# フレームに1枚ずつあてがうために，単位行列を採用している\nlight   = (128 * np.eye(n_frame)).reshape(n_frame, n_frame, 1, 1, 1)\n\nframes = np.clip(x_r + light, 0, 255).astype(np.uint8)\n\n\n\n\nCode\ntmp = frames.reshape(n_frame, H // S, W // S, S, S, 3)\ntmp = tmp.transpose(0, 1, 3, 2, 4, 5)\ntmp = tmp.reshape(n_frame, H, W, 3)\n\n\n\n\nCode\nvideo_utils.save_video(\"./dat/q28.mp4\", tmp)\n\n\n\n\nCode\nnp.eye(4).reshape(4, 4, 1)\n\n\narray([[[1.],\n        [0.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [1.],\n        [0.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [1.],\n        [0.]],\n\n       [[0.],\n        [0.],\n        [0.],\n        [1.]]])\n\n\n\n\nCode\nnp.eye(4).reshape(4, 4, 1, 1)\n\n\narray([[[[1.]],\n\n        [[0.]],\n\n        [[0.]],\n\n        [[0.]]],\n\n\n       [[[0.]],\n\n        [[1.]],\n\n        [[0.]],\n\n        [[0.]]],\n\n\n       [[[0.]],\n\n        [[0.]],\n\n        [[1.]],\n\n        [[0.]]],\n\n\n       [[[0.]],\n\n        [[0.]],\n\n        [[0.]],\n\n        [[1.]]]])\n\n\n\n\n\n1.11.26 Q29：跳ね返るボール\nボールが行って戻るまでの範囲を繰り返すようにしている.\n軸ごとに繰り返しで表現できるようにすることがポイント.\n例えばxが1, 2, 3, 4, 5, 6，．．．となっていたときに３で折り返すようにすれば, 1, 2, 3, 2, 1, 2, …という風になる.\n\n\nCode\nF, H, W, R = 500, 256, 256, 15\nidy, idx   = np.indices((H, W))\nv_y, v_x   = 10, 7\nred    = np.array([255, 0, 0])\ncanvas = np.full((H, W, 3), 0, dtype = np.uint8)\n\n\n\n\nCode\nx = np.arange(100)\nlimit = 10\ny = np.minimum(2 * limit - x % (2 * limit), x % (2 * limit))\nplt.plot(x, y)\n\n\n\n\n\n\n\n\n\n\n\nCode\nframe_list = []\nfor i in range(F):\n    y_pos = np.minimum(2 * (H-R) - i * v_y % (2 * (H-R)), i * v_y % (2 * (H-R))) + R\n    x_pos = np.minimum(2 * (H-R) - i * v_x % (2 * (H-R)), i * v_x % (2 * (H-R))) + R\n    mask = np.sqrt((idy-y_pos) ** 2 + (idx-x_pos) ** 2) &lt;= R\n    out  = mask[..., None] * red[None, None, :] + (1-mask[..., None]) * canvas\n    frame_list.append(out.astype(np.uint8))\n\n\n\n\nCode\nframes = np.stack(frame_list, axis = 0)\nvideo_utils.save_video(\"./dat/q29.mp4\", frames)\n\n\n\n\n1.11.27 Q30：跳ね返る複数のボール\n\n\nCode\nspeed_ratio = np.array([\n    [10, 8, 5, 4, 12, 3], \n    [7, 9, 6, 8, 5, 5]\n])\nball_colors = np.array([\n    [128, 0, 0], \n    [0, 128, 0], \n    [0,0, 128], \n    [128, 128, 0], \n    [0, 128, 128],\n    [128, 0,128]\n])\nM = 6\n\n\n\n\nCode\nframe_list = []\ns_y, s_x = speed_ratio\ncanvas = np.full((M, H, W, 3), 0, dtype = np.uint8)\nfor i in range(F):\n    y_pos = np.min(np.stack([2 * (H-R) - i * s_y % (2 * (H-R)), i * s_y % (2 * (H-R))], axis = 1), axis = 1)\n    y_pos += R\n    x_pos = np.min(np.stack([2 * (H-R) - i * s_x % (2 * (H-R)), i * s_x % (2 * (H-R))], axis = 1), axis = 1)\n    x_pos += R\n    mask = np.sqrt((idy[None, ...] - y_pos[:, None, None])**2 + (idx[None, ...] - x_pos[:, None, None]) ** 2) &lt;= R\n    out  = mask[..., None] * ball_colors[:, None, None, :]  + (1 - mask[..., None]) * canvas\n    out  = np.clip(np.sum(out, axis = 0), 0, 255).astype(np.uint8)\n    frame_list.append(out)\n\n\n    \n\n\n\n\nCode\nframes = np.stack(frame_list, axis = 0)\nvideo_utils.save_video(\"./dat/q30.mp4\", frames)\n\n\n\n\n1.11.28 Q31：粉雪\n\n\nCode\nH, W = 256 + 2, 256 + 2\nM    = 10000\nn_frames  = 500\n# M x 2\npoints_pos = np.random.rand(M, 2) * np.array([8 * H, 8 * W])[None, :] - np.array([4 * H, 4 * W])[None, :]\npoints_pos = points_pos.astype(np.uint8)\nwhite = np.array([255, 255, 255])\n\n\n\n\nCode\nframe_canvas = np.full((n_frames, H, W, 3), 0, dtype = np.uint8)\nfor i in range(n_frames):\n    speed = np.random.uniform(1, 7, M)\n    theta = np.random.uniform(0, np.pi * 2, M)\n    dyx   = np.stack([speed * np.sin(theta), speed * np.cos(theta)], axis = 1)\n    points_pos += dyx.astype(np.uint8)\n    points_pos[:, 0] = np.clip(points_pos[:, 0], 0, H-1)\n    points_pos[:, 1] = np.clip(points_pos[:, 1], 0, W-1)\n    frame_canvas[i, points_pos[:, 0], points_pos[:, 1], :] = white[None, None, :]\n    \nframe_canvas = frame_canvas[:, 1:H-1, 1:W-1, :]\n\n\n\n\nCode\nvideo_utils.save_video(\"./dat/q31.mp4\", frame_canvas)",
    "crumbs": [
      "Python",
      "Numpy Infinity",
      "ブロードキャストの応用"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html",
    "href": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html",
    "title": "はじめに",
    "section": "",
    "text": "Code\nimport importlib.util\nimport os\n\ntry:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    work_dirctory = '/content/drive/Othercomputers/LetsNoteSilver'\n    os.chdir(work_dirctory)\n    print(\"mounted\")\nexcept ModuleNotFoundError as e:\n    print(\"not mounted\")\n\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport graphviz\npd.set_option('display.float_format', '{:.3f}'.format)\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\n\n\nnot mounted\nCode\ndf = pd.read_csv(\n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", \n    header = None, \n    names = [\n        \"age\", \n        \"workclass\", \n        \"fnlwgt\", \n        \"education\", \n        \"education-num\", \n        \"marital-status\",\n        \"occupation\",\n        \"relationship\",\n        \"race\",\n        \"gender\",\n        \"capital-gain\",\n        \"capital-loss\",\n        \"hours-per-week\",\n        \"native-country\",\n        \"income\"\n    ]\n)\n\ndf.head()\n\n\n\n\n\n\n\n\n\nage\nworkclass\nfnlwgt\neducation\neducation-num\nmarital-status\noccupation\nrelationship\nrace\ngender\ncapital-gain\ncapital-loss\nhours-per-week\nnative-country\nincome\n\n\n\n\n0\n39\nState-gov\n77516\nBachelors\n13\nNever-married\nAdm-clerical\nNot-in-family\nWhite\nMale\n2174\n0\n40\nUnited-States\n&lt;=50K\n\n\n1\n50\nSelf-emp-not-inc\n83311\nBachelors\n13\nMarried-civ-spouse\nExec-managerial\nHusband\nWhite\nMale\n0\n0\n13\nUnited-States\n&lt;=50K\n\n\n2\n38\nPrivate\n215646\nHS-grad\n9\nDivorced\nHandlers-cleaners\nNot-in-family\nWhite\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n3\n53\nPrivate\n234721\n11th\n7\nMarried-civ-spouse\nHandlers-cleaners\nHusband\nBlack\nMale\n0\n0\n40\nUnited-States\n&lt;=50K\n\n\n4\n28\nPrivate\n338409\nBachelors\n13\nMarried-civ-spouse\nProf-specialty\nWife\nBlack\nFemale\n0\n0\n40\nCuba\n&lt;=50K\nCode\ndf.shape\n\n\n(32561, 15)\nCode\n# 欠損値\ndf.isnull().sum()\n\n\nage               0\nworkclass         0\nfnlwgt            0\neducation         0\neducation-num     0\nmarital-status    0\noccupation        0\nrelationship      0\nrace              0\ngender            0\ncapital-gain      0\ncapital-loss      0\nhours-per-week    0\nnative-country    0\nincome            0\ndtype: int64\nCode\n# データ型\ndf.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 32561 entries, 0 to 32560\nData columns (total 15 columns):\n #   Column          Non-Null Count  Dtype \n---  ------          --------------  ----- \n 0   age             32561 non-null  int64 \n 1   workclass       32561 non-null  object\n 2   fnlwgt          32561 non-null  int64 \n 3   education       32561 non-null  object\n 4   education-num   32561 non-null  int64 \n 5   marital-status  32561 non-null  object\n 6   occupation      32561 non-null  object\n 7   relationship    32561 non-null  object\n 8   race            32561 non-null  object\n 9   gender          32561 non-null  object\n 10  capital-gain    32561 non-null  int64 \n 11  capital-loss    32561 non-null  int64 \n 12  hours-per-week  32561 non-null  int64 \n 13  native-country  32561 non-null  object\n 14  income          32561 non-null  object\ndtypes: int64(6), object(9)\nmemory usage: 3.7+ MB",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#数値変数",
    "href": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#数値変数",
    "title": "はじめに",
    "section": "0.1 数値変数",
    "text": "0.1 数値変数\n\n\nCode\ndf.select_dtypes(include = [\"float\", \"int\"]).describe().T\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nage\n32561.000\n38.582\n13.640\n17.000\n28.000\n37.000\n48.000\n90.000\n\n\nfnlwgt\n32561.000\n189778.367\n105549.978\n12285.000\n117827.000\n178356.000\n237051.000\n1484705.000\n\n\neducation-num\n32561.000\n10.081\n2.573\n1.000\n9.000\n10.000\n12.000\n16.000\n\n\ncapital-gain\n32561.000\n1077.649\n7385.292\n0.000\n0.000\n0.000\n0.000\n99999.000\n\n\ncapital-loss\n32561.000\n87.304\n402.960\n0.000\n0.000\n0.000\n0.000\n4356.000\n\n\nhours-per-week\n32561.000\n40.437\n12.347\n1.000\n40.000\n40.000\n45.000\n99.000\n\n\n\n\n\n\n\n\n\nCode\n# ヒストグラム\nfig = plt.figure(figsize = (10, 6))\ndf.hist(bins = 20)\nplt.tight_layout()\nplt.show()\n\n\n&lt;Figure size 1000x600 with 0 Axes&gt;",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#カテゴリ変数eda",
    "href": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#カテゴリ変数eda",
    "title": "はじめに",
    "section": "0.2 カテゴリ変数EDA",
    "text": "0.2 カテゴリ変数EDA\n\n\nCode\ndf.select_dtypes(include = [\"object\"]).describe().T.reset_index()\n\n\n\n\n\n\n\n\n\nindex\ncount\nunique\ntop\nfreq\n\n\n\n\n0\nworkclass\n32561\n9\nPrivate\n22696\n\n\n1\neducation\n32561\n16\nHS-grad\n10501\n\n\n2\nmarital-status\n32561\n7\nMarried-civ-spouse\n14976\n\n\n3\noccupation\n32561\n15\nProf-specialty\n4140\n\n\n4\nrelationship\n32561\n6\nHusband\n13193\n\n\n5\nrace\n32561\n5\nWhite\n27816\n\n\n6\ngender\n32561\n2\nMale\n21790\n\n\n7\nnative-country\n32561\n42\nUnited-States\n29170\n\n\n8\nincome\n32561\n2\n&lt;=50K\n24720\n\n\n\n\n\n\n\n\n\nCode\n# ユニーク値を確認する\n# カテゴリー値の前に半角スペースが含まれていることがわかる\nunique_levels = (\n    df\n    .select_dtypes(include=[\"object\"])\n    .agg(lambda x: [x.unique()], axis = 0)\n)\nfor column, values in unique_levels.items():\n    print(f\"{column}: {values[0]}\")\n\n\nworkclass: [' State-gov' ' Self-emp-not-inc' ' Private' ' Federal-gov' ' Local-gov'\n ' ?' ' Self-emp-inc' ' Without-pay' ' Never-worked']\neducation: [' Bachelors' ' HS-grad' ' 11th' ' Masters' ' 9th' ' Some-college'\n ' Assoc-acdm' ' Assoc-voc' ' 7th-8th' ' Doctorate' ' Prof-school'\n ' 5th-6th' ' 10th' ' 1st-4th' ' Preschool' ' 12th']\nmarital-status: [' Never-married' ' Married-civ-spouse' ' Divorced'\n ' Married-spouse-absent' ' Separated' ' Married-AF-spouse' ' Widowed']\noccupation: [' Adm-clerical' ' Exec-managerial' ' Handlers-cleaners' ' Prof-specialty'\n ' Other-service' ' Sales' ' Craft-repair' ' Transport-moving'\n ' Farming-fishing' ' Machine-op-inspct' ' Tech-support' ' ?'\n ' Protective-serv' ' Armed-Forces' ' Priv-house-serv']\nrelationship: [' Not-in-family' ' Husband' ' Wife' ' Own-child' ' Unmarried'\n ' Other-relative']\nrace: [' White' ' Black' ' Asian-Pac-Islander' ' Amer-Indian-Eskimo' ' Other']\ngender: [' Male' ' Female']\nnative-country: [' United-States' ' Cuba' ' Jamaica' ' India' ' ?' ' Mexico' ' South'\n ' Puerto-Rico' ' Honduras' ' England' ' Canada' ' Germany' ' Iran'\n ' Philippines' ' Italy' ' Poland' ' Columbia' ' Cambodia' ' Thailand'\n ' Ecuador' ' Laos' ' Taiwan' ' Haiti' ' Portugal' ' Dominican-Republic'\n ' El-Salvador' ' France' ' Guatemala' ' China' ' Japan' ' Yugoslavia'\n ' Peru' ' Outlying-US(Guam-USVI-etc)' ' Scotland' ' Trinadad&Tobago'\n ' Greece' ' Nicaragua' ' Vietnam' ' Hong' ' Ireland' ' Hungary'\n ' Holand-Netherlands']\nincome: [' &lt;=50K' ' &gt;50K']\n\n\n\n\nCode\n# カテゴリー別の頻度を確認\n# この確認方法は簡単でわかりやすい気がするいい方法だね\nfig = plt.figure(figsize = (20, 20))\nfor i, name in enumerate(unique_levels.columns):\n    ax = plt.subplot(5, 2, i + 1)\n    df[name].value_counts().plot(kind = \"bar\", ax = ax)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# カテゴリー変数の前処理を実装する\nfor s in unique_levels.columns:\n    df[s] = df[s].str.replace(\" \", \"\")\n\n\n\n\nCode\n# 特徴量native-countryがUnited-Statesの条件でデータセットのレコードを絞る\ndf = (\n    df\n    .query(\"`native-country`.isin(['United-States', ])\")\n    .drop(labels=[\"native-country\"], axis = 1)\n    .reset_index(drop = True)\n)\n\ndf.shape\n\n\n(29170, 14)\n\n\n\n\nCode\n# 目的変数の分布\n# 図書と値が異なるのが気になる・・・？\ndf[\"income\"].value_counts()\n\n\nincome\n&lt;=50K    21999\n&gt;50K      7171\nName: count, dtype: int64\n\n\n\n\nCode\nfig = plt.figure(figsize = (6, 3))\nsns.countplot(x = \"income\", data = df)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 正解ラベルをエンコーディング\ndf[\"income\"] = (df[\"income\"] != \"&lt;=50K\").astype(int)",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#混同行列と正解率の検証",
    "href": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#混同行列と正解率の検証",
    "title": "はじめに",
    "section": "0.3 混同行列と正解率の検証",
    "text": "0.3 混同行列と正解率の検証\n\nstratifyを使うと、学習セット、テストセットで分布のバランスを保つことが出来る\n\n\n\nCode\nX = df.drop([\"income\"], axis = 1)\ny = df[\"income\"]\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, shuffle = True, stratify = y, random_state = 0)\n\n\n\n\nCode\n# stratifyを指定しているのでyの分布を保つことが出来てる\ny_test.value_counts() / len(y_test), y_train.value_counts() / len(y_train)\n\n\n(income\n 0   0.754\n 1   0.246\n Name: count, dtype: float64,\n income\n 0   0.754\n 1   0.246\n Name: count, dtype: float64)\n\n\n予測値を適当に当てはめて各種の評価指標の値を確認する.\n\n\nCode\ny_test_zeros = np.zeros(len(y_test))\n\n# 混同行列\ncm = confusion_matrix(y_test, y_test_zeros)\nfig = plt.figure(figsize = (6, 4))\nsns.heatmap(cm, annot = True, fmt = \"d\", cmap = \"Blues\")\n\nplt.xlabel(\"pred\")\nplt.ylabel(\"label\")\n\n\nText(45.722222222222214, 0.5, 'label')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 正解率、精度、再現率、F1スコアを並べる\nscores = {\"accuracy\":accuracy_score, \"precision\": precision_score, \"recall\": recall_score, \"f1\":f1_score, }\nfor name in scores:\n    print(f\"{name} = {scores[name](y_test, y_test_zeros)}\")\n\n\naccuracy = 0.7541995200548509\nprecision = 0.0\nrecall = 0.0\nf1 = 0.0\n\n\nc:\\pyenv\\py311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\nCode\n# 正解率、精度、再現率、F1スコアをすべて1と予測した場合で試す\nscores = {\"accuracy\":accuracy_score, \"precision\": precision_score, \"recall\": recall_score, \"f1\":f1_score, }\nfor name in scores:\n    print(f\"{name} = {scores[name](y_test, np.ones(len(y_test)))}\")\n\n\naccuracy = 0.24580047994514911\nprecision = 0.24580047994514911\nrecall = 1.0\nf1 = 0.3946064942212438",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#ロジスティック回帰の学習予測評価",
    "href": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#ロジスティック回帰の学習予測評価",
    "title": "はじめに",
    "section": "0.4 ロジスティック回帰の学習、予測、評価",
    "text": "0.4 ロジスティック回帰の学習、予測、評価\nこれまでの処理に加えて、カテゴリ変数文字列をone-hot encodingする。\n\n\nCode\nx_category_columns = X.select_dtypes(include = [\"object\"]).columns\nX_dummied_category = [\n    pd.get_dummies(X[col], prefix = col, drop_first=True).astype(int)\n    for col in X.select_dtypes(include = [\"object\"]).columns\n]\nX_dummied_category.append(X.select_dtypes(include = [\"float\", \"int\"]))\nX = pd.concat(X_dummied_category, axis = 1)\n\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, shuffle = True, stratify=y, random_state=0)\ny_train.value_counts() / len(y_train), y_test.value_counts() / len(y_test)\n\n\n(income\n 0   0.754\n 1   0.246\n Name: count, dtype: float64,\n income\n 0   0.754\n 1   0.246\n Name: count, dtype: float64)\n\n\n\n\nCode\n# 特徴量の標準化\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nnum_cols = X.columns[-6:]\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])\nX_test[num_cols] = scaler.transform(X_test[num_cols])\n\n\n\n\nCode\nX_train[num_cols].describe()\n\n\n\n\n\n\n\n\n\nage\nfnlwgt\neducation-num\ncapital-gain\ncapital-loss\nhours-per-week\n\n\n\n\ncount\n23336.000\n23336.000\n23336.000\n23336.000\n23336.000\n23336.000\n\n\nmean\n-0.000\n0.000\n0.000\n0.000\n-0.000\n-0.000\n\n\nstd\n1.000\n1.000\n1.000\n1.000\n1.000\n1.000\n\n\nmin\n-1.577\n-1.667\n-3.812\n-0.149\n-0.220\n-3.177\n\n\n25%\n-0.778\n-0.677\n-0.483\n-0.149\n-0.220\n-0.027\n\n\n50%\n-0.124\n-0.098\n-0.067\n-0.149\n-0.220\n-0.027\n\n\n75%\n0.674\n0.446\n0.766\n-0.149\n-0.220\n0.377\n\n\nmax\n3.725\n12.135\n2.430\n12.962\n10.437\n4.738\n\n\n\n\n\n\n\n\n\nCode\nparams = {\n    \"penalty\": \"l1\", # Noneは正則化なし, elasticnetもある\n    \"C\": .1, # 正則化項の強さ\n    \"solver\": \"liblinear\",  # L1のときは固定\n    \"max_iter\": 100, \n    \"multi_class\": \"ovr\", # 出力クラスのクラス数はautoで自動判定\n    # \"l1_rato\": None, # elasticetのときのL1の強さ\n    \"random_state\":0\n}\n\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(**params)\nmodel.fit(X_train, y_train)\nmodel.get_params()\n\n\n{'C': 0.1,\n 'class_weight': None,\n 'dual': False,\n 'fit_intercept': True,\n 'intercept_scaling': 1,\n 'l1_ratio': None,\n 'max_iter': 100,\n 'multi_class': 'ovr',\n 'n_jobs': None,\n 'penalty': 'l1',\n 'random_state': 0,\n 'solver': 'liblinear',\n 'tol': 0.0001,\n 'verbose': 0,\n 'warm_start': False}\n\n\n\n\nCode\n# 予測確立の出力\nmodel.predict_proba(X_test)\n\n\narray([[0.61191144, 0.38808856],\n       [0.76940162, 0.23059838],\n       [0.82270077, 0.17729923],\n       ...,\n       [0.00571894, 0.99428106],\n       [0.98302141, 0.01697859],\n       [0.97352058, 0.02647942]])\n\n\n\n\nCode\n# 予測値の出力\nmodel.predict(X_test)\n\n\narray([0, 0, 0, ..., 1, 0, 0])\n\n\n\n\nCode\n# スコアを評価する\ny_test_pred = model.predict(X_test)\nfor name in scores:\n    print(f\"{name} = {scores[name](y_test, y_test_pred)}\")\n\n\naccuracy = 0.8402468289338362\nprecision = 0.7205623901581723\nrecall = 0.5718270571827058\nf1 = 0.6376360808709176\n\n\n\n\nCode\n# 混同行列\ncm = confusion_matrix(y_test, y_test_pred)\nplt.figure(figsize =  (6, 4))\nsns.heatmap(cm, annot = True, fmt = \"d\", cmap = \"Blues\")\nplt.xlabel(\"pred\")\nplt.ylabel(\"label\")\n\n\nText(45.722222222222214, 0.5, 'label')",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#回帰係数について",
    "href": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#回帰係数について",
    "title": "はじめに",
    "section": "0.5 回帰係数について",
    "text": "0.5 回帰係数について\n\n\nCode\nprint(f\"w = [w1, w2, ...] = {model.coef_[0]}\")\n\n\nw = [w1, w2, ...] = [ 6.06250675e-01  0.00000000e+00  0.00000000e+00  2.09047854e-01\n  3.30941698e-01 -2.45180504e-01  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  0.00000000e+00  0.00000000e+00 -2.26824502e-01 -1.06208273e-02\n -2.24462068e-02  0.00000000e+00 -3.97179298e-04  0.00000000e+00\n  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n  1.73028245e+00  0.00000000e+00 -4.34313773e-01 -9.89661506e-04\n  0.00000000e+00  5.95052558e-02  0.00000000e+00  1.53865676e-01\n  8.24454643e-01 -7.34774017e-01 -4.34557007e-01 -1.42962464e-01\n -5.92207717e-01  0.00000000e+00  5.00592347e-01  6.02215962e-01\n  3.64397963e-01  5.71838407e-01  0.00000000e+00  0.00000000e+00\n -3.77265814e-01 -9.28224745e-01 -4.22197646e-02  1.14995676e+00\n  0.00000000e+00 -1.18404531e-01  0.00000000e+00  0.00000000e+00\n  7.38602326e-01  3.27664860e-01  6.68219214e-02  7.45148775e-01\n  2.34070420e+00  2.49978055e-01  3.74810953e-01]\n\n\n\n\nCode\nmodel.intercept_\n\n\narray([-3.22038751])\n\n\n\n\nCode\n# 回帰係数の上位30件を可視化する\nimportances = model.coef_[0]\nindices = np.argsort(importances)[::-1][:30]\n\nplt.figure(figsize = (10, 4))\nplt.title(\"Regression coefficeint\")\nplt.bar(range(len(indices)), importances[indices])\nplt.xticks(range(len(indices)), X.columns[indices], rotation = 90)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 回帰係数の下位30件を可視化する\nimportances = model.coef_[0]\nindices = np.argsort(importances)[:30]\n\nplt.figure(figsize = (10, 4))\nplt.title(\"Regression coefficeint\")\nplt.bar(range(len(indices)), importances[indices])\nplt.xticks(range(len(indices)), X.columns[indices], rotation = 90)\n\nplt.show()",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#shapでの可視化",
    "href": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#shapでの可視化",
    "title": "はじめに",
    "section": "1.1 shapでの可視化",
    "text": "1.1 shapでの可視化\n\n\nCode\nimport shap\nexplainer = shap.TreeExplainer(\n    model = model, \n    feature_pertubation = \"tree_path_dependent\"\n)\n\nshap_values = explainer(X_test)\n\n# ロジット値の平均である\nexplainer.expected_value\n\n\n[2.8079953643772746, -2.8079953643772746]\n\n\n\n\nCode\n# ラベル1の値に絞り込み\nshap_values.values = shap_values.values[:, :, 1]\nshap_values.base_values = explainer.expected_value[1]\n\n\n\n\nCode\n# 個別の値をみてshap値が正しく計算できているのかを試す\nlogit = shap_values[-3].base_values + shap_values.values[-3].sum()\n\n1 / (1 + np.exp(-logit))\n\n\n0.9990514154242494\n\n\n\n\nCode\ny_test_pred_proba[-3]\n\n\n0.9990514154242494\n\n\n\n\nCode\n# 最後から3件目のshap値の可視化\nshap.plots.waterfall(shap_values[-3])\n\n\n\n\n\n\n\n\n\n\n\nCode\nshap.plots.bar(shap_values=shap_values)",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#クロスヴァリデーション",
    "href": "contents/books/08_handbook_lightgbm/cl03_分類の予測モデル.html#クロスヴァリデーション",
    "title": "はじめに",
    "section": "2.1 クロスヴァリデーション",
    "text": "2.1 クロスヴァリデーション\n\n\nCode\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\n\nparams = {\n    \"objective\": \"binary\", \n    \"num_leaves\": 5, \n    \"seed\": 0, \n    \"verbose\": -1, \n}\n\nacc_scores = []\nlos_scores = []\nmodels = []\noof = np.zeros(len(X_train))\n\n\n\n\nCode\nkf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 1)\nfor fold, (tr_idx, va_idx) in enumerate(kf.split(X_train, y_train)):\n    X_tr = X_train.iloc[tr_idx]\n    X_va = X_train.iloc[va_idx]\n    y_tr = y_train.iloc[tr_idx]\n    y_va = y_train.iloc[va_idx]\n    \n    lgb_train = lgb.Dataset(X_tr, y_tr)\n    lgb_eval = lgb.Dataset(X_va, y_va, reference = lgb_train)\n    \n    model = lgb.train(\n        params, \n        lgb_train, \n        num_boost_round= 500, \n        valid_sets = [lgb_train, lgb_eval], \n        valid_names=[\"train\", \"valid\"], \n        callbacks = [lgb.early_stopping(10), lgb.log_evaluation(100)]\n    )\n    \n    y_va_pred=model.predict(X_va, num_iteration=model.best_iteration)\n    acc_scores.append(accuracy_score(y_va, np.round(y_va_pred)))\n    los_scores.append(log_loss(y_va, y_va_pred))\n    models.append(model)\n    oof[va_idx] = y_va_pred\n    \ncv_accuracy_score =  np.mean(acc_scores)\ncv_logloss_score = np.mean(los_scores)\n\n\nTraining until validation scores don't improve for 10 rounds\n[100]   train's binary_logloss: 0.288706    valid's binary_logloss: 0.287306\n[200]   train's binary_logloss: 0.275793    valid's binary_logloss: 0.278749\nEarly stopping, best iteration is:\n[278]   train's binary_logloss: 0.268845    valid's binary_logloss: 0.275477\nTraining until validation scores don't improve for 10 rounds\n[100]   train's binary_logloss: 0.287829    valid's binary_logloss: 0.29263\n[200]   train's binary_logloss: 0.274043    valid's binary_logloss: 0.284794\nEarly stopping, best iteration is:\n[212]   train's binary_logloss: 0.272111    valid's binary_logloss: 0.283514\nTraining until validation scores don't improve for 10 rounds\n[100]   train's binary_logloss: 0.286818    valid's binary_logloss: 0.29414\n[200]   train's binary_logloss: 0.27296 valid's binary_logloss: 0.286963\nEarly stopping, best iteration is:\n[243]   train's binary_logloss: 0.268831    valid's binary_logloss: 0.284629\nTraining until validation scores don't improve for 10 rounds\n[100]   train's binary_logloss: 0.285241    valid's binary_logloss: 0.299504\n[200]   train's binary_logloss: 0.27159 valid's binary_logloss: 0.291395\n[300]   train's binary_logloss: 0.263487    valid's binary_logloss: 0.288628\nEarly stopping, best iteration is:\n[297]   train's binary_logloss: 0.263727    valid's binary_logloss: 0.288521\nTraining until validation scores don't improve for 10 rounds\n[100]   train's binary_logloss: 0.286321    valid's binary_logloss: 0.296634\n[200]   train's binary_logloss: 0.271693    valid's binary_logloss: 0.289273\nEarly stopping, best iteration is:\n[243]   train's binary_logloss: 0.26774 valid's binary_logloss: 0.287875\n\n\n\n\nCode\ncv_accuracy_score, cv_logloss_score\n\n\n(0.8712717729122911, 0.2840031778817057)",
    "crumbs": [
      "Python",
      "LightGBMハンドブック",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/books/09_Pythonで学ぶ数理最適化/ch01-基礎編.html",
    "href": "contents/books/09_Pythonで学ぶ数理最適化/ch01-基礎編.html",
    "title": "基礎編",
    "section": "",
    "text": "WindowsでMIPライブラリが扱えないのでWSL上のubuntuで実施している。 仮想環境名はmipである。\nなぜWindowsで動作しないのかは不明である。m = Model()を実行すると、Pythonのインタプリタがクラッシュしてしまう。\nWindowsで動かない原因はPythonのバージョンであった。3.11以下だと使うことができる。 つまりWindowsで動かないのではなく、Pythonのバージョンのせいで動かないということであった。\n\n本書では次を扱う\n\n線形最適化\n混合整数最適化",
    "crumbs": [
      "Python",
      "Pythonで学ぶ数理最適化",
      "基礎編"
    ]
  },
  {
    "objectID": "contents/books/09_Pythonで学ぶ数理最適化/ch01-基礎編.html#クッキーとケーキ",
    "href": "contents/books/09_Pythonで学ぶ数理最適化/ch01-基礎編.html#クッキーとケーキ",
    "title": "基礎編",
    "section": "2.1 クッキーとケーキ",
    "text": "2.1 クッキーとケーキ\n次の例題から数理モデルを作成してみる。\n\n\n\npicture 2\n\n\n$$ \\[\\begin{align}\nS &= \\{クッキー,ケーキ\\}\\\\\nx_{s} &= \\text{ターゲット}s\\text{の生地生産量}[10g]\\\\\n\n\\hat{x}_s &= \\max_{x_s, s\\in S} \\sum_{s\\in S} x_s\\\\\ns.t. &\\quad 0.3x_{クッキー}+0.4x_{ケーキ}\\le 1200[g]\\\\\n&\\quad 0.3x_{クッキー}+0.2x_{ケーキ}\\le 900[g]\\\\\n&\\quad x\\ge 0\\\\\n&\\quad x\\ge 0\n\\end{align}\\] $$",
    "crumbs": [
      "Python",
      "Pythonで学ぶ数理最適化",
      "基礎編"
    ]
  },
  {
    "objectID": "contents/books/09_Pythonで学ぶ数理最適化/ch01-基礎編.html#次元配列の便利機能",
    "href": "contents/books/09_Pythonで学ぶ数理最適化/ch01-基礎編.html#次元配列の便利機能",
    "title": "基礎編",
    "section": "6.1 ２次元配列の便利機能",
    "text": "6.1 ２次元配列の便利機能\n\n\n\npicture 12\n\n\n\n\n\npicture 13\n\n\n\n\n\npicture 14\n\n\n\n\n\npicture 15",
    "crumbs": [
      "Python",
      "Pythonで学ぶ数理最適化",
      "基礎編"
    ]
  },
  {
    "objectID": "contents/books/10_爆速Python/ch02_組込み機能のパフォーマンスを最大限に引き出す.html",
    "href": "contents/books/10_爆速Python/ch02_組込み機能のパフォーマンスを最大限に引き出す.html",
    "title": "組込み機能のパフォーマンスを最大限に引き出す",
    "section": "",
    "text": "Code\nimport os\nimport sys\nfrom pathlib import Path\n\nprint(os.getcwd())\n\n\nc:\\Users\\suzuk\\Dropbox\\R\\Workspace\\RTipsSite\\contents\\books\\10_爆速Python\nこの章で学ぶ内容。",
    "crumbs": [
      "Python",
      "爆速Python",
      "組込み機能のパフォーマンスを最大限に引き出す"
    ]
  },
  {
    "objectID": "contents/books/10_爆速Python/ch02_組込み機能のパフォーマンスを最大限に引き出す.html#pythonのく見込みプロファイリングモジュール",
    "href": "contents/books/10_爆速Python/ch02_組込み機能のパフォーマンスを最大限に引き出す.html#pythonのく見込みプロファイリングモジュール",
    "title": "組込み機能のパフォーマンスを最大限に引き出す",
    "section": "1.1 Pythonのく見込みプロファイリングモジュール",
    "text": "1.1 Pythonのく見込みプロファイリングモジュール\ncProfileモジュールを使うことで、ボトルネックを調べられる。\n\n\nCode\n!python -m cProfile -s cumulative ./su_utils/sec1-io-cpu_load.py 01044099999,02293099999 2021-2021 &gt; output/profile.txt\n\n\n\n\nCode\n!cat ./output/profile.txt | head -n20\n\n\n{'01044099999': -10.0, '02293099999': -27.6}\n         376120 function calls (371453 primitive calls) in 86.142 seconds\n\n   Ordered by: cumulative time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n    136/1    0.001    0.000   86.142   86.142 {built-in method builtins.exec}\n        1    0.000    0.000   86.142   86.142 sec1-io-cpu_load.py:1(&lt;module&gt;)\n        1    0.001    0.001   85.120   85.120 sec1-io-cpu_load.py:32(download_all_data)\n        2    0.001    0.000   85.118   42.559 sec1-io-cpu_load.py:16(download_data)\n        2    0.000    0.000   85.066   42.533 api.py:62(get)\n        2    0.000    0.000   85.066   42.533 api.py:14(request)\n        2    0.000    0.000   85.065   42.533 sessions.py:500(request)\n        2    0.000    0.000   85.059   42.530 sessions.py:673(send)\n     2471    0.004    0.000   84.095    0.034 socket.py:694(readinto)\n     2471    0.006    0.000   84.087    0.034 ssl.py:1237(recv_into)\n     2471    0.003    0.000   84.080    0.034 ssl.py:1095(read)\n     2471   84.078    0.034   84.078    0.034 {method 'read' of '_ssl._SSLSocket' objects}\n        2    0.000    0.000   77.557   38.778 adapters.py:613(send)\n        2    0.000    0.000   77.556   38.778 connectionpool.py:598(urlopen)\n\n\nファイルが存在しているときには処理を実行しないようにする。\n\n\nCode\n!python -m cProfile -s cumulative ./su_utils/sec1-io-cpu_load_cache.py 01044099999,02293099999 2021-2021 &gt; output/profile_cache.txt\n\n\n\n\nCode\n!cat ./output/profile_cache.txt | head -n20\n\n\n{'01044099999': -10.0, '02293099999': -27.6}\n         304987 function calls (300157 primitive calls) in 0.466 seconds\n\n   Ordered by: cumulative time\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n       16    0.000    0.000    0.563    0.035 __init__.py:1(&lt;module&gt;)\n    176/1    0.002    0.000    0.466    0.466 {built-in method builtins.exec}\n        1    0.001    0.001    0.466    0.466 sec1-io-cpu_load_cache.py:1(&lt;module&gt;)\n    210/2    0.001    0.000    0.293    0.146 &lt;frozen importlib._bootstrap&gt;:1343(_find_and_load)\n    209/2    0.001    0.000    0.293    0.146 &lt;frozen importlib._bootstrap&gt;:1298(_find_and_load_unlocked)\n    198/2    0.001    0.000    0.290    0.145 &lt;frozen importlib._bootstrap&gt;:905(_load_unlocked)\n    174/2    0.000    0.000    0.290    0.145 &lt;frozen importlib._bootstrap_external&gt;:988(exec_module)\n    431/4    0.000    0.000    0.288    0.072 &lt;frozen importlib._bootstrap&gt;:480(_call_with_frames_removed)\n        1    0.008    0.008    0.172    0.172 sec1-io-cpu_load_cache.py:56(get_all_temperatures)\n    33650    0.126    0.000    0.161    0.000 sec1-io-cpu_load_cache.py:43(get_file_temperatures)\n     17/7    0.000    0.000    0.139    0.020 {built-in method builtins.__import__}\n        2    0.000    0.000    0.116    0.058 exceptions.py:1(&lt;module&gt;)\n      174    0.002    0.000    0.113    0.001 &lt;frozen importlib._bootstrap_external&gt;:1061(get_code)\n    51/22    0.000    0.000    0.111    0.005 &lt;frozen importlib._bootstrap&gt;:1384(_handle_fromlist)",
    "crumbs": [
      "Python",
      "爆速Python",
      "組込み機能のパフォーマンスを最大限に引き出す"
    ]
  },
  {
    "objectID": "contents/books/10_爆速Python/ch02_組込み機能のパフォーマンスを最大限に引き出す.html#プロファイリング情報を可視化する",
    "href": "contents/books/10_爆速Python/ch02_組込み機能のパフォーマンスを最大限に引き出す.html#プロファイリング情報を可視化する",
    "title": "組込み機能のパフォーマンスを最大限に引き出す",
    "section": "2.1 プロファイリング情報を可視化する",
    "text": "2.1 プロファイリング情報を可視化する\n\n\nCode\n!python -m cProfile -o ./output/distance_cache.prof ./su_utils/sec2-cpu-distance_cache.py",
    "crumbs": [
      "Python",
      "爆速Python",
      "組込み機能のパフォーマンスを最大限に引き出す"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch01_時系列分析をはじめよう.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch01_時系列分析をはじめよう.html",
    "title": "時系列分析をはじめよう",
    "section": "",
    "text": "1 はじめに\n\n時系列データとは⇒ジエ亀裂データを集計するときの注意点\n時系列でないデータを用いた推測⇒単純ランダムサンプリングからの乖離\n時系列データの推測における問題⇒データ生成過程と時系列モデル\n時系列データへの回帰分析の問題⇒時系列モデルの作成\n\n\n\n2 時系列データとは\n「日付ごとの駅の乗降者数」のように時間の順番で並びかえたデータ（逆にいうと時間で並び変えることができるデータ）のことをいう。 時系列データは並び順に意味がある、つまり情報を有しているデータである。\n時系列ではないデータは「クロスセクションデータ」という。「ある日の複数の駅の乗降者数」はクロスセクションデータとなる。\n\n\n3 時系列でないデータを用いた推測\n単純ランダムサンプリングとは母集団からある事象が発生する確率が等しいということ。\n\n\n4 単純ランダムサンプリングからの乖離\n時系列データのある観測値があったとき、ランダムサンプリングのようにその日の値を再び観測するということはできない。\n\n\n5 時系列データの回帰分析の問題\n時系列データはデータが独立であるとは容易には仮定できない。\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "時系列分析をはじめよう"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch03_データ生成過程の基本.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch03_データ生成過程の基本.html",
    "title": "データ生成過程の基本",
    "section": "",
    "text": "1 はじめに\n表記方法⇒分析が楽なデータ生成過程⇒自己共分散と自己相関⇒定常過程⇒ホワイトノイズと正規ホワイトノイズ⇒ランダウォーク⇒確定的トレンドと確率的トレンド\n\n\n2 表記\n\n\\(y_t\\)は確率変数でもあり、その実現値でもある\n\n\n\n3 分析が楽なデータ生成過程\n\n定常データは楽\n\\(\\hat{Cov}_k\\)は\\(k\\)次の標本自己共分散である\n\\(\\hat{\\rho}_k\\)は標本自己相関係数である\n\n\\[\n\\begin{align}\n\\hat{\\mu}&=\\frac{1}{T}\\sum_{t=1}^Ty_t \\\\\n\\hat{Cov}_k&=\\frac{1}{T}\\sum_{t=1+k}^T(y_t - \\hat{\\mu})(y_{t-k}-\\hat{\\mu}) \\\\\n\\hat{\\rho}_k&=\\frac{\\hat{Cov}_k}{\\hat{Cov}_0}\n\\end{align}\n\\]\n\n\n4 自己共分散と自己相関\n\\[\n\\begin{align}\n\\mathrm{Cov}(y_t, y_{t-k}) &= \\mathrm{E}[(y_t-\\mu_t)(y_{t-k}-\\mu_{t-k})] \\\\\n\\mathrm{Cov}(y_t, y_t) &=\\mathrm{V}(y_t) \\\\\n\\rho_{tk} &= \\mathrm{Corr}(y_t, y_{t-k}) = \\frac{Cov(y_t, y_{t-k})}{\\sqrt{V(y_t)V(y_{t-k})}}\n\\end{align}\n\\]\n\n\n5 強定常過程\n時系列データ\\(y_t, y_{t+1}, \\dots, y_{t+k}\\)を\\(h\\)シフトした\\(y_{t+h}, y_{t+h+1}, \\dots, y_{t+h+k}\\)を考えたときに、両者の同時分布が常に等しくなるようなデータの生成過程のこと\n\n\n6 弱定常過程\n平均値と共分散が時間によらずシフト量だけに影響する\n\\[\n\\begin{align}\nE(y_t) &= \\mu \\\\\nCov(y_t, y_{t-k}) &= E[(y_t-\\mu)(y_{t-k}-\\mu)] = Cov_k \\\\\nV(y_t) &= Cov_0 = \\sigma^2 \\\\\n\\rho_k &= \\frac{Cov_k}{\\sqrt{\\sigma^2 \\sigma^2}}\n\\end{align}\n\\]\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "データ生成過程の基本"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch05_Pythonによる時系列分析の基本.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch05_Pythonによる時系列分析の基本.html",
    "title": "Pythonによる時系列分析の基本",
    "section": "",
    "text": "時系列データの基本統計量\n可視化、移動平均、差分系列、季節差分系列、対数系列、増分率、自己相関係数とコレログラム、移動平均\nデータ取得頻度の変更",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "Pythonによる時系列分析の基本"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch05_Pythonによる時系列分析の基本.html#matplotlib",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch05_Pythonによる時系列分析の基本.html#matplotlib",
    "title": "Pythonによる時系列分析の基本",
    "section": "4.1 matplotlib",
    "text": "4.1 matplotlib\n\n\nCode\nfig = plt.figure(figsize=(8, 5))\nax = fig.add_subplot(1, 1, 1)\nax.plot(\n    air_passengers[\"value\"], \n    label='原系列'\n)\nax.set_xlabel('年月', size=14)\nax.set_ylabel('乗客数', size=14)\nax.set_title('飛行機乗客数データの折れ線グラフ', size=16)\nax.legend()\n\nax.xaxis.set_major_locator(mdates.YearLocator(3))\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "Pythonによる時系列分析の基本"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch05_Pythonによる時系列分析の基本.html#回帰直線",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch05_Pythonによる時系列分析の基本.html#回帰直線",
    "title": "Pythonによる時系列分析の基本",
    "section": "14.1 回帰直線",
    "text": "14.1 回帰直線\n\n\nCode\nnp.random.seed(0)\n\nfig, ax = plt.subplots(figsize = (8, 4), ncols = 2, tight_layout = True)\n\nfor i in range(0, 2):\n    x = stats.norm.rvs(loc=0, scale=1, size=50)\n    y = stats.norm.rvs(loc=0, scale=1, size=50)\n    data_sim = pd.DataFrame({\"x\":x, \"y\":y})\n    \n    sns.regplot(x = 'x', y = 'y', data = data_sim, ax = ax[i])\n\n\n\n\n\n\n\n\n\n正規ホワイトノイズ系列のペアを、1000ペア作成して回帰分析を実行し、F比を計算する。\n\n\nCode\nn_sim = 1_000\n\nf_ratio_array = np.zeros(n_sim)\nnp.random.seed(1)\nfor i in range(1, n_sim):\n    x = stats.norm.rvs(loc=0, scale=1, size=50)\n    y = stats.norm.rvs(loc=0, scale=1, size=50)\n    data_sim = pd.DataFrame({\"x\":x, \"y\":y})\n    \n    lm_model_sim = smf.ols(\"y ~ x\", data = data_sim).fit()\n    \n    f_ratio_array[i] = lm_model_sim.fvalue\n\n\n\n\nCode\n# F比のヒストグラム\nsns.histplot(f_ratio_array, bins=100, stat=\"density\")\n\nf_df = pd.DataFrame({\"x\":np.arange(0, 12, .01)})\nf_df[\"density\"] = stats.f.pdf(f_df.x, dfn=1, dfd=48)\nsns.lineplot(x='x', y='density', data=f_df, color='red')\n\n\n\n\n\n\n\n\n\nF比がFぶんぷに従うということを利用して、次のF検定をおこなう.\n\n帰無仮説：説明変数と応答変数には関係がない\n対立仮説：説明変数と応答変数には関係がある\n\n\n\nCode\nstats.f.ppf(q=.95, dfn=1, dfd=48)\n\n\n4.042652128566653\n\n\n今回は帰無仮説を棄却する確率が5%以下になるはずである\n\n\nCode\nsum(f_ratio_array &gt; stats.f.ppf(q=.95, dfn=1, dfd=48)) / n_sim\n\n\n0.046",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "Pythonによる時系列分析の基本"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch31_単純な時系列予測の手法.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch31_単純な時系列予測の手法.html",
    "title": "単純な時系列予測",
    "section": "",
    "text": "持続予測\n季節ナイーブ予測\n平均値予測\n移動平均予測\nドリフト予測\n予測手法の比較\nMAE\nMSE\nRMSE\nMAPE\nMASE",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "単純な時系列予測"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch31_単純な時系列予測の手法.html#持続予測",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch31_単純な時系列予測の手法.html#持続予測",
    "title": "単純な時系列予測",
    "section": "1.1 持続予測",
    "text": "1.1 持続予測\n同じ値が続くことを予測する手法である。\n\\[\n\\hat{y}_{t+1} = y_t\n\\]\n\n\nCode\nnaive_pred = pd.DataFrame({\"value\":np.tile(train.loc['1957-12-01'], len(test))}, index=test.index)\nnaive_pred.head()\n\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\n1958-01-01\n336\n\n\n1958-02-01\n336\n\n\n1958-03-01\n336\n\n\n1958-04-01\n336\n\n\n1958-05-01\n336",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "単純な時系列予測"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html",
    "title": "sktimeの使い方",
    "section": "",
    "text": "PeriodIndex\nsktimeを用いた予測の流れ\nデータの分割と予測期間の指定\nsktimeによる予測\n時系列データに対する交差検証\nsktimeによる交差検証\nパイプラインの利用\nハイパーパラメータのチューニング\n分析手法の自動選択",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "sktimeの使い方"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#持続予測",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#持続予測",
    "title": "sktimeの使い方",
    "section": "4.1 持続予測",
    "text": "4.1 持続予測\n\n\nCode\nnaive_forecaster = NaiveForecaster(strategy=\"last\")\n\nnaive_forecaster.fit(train)\n\nnaive_pred = naive_forecaster.predict(fh)\nnaive_pred\n\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\n1958-01\n336.0\n\n\n1958-02\n336.0\n\n\n1958-03\n336.0\n\n\n1958-04\n336.0\n\n\n1958-05\n336.0\n\n\n1958-06\n336.0\n\n\n1958-07\n336.0\n\n\n1958-08\n336.0\n\n\n1958-09\n336.0\n\n\n1958-10\n336.0\n\n\n1958-11\n336.0\n\n\n1958-12\n336.0\n\n\n1959-01\n336.0\n\n\n1959-02\n336.0\n\n\n1959-03\n336.0\n\n\n1959-04\n336.0\n\n\n1959-05\n336.0\n\n\n1959-06\n336.0\n\n\n1959-07\n336.0\n\n\n1959-08\n336.0\n\n\n1959-09\n336.0\n\n\n1959-10\n336.0\n\n\n1959-11\n336.0\n\n\n1959-12\n336.0\n\n\n1960-01\n336.0\n\n\n1960-02\n336.0\n\n\n1960-03\n336.0\n\n\n1960-04\n336.0\n\n\n1960-05\n336.0\n\n\n1960-06\n336.0\n\n\n1960-07\n336.0\n\n\n1960-08\n336.0\n\n\n1960-09\n336.0\n\n\n1960-10\n336.0\n\n\n1960-11\n336.0\n\n\n1960-12\n336.0",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "sktimeの使い方"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#ドリフト予測",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#ドリフト予測",
    "title": "sktimeの使い方",
    "section": "4.2 ドリフト予測",
    "text": "4.2 ドリフト予測\n\n\nCode\ndrift_forecaster = NaiveForecaster(strategy=\"drift\")\n\ndrift_forecaster.fit(train)\n\ndrift_pred = drift_forecaster.predict(fh)\n\n\n\n\nCode\ndrift_pred\n\n\n\n\n\n\n\n\n\nvalue\n\n\n\n\n1958-01\n338.093458\n\n\n1958-02\n340.186916\n\n\n1958-03\n342.280374\n\n\n1958-04\n344.373832\n\n\n1958-05\n346.467290\n\n\n1958-06\n348.560748\n\n\n1958-07\n350.654206\n\n\n1958-08\n352.747664\n\n\n1958-09\n354.841121\n\n\n1958-10\n356.934579\n\n\n1958-11\n359.028037\n\n\n1958-12\n361.121495\n\n\n1959-01\n363.214953\n\n\n1959-02\n365.308411\n\n\n1959-03\n367.401869\n\n\n1959-04\n369.495327\n\n\n1959-05\n371.588785\n\n\n1959-06\n373.682243\n\n\n1959-07\n375.775701\n\n\n1959-08\n377.869159\n\n\n1959-09\n379.962617\n\n\n1959-10\n382.056075\n\n\n1959-11\n384.149533\n\n\n1959-12\n386.242991\n\n\n1960-01\n388.336449\n\n\n1960-02\n390.429907\n\n\n1960-03\n392.523364\n\n\n1960-04\n394.616822\n\n\n1960-05\n396.710280\n\n\n1960-06\n398.803738\n\n\n1960-07\n400.897196\n\n\n1960-08\n402.990654\n\n\n1960-09\n405.084112\n\n\n1960-10\n407.177570\n\n\n1960-11\n409.271028\n\n\n1960-12\n411.364486",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "sktimeの使い方"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#事例1-季節調整-トレンド除去-持続予測",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#事例1-季節調整-トレンド除去-持続予測",
    "title": "sktimeの使い方",
    "section": "7.1 事例1: 季節調整 + トレンド除去 + 持続予測",
    "text": "7.1 事例1: 季節調整 + トレンド除去 + 持続予測\n\n\nCode\npipe_forecaster_1 = TransformedTargetForecaster(\n    [\n        (\"deseasonalize\", Deseasonalizer(model=\"multiplicative\", sp=12)),\n        (\"detrend\", Detrender(forecaster=PolynomialTrendForecaster(degree=1), model=\"multiplicative\")),\n        (\"forecast\", NaiveForecaster(strategy=\"last\")),\n    ]\n)\n\n# データの当てはめ\npipe_forecaster_1.fit(train)\n\n# 予測の実施\npipe_pred_1 = pipe_forecaster_1.predict(fh)\n\n# 予測精度\nmean_absolute_error(test, pipe_pred_1)\n\n\n23.600721546680006\n\n\n\n\nCode\n# CV\ncv = ExpandingWindowSplitter(fh=np.arange(1, 13), initial_window=24, step_length=12)\ncv_df = evaluate(forecaster=pipe_forecaster_1, cv=cv, y=train, scoring=MeanAbsoluteError(), return_data=True)\ncv_df\n\n\n\n\n\n\n\n\n\ntest_MeanAbsoluteError\nfit_time\npred_time\nlen_train_window\ncutoff\ny_train\ny_test\ny_pred\n\n\n\n\n0\n10.852564\n0.022165\n0.012915\n24\n1950-12\nvalue 1949-01 112 1949-02 118 1...\nvalue 1951-01 145 1951-02 150 1...\nvalue 1951-01 135.834056 1951-0...\n\n\n1\n7.251605\n0.017533\n0.007962\n36\n1951-12\nvalue 1949-01 112 1949-02 118 1...\nvalue 1952-01 171 1952-02 180 1...\nvalue 1952-01 166.265880 1952-0...\n\n\n2\n7.659675\n0.016288\n0.009640\n48\n1952-12\nvalue 1949-01 112 1949-02 118 1...\nvalue 1953-01 196 1953-02 196 1...\nvalue 1953-01 195.685338 1953-0...\n\n\n3\n10.326878\n0.016838\n0.009004\n60\n1953-12\nvalue 1949-01 112 1949-02 118 1...\nvalue 1954-01 204 1954-02 188 1...\nvalue 1954-01 202.578761 1954-0...\n\n\n4\n20.332239\n0.018527\n0.008935\n72\n1954-12\nvalue 1949-01 112 1949-02 118 1...\nvalue 1955-01 242 1955-02 233 1...\nvalue 1955-01 230.777564 1955-0...\n\n\n5\n9.418944\n0.016428\n0.007115\n84\n1955-12\nvalue 1949-01 112 1949-02 118 1...\nvalue 1956-01 284 1956-02 277 1...\nvalue 1956-01 281.797217 1956-0...\n\n\n6\n16.832911\n0.015951\n0.010480\n96\n1956-12\nvalue 1949-01 112 1949-02 118 1...\nvalue 1957-01 315 1957-02 301 1...\nvalue 1957-01 310.113894 1957-0...\n\n\n\n\n\n\n\n\n\nCode\ncv_df.loc[:, \"test_MeanAbsoluteError\"].mean()\n\n\n11.81068805846526",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "sktimeの使い方"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#事例2-差分によるトレンド除去-季節ナイーブ予測",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#事例2-差分によるトレンド除去-季節ナイーブ予測",
    "title": "sktimeの使い方",
    "section": "7.2 事例2: 差分によるトレンド除去 + 季節ナイーブ予測",
    "text": "7.2 事例2: 差分によるトレンド除去 + 季節ナイーブ予測\n\n\nCode\npipe_forecaster_2 = TransformedTargetForecaster(\n    [\n        ('transform', Differencer(lags=[1])), \n        ('forecast', NaiveForecaster(strategy=\"last\", sp=12))\n    ]\n)\n\npipe_forecaster_2.fit(train)\n\npipe_pred_2 = pipe_forecaster_2.predict(fh)\n\nmean_absolute_error(test, pipe_pred_2)\n\n\n17.805555555555557\n\n\n\n\nCode\nevaluate(\n    forecaster=pipe_forecaster_2,\n    cv=cv, \n    y=train, \n    scoring=MeanAbsoluteError()\n).iloc[:, 0].mean()\n\n\n11.13095238095238\n\n\n\n\nCode\nfg, ax = plot_series(\n    train, \n    test, \n    pipe_pred_1, \n    pipe_pred_2, \n    labels = [\"train\", \"test\", \"pipe_1\", \"pipe_2\"], \n    markers=np.tile('', 4)\n)\nfig.set_size_inches(12, 4)",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "sktimeの使い方"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#ハイパーパラメータのチューニング",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#ハイパーパラメータのチューニング",
    "title": "sktimeの使い方",
    "section": "7.3 ハイパーパラメータのチューニング",
    "text": "7.3 ハイパーパラメータのチューニング\n\n\nCode\nfrom dask import dataframe as dd\n\n\n\n\nCode\n# 持続予測\nnaive_forecaster = NaiveForecaster(strategy=\"last\", sp=1)\n\n# 持続予測のハイパーパラメータの準備\nparam_grid = {'sp': np.arange(1, 13)}\n\n# 予測器\nbest_naive_forecaster = ForecastingGridSearchCV(\n    naive_forecaster, \n    param_grid=param_grid, \n    strategy=\"refit\", \n    cv=cv,\n    scoring=MeanAbsoluteError()\n)\n\n\n\n\nCode\nbest_naive_forecaster.fit(train)\n\n\nForecastingGridSearchCV(cv=ExpandingWindowSplitter(fh=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]),\n                                                   initial_window=24,\n                                                   step_length=12),\n                        forecaster=NaiveForecaster(),\n                        param_grid={'sp': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])},\n                        scoring=MeanAbsoluteError())Please rerun this cell to show the HTML repr or trust the notebook.ForecastingGridSearchCV?Documentation for ForecastingGridSearchCVForecastingGridSearchCV(cv=ExpandingWindowSplitter(fh=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]),\n                                                   initial_window=24,\n                                                   step_length=12),\n                        forecaster=NaiveForecaster(),\n                        param_grid={'sp': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])},\n                        scoring=MeanAbsoluteError())forecaster: NaiveForecasterNaiveForecaster()NaiveForecaster?Documentation for NaiveForecasterNaiveForecaster()\n\n\n\n\nCode\nbest_naive_forecaster.best_params_\n\n\n{'sp': 12}\n\n\n\n\nCode\n# 予測\nbest_naive_pred = best_naive_forecaster.predict(fh)\n\nmean_absolute_error(test, best_naive_pred)\n\n\n60.083333333333336",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "sktimeの使い方"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#事例複数の予測モデルからの選択",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#事例複数の予測モデルからの選択",
    "title": "sktimeの使い方",
    "section": "8.1 事例:複数の予測モデルからの選択",
    "text": "8.1 事例:複数の予測モデルからの選択\n\n\nCode\nforecast_options = MultiplexForecaster(\n    forecasters = [\n        (\"pipe_1\", pipe_forecaster_1),\n        (\"pipe_2\", pipe_forecaster_2),\n        (\"best_naive\", best_naive_forecaster)\n    ]\n)\n\nparam_grid = {'selected_forecaster': [\"pipe_1\", \"pipe_2\", \"best_naive\"]}\n\ncv_forecaster = ForecastingGridSearchCV(\n    forecast_options, \n    param_grid=param_grid, \n    cv=cv, \n    strategy=\"refit\", \n    scoring=MeanAbsoluteError()\n)\n\ncv_forecaster.fit(train)\n\ncv_forecaster.best_params_\n\n\n{'selected_forecaster': 'pipe_2'}",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "sktimeの使い方"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#対数変換の必要性の選択",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch33_sktimeの使い方.html#対数変換の必要性の選択",
    "title": "sktimeの使い方",
    "section": "8.2 対数変換の必要性の選択",
    "text": "8.2 対数変換の必要性の選択\n\n\nCode\npipe_select = TransformedTargetForecaster(\n    steps = [\n        ('log', OptionalPassthrough(LogTransformer())),\n        ('forecaster', MultiplexForecaster(\n            forecasters = [\n                (\"pipe_1\", pipe_forecaster_1),\n                (\"pipe_2\", pipe_forecaster_2),\n                (\"best_naive\", best_naive_forecaster)\n            ]\n        ))\n    ]\n)\n\nparam_grid = {\n    'log__passthrough': [True, False],\n    'forecaster__selected_forecaster': [\"pipe_1\", \"pipe_2\", \"best_naive\"]\n}\n\ncv_pipe_forecaster = ForecastingGridSearchCV(\n    forecaster=pipe_select,\n    param_grid=param_grid,\n    cv=cv,\n    scoring=MeanAbsoluteError()\n)\n\n\n\n\n\nCode\ncv_pipe_forecaster.fit(train)\n\ncv_pipe_forecaster.best_params_\n\n\n{'forecaster__selected_forecaster': 'pipe_1', 'log__passthrough': False}\n\n\n\n\nCode\nresult_df = cv_pipe_forecaster.cv_results_[[\"mean_test_MeanAbsoluteError\", \"params\"]].copy()\nresult_df[\"params\"] = result_df[\"params\"].apply(lambda x: list(x.values()))\n\nresult_df\n\n\n\n\n\n\n\n\n\nmean_test_MeanAbsoluteError\nparams\n\n\n\n\n0\n11.810688\n[pipe_1, True]\n\n\n1\n9.090768\n[pipe_1, False]\n\n\n2\n11.130952\n[pipe_2, True]\n\n\n3\n10.881038\n[pipe_2, False]\n\n\n4\n37.527778\n[best_naive, True]\n\n\n5\n37.527778\n[best_naive, False]\n\n\n\n\n\n\n\n\n\nCode\nbest_pred = cv_pipe_forecaster.predict(fh)\n\nmean_absolute_error(test, best_pred)\n\n\n36.03614258141156",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "sktimeの使い方"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch41_box-jenkins法からの自動予測アプローチ.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch41_box-jenkins法からの自動予測アプローチ.html",
    "title": "Box-Jenkins法からの自動予測アプローチ",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "Box-Jenkins法からの自動予測アプローチ"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch43_SARIMAモデル.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch43_SARIMAモデル.html",
    "title": "SARIMAモデル",
    "section": "",
    "text": "1 分析の準備\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as tsa\n\nfrom sklearn.linear_model import LinearRegression\nfrom sktime.forecasting.compose import make_reduction\n\nfrom matplotlib import rcParams\nrcParams['font.family'] = 'sans-serif'\nrcParams['font.sans-serif'] = 'Meiryo'\n\n\n\n\n2 SARIMAモデル\n\n\nCode\nair_passengers = sm.datasets.get_rdataset(\"AirPassengers\").data\n\nair_passengers.index = pd.date_range(\n    start='1949-01-01', \n    periods=len(air_passengers), \n    freq='MS'\n)\n\nair_passengers = air_passengers.drop(air_passengers.columns[0], axis=1)\n\ntrain = air_passengers.loc['1949-01':'1957-12']\ntest = air_passengers.loc['1958-01':'1960-12']\n\n\n\n\n3 SARIMAモデルの推定と予測\n\n\nCode\nmod_sarima = tsa.SARIMAX(\n    train, \n    order = (3, 1, 2), # SARIMA(3, 1, 2)\n    seasonal_order = (1, 1, 1, 12), # Seasonal component\n).fit(maxiter=1_000)\n\nmod_sarima.params\n\n\nar.L1        0.490074\nar.L2        0.323664\nar.L3       -0.145454\nma.L1       -0.750347\nma.L2       -0.161106\nar.S.L12    -0.251565\nma.S.L12     0.110749\nsigma2      84.291009\ndtype: float64\n\n\n\n\nCode\npred_sarima = mod_sarima.forecast(36)\n\nfig, ax = plt.subplots(figsize=(8, 4))\n\nax.plot(train['value'], label='訓練データ')\nax.plot(test['value'], label='テストデータ')\n\nax.plot(pred_sarima, label='SARIMA(3, 1, 2)(1, 1, 1, 12)')\n\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\n4 SARIMAXモデルとSARIMA ERRORモデル\n外生変数入りのSARIMAモデルをSARIMAXモデルと呼びます。また、SARIMAモデルの残差をモデル化したものをSARIMA ERRORモデルと呼びます。\nARAMAXモデルは、外生変数を含むARIMAモデルです。SARIMAXモデルは、外生変数を含むSARIMAモデルです。SARIMA ERRORモデルは、残差をモデル化したSARIMAモデルです。 数式では次のように表される\nARMAXモデルは次です。\n\\[\n\\phi(B)y_t = \\beta x_t + \\theta(B) \\epsilon_t\n\\]\nARMA Errorモデルは次です。\n\\[\nY_t = \\phi_1 Y_{t-1} + \\cdots + \\phi_p Y_{t-p} + \\theta_1 \\epsilon_{t-1} + \\cdots + \\theta_q \\epsilon_{t-q} + \\epsilon_t + \\beta_1 X_{1,t} + \\cdots + \\beta_k X_{k,t} + \\epsilon_t\n\\]\n\n\n5 SARIMAXモデルの推定と予測\n\n\nCode\nts_sales_bj = pd.read_csv(\n    './book-python-tsa-intro/book-data/4-3-1-sales-data.csv', \n    index_col='date',\n    parse_dates=True,\n    dtype='float'\n)\n\nts_sales_bj.index.freq = 'MS'\n\nts_sales_bj.head()\n\n\n\n\n\n\n\n\n\nsales\ndiscount\n\n\ndate\n\n\n\n\n\n\n2010-01-01\n71.0\n0.0\n\n\n2010-02-01\n67.0\n0.0\n\n\n2010-03-01\n64.0\n0.0\n\n\n2010-04-01\n64.0\n0.0\n\n\n2010-05-01\n151.0\n1.0\n\n\n\n\n\n\n\n\n\nCode\nts_sales_bj.plot(subplots=True)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 売上を対数返還した結果のグラフ\n\nfig, ax = plt.subplots(figsize=(8,2), tight_layout=True)\n\nax.plot(np.log(ts_sales_bj['sales']), label='対数変換後')\n\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# テストデータと訓練データの作成\n\ntrain = ts_sales_bj.loc['2010-01':'2018-12']\ntest = ts_sales_bj.loc['2019-01':'2019-12']\n\n\n\n\nCode\n# SARIMA(1, 1, 1)(1, 1, 1)\n\nmod_sarimax = tsa.SARIMAX(\n    np.log(train['sales']),\n    exog=train['discount'],\n    order=(1,1,1),\n    seasonal_order=(1,1,1,12), \n).fit(maxiter=1_000, method='nm')\n\nprint(mod_sarimax.params)\n\n\nOptimization terminated successfully.\n         Current function value: -0.725494\n         Iterations: 406\n         Function evaluations: 634\ndiscount    0.780721\nar.L1      -0.289926\nma.L1      -0.164008\nar.S.L12   -0.017217\nma.S.L12   -0.042015\nsigma2      0.011220\ndtype: float64\n\n\n\n\nCode\n# SARIMAXモデルによる予測\npred_sarimax = mod_sarimax.forecast(\n    12, \n    exog = test['discount'].values.reshape(-1, 1)\n)\n\nfig, ax = plt.subplots(figsize=(8, 4), tight_layout=True)\n\nax.plot(np.log(train['sales']), label='訓練データ')\nax.plot(np.log(test['sales']), label='テストデータ')\n\nax.plot(pred_sarimax, label='SARIMAX(1, 1, 1)(1, 1, 1)')\n\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# 信頼区間付きの予測\npred_ci = mod_sarimax.get_forecast(\n    steps=12, \n    exog=test['discount'].values.reshape(-1, 1) # (データ数, 特徴量数)\n)\n\npred_ci.summary_frame(alpha=.05)\n\n\n\n\n\n\n\n\nsales\nmean\nmean_se\nmean_ci_lower\nmean_ci_upper\n\n\n\n\n2019-01-01\n3.839755\n0.105925\n3.632145\n4.047364\n\n\n2019-02-01\n3.273200\n0.120689\n3.036654\n3.509746\n\n\n2019-03-01\n2.982692\n0.140423\n2.707468\n3.257915\n\n\n2019-04-01\n3.719158\n0.155908\n3.413583\n4.024732\n\n\n2019-05-01\n2.775086\n0.170459\n2.440992\n3.109180\n\n\n2019-06-01\n2.702974\n0.183735\n2.342860\n3.063088\n\n\n2019-07-01\n2.462137\n0.196149\n2.077693\n2.846582\n\n\n2019-08-01\n2.878494\n0.207813\n2.471188\n3.285800\n\n\n2019-09-01\n2.917276\n0.218859\n2.488320\n3.346231\n\n\n2019-10-01\n2.786179\n0.229373\n2.336617\n3.235741\n\n\n2019-11-01\n3.392243\n0.239425\n2.922978\n3.861508\n\n\n2019-12-01\n3.566839\n0.249073\n3.078665\n4.055013\n\n\n\n\n\n\n\n\n\nCode\nconf_int_def_exp = pred_ci.summary_frame(alpha=.05).apply(np.exp)\n\nfig, ax = plt.subplots(figsize=(8, 4), tight_layout=True)\n\nax.plot(train['sales'], label='訓練データ')\n\nax.plot(conf_int_def_exp['mean'], label='予測値')\n\nax.fill_between(\n    conf_int_def_exp.index, \n    conf_int_def_exp['mean_ci_lower'], \n    conf_int_def_exp['mean_ci_upper'], \n    color='gray', \n    alpha=0.2\n)\n\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "SARIMAモデル"
    ]
  },
  {
    "objectID": "contents/books/11_Pythonではじめる時系列分析入門/ch51_状態空間モデルの概要.html",
    "href": "contents/books/11_Pythonではじめる時系列分析入門/ch51_状態空間モデルの概要.html",
    "title": "状態空間モデルの概要",
    "section": "",
    "text": "1 はじめに\n\n状態と観測がある\n状態は観測できない\n状態に観測誤差が加わったものを観測とする\n状態は時々刻々と変化する\n状態空間モデルでは、状態を状態方程式、観測を観測方程式で表す\n状態方程式で状態の変動パターンを表し、そのパラメータを観測から推定する\n\n状態空間モデルを使うことのメリットは\n\n推定するのは状態であるため、観測に誤差や欠測があっても対応することができる\n状態の変動パターンが非定常であっても差分を取る必要がない\n\n状態空間モデルにおけるモデルとモデルの推定手法\n\nモデル\n\n線形ガウス状態空間モデル\n\n推定手法\n\nカルマンフィルタ\nカルマンスムーザ\nパーティクルフィルタ\n\n\n状態空間モデルの主な成分\n\n\\(\\mu_t\\): 水準成分、系列のトレンドを表す成分\n\\(\\sigma_t\\): ドリフト成分、水準成分の増減量を表す成分\n\\(\\gamma_t\\): 季節成分、周期的な変動を表す成分\n\n\n\n2 線形ガウス状態空間モデル\n\\(\\bm{\\alpha}t\\)を状態ベクトル、\\(\\bm{y}_t\\)を観測ベクトルとすると、線形ガウス状態空間モデルは以下のように表される。\\(\\bm{\\eta}_t\\)は過程誤差、\\(\\bm{\\epsilon}_t\\)は観測誤差である。\n\\[\n\\begin{align}\n\\bm{\\alpha}_t &= \\bm{T}_t\\bm{\\alpha}_{t-1} + \\bm{R}_t\\bm{\\eta}_t \\\\\n\\bm{\\eta}_t &\\sim N(0, \\bm{Q}_t) \\\\\n\\bm{y}_t &= \\bm{Z}_t\\bm{\\alpha}_t + \\bm{\\epsilon}_t \\\\\n\\bm{\\epsilon}_t &\\sim N(0, \\bm{H}_t)\n\\end{align}\n\\]\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "Pythonではじめる時系列分析",
      "状態空間モデルの概要"
    ]
  },
  {
    "objectID": "contents/books/12_いきなりPython/ch01_マインドリーダー100.html",
    "href": "contents/books/12_いきなりPython/ch01_マインドリーダー100.html",
    "title": "マインドリーダー100",
    "section": "",
    "text": "1 はじめてのノートブック\n\n\nCode\nprint(\"Hello, World\")\nprint(\"Hello, Python\")\n\n\nHello, World\nHello, Python\n\n\n\n\n2 数当てられゲーム\n\n\nCode\nimport ipywidgets as widgets\nfrom IPython.display import display\nimport threading\n\nclass InputHandler:\n    def __init__(self):\n        self.value = None\n        self.widget = widgets.Text(\n            description=\"回答\",\n            placeholder=\"yes/no\"\n        )\n        self.widget.continuous_update = False\n        self.input_event = threading.Event()\n        self.widget.observe(self.on_submit, names='value')\n\n    def on_submit(self, change):\n        self.value = change['new']\n        print(f\"入力をうけました\")\n        self.input_event.set()\n        \n    def get_value(self):\n        self.input_event.wait()\n        self.input_event.clear()\n        return self.value\n        \nhandler = InputHandler()\ndisplay(handler.widget)\nhandler.get_value()\n\n\n\n\n\n入力をうけました\n\n\n\n\nCode\nprint(\"1から100までの間だで好きな数字を１つ思うべてください\")\nprint(\"あなたが考えている数字を回いないに当ててみます\")\n\nlow, high = 1, 100\nprint(f\"{low=}, {high=}\")\n\nfor i in range(1, 8):\n    answer = None\n    mid = (low + high) // 2\n    print(f\"{i}回目の質問: あなたが考えている数字は{mid}より大きいですか？(yes/no)\")\n    handler = InputHandler()\n    display(handler.widget)\n    answer = handler.get_value()\n    if answer == \"yes\":\n        low = mid+1\n    else:\n        high = mid\n    \n    if low == high:\n        break\n    \nprint(f\"あなたが考えている数字は{low}です\")\n\n\n1から100までの間だで好きな数字を１つ思うべてください\nあなたが考えている数字を回いないに当ててみます\nlow=1, high=100\n1回目の質問: あなたが考えている数字は50より大きいですか？(yes/no)\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "いきなりPython",
      "マインドリーダー100"
    ]
  },
  {
    "objectID": "contents/books/12_いきなりPython/ch03_いつでも声が割り器.html",
    "href": "contents/books/12_いきなりPython/ch03_いつでも声が割り器.html",
    "title": "いつでも声変わり器",
    "section": "",
    "text": "1 はじめに\n\npyaudioでコンピュータのマイクを準備する\npyaudio+numpyで音声データを取得する\nmatplotlibで音声データを表示する\npython-soundfileで音声データを保存する\nlibrosaで音声データを加工する\nmatplotlibで音声データを表示する\n\n\n\n2 PCマイクの準備\n\n\nCode\nimport numpy as np\nimport pyaudio\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt\nimport soundfile as sf\n\npa = pyaudio.PyAudio()\n\ndef get_mic_index(pa):\n\n    mic_list = []\n    for i in range(pa.get_device_count()):\n        device_info = pa.get_device_info_by_index(i)\n        num_of_input_ch = device_info['maxInputChannels'] # mic has input channeles greater than 0\n        if num_of_input_ch &gt; 0:\n            mic_list.append(device_info)\n    \n    return mic_list\n        \npprint(get_mic_index(pa))\n\n\n[{'defaultHighInputLatency': 0.18,\n  'defaultHighOutputLatency': 0.18,\n  'defaultLowInputLatency': 0.09,\n  'defaultLowOutputLatency': 0.09,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 0,\n  'index': 0,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': 'Microsoft サウンド マッパー - Input',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.18,\n  'defaultHighOutputLatency': 0.18,\n  'defaultLowInputLatency': 0.09,\n  'defaultLowOutputLatency': 0.09,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 0,\n  'index': 1,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': '繝槭う繧ｯ (UGREEN camera 2K)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.18,\n  'defaultHighOutputLatency': 0.18,\n  'defaultLowInputLatency': 0.09,\n  'defaultLowOutputLatency': 0.09,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 0,\n  'index': 2,\n  'maxInputChannels': 4,\n  'maxOutputChannels': 0,\n  'name': 'マイク配列 (デジタルマイク向けインテル® スマート・サウンド',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.18,\n  'defaultHighOutputLatency': 0.18,\n  'defaultLowInputLatency': 0.09,\n  'defaultLowOutputLatency': 0.09,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 0,\n  'index': 3,\n  'maxInputChannels': 8,\n  'maxOutputChannels': 0,\n  'name': 'CABLE Output (VB-Audio Virtual ',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.24,\n  'defaultHighOutputLatency': 0.0,\n  'defaultLowInputLatency': 0.12,\n  'defaultLowOutputLatency': 0.0,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 1,\n  'index': 9,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': 'プライマリ サウンド キャプチャ ドライバー',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.24,\n  'defaultHighOutputLatency': 0.0,\n  'defaultLowInputLatency': 0.12,\n  'defaultLowOutputLatency': 0.0,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 1,\n  'index': 10,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': '繝槭う繧ｯ (UGREEN camera 2K)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.24,\n  'defaultHighOutputLatency': 0.0,\n  'defaultLowInputLatency': 0.12,\n  'defaultLowOutputLatency': 0.0,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 1,\n  'index': 11,\n  'maxInputChannels': 4,\n  'maxOutputChannels': 0,\n  'name': 'マイク配列 (デジタルマイク向けインテル® スマート・サウンド・テクノロジー)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.24,\n  'defaultHighOutputLatency': 0.0,\n  'defaultLowInputLatency': 0.12,\n  'defaultLowOutputLatency': 0.0,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 1,\n  'index': 12,\n  'maxInputChannels': 8,\n  'maxOutputChannels': 0,\n  'name': 'CABLE Output (VB-Audio Virtual Cable)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.01,\n  'defaultHighOutputLatency': 0.0,\n  'defaultLowInputLatency': 0.002,\n  'defaultLowOutputLatency': 0.0,\n  'defaultSampleRate': 48000.0,\n  'hostApi': 2,\n  'index': 22,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': 'マイク配列 (デジタルマイク向けインテル® スマート・サウンド・テクノロジー)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.01,\n  'defaultHighOutputLatency': 0.0,\n  'defaultLowInputLatency': 0.003,\n  'defaultLowOutputLatency': 0.0,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 2,\n  'index': 23,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': 'CABLE Output (VB-Audio Virtual Cable)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.01,\n  'defaultHighOutputLatency': 0.0,\n  'defaultLowInputLatency': 0.003,\n  'defaultLowOutputLatency': 0.0,\n  'defaultSampleRate': 48000.0,\n  'hostApi': 2,\n  'index': 24,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': '繝槭う繧ｯ (UGREEN camera 2K)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.04,\n  'defaultHighOutputLatency': 0.04,\n  'defaultLowInputLatency': 0.01,\n  'defaultLowOutputLatency': 0.01,\n  'defaultSampleRate': 48000.0,\n  'hostApi': 3,\n  'index': 25,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': 'マイク配列 1 ()',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.04,\n  'defaultHighOutputLatency': 0.04,\n  'defaultLowInputLatency': 0.01,\n  'defaultLowOutputLatency': 0.01,\n  'defaultSampleRate': 48000.0,\n  'hostApi': 3,\n  'index': 26,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': 'マイク配列 2 ()',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.04,\n  'defaultHighOutputLatency': 0.04,\n  'defaultLowInputLatency': 0.01,\n  'defaultLowOutputLatency': 0.01,\n  'defaultSampleRate': 16000.0,\n  'hostApi': 3,\n  'index': 27,\n  'maxInputChannels': 4,\n  'maxOutputChannels': 0,\n  'name': 'マイク配列 3 ()',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.04,\n  'defaultHighOutputLatency': 0.04,\n  'defaultLowInputLatency': 0.01,\n  'defaultLowOutputLatency': 0.01,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 3,\n  'index': 28,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': '繝槭う繧ｯ (Realtek HD Audio Mic input)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.04,\n  'defaultHighOutputLatency': 0.04,\n  'defaultLowInputLatency': 0.01,\n  'defaultLowOutputLatency': 0.01,\n  'defaultSampleRate': 48000.0,\n  'hostApi': 3,\n  'index': 31,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': 'PC スピーカー (Realtek HD Audio 2nd output with SST)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.04,\n  'defaultHighOutputLatency': 0.04,\n  'defaultLowInputLatency': 0.01,\n  'defaultLowOutputLatency': 0.01,\n  'defaultSampleRate': 48000.0,\n  'hostApi': 3,\n  'index': 34,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': 'PC スピーカー (Realtek HD Audio output with SST)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.04,\n  'defaultHighOutputLatency': 0.04,\n  'defaultLowInputLatency': 0.01,\n  'defaultLowOutputLatency': 0.01,\n  'defaultSampleRate': 48000.0,\n  'hostApi': 3,\n  'index': 35,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': 'ステレオ ミキサー (Realtek HD Audio Stereo input)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.08533333333333333,\n  'defaultHighOutputLatency': 0.08533333333333333,\n  'defaultLowInputLatency': 0.01,\n  'defaultLowOutputLatency': 0.01,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 3,\n  'index': 37,\n  'maxInputChannels': 8,\n  'maxOutputChannels': 0,\n  'name': 'CABLE Output (VB-Audio Point)',\n  'structVersion': 2},\n {'defaultHighInputLatency': 0.08533333333333333,\n  'defaultHighOutputLatency': 0.08533333333333333,\n  'defaultLowInputLatency': 0.01,\n  'defaultLowOutputLatency': 0.01,\n  'defaultSampleRate': 44100.0,\n  'hostApi': 3,\n  'index': 39,\n  'maxInputChannels': 2,\n  'maxOutputChannels': 0,\n  'name': '繝槭う繧ｯ (UGREEN camera 2K)',\n  'structVersion': 2}]\n\n\n\n\nCode\ndef record(pa, index, duration):\n    \"\"\"PCのマイクで録音する関数\"\"\"\n    # 録音するパラメータを設定\n    sampling_rate = 44100\n    frame_size = 1024\n    format = pyaudio.paInt16\n\n    # ストリームを開く\n    stream = pa.open(format=format,\n                        channels=1,\n                        rate=sampling_rate,\n                        input=True,\n                        input_device_index=index,\n                        frames_per_buffer=frame_size)\n    \n    # ループ数の設定\n    dt = 1 / sampling_rate\n    n = int(((duration / dt) / frame_size))\n    print(n)\n\n    # 録音\n    waveform = []\n    print(\"start\")\n    for i in range(n):\n        frame = stream.read(frame_size)\n        waveform.append(frame)\n\n    # ストリームの終了\n    stream.stop_stream()\n    stream.close()\n\n    # データあをまとめる\n    waveform = b\"\".join(waveform)\n\n    # バイトデータを数値データに変換\n    byte_to_num = np.frombuffer(waveform, dtype=\"int16\")\n\n    # 最大値を計算\n    max_value = float((2 ** 16) / 2)\n\n    # 波形を正規化\n    normalized_waveform = byte_to_num / max_value\n\n    return normalized_waveform, sampling_rate\n\n\n\n\nCode\ndef graph_plot(x, y):\n    \"\"\"波形をグラフにする関数\"\"\"\n    \n    fig, ax = plt.subplots()\n    ax.set_xlabel(\"Times[s]\")\n    ax.set_ylabel(\"Amplitude\")\n    \n    # データのプロット\n    ax.plot(x, y)\n    plt.show()\n    plt.close()\n    \n    return \n\n\n\n\n\nCode\nindex = 1\nduration = 5\nwaveform, sampling_rate = record(pa, index, duration)\npa.terminate()\n\ndt = 1 / sampling_rate\nt = np.arange(0, len(waveform) * dt, dt)\ngraph_plot(t, waveform)\n\n\n215\nstart\n\n\n\n\n\n\n\n\n\n\n\nCode\nfilename = './docs/konichiwa.wav'\nsf.write(filename, waveform, sampling_rate)\n\n\n\n\n3 音声の高さを変更\n\n\nCode\nimport librosa\n\n\n\n\nCode\nfilename = './docs/konichiwa.wav'\nn_steps = 8\n\nwaveform_shifted = librosa.effects.pitch_shift(waveform, sr=sampling_rate, n_steps=n_steps)\n\n\n\n\nCode\nsf.write('./docs/konichiwa_shifted.wav', waveform_shifted, sampling_rate)\n\n\n\n\nCode\ndef graph_plot(x, y):\n    \"\"\"波形をグラフにする関数\"\"\"\n    \n    fig, ax = plt.subplots()\n    ax.set_xlabel(\"Times[s]\")\n    ax.set_ylabel(\"Amplitude\")\n    \n    # データのプロット\n    for x_axis, y_axis in zip(x, y):\n        ax.plot(x_axis, y_axis)\n    plt.show()\n    plt.close()\n    \n    return\n\n\n\n\nCode\ndt = 1 / sampling_rate\nt = np.arange(0, len(waveform) * dt, dt)\nt_shifted = np.arange(0, len(waveform_shifted) * dt, dt)\ngraph_plot([t, t_shifted], [waveform, waveform_shifted])\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "いきなりPython",
      "いつでも声変わり器"
    ]
  },
  {
    "objectID": "contents/books/12_いきなりPython/ch05_タイムラプスクリエーター.html",
    "href": "contents/books/12_いきなりPython/ch05_タイムラプスクリエーター.html",
    "title": "タイムラプスクリエーター",
    "section": "",
    "text": "1 はじめに\n\n\nCode\nimport cv2\n\n\n\n\n2 静止画を撮影\n\n\nCode\ncap = cv2.VideoCapture(0)\nret, frame = cap.read()\n\nif ret:\n    cv2.imwrite('docs/webcam.jpg', frame)\n    print('webcam.jpg saved')\n    \ncap.release()\n\n\nwebcam.jpg saved\n\n\n\n\nCode\ncv2.imshow('docs/webcam.jpg', frame)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\n\n\n\n3 動画を撮影する\n\n\nCode\nimport cv2\nimport time\n\ncap = cv2.VideoCapture(0)\n\nframe_rate = 30\nduration = 10\ninterval = 1 / frame_rate\nframe_count = int(duration / interval)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter('docs/webcam.mp4', fourcc, frame_rate, (640, 480))\n\nfor i in range(frame_count):\n    ret, frame = cap.read()\n    if not ret:\n        break\n    out.write(frame)\n\n    cv2.imshow('Recording', frame)\n    \n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n    \n    time.sleep(interval)\n    \ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n\n\n\n4 タイムラプス動画を撮影する\n基本的にはフレームレートを落とすのが方針である。\n\n\nCode\nimport cv2\nimport time\n\ncap = cv2.VideoCapture(0)\n\nframe_rate = 30\nduration = 60\ninterval = 2\nframe_count = int(duration / interval)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter('docs/webcam_timelapse.mp4', fourcc, frame_rate, (640, 480))\n\nfor i in range(frame_count):\n    ret, frame = cap.read()\n    if not ret:\n        break\n    out.write(frame)\n\n    cv2.imshow('Recording', frame)\n    \n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n    \n    time.sleep(interval)\n    \ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n\n\n\n5 画像ファイルにエフェクトをかける\n\n\nCode\nimport cv2\nimport numpy as np\n\ndef apply_color_tone(img):\n    sepia_filter = np.array([[0.272, 0.534, 0.131],\n                             [0.349, 0.686, 0.168],\n                             [0.393, 0.769, 0.189]])\n\n    applied_img = cv2.transform(img, sepia_filter)\n    applied_img = np.clip(applied_img, 0, 255).astype(np.uint8)\n    return applied_img\n\n\nimg = cv2.imread('docs/webcam.jpg')\ncolor_tone_img = apply_color_tone(img)\ncv2.imshow('Color Tone', color_tone_img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Python",
      "いきなりPython",
      "タイムラプスクリエーター"
    ]
  },
  {
    "objectID": "contents/books/17_Pythonで実践する時系列予測の基礎/ch02_統計的推測と時系列分析の基礎.html",
    "href": "contents/books/17_Pythonで実践する時系列予測の基礎/ch02_統計的推測と時系列分析の基礎.html",
    "title": "ch02 統計的推測と時系列分析",
    "section": "",
    "text": "1 はじめに\n\n定常性\n自己回帰\n周期の探し方\n欠測値補間\n\n\n\n2 Code 2.1\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport japanize_matplotlib\nimport seaborn as sns\n\nnp.random.seed(42)\n\npopulation_income = np.random.lognormal(\n    mean=15.5, \n    sigma=.5, \n    size=100000\n)\n\n\n\n\n\n3 Code 2.2\n\n\nCode\npopulation_mean = np.mean(population_income)\npopulation_std = np.std(population_income)\n\nprint(f\"母集団の平均: {population_mean:.2f}, 母集団の標準偏差: {population_std:.2f}\")\n\n\n母集団の平均: 6111262.04, 母集団の標準偏差: 3254959.76\n\n\n\n\nCode\nfigx, ax = plt.subplots(figsize=(10, 6))\n\nplt.hist(\n    population_income, \n    bins=100, \n    density=True,\n    color='skyblue', \n    edgecolor='black', \n    alpha=0.7\n)\nplt.axvline(\n    population_mean, \n    color=\"red\", \n    linestyle=\"dashed\", \n    linewidth=2,\n    label=\"母集団の平均\"\n)\n\n\nplt.title(\"母集団の年収\")\nplt.xlabel(\"年収\")\nplt.ylabel(\"確率密度\")\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n4 Code 2.4\n\n\nCode\nfrom scipy import stats\n\nsample_size = 100\nsample_income = np.random.choice(population_income, size=sample_size, replace=False)\nsample_mean = np.mean(sample_income)\nsample_std = np.std(sample_income, ddof=1)  # 標本標\n\nsample_se = sample_std / np.sqrt(sample_size)\nconfidence_interval = stats.t.interval(\n    0.95,\n    df=sample_size-1,\n    loc=sample_mean,\n    scale=sample_se\n)\nprint(f\"標本の平均: {sample_mean:.2f}, 標本の標準偏差: {sample_std:.2f}\")\nprint(f\"95% 信頼区間: {confidence_interval}\")\n\n\n標本の平均: 6189692.82, 標本の標準偏差: 3847968.16\n95% 信頼区間: (np.float64(5426172.458411219), np.float64(6953213.190320864))\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/notes/bash/index.html",
    "href": "contents/notes/bash/index.html",
    "title": "bash",
    "section": "",
    "text": "1 はじめに\nbashで動くスクリプトです。\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contents/notes/development/02_mermaid.html",
    "href": "contents/notes/development/02_mermaid.html",
    "title": "mermaid",
    "section": "",
    "text": "mermadiのサンプルです",
    "crumbs": [
      "Notes",
      "development",
      "mermaid"
    ]
  },
  {
    "objectID": "contents/notes/development/02_mermaid.html#td",
    "href": "contents/notes/development/02_mermaid.html#td",
    "title": "mermaid",
    "section": "2.1 TD",
    "text": "2.1 TD",
    "crumbs": [
      "Notes",
      "development",
      "mermaid"
    ]
  },
  {
    "objectID": "contents/notes/development/02_mermaid.html#bt",
    "href": "contents/notes/development/02_mermaid.html#bt",
    "title": "mermaid",
    "section": "2.2 BT",
    "text": "2.2 BT",
    "crumbs": [
      "Notes",
      "development",
      "mermaid"
    ]
  },
  {
    "objectID": "contents/notes/development/02_mermaid.html#lr",
    "href": "contents/notes/development/02_mermaid.html#lr",
    "title": "mermaid",
    "section": "2.3 LR",
    "text": "2.3 LR",
    "crumbs": [
      "Notes",
      "development",
      "mermaid"
    ]
  },
  {
    "objectID": "contents/notes/development/02_mermaid.html#er",
    "href": "contents/notes/development/02_mermaid.html#er",
    "title": "mermaid",
    "section": "2.4 ER",
    "text": "2.4 ER",
    "crumbs": [
      "Notes",
      "development",
      "mermaid"
    ]
  },
  {
    "objectID": "contents/notes/development/02_mermaid.html#サブグラフ",
    "href": "contents/notes/development/02_mermaid.html#サブグラフ",
    "title": "mermaid",
    "section": "2.5 サブグラフ",
    "text": "2.5 サブグラフ",
    "crumbs": [
      "Notes",
      "development",
      "mermaid"
    ]
  },
  {
    "objectID": "contents/notes/development/02_mermaid.html#シーケンス図",
    "href": "contents/notes/development/02_mermaid.html#シーケンス図",
    "title": "mermaid",
    "section": "2.6 シーケンス図",
    "text": "2.6 シーケンス図",
    "crumbs": [
      "Notes",
      "development",
      "mermaid"
    ]
  },
  {
    "objectID": "contents/notes/development/02_mermaid.html#ガントチャート",
    "href": "contents/notes/development/02_mermaid.html#ガントチャート",
    "title": "mermaid",
    "section": "2.7 ガントチャート",
    "text": "2.7 ガントチャート",
    "crumbs": [
      "Notes",
      "development",
      "mermaid"
    ]
  },
  {
    "objectID": "contents/notes/development/02_mermaid.html#クラス図",
    "href": "contents/notes/development/02_mermaid.html#クラス図",
    "title": "mermaid",
    "section": "2.8 クラス図",
    "text": "2.8 クラス図",
    "crumbs": [
      "Notes",
      "development",
      "mermaid"
    ]
  },
  {
    "objectID": "contents/notes/development/02_mermaid.html#マインドマップ",
    "href": "contents/notes/development/02_mermaid.html#マインドマップ",
    "title": "mermaid",
    "section": "2.9 マインドマップ",
    "text": "2.9 マインドマップ\n\n参考\nVSCodeのプレビューで表示されない\nレンダリングすると表示される",
    "crumbs": [
      "Notes",
      "development",
      "mermaid"
    ]
  },
  {
    "objectID": "contents/notes/development/02_mermaid.html#おまけ",
    "href": "contents/notes/development/02_mermaid.html#おまけ",
    "title": "mermaid",
    "section": "2.10 おまけ",
    "text": "2.10 おまけ",
    "crumbs": [
      "Notes",
      "development",
      "mermaid"
    ]
  },
  {
    "objectID": "contents/notes/development/index.html",
    "href": "contents/notes/development/index.html",
    "title": "開発環境ノート",
    "section": "",
    "text": "1 はじめに\n開発環境に関するノートです。\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Notes",
      "development",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/notes/ggplot2/index.html",
    "href": "contents/notes/ggplot2/index.html",
    "title": "ggplot2",
    "section": "",
    "text": "1 はじめに\nggplot2に関するノートです。\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Notes",
      "ggplot2",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/test/R2/renv.html",
    "href": "contents/test/R2/renv.html",
    "title": "renv",
    "section": "",
    "text": "1 hereでrenvはどのように指定すればいいか\n\nプロジェクトファイルがあるフォルダで指定するときはそのフォルダになる\nプロジェクトファイルがないフォルダで指定するときはルートフォルダになる\n正確には１番近い親フォルダになっているみたい\nなのでこの場合のhereの挙動には注意する\n\n\n\nCode\nhere::here()\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite\"\ncur_dir &lt;- here::here(\"contents\", \"test\", \"R\")\nprint(cur_dir)\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\n# renv::init(cur_dir)\n\nprint(getwd())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R2\"\nprint(here::here())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite\"\n\n\n\n\nCode\nlibrary(here)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Test",
      "R2",
      "renv"
    ]
  },
  {
    "objectID": "contents/test/bash/quick_sample.html",
    "href": "contents/test/bash/quick_sample.html",
    "title": "Quick Sample",
    "section": "",
    "text": "engine: knitrを追加することで動作\nカレントディレクトリはこのファイルがある場所\nルートディレクトリになっていないことに注意\n\n\n\nCode\nls -la | cat\n#&gt; total 0\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 22:06 .\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 21:52 ..\n#&gt; -rwxrwxrwx 1 asozan asozan  376 Sep 18 22:06 quick_sample.qmd\n#&gt; -rwxrwxrwx 1 asozan asozan  402 Sep 18 22:06 quick_sample.rmarkdown\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 22:06 quick_sample_cache\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 21:48 日本語フォルダ\n\n\n\n\nCode\nls -la\n#&gt; total 0\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 22:06 .\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 21:52 ..\n#&gt; -rwxrwxrwx 1 asozan asozan  376 Sep 18 22:06 quick_sample.qmd\n#&gt; -rwxrwxrwx 1 asozan asozan  402 Sep 18 22:06 quick_sample.rmarkdown\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 22:06 quick_sample_cache\n#&gt; drwxrwxrwx 1 asozan asozan 4096 Sep 18 21:48 日本語フォルダ\n\n\n\n\nCode\ncat 日本語フォルダ/日本語ファイル.txt\n#&gt; \n#&gt; 日本語テキストです\n\n\npwdで確認するとWSLでマウントしたときのディレクトリが カレントディレクトリになっていることがわかる。\n\n\nCode\npwd\n#&gt; /mnt/h/Dropbox/R/Workspace/RTipsSite/contents/test/bash\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Test",
      "Bash",
      "Quick Sample"
    ]
  },
  {
    "objectID": "contents/test/R/sub_directory/renv.html",
    "href": "contents/test/R/sub_directory/renv.html",
    "title": "renv",
    "section": "",
    "text": "1 renvはどのように指定すればいいか\n\nプロジェクトファイルがあるフォルダで指定するときはそのフォルダになる\nプロジェクトファイルがないフォルダで指定するときはルートフォルダになる\n正確には１番近い親フォルダになっているみたい\nなのでこの場合のhereの挙動には注意する\n\n\n\nCode\nhere::here()\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\ncur_dir &lt;- here::here(\"contents\", \"test\", \"R\")\nprint(cur_dir)\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R/contents/test/R\"\n# renv::init(cur_dir)\n\nprint(getwd())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R/sub_directory\"\nprint(here::here())\n#&gt; [1] \"H:/Dropbox/R/Workspace/RTipsSite/contents/test/R\"\n\n\n\n\nCode\nlibrary(here)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Test",
      "R",
      "Sub Directory",
      "renv"
    ]
  },
  {
    "objectID": "contents/sql/ap-test/index.html",
    "href": "contents/sql/ap-test/index.html",
    "title": "応用情報処理試験",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/sql/ap-test\")\nCode\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")"
  },
  {
    "objectID": "contents/sql/ap-test/index.html#セットアップ",
    "href": "contents/sql/ap-test/index.html#セットアップ",
    "title": "応用情報処理試験",
    "section": "2.1 セットアップ",
    "text": "2.1 セットアップ\n\n\nCode\nCREATE OR REPLACE TABLE 社員\n(\n    支社番号 INTEGER NOT NULL,\n    番号 INTEGER PRIMARY KEY,\n    名前 VARCHAR(100) NULL,\n    性別 VARCHAR(10) NULL,\n    住所 TEXT NULL,\n    部署 INTEGER NULL \n);\n\n\nCREATE OR REPLACE SEQUENCE seq_number START 1;\nINSERT INTO 社員 (支社番号, 番号, 名前, 性別, 住所, 部署) VALUES (1, nextval('seq_number'), '石田', '男', '◯◯◯◯', 1);\nINSERT INTO 社員 (支社番号, 番号, 名前, 性別, 住所, 部署) VALUES (1, nextval('seq_number'), '田中', '女', '△△△△', 1);\nINSERT INTO 社員 (支社番号, 番号, 名前, 性別, 住所, 部署) VALUES (1, nextval('seq_number'), '山田', '男', 'あいうえ', 2);\nINSERT INTO 社員 (支社番号, 番号, 名前, 性別, 住所, 部署) VALUES (2, nextval('seq_number'), '高橋', '女', '◯◯◯◯', 2);\nINSERT INTO 社員 (支社番号, 番号, 名前, 性別, 住所, 部署) VALUES (2, nextval('seq_number'), '鈴木', '男', '■■■■', 2);\nINSERT INTO 社員 (支社番号, 番号, 名前, 性別, 住所, 部署) VALUES (2, nextval('seq_number'), '木村', '女', '◯◯◯◯', 8); -- 部署8は存在しないので注意\nINSERT INTO 社員 (支社番号, 番号, 名前, 性別, 住所, 部署) VALUES (2, nextval('seq_number'), '佐藤', '男', '◇◇◇◇', 3);\nINSERT INTO 社員 (支社番号, 番号, 名前, 性別, 住所, 部署) VALUES (3, nextval('seq_number'), '大田', '男', '◯◯◯◯', 3);\nINSERT INTO 社員 (支社番号, 番号, 名前, 性別, 住所, 部署) VALUES (3, nextval('seq_number'), '今村', '男', '☆☆☆☆', 3);\n\n\nCREATE OR REPLACE TABLE 部署\n(\n    番号 INTEGER PRIMARY KEY NOT NULL,\n    名前 VARCHAR(100) NULL,\n    部長 INTEGER REFERENCES 社員 (番号) -- 外部キー\n);\n\nINSERT INTO 部署 (番号, 名前, 部長) VALUES (1, '社長室', 1);\nINSERT INTO 部署 (番号, 名前, 部長) VALUES (2, '営業部', 5);\nINSERT INTO 部署 (番号, 名前, 部長) VALUES (3, '開発部', 7);\nINSERT INTO 部署 (番号, 名前, 部長) VALUES (7, '経理部', 9);\nINSERT INTO 部署 (番号, 名前, 部長) VALUES (8, '秘書室', 9);\n\n\n\n\nCode\nSELECT * FROM 部署 WHERE 番号 BETWEEN 1 AND 5;\n\n\n\n3 records\n\n\n番号\n名前\n部長\n\n\n\n\n1\n社長室\n1\n\n\n2\n営業部\n5\n\n\n3\n開発部\n7\n\n\n\n\n\n\n\nCode\nSELECT * FROM 部署 WHERE 名前 LIKE '%室'\n\n\n\n2 records\n\n\n番号\n名前\n部長\n\n\n\n\n1\n社長室\n1\n\n\n8\n秘書室\n9\n\n\n\n\n\n\n\nCode\nSELECT * FROM 社員 s  \nINNER JOIN 部署 t\nON s.部署 = t.番号\n\n\n\n9 records\n\n\n支社番号\n番号\n名前\n性別\n住所\n部署\n番号\n名前\n部長\n\n\n\n\n1\n1\n石田\n男\n◯◯◯◯\n1\n1\n社長室\n1\n\n\n1\n2\n田中\n女\n△△△△\n1\n1\n社長室\n1\n\n\n1\n3\n山田\n男\nあいうえ\n2\n2\n営業部\n5\n\n\n2\n4\n高橋\n女\n◯◯◯◯\n2\n2\n営業部\n5\n\n\n2\n5\n鈴木\n男\n■■■■\n2\n2\n営業部\n5\n\n\n2\n6\n木村\n女\n◯◯◯◯\n8\n8\n秘書室\n9\n\n\n2\n7\n佐藤\n男\n◇◇◇◇\n3\n3\n開発部\n7\n\n\n3\n8\n大田\n男\n◯◯◯◯\n3\n3\n開発部\n7\n\n\n3\n9\n今村\n男\n☆☆☆☆\n3\n3\n開発部\n7\n\n\n\n\n\n\n\nCode\nSELECT * FROM 社員 s  \nLEFT OUTER JOIN 部署 t\nON s.部署 = t.番号\n\n\n\n9 records\n\n\n支社番号\n番号\n名前\n性別\n住所\n部署\n番号\n名前\n部長\n\n\n\n\n1\n1\n石田\n男\n◯◯◯◯\n1\n1\n社長室\n1\n\n\n1\n2\n田中\n女\n△△△△\n1\n1\n社長室\n1\n\n\n1\n3\n山田\n男\nあいうえ\n2\n2\n営業部\n5\n\n\n2\n4\n高橋\n女\n◯◯◯◯\n2\n2\n営業部\n5\n\n\n2\n5\n鈴木\n男\n■■■■\n2\n2\n営業部\n5\n\n\n2\n6\n木村\n女\n◯◯◯◯\n8\n8\n秘書室\n9\n\n\n2\n7\n佐藤\n男\n◇◇◇◇\n3\n3\n開発部\n7\n\n\n3\n8\n大田\n男\n◯◯◯◯\n3\n3\n開発部\n7\n\n\n3\n9\n今村\n男\n☆☆☆☆\n3\n3\n開発部\n7\n\n\n\n\n\n\n\nCode\nSELECT * FROM 社員 s  \nRIGHT OUTER JOIN 部署 t\nON s.部署 = t.番号\n\n\n\nDisplaying records 1 - 10\n\n\n支社番号\n番号\n名前\n性別\n住所\n部署\n番号\n名前\n部長\n\n\n\n\n1\n2\n田中\n女\n△△△△\n1\n1\n社長室\n1\n\n\n2\n5\n鈴木\n男\n■■■■\n2\n2\n営業部\n5\n\n\n3\n9\n今村\n男\n☆☆☆☆\n3\n3\n開発部\n7\n\n\n2\n6\n木村\n女\n◯◯◯◯\n8\n8\n秘書室\n9\n\n\n1\n1\n石田\n男\n◯◯◯◯\n1\n1\n社長室\n1\n\n\n2\n4\n高橋\n女\n◯◯◯◯\n2\n2\n営業部\n5\n\n\n3\n8\n大田\n男\n◯◯◯◯\n3\n3\n開発部\n7\n\n\n1\n3\n山田\n男\nあいうえ\n2\n2\n営業部\n5\n\n\n2\n7\n佐藤\n男\n◇◇◇◇\n3\n3\n開発部\n7\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n7\n経理部\n9\n\n\n\n\n\n\n\nCode\nSELECT * FROM 社員 s  \nJOIN 部署 t\nON s.部署 = t.番号\n\n\n\n9 records\n\n\n支社番号\n番号\n名前\n性別\n住所\n部署\n番号\n名前\n部長\n\n\n\n\n1\n1\n石田\n男\n◯◯◯◯\n1\n1\n社長室\n1\n\n\n1\n2\n田中\n女\n△△△△\n1\n1\n社長室\n1\n\n\n1\n3\n山田\n男\nあいうえ\n2\n2\n営業部\n5\n\n\n2\n4\n高橋\n女\n◯◯◯◯\n2\n2\n営業部\n5\n\n\n2\n5\n鈴木\n男\n■■■■\n2\n2\n営業部\n5\n\n\n2\n6\n木村\n女\n◯◯◯◯\n8\n8\n秘書室\n9\n\n\n2\n7\n佐藤\n男\n◇◇◇◇\n3\n3\n開発部\n7\n\n\n3\n8\n大田\n男\n◯◯◯◯\n3\n3\n開発部\n7\n\n\n3\n9\n今村\n男\n☆☆☆☆\n3\n3\n開発部\n7\n\n\n\n\n\n\n\nCode\nSELECT * FROM 社員 s, 部署 t\n\n\n\nDisplaying records 1 - 10\n\n\n支社番号\n番号\n名前\n性別\n住所\n部署\n番号\n名前\n部長\n\n\n\n\n1\n1\n石田\n男\n◯◯◯◯\n1\n1\n社長室\n1\n\n\n1\n2\n田中\n女\n△△△△\n1\n1\n社長室\n1\n\n\n1\n3\n山田\n男\nあいうえ\n2\n1\n社長室\n1\n\n\n2\n4\n高橋\n女\n◯◯◯◯\n2\n1\n社長室\n1\n\n\n2\n5\n鈴木\n男\n■■■■\n2\n1\n社長室\n1\n\n\n2\n6\n木村\n女\n◯◯◯◯\n8\n1\n社長室\n1\n\n\n2\n7\n佐藤\n男\n◇◇◇◇\n3\n1\n社長室\n1\n\n\n3\n8\n大田\n男\n◯◯◯◯\n3\n1\n社長室\n1\n\n\n3\n9\n今村\n男\n☆☆☆☆\n3\n1\n社長室\n1\n\n\n1\n1\n石田\n男\n◯◯◯◯\n1\n2\n営業部\n5\n\n\n\n\n\n\n\nCode\nSELECT * FROM 部署 WHERE 名前 IN (SELECT 名前 FROM 部署 WHERE 名前 = '社長室')\n\n\n\n1 records\n\n\n番号\n名前\n部長\n\n\n\n\n1\n社長室\n1\n\n\n\n\n\n\n\nCode\nSELECT * FROM 部署 WHERE 名前 IN (SELECT 名前 FROM 部署 WHERE 名前 LIKE '%室')\n\n\n\n2 records\n\n\n番号\n名前\n部長\n\n\n\n\n1\n社長室\n1\n\n\n8\n秘書室\n9\n\n\n\n\n\nEXISTSを使うと条件となるテーブルが存在するかしないかを条件として、SELECTを制御することが可能である。\n\n\nCode\nSELECT * FROM 部署 WHERE EXISTS (SELECT 名前 FROM 部署 WHERE 名前 LIKE '%室')\n\n\n\n5 records\n\n\n番号\n名前\n部長\n\n\n\n\n1\n社長室\n1\n\n\n2\n営業部\n5\n\n\n3\n開発部\n7\n\n\n7\n経理部\n9\n\n\n8\n秘書室\n9\n\n\n\n\n\nこれは実はフィルターになっている。 つまり、EXISTSでおこなったフィルタが外のクエリのフィルターになっている。\n\n\nCode\nSELECT * FROM 部署 WHERE EXISTS (SELECT 1 FROM 部署 WHERE 名前 LIKE '%室')\n\n\n\n5 records\n\n\n番号\n名前\n部長\n\n\n\n\n1\n社長室\n1\n\n\n2\n営業部\n5\n\n\n3\n開発部\n7\n\n\n7\n経理部\n9\n\n\n8\n秘書室\n9\n\n\n\n\n\n\n\nCode\nSELECT * FROM 部署 USING SAMPLE 5;\n\n\n\n5 records\n\n\n番号\n名前\n部長\n\n\n\n\n1\n社長室\n1\n\n\n2\n営業部\n5\n\n\n3\n開発部\n7\n\n\n7\n経理部\n9\n\n\n8\n秘書室\n9\n\n\n\n\n\n\n\nCode\nSELECT * FROM range(2) t1(x)\nUNION\nSELECT * FROM range(3) t2(x);\n\n\n\n3 records\n\n\nx\n\n\n\n\n1\n\n\n0\n\n\n2\n\n\n\n\n\n\n\nCode\nCREATE OR REPLACE TABLE 社員2 (\n    社員番号 INTEGER PRIMARY KEY,\n    年齢 INTEGER,\n    性別 VARCHAR(10),\n    CONSTRAINT 年齢チェック CHECK (年齢 &gt;= 18), \n    CONSTRAINT 年齢チェック CHECK (性別 IN ('男', '女')), \n);"
  },
  {
    "objectID": "contents/sql/ap-test/index.html#部署",
    "href": "contents/sql/ap-test/index.html#部署",
    "title": "応用情報処理試験",
    "section": "2.2 部署",
    "text": "2.2 部署"
  },
  {
    "objectID": "contents/sql/duckdb/SpatialDataAnalysiswithDuckDB/working.html",
    "href": "contents/sql/duckdb/SpatialDataAnalysiswithDuckDB/working.html",
    "title": "Spatial Data Analytics With Duckdb",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\n\n\n\n\nCode\n\ncur_dir &lt;- here::here(\"contents/sql/duckdb/SpatialDataAnalysiswithDuckDB\")\n\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nlibrary(here)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")\n\n\n\n1 Setup\n\n\nCode\ncon &lt;- dbConnect(duckdb(here(cur_dir, \"data.db\")))\n\n\n\n\nCode\nINSTALL icu;\nINSTALL spatial;\nINSTALL httpfs;\nINSTALL json;\n\n\n\n\nCode\nLOAD icu;\nLOAD spatial;\nLOAD httpfs;\nLOAD json;\nSET s3_region='us-west-2';\n\n\n\n\nCode\nSHOW ALL TABLES;\n\n\n\n4 records\n\n\n\n\n\n\n\n\n\n\ndatabase\nschema\nname\ncolumn_names\ncolumn_types\ntemporary\n\n\n\n\ndata\nmain\npark_ist\nlocality , region , postcode , freeform , categories_main , names , CAST(confidence AS “json”), CAST(bbox AS “json”) , geom\nVARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , JSON , JSON , GEOMETRY\nFALSE\n\n\ndata\nmain\nplaces\nid , updatetime, version , names , categories, confidence, websites , socials , emails , phones , brand , addresses , sources , bbox , geometry , theme , type , country\nVARCHAR , VARCHAR , INTEGER , MAP(VARCHAR, MAP(VARCHAR, VARCHAR)[]) , STRUCT(main VARCHAR, alternate VARCHAR[]) , DOUBLE , VARCHAR[] , VARCHAR[] , VARCHAR[] , VARCHAR[] , STRUCT(“names” MAP(VARCHAR, MAP(VARCHAR, VARCHAR)[]), wikidata VARCHAR), MAP(VARCHAR, VARCHAR)[] , MAP(VARCHAR, VARCHAR)[] , STRUCT(minx DOUBLE, maxx DOUBLE, miny DOUBLE, maxy DOUBLE) , BLOB , VARCHAR , VARCHAR , VARCHAR\nFALSE\n\n\ndata\nmain\npoi_ist\nlocality , region , postcode , freeform , categories_main , names , CAST(confidence AS “json”), CAST(bbox AS “json”) , geom\nVARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , JSON , JSON , GEOMETRY\nFALSE\n\n\ndata\nmain\nturkey_places\nlocality , region , postcode , freeform , categories_main , names , CAST(confidence AS “json”), CAST(bbox AS “json”) , geom\nVARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , VARCHAR , JSON , JSON , GEOMETRY\nFALSE\n\n\n\n\n\n\n\n2 はじめに\nここの追試です。 duckdbを使った空間分析です。\n\n\n3 note\n約６０００万件のデータをサンプルデータとして扱う。・・・と思ったけどどうやらメモリ不足になるのでここでは少しだけ抽出する。\n\n\nCode\ncreate or replace table places as \nselect * from read_parquet('s3://overturemaps-us-west-2/release/2023-07-26-alpha.0/theme=places/type=*/*') limit 500000;\n\n\n\n\nCode\ndescribe places;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\n\nid\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nupdatetime\nVARCHAR\nYES\nNA\nNA\nNA\n\n\nversion\nINTEGER\nYES\nNA\nNA\nNA\n\n\nnames\nMAP(VARCHAR, MAP(VARCHAR, VARCHAR)[])\nYES\nNA\nNA\nNA\n\n\ncategories\nSTRUCT(main VARCHAR, alternate VARCHAR[])\nYES\nNA\nNA\nNA\n\n\nconfidence\nDOUBLE\nYES\nNA\nNA\nNA\n\n\nwebsites\nVARCHAR[]\nYES\nNA\nNA\nNA\n\n\nsocials\nVARCHAR[]\nYES\nNA\nNA\nNA\n\n\nemails\nVARCHAR[]\nYES\nNA\nNA\nNA\n\n\nphones\nVARCHAR[]\nYES\nNA\nNA\nNA\n\n\n\n\n\n\n\nCode\nselect count(*) as cnt from places;\n\n\n\n1 records\n\n\ncnt\n\n\n\n\n5e+05\n\n\n\n\n\nMAP型を表示することができないので変換する。\n\n\nCode\nselect\n  confidence\n  , cast(names as json) naems\n  , categories\n  , cast(brand as json) brand\n  , cast(addresses as json) addresses\nfrom \n  places\nlimit \n  10;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\nconfidence\nnaems\ncategories\nbrand\naddresses\n\n\n\n\n0.5989174\n{“common”:[{“value”:“Bronwydd Veterinary Surgery”,“language”:“local”}]}\nveterinarian\n{“names”:null,“wikidata”:null}\n[{“postcode”:“LL57 2NX”,“freeform”:“Ffordd Bronwydd”,“country”:“GB”}]\n\n\n0.9108787\n{“common”:[{“value”:“Truchi Kennedy”,“language”:“local”}]}\npark\n{“names”:null,“wikidata”:null}\n[{“country”:“AR”}]\n\n\n0.9628990\n{“common”:[{“value”:“Studio Bella”,“language”:“local”}]}\nbeauty_salon\n{“names”:null,“wikidata”:null}\n[{“locality”:“São Paulo”,“postcode”:“02114-001”,“freeform”:“Rua José Wasth Rodrigues, 134”,“region”:“SP”,“country”:“BR”}]\n\n\n0.4756328\n{“common”:[{“value”:“เต้าหู้นมสด \"แม่หนูชวนชิม\" ทับสะแก”,“language”:“local”}]}\nthai_restaurant\n{“names”:null,“wikidata”:null}\n[{“locality”:“Thap Sakae”,“postcode”:“77130”,“freeform”:“6/3 ม.5 ต.เขาล้าน อ.ทับสะแก จ.ประจวบคีรีขันธ์”,“country”:“TH”}]\n\n\n0.5783658\n{“common”:[{“value”:“ร้านซ่อมรถโกริน”,“language”:“local”}]}\ncommunity_services_non_profits\n{“names”:null,“wikidata”:null}\n[{“locality”:“Krabi”,“freeform”:“ม.2อ่าวลึกเหนือ อ.อ่าวลึก จ.กระบี่”,“country”:“TH”}]\n\n\n0.5397370\n{“common”:[{“value”:“Sunrise Textile Park 2”,“language”:“local”}]}\npark\n{“names”:null,“wikidata”:null}\n[{“locality”:“Olpad”,“postcode”:“394130”,“freeform”:“Kareli”,“country”:“IN”}]\n\n\n0.7074041\n{“common”:[{“value”:“วัดพระธาตุดอยสุเทพราชวรมหาวิหาร จ,เชียงใหม่”,“language”:“local”}]}\ntravel\n{“names”:null,“wikidata”:null}\n[{“locality”:“Chiang Mai”,“country”:“TH”}]\n\n\n0.9865023\n{“common”:[{“value”:“Cornerstone”,“language”:“local”}]}\nchurch_cathedral\n{“names”:null,“wikidata”:null}\n[{“locality”:“Windham”,“postcode”:“04062-4405”,“freeform”:“48 Cottage Rd”,“region”:“ME”,“country”:“US”}]\n\n\n0.2839491\n{“common”:[{“value”:“Reims Rugby Team”,“language”:“local”}]}\nsports_club_and_league\n{“names”:null,“wikidata”:null}\n[{“locality”:“Reims”,“postcode”:“51100”,“freeform”:“59 Rue Pierre Taittinger”,“region”:“null”,“country”:“FR”}]\n\n\n0.4486956\n{“common”:[{“value”:“Better Garden G R A Onitsha”,“language”:“local”}]}\nbeer_garden\n{“names”:null,“wikidata”:null}\n[{“locality”:“Onitsha”,“country”:“NG”}]\n\n\n\n\n\n\n\nCode\nselect\n  replace(json_extract(cast(addresses as json), '$[0].country')::varchar, '\"', '') as country\nfrom \n  places\nlimit 5;\n\n\n\n5 records\n\n\ncountry\n\n\n\n\nGB\n\n\nAR\n\n\nBR\n\n\nTH\n\n\nTH\n\n\n\n\n\nカラムを作成してデータをセットする。\n\n\nCode\nalter table places add column country varchar;\nupdate places set country = replace(json_extract(cast(places.addresses as json), '$[0].country')::varchar, '\"', '')\n\n\nトルコの情報を抽出する。\n\n\nCode\n create or replace table turkey_places as (\n              select\n                     replace(json_extract(places.addresses::json,'$[0].locality'),'\"','')::varchar as locality,\n                     replace(json_extract(places.addresses::json,'$[0].region'),'\"','')::varchar as region,\n                     replace(json_extract(places.addresses::json,'$[0].postcode'),'\"','')::varchar as postcode,\n                     replace(json_extract(places.addresses::json,'$[0].freeform'),'\"','')::varchar as freeform,\n\n                     categories.main as categories_main,\n\n                     replace(json_extract(places.names::json,'$.common[0].value'),'\"','')::varchar as names,\n                     cast(confidence as json),\n                     cast(bbox as json),\n                     st_transform(st_point(st_y(st_geomfromwkb(geometry)),st_x(st_geomfromwkb(geometry))),'EPSG:4326','EPSG:3857') as geom\n\n              from places \n                     where country ='TR' \n       )\n\n\n\n\nCode\nselect * from turkey_places limit 5\n\n\n\n5 records\n\n\n\n\n\n\n\n\n\n\n\n\n\nlocality\nregion\npostcode\nfreeform\ncategories_main\nnames\nCAST(confidence AS “json”)\nCAST(bbox AS “json”)\ngeom\n\n\n\n\nNA\nNA\nNA\nNA\npublic_plaza\nElite Mamak Society\n0.6304769515991211\n{“minx”:32.919501,“maxx”:32.919501,“miny”:39.916317,“maxy”:39.916317}\n00, 00, 18, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, ae, 98, 53, 0b, 63, f5, 4b, 41, 47, 08, a8, 48, 07, 84, 52, 41\n\n\nKonya\n42\n42120\nİlkay Sokak 8\nprofessional_services\nAGRO TÖKE\n0.655314564704895\n{“minx”:32.522156,“maxx”:32.522156,“miny”:37.93727,“maxy”:37.93727}\n00, 00, 18, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, 00, b4, 36, ec, fe, 9e, 4b, 41, b5, 55, 0c, fd, 71, 6f, 51, 41\n\n\nKayseri\nNA\n38010\nUfuk Sokak 1\nlocal_and_state_government_offices\nSosyal Güvenlik Kurumu İl Müdürlüğü\n0.6062818169593811\n{“minx”:35.48468,“maxx”:35.48468,“miny”:38.72812,“maxy”:38.72812}\n00, 00, 18, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, 6f, 91, 18, 41, 1c, 23, 4e, 41, 4e, 9e, 49, 65, 0c, dd, 51, 41\n\n\nAnkara\n06\n06300\n492. Cadde 31/B\ncar_dealer\nEYMEN OTO Kiralama\n0.5934379696846008\n{“minx”:32.8882484,“maxx”:32.8882484,“miny”:39.9881714,“maxy”:39.9881714}\n00, 00, 18, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, af, f5, 50, 88, 97, ee, 4b, 41, 98, f9, d1, e1, 37, 8e, 52, 41\n\n\nAdana\n01\n01120\n62017. Sokak 13\ncontractor\nGökpınar İnşaat\n0.6233547925949097\n{“minx”:35.3315582,“maxx”:35.3315582,“miny”:36.9957809,“maxy”:36.9957809}\n00, 00, 18, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, 27, 42, ac, 88, d1, 01, 4e, 41, a8, b3, c9, ad, 7d, ee, 50, 41\n\n\n\n\n\nイスタンブールのPOIデータを作成する。\n\n\nCode\n    create or replace table park_ist as (\n        select * from turkey_places where locality = 'İstanbul' and categories_main='park'   \n    );\n\n    create or replace table poi_ist as (\n        select * from turkey_places where locality = 'İstanbul' and categories_main &lt;&gt; 'park'\n    )\n\n\n\n\nCode\nselect count(*) from poi_ist;\n\n\n\n1 records\n\n\ncount_star()\n\n\n\n\n1557\n\n\n\n\n\n\n\nCode\nselect count(*) from park_ist;\n\n\n\n1 records\n\n\ncount_star()\n\n\n\n\n2\n\n\n\n\n\nPOIのうち半径500メートル以内に公園があるものを抽出する。普通に直積を作ってフィルターしている。\n\n\nCode\nselect  \n  poi_ist.region as poi_ist_region,\n  poi_ist.freeform as poi_ist_freeform,\n  poi_ist.categories_main as poi_ist_categori,\n  park_ist.categories_main as park_categori, \n  park_ist.names as park_names,\n  park_ist.freeform as park_ist_freeform,\n  st_distance(poi_ist.geom,park_ist.geom) as dist,\n  ST_AsText(poi_ist.geom) as geom,\n  ST_AsText(park_ist.geom) as geom2\n\nfrom poi_ist, park_ist \n\nwhere ST_DWithin(poi_ist.geom, park_ist.geom, 500) \n\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\npoi_ist_region\npoi_ist_freeform\npoi_ist_categori\npark_categori\npark_names\npark_ist_freeform\ndist\ngeom\ngeom2\n\n\n\n\n34\nÇakmak Sokak 2\nsocial_service_organizations\npark\nHayat Park Inci Kule\nVeysel Karani Caddesi 16\n393.8883\nPOINT (3211578.185300193 5021454.134304489)\nPOINT (3211587.1019914057 5021060.346982889)\n\n\n\n\n\n\n\n4 Disconnect\n\n\nCode\ndbDisconnect(con, shutdown=TRUE)\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "DuckDB",
      "Spatial DataAnalysis with DuckDB",
      "Spatial Data Analytics With Duckdb"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_02_configuration.html",
    "href": "contents/sql/duckdb/documentaion/guide_02_configuration.html",
    "title": "Configuration",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\ncur_dir &lt;- here::here(\"contents/sql/duckdb/documentation\")\nCode\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Configuration"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_02_configuration.html#list-of-supported-pragma-statements",
    "href": "contents/sql/duckdb/documentaion/guide_02_configuration.html#list-of-supported-pragma-statements",
    "title": "Configuration",
    "section": "3.1 List of Supported PRAGMA Statements",
    "text": "3.1 List of Supported PRAGMA Statements\n\n\nCode\nPRAGMA database_list;\n\n\n\n1 records\n\n\nseq\nname\nfile\n\n\n\n\n1078\nmemory\nNA\n\n\n\n\n\n\n\nCode\nPRAGMA show_tables;\n\n\n\n0 records\n\n\nname\n\n\n\n\n\n\n\nいわゆるDESCRIBEと同じ効果を出す。\n\n\nCode\nPRAGMA show_tables_expanded;\n\n\n\n0 records\n\n\ndatabase\nschema\nname\ncolumn_names\ncolumn_types\ntemporary\n\n\n\n\n\n\n\n\n\nCode\nPRAGMA functions;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\n\nname\ntype\nparameters\nvarargs\nreturn_type\nside_effects\n\n\n\n\n!__postfix\nSCALAR\nINTEGER\nNA\nHUGEINT\nFALSE\n\n\n!~~\nSCALAR\nVARCHAR, VARCHAR\nNA\nBOOLEAN\nFALSE\n\n\n!~~*\nSCALAR\nVARCHAR, VARCHAR\nNA\nBOOLEAN\nFALSE\n\n\n%\nSCALAR\nDOUBLE, DOUBLE\nNA\nDOUBLE\nFALSE\n\n\n%\nSCALAR\nFLOAT, FLOAT\nNA\nFLOAT\nFALSE\n\n\n%\nSCALAR\nHUGEINT, HUGEINT\nNA\nHUGEINT\nFALSE\n\n\n%\nSCALAR\nBIGINT, BIGINT\nNA\nBIGINT\nFALSE\n\n\n%\nSCALAR\nSMALLINT, SMALLINT\nNA\nSMALLINT\nFALSE\n\n\n%\nSCALAR\nTINYINT, TINYINT\nNA\nTINYINT\nFALSE\n\n\n%\nSCALAR\nINTEGER, INTEGER\nNA\nINTEGER\nFALSE",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Configuration"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_02_configuration.html#table-info",
    "href": "contents/sql/duckdb/documentaion/guide_02_configuration.html#table-info",
    "title": "Configuration",
    "section": "3.2 Table info",
    "text": "3.2 Table info\n\n\nCode\nCREATE OR REPLACE TABLE tbl AS SELECT * FROM read_parquet('./penguins.parquet');\n\n\n\n\nCode\nPRAGMA table_info('tbl');\n\n\n\n8 records\n\n\ncid\nname\ntype\nnotnull\ndflt_value\npk\n\n\n\n\n0\nspecies\nVARCHAR\nFALSE\nNA\nFALSE\n\n\n1\nisland\nVARCHAR\nFALSE\nNA\nFALSE\n\n\n2\nbill_length_mm\nDOUBLE\nFALSE\nNA\nFALSE\n\n\n3\nbill_depth_mm\nDOUBLE\nFALSE\nNA\nFALSE\n\n\n4\nflipper_length_mm\nINTEGER\nFALSE\nNA\nFALSE\n\n\n5\nbody_mass_g\nINTEGER\nFALSE\nNA\nFALSE\n\n\n6\nsex\nVARCHAR\nFALSE\nNA\nFALSE\n\n\n7\nyear\nINTEGER\nFALSE\nNA\nFALSE\n\n\n\n\n\n\n\nCode\nCALL pragma_table_info('tbl');\n\n\n\n8 records\n\n\ncid\nname\ntype\nnotnull\ndflt_value\npk\n\n\n\n\n0\nspecies\nVARCHAR\nFALSE\nNA\nFALSE\n\n\n1\nisland\nVARCHAR\nFALSE\nNA\nFALSE\n\n\n2\nbill_length_mm\nDOUBLE\nFALSE\nNA\nFALSE\n\n\n3\nbill_depth_mm\nDOUBLE\nFALSE\nNA\nFALSE\n\n\n4\nflipper_length_mm\nINTEGER\nFALSE\nNA\nFALSE\n\n\n5\nbody_mass_g\nINTEGER\nFALSE\nNA\nFALSE\n\n\n6\nsex\nVARCHAR\nFALSE\nNA\nFALSE\n\n\n7\nyear\nINTEGER\nFALSE\nNA\nFALSE",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Configuration"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_02_configuration.html#memoory-limit",
    "href": "contents/sql/duckdb/documentaion/guide_02_configuration.html#memoory-limit",
    "title": "Configuration",
    "section": "3.3 Memoory Limit",
    "text": "3.3 Memoory Limit\n#| connection: con\nSET memory_limit = '1GB';\nSET max_memory = '1GB';",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Configuration"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_02_configuration.html#threads",
    "href": "contents/sql/duckdb/documentaion/guide_02_configuration.html#threads",
    "title": "Configuration",
    "section": "3.4 Threads",
    "text": "3.4 Threads\n\n\nCode\nSET threads = 4;",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Configuration"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_02_configuration.html#database-size",
    "href": "contents/sql/duckdb/documentaion/guide_02_configuration.html#database-size",
    "title": "Configuration",
    "section": "3.5 Database Size",
    "text": "3.5 Database Size\n\n\nCode\nCALL pragma_database_size();\n\n\n\n1 records\n\n\n\n\n\n\n\n\n\n\n\n\n\ndatabase_name\ndatabase_size\nblock_size\ntotal_blocks\nused_blocks\nfree_blocks\nwal_size\nmemory_usage\nmemory_limit\n\n\n\n\nmemory\n0 bytes\n0\n0\n0\n0\n0 bytes\n168.0 KiB\n9.3 GiB",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Configuration"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_02_configuration.html#progress-bar",
    "href": "contents/sql/duckdb/documentaion/guide_02_configuration.html#progress-bar",
    "title": "Configuration",
    "section": "3.6 Progress Bar",
    "text": "3.6 Progress Bar\n\n\nCode\nPRAGMA enable_progress_bar;\n\n\n\n0 records\n\n\nSuccess",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Configuration"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_02_configuration.html#returning-erros-as-json",
    "href": "contents/sql/duckdb/documentaion/guide_02_configuration.html#returning-erros-as-json",
    "title": "Configuration",
    "section": "3.7 Returning Erros as JSON",
    "text": "3.7 Returning Erros as JSON\n\n\nCode\nSET errors_as_json = true;",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Configuration"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/guide_02_configuration.html#secrets",
    "href": "contents/sql/duckdb/documentaion/guide_02_configuration.html#secrets",
    "title": "Configuration",
    "section": "3.8 Secrets",
    "text": "3.8 Secrets\nこんな感じで作成するみたい。\nCREATE SECRET secret1 (\n    TYPE S3,\n    KEY_ID 'my_secret_key1',\n    SECRET 'my_secret_value1',\n    SCOPE 's3://my-bucket'\n);\n削除するときには次のようにする。\nDROP PERSISTENT SECRET my_persistent_secret;",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "Configuration"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/documentaion/index.html",
    "href": "contents/sql/duckdb/documentaion/index.html",
    "title": "DuckDB Documentation",
    "section": "",
    "text": "1 はじめに\n\nDuckDBの勉強ノートです.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "DuckDB",
      "Documentaion",
      "はじめに"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/mytips/02_.html",
    "href": "contents/sql/duckdb/mytips/02_.html",
    "title": "クイックメモ",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\nCode\ncur_dir &lt;- here::here(\"contents/sql/duckdb/mytips\")\n\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nlibrary(here)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "DuckDB",
      "My Tips",
      "クイックメモ"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/mytips/02_.html#再帰共通表式グラフ",
    "href": "contents/sql/duckdb/mytips/02_.html#再帰共通表式グラフ",
    "title": "クイックメモ",
    "section": "8.1 再帰共通表式：グラフ",
    "text": "8.1 再帰共通表式：グラフ\n\n\nCode\nCREATE TABLE edge (node1id INTEGER, node2id INTEGER);\nINSERT INTO edge\n    VALUES\n        (1, 3), (1, 5), (2, 4), (2, 5), (2, 10), (3, 1), (3, 5), (3, 8), (3, 10),\n        (5, 3), (5, 4), (5, 8), (6, 3), (6, 4), (7, 4), (8, 1), (9, 4);\n\n\n上記の隣接グラフからあるノードから始まる全てのパスを列挙する。\n\n\nCode\nWITH RECURSIVE paths(startNode, endNode, path) AS (\n    SELECT -- define the path as the first edge of the traversal\n        node1id AS startNode,\n        node2id AS endNode,\n        [node1id, node2id] AS path\n    FROM edge\n    WHERE startNode = 1\n    UNION ALL\n    SELECT -- concatenate new edge to the path\n        paths.startNode AS startNode,\n        node2id AS endNode,\n        array_append(path, node2id) AS path\n    FROM paths\n    JOIN edge ON paths.endNode = node1id\n    -- Prevent adding a repeated node to the path.\n    -- This ensures that no cycles occur.\n    WHERE node2id != ALL(paths.path)\n)\nSELECT startNode, endNode, path\nFROM paths\nORDER BY length(path), path;\n\n\n\nDisplaying records 1 - 10\n\n\nstartNode\nendNode\npath\n\n\n\n\n1\n3\n1, 3\n\n\n1\n5\n1, 5\n\n\n1\n5\n1, 3, 5\n\n\n1\n8\n1, 3, 8\n\n\n1\n10\n1, 3, 10\n\n\n1\n3\n1, 5, 3\n\n\n1\n4\n1, 5, 4\n\n\n1\n8\n1, 5, 8\n\n\n1\n4\n1, 3, 5, 4\n\n\n1\n8\n1, 3, 5, 8",
    "crumbs": [
      "DuckDB",
      "My Tips",
      "クイックメモ"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/mytips/02_.html#ノードから重み付けされていない最短パスの列挙",
    "href": "contents/sql/duckdb/mytips/02_.html#ノードから重み付けされていない最短パスの列挙",
    "title": "クイックメモ",
    "section": "8.2 ノードから重み付けされていない最短パスの列挙",
    "text": "8.2 ノードから重み付けされていない最短パスの列挙\n\n\nCode\nWITH RECURSIVE paths(startNode, endNode, path) AS (\n   SELECT -- define the path as the first edge of the traversal\n        node1id AS startNode,\n        node2id AS endNode,\n        [node1id, node2id] AS path\n     FROM edge\n     WHERE startNode = 1\n   UNION ALL\n   SELECT -- concatenate new edge to the path\n        paths.startNode AS startNode,\n        node2id AS endNode,\n        array_append(path, node2id) AS path\n     FROM paths\n     JOIN edge ON paths.endNode = node1id\n    -- Prevent adding a node that was visited previously by any path.\n    -- This ensures that (1) no cycles occur and (2) only nodes that\n    -- were not visited by previous (shorter) paths are added to a path.\n    WHERE NOT EXISTS (SELECT 1\n                      FROM paths previous_paths\n                      WHERE list_contains(previous_paths.path, node2id))\n)\nSELECT startNode, endNode, path\nFROM paths\nORDER BY length(path), path;\n\n\n\n6 records\n\n\nstartNode\nendNode\npath\n\n\n\n\n1\n3\n1, 3\n\n\n1\n5\n1, 5\n\n\n1\n8\n1, 3, 8\n\n\n1\n10\n1, 3, 10\n\n\n1\n4\n1, 5, 4\n\n\n1\n8\n1, 5, 8",
    "crumbs": [
      "DuckDB",
      "My Tips",
      "クイックメモ"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/mytips/02_.html#つのノード間の重み付けされていあに最短パスの列挙",
    "href": "contents/sql/duckdb/mytips/02_.html#つのノード間の重み付けされていあに最短パスの列挙",
    "title": "クイックメモ",
    "section": "8.3 2つのノード間の重み付けされていあに最短パスの列挙",
    "text": "8.3 2つのノード間の重み付けされていあに最短パスの列挙\n\n\nCode\nWITH RECURSIVE paths(startNode, endNode, path, endReached) AS (\n   SELECT -- define the path as the first edge of the traversal\n        node1id AS startNode,\n        node2id AS endNode,\n        [node1id, node2id] AS path,\n        (node2id = 8) AS endReached\n     FROM edge\n     WHERE startNode = 1\n   UNION ALL\n   SELECT -- concatenate new edge to the path\n        paths.startNode AS startNode,\n        node2id AS endNode,\n        array_append(path, node2id) AS path,\n        max(CASE WHEN node2id = 8 THEN 1 ELSE 0 END)\n            OVER (ROWS BETWEEN UNBOUNDED PRECEDING\n                           AND UNBOUNDED FOLLOWING) AS endReached\n     FROM paths\n     JOIN edge ON paths.endNode = node1id\n    WHERE NOT EXISTS (SELECT 1\n                      FROM paths previous_paths\n                      WHERE list_contains(previous_paths.path, node2id))\n      AND paths.endReached = 0\n)\nSELECT startNode, endNode, path\nFROM paths\nWHERE endNode = 8\nORDER BY length(path), path;\n\n\n\n2 records\n\n\nstartNode\nendNode\npath\n\n\n\n\n1\n8\n1, 3, 8\n\n\n1\n8\n1, 5, 8\n\n\n\n\n\n\n\nCode\nSELECT -- define the path as the first edge of the traversal\n    node1id AS startNode,\n    node2id AS endNode,\n    [node1id, node2id] AS path,\n    (node2id = 8) AS endReached\nFROM edge\nWHERE startNode = 1\n\n\n\n2 records\n\n\nstartNode\nendNode\npath\nendReached\n\n\n\n\n1\n3\n1, 3\nFALSE\n\n\n1\n5\n1, 5\nFALSE",
    "crumbs": [
      "DuckDB",
      "My Tips",
      "クイックメモ"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap04_.html",
    "href": "contents/sql/duckdb/sql-bigdata/chap04_.html",
    "title": "売上を把握するためのデータ抽出",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\nCode\n\ncur_dir &lt;- here::here(\"contents/sql/duckdb/sql-bigdata\")\n\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nlibrary(here)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "売上を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap04_.html#日別の売上を集計",
    "href": "contents/sql/duckdb/sql-bigdata/chap04_.html#日別の売上を集計",
    "title": "売上を把握するためのデータ抽出",
    "section": "2.1 日別の売上を集計",
    "text": "2.1 日別の売上を集計\n\n\nCode\nSELECT\n  strptime(dt, '%Y-%m-%d') as dt\n  , COUNT(*) as purchase_count\n  , SUM(purchase_amount) as total_amount\n  , AVG(purchase_amount) as avg_amount\nFROM purchase_log\nGROUP BY dt\nORDER BY dt\n;\n\n\n\n\nCode\nmy_data |&gt; \n  ggplot() + \n  geom_col(aes(dt, total_amount)) + \n  theme_bw()",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "売上を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap04_.html#移動平均を用いて日別の推移を見る",
    "href": "contents/sql/duckdb/sql-bigdata/chap04_.html#移動平均を用いて日別の推移を見る",
    "title": "売上を把握するためのデータ抽出",
    "section": "2.2 移動平均を用いて日別の推移を見る",
    "text": "2.2 移動平均を用いて日別の推移を見る\nSUM(purchase_amount)はGROUP BYに対しておこなわれて、 その後にAVG() OVER()がおこなわれてる。\nこれはOVERは直前の集約関数に対しておこなわれていると理解しておけば大丈夫である。\n計算結果をみてみると、dtが密になっていないのでこの状態で移動平均を計算するのは意味がない状態なのがわかる。\n\n\nCode\nSELECT\n  dt\n  , SUM(purchase_amount) total_amount\n  , AVG(SUM(purchase_amount))\n    OVER(ORDER BY dt ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n    as seven_day_avg\n  , COUNT(*) OVER(ORDER BY dt ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) as pre_days\n  , CASE\n      WHEN\n        COUNT(*) OVER(ORDER BY dt ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n        = 7\n      THEN\n        AVG(SUM(purchase_amount))\n        OVER(ORDER BY dt ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n      END\n      as seven_day_avg_strict\nFROM purchase_log\nGROUP BY dt\nORDER BY dt\n;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\n\ndt\ntotal_amount\nseven_day_avg\npre_days\nseven_day_avg_strict\n\n\n\n\n2014-01-01\n13900\n13900.000\n1\nNA\n\n\n2014-02-08\n28469\n21184.500\n2\nNA\n\n\n2014-03-09\n18899\n20422.667\n3\nNA\n\n\n2014-04-11\n12394\n18415.500\n4\nNA\n\n\n2014-05-11\n2282\n15188.800\n5\nNA\n\n\n2014-06-12\n10180\n14354.000\n6\nNA\n\n\n2014-07-11\n4027\n12878.714\n7\n12878.714\n\n\n2014-08-10\n6243\n11784.857\n7\n11784.857\n\n\n2014-09-10\n3832\n8265.286\n7\n8265.286\n\n\n2014-10-11\n6716\n6524.857\n7\n6524.857",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "売上を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap04_.html#当月売上の累計を求める",
    "href": "contents/sql/duckdb/sql-bigdata/chap04_.html#当月売上の累計を求める",
    "title": "売上を把握するためのデータ抽出",
    "section": "2.3 当月売上の累計を求める",
    "text": "2.3 当月売上の累計を求める\n\n\nCode\nSELECT\n  dt\n  , EXTRACT(YEAR FROM dt::date) as year\n  , sum(purchase_amount) as total_amount\nFROM purchase_log\nGROUP BY dt\nORDER BY dt\n;\n\n\n\nDisplaying records 1 - 10\n\n\ndt\nyear\ntotal_amount\n\n\n\n\n2014-01-01\n2014\n13900\n\n\n2014-02-08\n2014\n28469\n\n\n2014-03-09\n2014\n18899\n\n\n2014-04-11\n2014\n12394\n\n\n2014-05-11\n2014\n2282\n\n\n2014-06-12\n2014\n10180\n\n\n2014-07-11\n2014\n4027\n\n\n2014-08-10\n2014\n6243\n\n\n2014-09-10\n2014\n3832\n\n\n2014-10-11\n2014\n6716",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "売上を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap04_.html#月別売上の昨対比を求める",
    "href": "contents/sql/duckdb/sql-bigdata/chap04_.html#月別売上の昨対比を求める",
    "title": "売上を把握するためのデータ抽出",
    "section": "2.4 月別売上の昨対比を求める",
    "text": "2.4 月別売上の昨対比を求める\n\n\nCode\nWITH \ndaily_purchase as (\n  SELECT\n    dt\n    , substring(dt, 1, 4) as year\n    , substring(dt, 6, 2) as month\n    , substring(dt, 9, 2) as date\n    , sum(purchase_amount) as purchase_amount\n  FROM purchase_log\n  GROUP BY dt\n)\nSELECT \n  month\n  , SUM(CASE year WHEN '2014' THEN purchase_amount END) as amount_2014\n  , SUM(CASE year WHEN '2015' THEN purchase_amount END) as amount_2015\n  , 100. \n    * SUM(CASE year WHEN '2015' THEN purchase_amount END)\n    / SUM(CASE year WHEN '2014' THEN purchase_amount END)\nFROM daily_purchase\nGROUP BY month\nORDER BY month\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\nmonth\namount_2014\namount_2015\n((100 * sum(CASE WHEN ((“year” = ‘2015’)) THEN (purchase_amount) ELSE NULL END)) / sum(CASE WHEN ((“year” = ‘2014’)) THEN (purchase_amount) ELSE NULL END))\n\n\n\n\n01\n13900\n22111\n159.07194\n\n\n02\n28469\n11965\n42.02817\n\n\n03\n18899\n20215\n106.96333\n\n\n04\n12394\n11792\n95.14281\n\n\n05\n2282\n18087\n792.59422\n\n\n06\n10180\n18859\n185.25540\n\n\n07\n4027\n14919\n370.47430\n\n\n08\n6243\n12906\n206.72753\n\n\n09\n3832\n5696\n148.64301\n\n\n10\n6716\n13398\n199.49375",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "売上を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap04_.html#zチャートで業績の推移を確認する",
    "href": "contents/sql/duckdb/sql-bigdata/chap04_.html#zチャートで業績の推移を確認する",
    "title": "売上を把握するためのデータ抽出",
    "section": "2.5 Zチャートで業績の推移を確認する",
    "text": "2.5 Zチャートで業績の推移を確認する\nZチャートとは、月次売上、売上累計、移動年計の３つの指標で構成されるグラフである。\n\n\nCode\nWITH \ndaily_purchase as (\n  SELECT\n    dt\n    , substring(dt, 1, 4) as year\n    , substring(dt, 6, 2) as month\n    , substring(dt, 9, 2) as date\n    , sum(purchase_amount) as purchase_amount\n  FROM purchase_log\n  GROUP BY dt\n)\n, monthly_purchase as (\n  SELECT\n    year\n    , month\n    , SUM(purchase_amount) as amount\n  FROM daily_purchase\n  GROUP BY year, month\n)\n, calc_index as (\n  SELECT\n    year\n    , month\n    , amount\n    , SUM(CASE WHEN year = '2015' THEN amount END)\n      OVER(ORDER BY year, month ROWS UNBOUNDED PRECEDING)\n      as agg_amount\n    , SUM(amount)\n      OVER(ORDER BY year, month ROWS BETWEEN 11 PRECEDING AND CURRENT ROW)\n      as year_avg_amount\n  FROM monthly_purchase\n  ORDER BY year, month\n)\nSELECT \n  year || '-' || month as year_month\n  , amount\n  , agg_amount\n  , year_avg_amount\nFROM calc_index\nWHERE year = '2015'\nORDER BY year_month;\n\n\n\nDisplaying records 1 - 10\n\n\nyear_month\namount\nagg_amount\nyear_avg_amount\n\n\n\n\n2015-01\n22111\n22111\n160796\n\n\n2015-02\n11965\n34076\n144292\n\n\n2015-03\n20215\n54291\n145608\n\n\n2015-04\n11792\n66083\n145006\n\n\n2015-05\n18087\n84170\n160811\n\n\n2015-06\n18859\n103029\n169490\n\n\n2015-07\n14919\n117948\n180382\n\n\n2015-08\n12906\n130854\n187045\n\n\n2015-09\n5696\n136550\n188909\n\n\n2015-10\n13398\n149948\n195591",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "売上を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap04_.html#売上を把握するためのポイント",
    "href": "contents/sql/duckdb/sql-bigdata/chap04_.html#売上を把握するためのポイント",
    "title": "売上を把握するためのデータ抽出",
    "section": "2.6 売上を把握するためのポイント",
    "text": "2.6 売上を把握するためのポイント\n売上のトレンドを描画したら「なぜ」それが起きているのかをわかるようにレポートするのが大事である。\nたとえば「下降トレンド」なのは「販売回数」は同じであるが「平均購入額」が低下している。 ほかにも「定常トレンド」なのは「販売回数」は増えているが「平均購入額」が低下している。\n\n\nCode\nWITH \ndaily_purchase as (\n  SELECT\n    dt\n    , substring(dt, 1, 4) as year\n    , substring(dt, 6, 2) as month\n    , substring(dt, 9, 2) as date\n    , sum(purchase_amount) as purchase_amount\n    , count(*) as orders\n  FROM purchase_log\n  GROUP BY dt\n)\n, monthly_purchase as (\n  SELECT\n    year\n    , month\n    , sum(orders) as orders\n    , avg(purchase_amount) as avg_amount\n    , sum(purchase_amount) as monthly\n  FROM daily_purchase\n  GROUP BY year, month\n)\nSELECT \n  year || '=' || month as year_month\n  , orders\n  , avg_amount\n  , monthly\n  , sum(monthly)\n      over(partition by year order by month rows unbounded preceding)\n      as agg_amount\n  -- 12ヶ月前の売上\n  , LAG(monthly, 12)\n      over(order by year, month)\n      as last_year\n  , 100. * monthly \n    / lag(monthly, 12) over(order by year, month)\n    as rate\nFROM monthly_purchase\nORDER BY year_month;\n\n\n\nDisplaying records 1 - 10\n\n\nyear_month\norders\navg_amount\nmonthly\nagg_amount\nlast_year\nrate\n\n\n\n\n2014=01\n1\n13900\n13900\n13900\nNA\nNA\n\n\n2014=02\n1\n28469\n28469\n42369\nNA\nNA\n\n\n2014=03\n1\n18899\n18899\n61268\nNA\nNA\n\n\n2014=04\n1\n12394\n12394\n73662\nNA\nNA\n\n\n2014=05\n1\n2282\n2282\n75944\nNA\nNA\n\n\n2014=06\n1\n10180\n10180\n86124\nNA\nNA\n\n\n2014=07\n1\n4027\n4027\n90151\nNA\nNA\n\n\n2014=08\n1\n6243\n6243\n96394\nNA\nNA\n\n\n2014=09\n1\n3832\n3832\n100226\nNA\nNA\n\n\n2014=10\n1\n6716\n6716\n106942\nNA\nNA",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "売上を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap04_.html#カテゴリ別の売上と小計を計算",
    "href": "contents/sql/duckdb/sql-bigdata/chap04_.html#カテゴリ別の売上と小計を計算",
    "title": "売上を把握するためのデータ抽出",
    "section": "3.1 カテゴリ別の売上と小計を計算",
    "text": "3.1 カテゴリ別の売上と小計を計算\nレポートは最初は全体的な数値の概況を伝えた腕、その内訳をさまざまな切り口でレポートすることが わかりやすい流れである。\n\n\nCode\nWITH\nsub_category_amount as (\n  SELECT\n      category     as category\n    , sub_category as sub_category\n    , sum(price)   as amount\n  FROM\n    purchase_detail_log\n  GROUP BY \n    category, sub_category\n)\n, category_amount as (\n  SELECT \n    category\n    , 'all' as sub_category\n    , sum(price) as amount\n  FROM \n    purchase_detail_log\n  GROUP BY \n    category\n)\n, total_amount as (\n  SELECT\n    'all' as category\n    , 'all' as sub_category\n    , SUM(price) as amount\n  FROM\n    purchase_detail_log\n)\n\n          SELECT category, sub_category, amount FROM sub_category_amount\nUNION ALL SELECT category, sub_category, amount FROM category_amount\nUNION ALL SELECT category, sub_category, amount FROM total_amount\n\n;\n\n\n\nDisplaying records 1 - 10\n\n\ncategory\nsub_category\namount\n\n\n\n\nmens_fashion\njacket\n200\n\n\nbook\nbusiness\n400\n\n\nladys_fashion\nbag\n200\n\n\ndvd\ndocumentary\n300\n\n\ncd\nclassic\n400\n\n\nsupplement\nprotain\n200\n\n\ngame\naccessories\n500\n\n\nfood\nmeats\n600\n\n\nfood\nfish\n600\n\n\nladys_fashion\njacket\n800\n\n\n\n\n\nUNION ALLはデータを複数回読み込むためパフォーマンスやコスト面で良好でない。 ROLLUPが実装されている場合には次のように小計を記述することができる。\nこれにより、１つずつ条件付けをおこなうことができる。 注意点としてはグループがないところはNULLになるのでCOALESCEにより置換が必要である。\n\n\nCode\nSELECT\n      COALESCE(category, 'all') as category\n    , COALESCE(sub_category, 'all') as sub_category\n    , SUM(price) as amount\nFROM\n  purchase_detail_log\nGROUP BY \n  ROLLUP(category, sub_category);\n\n\n\nDisplaying records 1 - 10\n\n\ncategory\nsub_category\namount\n\n\n\n\nfood\nall\n1200\n\n\ngame\naccessories\n500\n\n\nfood\nmeats\n600\n\n\nfood\nfish\n600\n\n\nmens_fashion\nall\n200\n\n\nsupplement\nall\n200\n\n\ndvd\nall\n300\n\n\ngame\nall\n500\n\n\nladys_fashion\nall\n1000\n\n\nbook\nall\n400",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "売上を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap04_.html#abc分析",
    "href": "contents/sql/duckdb/sql-bigdata/chap04_.html#abc分析",
    "title": "売上を把握するためのデータ抽出",
    "section": "3.2 ABC分析",
    "text": "3.2 ABC分析\n\n\nCode\nWITH\nmonthly_sales as (\n  SELECT\n    category\n    , SUM(price) as amount\n  FROM \n    purchase_detail_log\n  WHERE\n    dt BETWEEN '2015-12-01' and '2015-12-31'\n  GROUP BY\n    category\n)\n, sales_composition_ratio as (\n  SELECT\n    category\n    , amount\n    -- 構成比\n    , 100. * amount / SUM(amount) OVER() as composition_ratio\n    -- 累積構成比\n    , 100. * SUM(amount) \n      OVER(ORDER BY amount DESC\n           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)\n      / SUM(amount) OVER() as cumulative_ratio\n  FROM \n    monthly_sales\n)\n\nSELECT \n  * \n  -- 構成比率の類消えに応じてランク付け\n  , CASE \n      WHEN cumulative_ratio BETWEEN 0 AND 70 THEN 'A'\n      WHEN cumulative_ratio BETWEEN 70 AND 90 THEN 'B'\n      WHEN cumulative_ratio BETWEEN 90 AND 100 THEN 'C'\n    END as abc_rank\nFROM \n  sales_composition_ratio\nORDER BY \n  amount DESC\n;\n\n\n\n8 records\n\n\ncategory\namount\ncomposition_ratio\ncumulative_ratio\nabc_rank\n\n\n\n\nfood\n1200\n28.571429\n28.57143\nA\n\n\nladys_fashion\n1000\n23.809524\n52.38095\nA\n\n\ngame\n500\n11.904762\n64.28571\nA\n\n\ncd\n400\n9.523810\n73.80952\nB\n\n\nbook\n400\n9.523810\n83.33333\nB\n\n\ndvd\n300\n7.142857\n90.47619\nC\n\n\nmens_fashion\n200\n4.761905\n95.23810\nC\n\n\nsupplement\n200\n4.761905\n100.00000\nC",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "売上を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap04_.html#ファンチャート商品の売れ行きの伸び率",
    "href": "contents/sql/duckdb/sql-bigdata/chap04_.html#ファンチャート商品の売れ行きの伸び率",
    "title": "売上を把握するためのデータ抽出",
    "section": "3.3 ファンチャート商品の売れ行きの伸び率",
    "text": "3.3 ファンチャート商品の売れ行きの伸び率\n\n\nCode\nwith\ndaily_category_amount as (\n    select\n        dt\n        , category\n        , substring( dt, 1, 4 ) as year\n        , substring( dt, 6, 2 ) as month\n        , substring( dt, 9, 2 ) as date\n        , sum( price ) as amount\n    from\n        purchase_detail_log\n    group by\n        dt, category\n)\n, monthly_category_amount as (\n    select\n        concat( year, '-', month ) as year_month\n        , category\n        , sum( amount ) as amount\n    from\n        daily_category_amount\n    group by\n        year, month, category\n)\nselect\n    year_month\n    , category\n    , amount\n    , first_value( amount )\n      over( partition by category order by year_month, category  )\n      as base_amount\n    , 100. * amount / first_value( amount ) \n      over( partition by category order by year_month, category  )\n      as rate\nfrom    \n    monthly_category_amount\norder by\n    year_month, category\n;\n\n\n\n8 records\n\n\nyear_month\ncategory\namount\nbase_amount\nrate\n\n\n\n\n2015-12\nbook\n400\n400\n100\n\n\n2015-12\ncd\n400\n400\n100\n\n\n2015-12\ndvd\n300\n300\n100\n\n\n2015-12\nfood\n1200\n1200\n100\n\n\n2015-12\ngame\n500\n500\n100\n\n\n2015-12\nladys_fashion\n1000\n1000\n100\n\n\n2015-12\nmens_fashion\n200\n200\n100\n\n\n2015-12\nsupplement\n200\n200\n100",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "売上を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap04_.html#メモタイル化",
    "href": "contents/sql/duckdb/sql-bigdata/chap04_.html#メモタイル化",
    "title": "売上を把握するためのデータ抽出",
    "section": "3.4 メモ：タイル化",
    "text": "3.4 メモ：タイル化\n\n\nCode\n\nSELECT *, ntile(3) OVER() c\nFROM(\n  SELECT unnest(generate_series(10)) a, random() b\n  ORDER BY b\n)\n;\n\n\n\nDisplaying records 1 - 10\n\n\na\nb\nc\n\n\n\n\n4\n0.1124412\n1\n\n\n0\n0.1850280\n1\n\n\n10\n0.3535486\n1\n\n\n6\n0.3607514\n1\n\n\n1\n0.4936704\n2\n\n\n7\n0.5715048\n2\n\n\n5\n0.6519861\n2\n\n\n9\n0.8438557\n2\n\n\n3\n0.8482401\n3\n\n\n8\n0.9529501\n3",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "売上を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap06_.html",
    "href": "contents/sql/duckdb/sql-bigdata/chap06_.html",
    "title": "Webサイトでの行動を把握するためのデータ抽出",
    "section": "",
    "text": "Code\nrenv::activate(here::here())\nCode\n\ncur_dir &lt;- here::here(\"contents/sql/duckdb/sql-bigdata\")\n\nlibrary(ggplot2)\nlibrary(duckdb)\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(arrow)\nlibrary(showtext)\nlibrary(here)\nshowtext_auto()\nfont_add_google(\"Noto Sans\")",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "Webサイトでの行動を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap06_.html#サンプルデータ",
    "href": "contents/sql/duckdb/sql-bigdata/chap06_.html#サンプルデータ",
    "title": "Webサイトでの行動を把握するためのデータ抽出",
    "section": "2.1 サンプルデータ",
    "text": "2.1 サンプルデータ\n\n\nCode\nSELECT * FROM purchase_log;\n\n\n\n4 records\n\n\nstamp\nshort_session\nlong_session\npurchase_id\namount\n\n\n\n\n2016-10-01 15:00:00\n0CVKaz\n1CwlSX\n1\n1000\n\n\n2016-10-01 16:00:00\n2is8PX\n7Dn99b\n2\n1000\n\n\n2016-10-01 20:00:00\n2is8PX\n7Dn99b\n3\n1000\n\n\n2016-10-02 14:00:00\n2is8PX\n7Dn99b\n4\n1000",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "Webサイトでの行動を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap06_.html#日時の訪問者数訪問回数ページビュー",
    "href": "contents/sql/duckdb/sql-bigdata/chap06_.html#日時の訪問者数訪問回数ページビュー",
    "title": "Webサイトでの行動を把握するためのデータ抽出",
    "section": "2.2 日時の訪問者数・訪問回数・ページビュー",
    "text": "2.2 日時の訪問者数・訪問回数・ページビュー\n\n\nCode\nSELECT\n  substring(stamp, 1, 10) as dt\n  , count(distinct long_session) as access_users\n  \n  , count(distinct short_session) as access_count\n  \n  , count(*) as page_view\n  \n  , 1. * count(*) / NULLIF(count(distinct long_session), 0) as pv_per_user\n  \nfrom\n  \n  access_log\n  \ngroup by\n  \n  dt\n\norder by\n  \n  dt\n  \n;\n\n\n\n3 records\n\n\ndt\naccess_users\naccess_count\npage_view\npv_per_user\n\n\n\n\n2016-10-01\n4\n5\n8\n2.000000\n\n\n2016-10-02\n4\n5\n7\n1.750000\n\n\n2016-10-03\n3\n3\n4\n1.333333",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "Webサイトでの行動を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap06_.html#ページマイの訪問者訪問回数ページビューを集計",
    "href": "contents/sql/duckdb/sql-bigdata/chap06_.html#ページマイの訪問者訪問回数ページビューを集計",
    "title": "Webサイトでの行動を把握するためのデータ抽出",
    "section": "2.3 ページマイの訪問者・訪問回数・ページビューを集計",
    "text": "2.3 ページマイの訪問者・訪問回数・ページビューを集計\n\n2.3.1 URL別に集計\n\n\nCode\nselect\n\n  url\n  , count(distinct short_session) as access_count\n  , count(distinct long_session) as access_usres\n  , count(*) as page_view\n  \nfrom \n\n  access_log\n  \ngroup by\n\n  url\n  \n;\n\n\n\nDisplaying records 1 - 10\n\n\n\n\n\n\n\n\nurl\naccess_count\naccess_usres\npage_view\n\n\n\n\nhttp://www.example.com/detail?id=2\n2\n2\n2\n\n\nhttp://www.example.com/?utm_source=google&utm_medium=search\n1\n1\n1\n\n\nhttp://www.example.com/list/cd\n3\n3\n3\n\n\nhttp://www.example.com/list/dvd?utm_source=yahoo&utm_medium=search\n1\n1\n1\n\n\nhttp://www.example.com/list/newly\n3\n2\n3\n\n\nhttp://www.example.com/detail?id=3\n1\n1\n1\n\n\nhttp://www.example.com/?utm_source=mynavi&utm_medium=affiliate\n1\n1\n1\n\n\nhttp://www.example.com/detail?id=1\n2\n2\n2\n\n\nhttp://www.example.com/\n3\n3\n3\n\n\nhttp://www.example.com/list/dvd\n2\n2\n2\n\n\n\n\n\n\n\n2.3.2 パス別に集計する\nクエリパラメータを省略してパス別に集計する。\n\n\nCode\nwith\naccess_log_with_path as (\n  -- urlからパスを抽出する\n  select\n    *\n    \n    , regexp_extract(url, '//[^/]+([^?#]+)', 1) as url_path\n    \n  from access_log\n  \n)\n\nselect \n\n  url_path\n  , count(distinct short_session) as access_count\n  , count(distinct long_session) as access_users\n  , count(*) as page_view\n\nfrom \n\n  access_log_with_path\n  \ngroup by\n\n  url_path\n\n\n\n5 records\n\n\nurl_path\naccess_count\naccess_users\npage_view\n\n\n\n\n/list/cd\n3\n3\n3\n\n\n/list/dvd\n3\n2\n3\n\n\n/detail\n5\n3\n5\n\n\n/\n5\n4\n5\n\n\n/list/newly\n3\n2\n3\n\n\n\n\n\n\n\n2.3.3 URLに大きな意味を持たせて集計する\n\n\nCode\nwith \naccess_log_with_path as (\n\n  select\n    *\n    \n    , regexp_extract(url, '//[^/]+([^?#]+)', 1) as url_path\n    \n  from access_log\n\n)\n, access_log_with_split_path as (\n  select\n  \n    * \n    , split_part(url_path, '/', 2) as path1\n    , split_part(url_path, '/', 3) as path2\n    \n  from access_log_with_path\n  \n)\n, access_log_with_page_name as (\n  select\n  \n    * \n  \n    , case \n        when path1 = 'list' then \n          case \n            when path2 = 'newly' then 'newly_list'\n            else 'category_list'\n          end\n        -- 上記以外はパスを採用する\n        else url_path\n      end as page_name\n      \n  from \n    \n    access_log_with_split_path\n\n)\n\nselect \n\n  page_name\n  \n  , count(distinct short_session) as access_count\n  \n  , count(distinct long_session) as access_users\n  \n  , count(*) as page_view\n  \nfrom \n\n  access_log_with_page_name\n  \ngroup by\n\n  page_name\n  \norder by\n\n  page_name\n  \n  \n;\n\n\n\n4 records\n\n\npage_name\naccess_count\naccess_users\npage_view\n\n\n\n\n/\n5\n4\n5\n\n\n/detail\n5\n3\n5\n\n\ncategory_list\n6\n4\n6\n\n\nnewly_list\n3\n2\n3",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "Webサイトでの行動を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap06_.html#流入元別に訪問回数やcvrを集計する",
    "href": "contents/sql/duckdb/sql-bigdata/chap06_.html#流入元別に訪問回数やcvrを集計する",
    "title": "Webサイトでの行動を把握するためのデータ抽出",
    "section": "2.4 流入元別に訪問回数やCVRを集計する",
    "text": "2.4 流入元別に訪問回数やCVRを集計する\nリファラーに直前のページのURLが保存される。そのリファラーとURLを使用し、下記の方法で流入元を判定する\n\nURLパラメータを元に判断する\nリファラーのドメインやランディングページで判定する\n\n\n\nCode\nwith \naccess_log_with_parse_info as (\n\n  select\n  \n    * \n    , regexp_extract(url, 'https?://([^/]*)',   1) as url_domain\n    , regexp_extract(url, 'utm_source=([^&]*)', 1) as url_utm_source\n    , regexp_extract(url, 'utm_medium=([^&]*)', 1) as url_utm_medium\n    , regexp_extract(referrer, 'https?://([^/]*)', 1) as referrer_domain\n    \n  from access_log\n\n)\n, access_log_with_via_info as (\n  select \n  \n    * \n    , row_number() over(order by stamp) as log_id\n    , case \n        when url_utm_source &lt;&gt; '' and url_utm_medium &lt;&gt; '' then concat_ws('/', url_utm_source, url_utm_medium)\n        when referrer_domain in ('search.yahoo.co.jp', 'www.google.co.jp') then 'search'\n        when referrer_domain in ('twitter.com', 'www.facebook.com') then 'social'\n        else 'other'\n      end as via\n      \n  from \n  \n    access_log_with_parse_info\n    \n  where\n  \n    coalesce(referrer_domain, '') not in ('', url_domain)\n    \n)\n\nselect \n  \n  via, \n  count(1) as access_count\n\nfrom\n  \n  access_log_with_via_info\n  \ngroup by\n\n  via\n  \norder by \n\n  access_count DESC\n\n;\n\n\n\n6 records\n\n\nvia\naccess_count\n\n\n\n\nsocial\n3\n\n\nother\n2\n\n\nsearch\n2\n\n\nmynavi/affiliate\n1\n\n\nyahoo/search\n1\n\n\ngoogle/search\n1\n\n\n\n\n\n\n2.4.1 流入元別にCVRを集計する\n\n\nCode\nwith \naccess_log_with_parse_info as (\n\n  select\n  \n    * \n    , regexp_extract(url, 'https?://([^/]*)',   1) as url_domain\n    , regexp_extract(url, 'utm_source=([^&]*)', 1) as url_utm_source\n    , regexp_extract(url, 'utm_medium=([^&]*)', 1) as url_utm_medium\n    , regexp_extract(referrer, 'https?://([^/]*)', 1) as referrer_domain\n    \n  from access_log\n\n)\n, access_log_with_via_info as (\n  select \n  \n    * \n    , row_number() over(order by stamp) as log_id\n    , case \n        when url_utm_source &lt;&gt; '' and url_utm_medium &lt;&gt; '' then concat_ws('/', url_utm_source, url_utm_medium)\n        when referrer_domain in ('search.yahoo.co.jp', 'www.google.co.jp') then 'search'\n        when referrer_domain in ('twitter.com', 'www.facebook.com') then 'social'\n        else 'other'\n      end as via\n      \n  from \n  \n    access_log_with_parse_info\n    \n  where\n  \n    coalesce(referrer_domain, '') not in ('', url_domain)\n    \n)\n, access_log_with_purchase_amount as (\n\n  select \n  \n    a.log_id\n    , a.via\n    , sum(\n        case \n          when p.stamp::date BETWEEN a.stamp::date and a.stamp::date + '1 day'::interval\n            then amount\n        end \n      ) as amount\n      \n    from \n    \n      access_log_with_via_info as a\n      \n      left outer join \n        \n        purchase_log as p\n        \n        on a.long_session = p.long_session\n        \n    group by\n    \n      a.log_id, a.via\n\n)\n\nselect\n\n  via\n  \n  , count(1) as via_count\n  , count(amount) as conversions\n  , avg(100. * sign(coalesce(amount, 0))) as cvr\n  , sum(coalesce(amount, 0)) as amount\n  , avg(1. * coalesce(amount, 0)) as avg_amount\n  \nfrom \n\n  access_log_with_purchase_amount\n  \n  \ngroup by via\n\norder by cvr desc\n\n;\n\n\n\n6 records\n\n\n\n\n\n\n\n\n\n\nvia\nvia_count\nconversions\ncvr\namount\navg_amount\n\n\n\n\ngoogle/search\n1\n1\n100.00000\n1000\n1000\n\n\nsocial\n3\n1\n33.33333\n3000\n1000\n\n\nother\n2\n0\n0.00000\n0\n0\n\n\nyahoo/search\n1\n0\n0.00000\n0\n0\n\n\nmynavi/affiliate\n1\n0\n0.00000\n0\n0\n\n\nsearch\n2\n0\n0.00000\n0\n0",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "Webサイトでの行動を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap06_.html#アクセスされる曜日時間帯を把握する",
    "href": "contents/sql/duckdb/sql-bigdata/chap06_.html#アクセスされる曜日時間帯を把握する",
    "title": "Webサイトでの行動を把握するためのデータ抽出",
    "section": "2.5 アクセスされる曜日、時間帯を把握する",
    "text": "2.5 アクセスされる曜日、時間帯を把握する\ndowというフォーマットは曜日を整数値で表す。\n\n\nCode\nwith \naccess_log_with_dow as (\n  \n  select\n    \n    stamp\n    \n    , date_part('dow', stamp::timestamp) as dow\n    \n    ,   cast(substring(stamp, 12, 2) as int) * 60 * 60\n      + cast(substring(stamp, 15, 2) as int) * 60\n      + cast(substring(stamp, 18, 2) as int)\n      as whole_seconds\n      \n    , 30 * 60 as interval_seconds\n    \n    \n  from \n    \n    access_log\n\n)\n, access_log_with_floor_seconds as (\n  \n  select\n    stamp\n    , dow\n    , cast((floor(whole_seconds/interval_seconds) * interval_seconds) as int) as floor_seconds\n    \n  from access_log_with_dow\n\n)\n, access_log_with_index as (\n  \n  select\n    stamp\n    , dow\n    ,    lpad(floor(floor_seconds / (60 * 60))::text , 2, '0') || ':'\n      || lpad(floor(floor_seconds % (60 * 60) / 60)::text, 2, '0') || ':'\n      || lpad(floor(floor_seconds % 60)::text , 2, '0') \n      as index_time\n  \n  from access_log_with_floor_seconds\n)\n\n\nselect\n\n  index_time\n  , count(case dow when 0 then 1 end) as sun\n  , count(case dow when 1 then 1 end) as mon\n  , count(case dow when 2 then 1 end) as tue\n  , count(case dow when 3 then 1 end) as wed\n  , count(case dow when 4 then 1 end) as thu\n  , count(case dow when 5 then 1 end) as fri\n  , count(case dow when 6 then 1 end) as sat\n\n\nfrom \n\n  access_log_with_index\n  \ngroup by \n\n  index_time\n  \norder by\n\n  index_time\n  \n;\n\n\n\n7 records\n\n\nindex_time\nsun\nmon\ntue\nwed\nthu\nfri\nsat\n\n\n\n\n12:0.:0.\n1\n1\n0\n0\n0\n0\n1\n\n\n13:0.:0.\n1\n1\n0\n0\n0\n0\n2\n\n\n14:0.:0.\n1\n1\n0\n0\n0\n0\n1\n\n\n15:0.:0.\n1\n1\n0\n0\n0\n0\n1\n\n\n16:0.:0.\n1\n0\n0\n0\n0\n0\n1\n\n\n17:0.:0.\n1\n0\n0\n0\n0\n0\n1\n\n\n18:0.:0.\n1\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\nCode\nselect 30 * 60;\n\n\n\n1 records\n\n\n(30 * 60)\n\n\n\n\n1800",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "Webサイトでの行動を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/chap06_.html#成果に結びつくページを把握する",
    "href": "contents/sql/duckdb/sql-bigdata/chap06_.html#成果に結びつくページを把握する",
    "title": "Webサイトでの行動を把握するためのデータ抽出",
    "section": "3.1 成果に結びつくページを把握する",
    "text": "3.1 成果に結びつくページを把握する\nあるセッションのページ遷移において/completeにと到達したことあるのかをフラグを立てている。\n\n\nCode\nwith\nactivity_log_with_conversion_flag as (\n  select\n    session\n    , stamp\n    , path\n    , sign(\n        sum(case when path = '/complete' then 1 else 0 end)\n        over(partition by session order by stamp desc\n             rows between unbounded preceding and current row\n             )\n      )  as has_conversion\n  from activity_log \n)\nselect * \nfrom activity_log_with_conversion_flag\norder by session, stamp;\n\n\n\nDisplaying records 1 - 10\n\n\nsession\nstamp\npath\nhas_conversion\n\n\n\n\n0fe39581\n2017-01-09 12:18:43\n/search_list\n0\n\n\n111f2996\n2017-01-09 12:18:43\n/search_list\n0\n\n\n111f2996\n2017-01-09 12:19:11\n/search_input\n0\n\n\n111f2996\n2017-01-09 12:20:10\n/\n0\n\n\n111f2996\n2017-01-09 12:21:14\n/search_input\n0\n\n\n1cf7678e\n2017-01-09 12:18:43\n/detail\n0\n\n\n1cf7678e\n2017-01-09 12:19:04\n/\n0\n\n\n36dd0df7\n2017-01-09 12:18:43\n/search_list\n0\n\n\n36dd0df7\n2017-01-09 12:19:49\n/detail\n0\n\n\n3efe001c\n2017-01-09 12:18:43\n/detail\n0",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "Webサイトでの行動を把握するためのデータ抽出"
    ]
  },
  {
    "objectID": "contents/sql/duckdb/sql-bigdata/index.html",
    "href": "contents/sql/duckdb/sql-bigdata/index.html",
    "title": "ビッグデータ分析・活用のためのSQLレシピ",
    "section": "",
    "text": "1 はじめに\n\nレシピの勉強ノートです.\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "DuckDB",
      "ビッグデータレシピ",
      "はじめに"
    ]
  }
]